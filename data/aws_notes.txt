AWS Whitepaper
Overview of Amazon Web Services
Copyright © 2025 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.
Overview of Amazon Web Services AWS Whitepaper
Overview of Amazon Web Services: AWS Whitepaper
Copyright © 2025 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.
Amazon's trademarks and trade dress may not be used in connection with any product or service that is not Amazon's, in any manner that is likely to cause confusion among customers, or in any manner that disparages or discredits Amazon. All other trademarks not owned by Amazon are the property of their respective owners, who may or may not be affiliated with, connected to, or sponsored by Amazon.
Overview of Amazon Web Services AWS Whitepaper
Table of Contents
Abstract and introduction ............................................................................................................... 1
Introduction ................................................................................................................................................... 1
What is cloud computing? .............................................................................................................. 2
Six advantages of cloud computing ............................................................................................... 3
Types of cloud computing .............................................................................................................. 4
Deployment models ..................................................................................................................................... 4
Cloud .......................................................................................................................................................... 4
Private cloud (on-premises) .................................................................................................................. 4
Hybrid ........................................................................................................................................................ 4
Global infrastructure ....................................................................................................................... 5
Security and compliance ................................................................................................................. 6
Security ........................................................................................................................................................... 6
Benefits of AWS security ....................................................................................................................... 7
Compliance ..................................................................................................................................................... 7
AWS services .................................................................................................................................... 8
Accessing AWS services ............................................................................................................................... 9
Analytics ......................................................................................................................................................... 9
Amazon Athena ..................................................................................................................................... 11
Amazon CloudSearch ........................................................................................................................... 11
Amazon DataZone ................................................................................................................................ 11
Amazon EMR .......................................................................................................................................... 12
Amazon FinSpace .................................................................................................................................. 12
Amazon Kinesis ..................................................................................................................................... 12
Amazon Data Firehose ......................................................................................................................... 13
Amazon Managed Service for Apache Flink .................................................................................... 13
Amazon Kinesis Data Streams ........................................................................................................... 14
Amazon Kinesis Video Streams .......................................................................................................... 14
Amazon OpenSearch Service ............................................................................................................. 14
Amazon OpenSearch Serverless ........................................................................................................ 14
Amazon Redshift ................................................................................................................................... 15
Amazon Redshift Serverless ............................................................................................................... 15
QuickSight .............................................................................................................................................. 15
AWS Clean Rooms ................................................................................................................................ 16
AWS Data Exchange ............................................................................................................................. 16
iii
Overview of Amazon Web Services AWS Whitepaper
AWS Data Pipeline ................................................................................................................................ 16
AWS Entity Resolution ......................................................................................................................... 17
AWS Glue ................................................................................................................................................ 17
AWS Lake Formation ............................................................................................................................ 18
Amazon Managed Streaming for Apache Kafka (Amazon MSK) ................................................. 18
Application integration ............................................................................................................................. 19
AWS Step Functions ............................................................................................................................. 21
Amazon AppFlow .................................................................................................................................. 21
AWS B2B Data Interchange ................................................................................................................ 21
Amazon EventBridge ............................................................................................................................ 22
Amazon Managed Workflows for Apache Airflow (MWAA) .......................................................... 22
Amazon MQ ........................................................................................................................................... 22
Amazon Simple Notification Service ................................................................................................ 23
Amazon Simple Queue Service .......................................................................................................... 23
Amazon Simple Workflow Service .................................................................................................... 23
Blockchain .................................................................................................................................................... 23
Business applications ................................................................................................................................. 24
Alexa for Business ................................................................................................................................. 25
AWS AppFabric ...................................................................................................................................... 25
Amazon Chime ...................................................................................................................................... 25
Amazon Chime SDK ............................................................................................................................. 26
Amazon Connect ................................................................................................................................... 26
Amazon Pinpoint ................................................................................................................................... 26
Amazon SES ........................................................................................................................................... 27
Amazon WorkDocs ................................................................................................................................ 27
Amazon WorkMail ................................................................................................................................. 27
Cloud Financial Management .................................................................................................................. 28
AWS Application Cost Profiler ............................................................................................................ 29
AWS Billing Conductor ........................................................................................................................ 29
AWS Cost Explorer ................................................................................................................................ 30
AWS Budgets ......................................................................................................................................... 30
AWS Cost and Usage Report .............................................................................................................. 30
Reserved Instance (RI) reporting ....................................................................................................... 30
Savings Plans ......................................................................................................................................... 31
Compute ....................................................................................................................................................... 31
Compare AWS compute services ....................................................................................................... 33
iv
Overview of Amazon Web Services AWS Whitepaper
Amazon EC2 ........................................................................................................................................... 35
Amazon EC2 Auto Scaling .................................................................................................................. 37
Amazon EC2 Image Builder ................................................................................................................ 38
Amazon Lightsail ................................................................................................................................... 38
Amazon Linux 2023 ............................................................................................................................. 38
AWS App Runner .................................................................................................................................. 39
AWS Batch .............................................................................................................................................. 39
AWS Elastic Beanstalk .......................................................................................................................... 39
AWS Fargate ........................................................................................................................................... 40
AWS Lambda .......................................................................................................................................... 40
AWS Serverless Application Repository ........................................................................................... 40
AWS Outposts ........................................................................................................................................ 41
AWS Wavelength ................................................................................................................................... 41
VMware Cloud on AWS ........................................................................................................................ 42
Customer enablement ............................................................................................................................... 43
Containers .................................................................................................................................................... 43
Amazon Elastic Container Registry ................................................................................................... 44
Amazon Elastic Container Service ..................................................................................................... 45
Amazon Elastic Kubernetes Service .................................................................................................. 45
AWS App2Container ............................................................................................................................. 45
Red Hat OpenShift Service on AWS ................................................................................................. 46
Databases ..................................................................................................................................................... 46
Compare AWS database services ....................................................................................................... 48
Amazon Aurora ...................................................................................................................................... 49
Amazon DynamoDB .............................................................................................................................. 50
Amazon ElastiCache ............................................................................................................................. 51
Amazon Keyspaces (for Apache Cassandra) .................................................................................... 51
Amazon MemoryDB .............................................................................................................................. 52
Amazon Neptune .................................................................................................................................. 52
Amazon Relational Database Service ............................................................................................... 53
Amazon RDS for Db2 ........................................................................................................................... 53
Amazon RDS on VMware .................................................................................................................... 53
Amazon Quantum Ledger Database (Amazon QLDB) ................................................................... 54
Amazon Timestream ............................................................................................................................ 55
Amazon DocumentDB (with MongoDB compatibility) .................................................................. 55
Amazon Lightsail managed databases ............................................................................................. 55
v
Overview of Amazon Web Services AWS Whitepaper
Developer tools ........................................................................................................................................... 56
AWS Infrastructure Composer ............................................................................................................ 56
AWS Cloud9 ........................................................................................................................................... 57
AWS CloudShell ..................................................................................................................................... 57
AWS CodeArtifact ................................................................................................................................. 57
AWS CodeBuild ...................................................................................................................................... 57
Amazon CodeCatalyst .......................................................................................................................... 58
AWS CodeCommit ................................................................................................................................. 58
AWS CodeDeploy .................................................................................................................................. 58
AWS CodePipeline ................................................................................................................................. 58
Amazon Corretto ................................................................................................................................... 59
AWS Fault Injection Service ................................................................................................................ 59
Amazon Q Developer ........................................................................................................................... 59
AWS X-Ray .............................................................................................................................................. 60
End user computing .................................................................................................................................. 60
Frontend web and mobile services ........................................................................................................ 62
AWS Amplify .......................................................................................................................................... 63
AWS AppSync ........................................................................................................................................ 63
AWS Device Farm .................................................................................................................................. 63
Amazon Location Service .................................................................................................................... 64
Game tech .................................................................................................................................................... 64
IoT .................................................................................................................................................................. 64
AWS IoT Analytics ................................................................................................................................. 66
AWS IoT Button ..................................................................................................................................... 67
AWS IoT Core ......................................................................................................................................... 67
AWS IoT Device Defender ................................................................................................................... 67
AWS IoT Device Management ............................................................................................................ 68
AWS IoT Events ..................................................................................................................................... 68
AWS IoT ExpressLink ............................................................................................................................ 69
AWS IoT FleetWise ................................................................................................................................ 69
AWS IoT Greengrass ............................................................................................................................. 70
AWS IoT SiteWise .................................................................................................................................. 70
AWS IoT TwinMaker ............................................................................................................................. 71
AWS Partner Device Catalog .............................................................................................................. 71
FreeRTOS ................................................................................................................................................ 71
ML and AI .................................................................................................................................................... 72
vi
Overview of Amazon Web Services AWS Whitepaper
Amazon Augmented AI ........................................................................................................................ 74
Amazon Bedrock ................................................................................................................................... 74
Amazon CodeGuru ................................................................................................................................ 75
Amazon Comprehend .......................................................................................................................... 75
Amazon DevOps Guru .......................................................................................................................... 75
Amazon Forecast ................................................................................................................................... 76
Amazon Fraud Detector ...................................................................................................................... 77
Amazon Comprehend Medical ........................................................................................................... 77
Amazon Kendra ..................................................................................................................................... 77
Amazon Lex ............................................................................................................................................ 78
Amazon Lookout for Equipment ....................................................................................................... 78
Amazon Lookout for Metrics .............................................................................................................. 79
Amazon Lookout for Vision ................................................................................................................ 79
Amazon Monitron ................................................................................................................................. 80
Amazon PartyRock ................................................................................................................................ 80
Amazon Personalize ............................................................................................................................. 80
Amazon Polly ......................................................................................................................................... 81
Amazon Q ............................................................................................................................................... 82
Amazon Rekognition ............................................................................................................................ 82
Amazon SageMaker AI ......................................................................................................................... 83
Amazon Textract ................................................................................................................................... 89
Amazon Transcribe ............................................................................................................................... 90
Amazon Translate ................................................................................................................................. 91
AWS DeepComposer ............................................................................................................................. 91
AWS DeepRacer ..................................................................................................................................... 91
AWS HealthLake .................................................................................................................................... 91
AWS HealthScribe ................................................................................................................................. 92
AWS Panorama ...................................................................................................................................... 92
Management and governance ................................................................................................................. 93
AWS Auto Scaling ................................................................................................................................. 94
AWS CloudFormation ........................................................................................................................... 94
AWS CloudTrail ...................................................................................................................................... 95
Amazon CloudWatch ............................................................................................................................ 95
AWS Compute Optimizer .................................................................................................................... 95
AWS Console Mobile Application ...................................................................................................... 96
AWS Control Tower .............................................................................................................................. 96
vii
Overview of Amazon Web Services AWS Whitepaper
AWS Config ............................................................................................................................................ 97
AWS Health ............................................................................................................................................ 97
AWS Launch Wizard ............................................................................................................................. 97
AWS License Manager .......................................................................................................................... 98
Amazon Managed Grafana ................................................................................................................. 98
Amazon Managed Service for Prometheus ..................................................................................... 99
AWS Organizations ............................................................................................................................... 99
AWS OpsWorks ...................................................................................................................................... 99
AWS Proton .......................................................................................................................................... 100
Amazon Q Developer in chat applications (formerly AWS Chatbot) ........................................ 100
AWS Service Catalog .......................................................................................................................... 100
AWS Systems Manager ...................................................................................................................... 101
AWS Trusted Advisor ......................................................................................................................... 103
AWS User Notifications ..................................................................................................................... 103
AWS Well-Architected Tool .............................................................................................................. 103
Media .......................................................................................................................................................... 103
Amazon Elastic Transcoder ............................................................................................................... 104
Amazon Interactive Video Service .................................................................................................. 104
Amazon Nimble Studio ..................................................................................................................... 105
AWS Elemental Appliances and Software ..................................................................................... 105
AWS Elemental MediaConnect ........................................................................................................ 105
AWS Elemental MediaConvert ......................................................................................................... 106
AWS Elemental MediaLive ................................................................................................................ 106
AWS Elemental MediaPackage ........................................................................................................ 106
AWS Elemental MediaStore .............................................................................................................. 106
AWS Elemental MediaTailor ............................................................................................................. 107
Migration and transfer ............................................................................................................................ 107
AWS Application Discovery Service ................................................................................................ 108
AWS Application Migration Service ................................................................................................ 109
AWS Database Migration Service .................................................................................................... 109
AWS Mainframe Modernization Service ......................................................................................... 109
AWS Migration Hub ........................................................................................................................... 110
AWS Snow Family ............................................................................................................................... 110
AWS DataSync ..................................................................................................................................... 112
AWS Transfer Family .......................................................................................................................... 112
Networking and content delivery ......................................................................................................... 113
viii
Overview of Amazon Web Services AWS Whitepaper
Amazon API Gateway ........................................................................................................................ 114
Amazon CloudFront ........................................................................................................................... 114
Amazon Route 53 ............................................................................................................................... 115
AWS Verified Access ........................................................................................................................... 115
Amazon VPC ........................................................................................................................................ 116
Amazon VPC Lattice ........................................................................................................................... 116
AWS App Mesh .................................................................................................................................... 116
AWS Cloud Map .................................................................................................................................. 117
AWS Direct Connect ........................................................................................................................... 118
AWS Global Accelerator .................................................................................................................... 118
AWS PrivateLink .................................................................................................................................. 119
AWS Private 5G ................................................................................................................................... 119
AWS Transit Gateway ......................................................................................................................... 119
AWS VPN .............................................................................................................................................. 120
Elastic Load Balancing ....................................................................................................................... 120
Integrated Private Wireless on AWS ............................................................................................... 121
Quantum technologies ........................................................................................................................... 121
Robotics ...................................................................................................................................................... 122
Satellite ...................................................................................................................................................... 123
Security, identity, and compliance ....................................................................................................... 124
Amazon Cognito ................................................................................................................................. 126
Amazon Detective .............................................................................................................................. 126
Amazon GuardDuty ............................................................................................................................ 127
Amazon Inspector ............................................................................................................................... 128
Amazon Macie ..................................................................................................................................... 128
Amazon Security Lake ....................................................................................................................... 129
Amazon Verified Permissions ........................................................................................................... 130
AWS Artifact ........................................................................................................................................ 130
AWS Audit Manager ........................................................................................................................... 130
AWS Certificate Manager .................................................................................................................. 131
AWS CloudHSM ................................................................................................................................... 131
AWS Directory Service ....................................................................................................................... 132
AWS Firewall Manager ....................................................................................................................... 132
AWS Identity and Access Management .......................................................................................... 132
AWS Key Management Service ........................................................................................................ 133
AWS Network Firewall ....................................................................................................................... 133
ix
Overview of Amazon Web Services AWS Whitepaper
AWS Resource Access Manager ........................................................................................................ 134
AWS Secrets Manager ........................................................................................................................ 134
AWS Security Hub .............................................................................................................................. 135
AWS Shield ........................................................................................................................................... 135
AWS IAM Identity Center .................................................................................................................. 136
AWS WAF .............................................................................................................................................. 137
AWS WAF Captcha .............................................................................................................................. 137
Storage ....................................................................................................................................................... 137
AWS Backup ......................................................................................................................................... 139
Amazon Elastic Block Store .............................................................................................................. 139
AWS Elastic Disaster Recovery ......................................................................................................... 139
Amazon Elastic File System .............................................................................................................. 140
Amazon File Cache ............................................................................................................................. 140
Amazon FSx for Lustre ...................................................................................................................... 141
Amazon FSx for NetApp ONTAP ..................................................................................................... 141
Amazon FSx for OpenZFS ................................................................................................................. 142
Amazon FSx for Windows File Server ............................................................................................ 142
Amazon Simple Storage Service ..................................................................................................... 142
AWS Storage Gateway ....................................................................................................................... 143
Next steps .................................................................................................................................... 145
Are you Well-Architected? ..................................................................................................................... 145
Conclusion .................................................................................................................................... 147
Resources ...................................................................................................................................... 148
Document history ........................................................................................................................ 149
.....................................................................................................................................................................
149
AWS Glossary ............................................................................................................................... 154
x
Overview of Amazon Web Services AWS Whitepaper
Overview of Amazon Web Services
Publication date: August 27, 2024 (Document history)
Amazon Web Services offers a broad set of global cloud-based products including compute, storage, databases, analytics, networking, mobile, developer tools, management tools, IoT, security, and enterprise applications: on-demand, available in seconds, with pay-as-you-go pricing. From data warehousing to deployment tools, directories to content delivery, over 200 AWS services are available.
New services can be provisioned quickly, without the upfront fixed expense. This allows enterprises, start-ups, small and medium-sized businesses, and customers in the public sector to access the building blocks they need to respond quickly to changing business requirements. This whitepaper provides you with an overview of the benefits of the AWS Cloud and introduces you to the services that make up the platform.
Introduction
In 2006, Amazon Web Services (AWS) began offering IT infrastructure services to businesses as web services—now commonly known as cloud computing. One of the key benefits of cloud computing is the opportunity to replace upfront capital infrastructure expenses with low variable costs that scale with your business. With the cloud, businesses no longer need to plan for and procure servers and other IT infrastructure weeks or months in advance. Instead, they can instantly spin up hundreds or thousands of servers in minutes and deliver results faster.
Today, AWS provides a highly reliable, scalable, low-cost infrastructure platform in the cloud that powers hundreds of thousands of businesses in 190 countries around the world.
This video explores how millions of customers use AWS to take advantage of the efficiencies of cloud computing: What is AWS? | Amazon Web Services
Introduction
1
Overview of Amazon Web Services AWS Whitepaper
What is cloud computing?
Cloud computing is the on-demand delivery of compute power, database, storage, applications, and other IT resources through a cloud services platform via the internet with pay-as-you-go pricing. Whether you are running applications that share photos to millions of mobile users or you’re supporting the critical operations of your business, a cloud services platform provides rapid access to flexible and low-cost IT resources. With cloud computing, you don’t need to make large upfront investments in hardware and spend a lot of time on the heavy lifting of managing that hardware. Instead, you can provision exactly the right type and size of computing resources you need to power your newest bright idea or operate your IT department. You can access as many resources as you need, almost instantly, and only pay for what you use.
Cloud computing provides a simple way to access servers, storage, databases and a broad set of application services over the internet. A cloud services platform such as Amazon Web Services owns and maintains the network-connected hardware required for these application services, while you provision and use what you need via a web application.
2
Overview of Amazon Web Services AWS Whitepaper
Six advantages of cloud computing
•
Trade fixed expense for variable expense – Instead of having to invest heavily in data centers and servers before you know how you’re going to use them, you can pay only when you consume computing resources, and pay only for how much you consume.
•
Benefit from massive economies of scale – By using cloud computing, you can achieve a lower variable cost than you can get on your own. Because usage from hundreds of thousands of customers is aggregated in the cloud, providers such as AWS can achieve higher economies of scale, which translates into lower pay as-you-go prices.
•
Stop guessing capacity – Eliminate guessing on your infrastructure capacity needs. When you make a capacity decision prior to deploying an application, you often end up either sitting on expensive idle resources or dealing with limited capacity. With cloud computing, these problems go away. You can access as much or as little capacity as you need, and scale up and down as required with only a few minutes’ notice.
•
Increase speed and agility – In a cloud computing environment, new IT resources are only a click away, which means that you reduce the time to make those resources available to your developers from weeks to just minutes. This results in a dramatic increase in agility for the organization, since the cost and time it takes to experiment and develop is significantly lower.
•
Stop spending money running and maintaining data centers – Focus on projects that differentiate your business, not the infrastructure. Cloud computing lets you focus on your own customers, rather than on the heavy lifting of racking, stacking, and powering servers.
•
Go global in minutes – Easily deploy your application in multiple regions around the world with just a few clicks. This means you can provide lower latency and a better experience for your customers at minimal cost.
3
Overview of Amazon Web Services AWS Whitepaper
Types of cloud computing
Cloud computing provides developers and IT departments with the ability to focus on what matters most and avoid undifferentiated work such as procurement, maintenance, and capacity planning. As cloud computing has grown in popularity, several different models and deployment strategies have emerged to help meet specific needs of different users. Each type provides you with different levels of control, flexibility, and management.
Deployment models
Cloud
A cloud-based application is fully deployed in the cloud and all parts of the application run in the cloud. Applications in the cloud have either been created in the cloud or have been migrated from an existing infrastructure to take advantage of the benefits of cloud computing. Cloud-based applications can be built on low-level infrastructure pieces or can use higher level services that provide abstraction from the management, architecting, and scaling requirements of core infrastructure.
Private cloud (on-premises)
The deployment of resources on-premises, using virtualization and resource management tools, is sometimes called the private cloud. On-premises deployment doesn’t provide many of the benefits of cloud computing but it is sometimes sought for its ability to provide dedicated resources. In most cases, this deployment model is the same as legacy IT infrastructure while using application management and virtualization technologies to try and increase resource utilization.
Hybrid
A hybrid deployment is a way to connect infrastructure and applications between cloud-based resources and existing resources that are not located in the cloud. The most common method of hybrid deployment is between the cloud and existing on-premises infrastructure to extend, and grow, an organization's infrastructure into the cloud while connecting cloud resources to the internal system. For more information on how AWS can help you with your hybrid deployment, visit our AWS Solutions for Hybrid and Multicloud page.
Deployment models
4
Overview of Amazon Web Services AWS Whitepaper
Global infrastructure
The AWS Cloud infrastructure is built around AWS Regions and Availability Zones. An AWS Region is a physical location in the world where we have multiple Availability Zones. Availability Zones consist of one or more discrete data centers, each with redundant power, networking, and connectivity, housed in separate facilities. These Availability Zones offer you the ability to operate production applications and databases that are more highly available, fault tolerant, and scalable than would be possible from a single data center. For the latest information on the AWS Cloud Availability Zones and AWS Regions, refer to AWS Global Infrastructure.
5
Overview of Amazon Web Services AWS Whitepaper
Security and compliance
Security
Cloud security at AWS is the highest priority. As organizations embrace the scalability and flexibility of the cloud, AWS is helping them evolve security, identity, and compliance into key business enablers. AWS builds security into the core of our cloud infrastructure, and offers foundational services to help organizations meet their unique security requirements in the cloud.
As an AWS customer, you will benefit from a data center and network architecture built to meet the requirements of the most security-sensitive organizations. Security in the cloud is much like security in your on-premises data centers—only without the costs of maintaining facilities and hardware. In the cloud, you don’t have to manage physical servers or storage devices. Instead, you use software-based security tools to monitor and protect the flow of information into and out of your cloud resources.
An advantage of the AWS Cloud is that it allows you to scale and innovate, while maintaining a secure environment and paying only for the services you use. This means that you can have the security you need at a lower cost than in an on-premises environment.
As an AWS customer you inherit all the best practices of AWS policies, architecture, and operational processes built to satisfy the requirements of our most security-sensitive customers. Get the flexibility and agility you need in security controls.
The AWS Cloud enables a shared responsibility model. While AWS manages security of the cloud, you are responsible for security in the cloud. This means that you retain control of the security you choose to implement to protect your own content, platform, applications, systems, and networks no differently than you would in an on-site data center.
AWS provides you with guidance and expertise through online resources, personnel, and partners. AWS provides you with advisories for current issues, plus you have the opportunity to work with AWS when you encounter security issues.
You get access to hundreds of tools and features to help you to meet your security objectives. AWS provides security-specific tools and features across network security, configuration management, access control, and data encryption.
Security
6
Overview of Amazon Web Services AWS Whitepaper
Finally, AWS environments are continuously audited, with certifications from accreditation bodies across geographies and verticals. In the AWS environment, you can take advantage of automated tools for asset inventory and privileged access reporting.
Benefits of AWS security
•
Keep Your data safe — The AWS infrastructure puts strong safeguards in place to help protect your privacy. All data is stored in highly secure AWS data centers.
•
Meet compliance requirements — AWS manages dozens of compliance programs in its infrastructure. This means that segments of your compliance have already been completed.
•
Save money — Cut costs by using AWS data centers. Maintain the highest standard of security without having to manage your own facility
•
Scale quickly — Security scales with your AWS Cloud usage. No matter the size of your business, the AWS infrastructure is designed to keep your data safe.
Compliance
AWS Cloud Compliance helps you understand the robust controls in place at AWS for security and data protection in the cloud. Compliance is a shared responsibility between AWS and the customer, and you can visit the Shared Responsibility Model to learn more. Customers can feel confident in operating and building on top of the security controls AWS uses on its infrastructure.
The IT infrastructure that AWS provides to its customers is designed and managed in alignment with best security practices and a variety of IT security standards. The following is a partial list of assurance programs with which AWS complies:
•
SOC 1/ISAE 3402, SOC 2, SOC 3
•
FISMA, DIACAP, and FedRAMP
•
PCI DSS Level 1
•
ISO 9001, ISO 27001, ISO 27017, ISO 27018
AWS provides customers a wide range of information on its IT control environment in whitepapers, reports, certifications, accreditations, and other third-party attestations. More information is available in the Risk and Compliance whitepaper and the AWS Security Center.
Benefits of AWS security
7
Overview of Amazon Web Services AWS Whitepaper
AWS services by category
AWS consists of many cloud services that you can use in combinations tailored to your business or organizational needs. This section introduces the major AWS services by category. Choose a category to explore its services.
To access the services, you can use the AWS Management Console, the AWS Command Line Interface (AWS CLI), or Software Development Kits (SDKs).
Topics
•
Accessing AWS services
•
Analytics
•
Application integration
•
Blockchain
•
Business applications
•
Cloud Financial Management
•
Compute
•
Customer enablement
•
Containers
•
Databases
•
Developer tools
•
End user computing
•
Frontend web and mobile services
•
Game tech
•
Internet of Things (IoT)
•
Machine Learning (ML) and Artificial Intelligence (AI)
•
Management and governance
•
Media
•
Migration and transfer
•
Networking and content delivery
•
Quantum technologies
8
Overview of Amazon Web Services AWS Whitepaper
•
Robotics
•
Satellite
•
Security, identity, and compliance
•
Storage
Accessing AWS services
AWS Management Console
Access and manage Amazon Web Services through the AWS Management Console, a simple and intuitive user interface. You can also use the AWS Management Console Application to quickly view resources on the go.
AWS Command Line Interface (AWS CLI)
The AWS Command Line Interface (AWS CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.
AWS CloudShell, which can be found next to the search bar in the AWS Management Console, provides a browser-based shell that is pre-authenticated with your console credentials. Using CloudShell, you can quickly run AWS commands and scripts without leaving your web browser.
Software Development Kits (SDKs)
Our Software Development Kits (SDKs) simplify using AWS services in your applications with an Application Program Interface (API) tailored to your programming language or platform.
Analytics
AWS provides a comprehensive set of analytics services that fit all your data analytics needs and enables organizations of all sizes and industries to reinvent their business with data. From storage and management, data governance, actions, and experiences, AWS offers purpose-built services that provide the best price-performance, scalability, and lowest cost.
Accessing AWS services
9
Overview of Amazon Web Services AWS Whitepaper
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing an AWS analytics service. For general information, see Analytics on AWS.
Analytics services
•
Amazon Athena
•
Amazon CloudSearch
•
Amazon DataZone
•
Amazon EMR
•
Amazon FinSpace
•
Amazon Kinesis
•
Amazon Data Firehose
•
Amazon Managed Service for Apache Flink
•
Amazon Kinesis Data Streams
•
Amazon Kinesis Video Streams
•
Amazon OpenSearch Service
•
Amazon OpenSearch Serverless
•
Amazon Redshift
Analytics
10
Overview of Amazon Web Services AWS Whitepaper
•
Amazon Redshift Serverless
•
QuickSight
•
AWS Clean Rooms
•
AWS Data Exchange
•
AWS Data Pipeline
•
AWS Entity Resolution
•
AWS Glue
•
AWS Lake Formation
•
Amazon Managed Streaming for Apache Kafka (Amazon MSK)
Amazon Athena
Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.
Athena is easy to use. Simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds. With Athena, there’s no need for complex extract, transform, and load (ETL) jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.
Athena is out-of-the-box integrated with AWS Glue Data Catalog, allowing you to create a unified metadata repository across various services, crawl data sources to discover schemas and populate your Catalog with new and modified table and partition definitions, and maintain schema versioning.
Amazon CloudSearch
Amazon CloudSearch is a managed service in the AWS Cloud that makes it simple and cost-effective to set up, manage, and scale a search solution for your website or application. Amazon CloudSearch supports 34 languages and popular search features such as highlighting, autocomplete, and geospatial search.
Amazon DataZone
Amazon DataZone is a data management service that you can use to publish data and make it available to the business data catalog through your personalized web application. You can
Amazon Athena
11
Overview of Amazon Web Services AWS Whitepaper
access your data more securely regardless of where it is stored—on AWS, on premises, or in SaaS applications such as Salesforce. Amazon DataZone simplifies your experience across AWS services such as Amazon Redshift, Amazon Athena, AWS Glue, AWS Lake Formation, and QuickSight.
Amazon EMR
Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink,Apache Hudi, and Presto. Amazon EMR makes it easy to set up, operate, and scale your big data environments by automating time-consuming tasks such as provisioning capacity and tuning clusters. With Amazon EMR, you can run petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. You can run workloads on Amazon EC2 instances, on Amazon Elastic Kubernetes Service (Amazon EKS) clusters, or on-premises using Amazon EMR on AWS Outposts.
Amazon FinSpace
Amazon FinSpace is a data management and analytics service purpose-built for the financial services industry (FSI). FinSpace reduces the time you spend finding and preparing petabytes of financial data to be ready for analysis from months to minutes.
Financial services organizations analyze data from internal data stores such as portfolio, actuarial, and risk management systems as well as petabytes of data from third-party data feeds, such as historical securities prices from stock exchanges. It can take months to find the right data, get permissions to access the data in a compliant way, and prepare it for analysis.
FinSpace removes the heavy lifting of building and maintaining a data management system for financial analytics. With FinSpace, you collect data and catalog it by relevant business concepts such as asset class, risk classification, or geographic region. FinSpace makes it easy to discover and share data across your organization in accordance with your compliance requirements. You define your data access policies in one place and FinSpace enforces them while keeping audit logs to allow for compliance and activity reporting. FinSpace also includes a library of 100+ functions, such as time bars and Bollinger bands, for you to prepare data for analysis.
Amazon Kinesis
Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the
Amazon EMR
12
Overview of Amazon Web Services AWS Whitepaper
tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning (ML), analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.
Amazon Kinesis currently offers four services: Firehose, Managed Service for Apache Flink, Kinesis Data Streams, and Kinesis Video Streams.
Amazon Data Firehose
Amazon Data Firehose is the easiest way to reliably load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.
You can easily create a Firehose delivery stream from the AWS Management Console, configure it with a few clicks, and start sending data to the stream from hundreds of thousands of data sources to be loaded continuously to AWS—all in just a few minutes. You can also configure your delivery stream to automatically convert the incoming data to columnar formats such as Apache Parquet and Apache ORC, before the data is delivered to Amazon S3, for cost-effective storage and analytics.
Amazon Managed Service for Apache Flink
Amazon Managed Service for Apache Flink is the easiest way to analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time. Amazon Managed Service for Apache Flink reduces the complexity of building, managing, and integrating streaming applications with other AWS services. SQL users can easily query streaming data or build entire streaming applications using templates and an interactive SQL editor. Java developers can quickly build sophisticated streaming applications using open source Java libraries and AWS integrations to transform and analyze data in real-time.
Amazon Managed Service for Apache Flink takes care of everything required to run your queries continuously and scales automatically to match the volume and throughput rate of your incoming data.
Amazon Data Firehose
13
Overview of Amazon Web Services AWS Whitepaper
Amazon Kinesis Data Streams
Amazon Kinesis Data Streams is a massively scalable and durable real-time data streaming service. Kinesis Data Streams can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.
Amazon Kinesis Video Streams
Amazon Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for analytics, ML, playback, and other processing. Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming video data from millions of devices. It also durably stores, encrypts, and indexes video data in your streams, and allows you to access your data through easy-to-use APIs. Kinesis Video Streams enables you to playback video for live and on-demand viewing, and quickly build applications that take advantage of computer vision and video analytics through integration with Amazon Rekognition Video, and libraries for ML frameworks such as Apache MxNet, TensorFlow, and OpenCV.
Amazon OpenSearch Service
Amazon OpenSearch Service (OpenSearch Service) makes it easy to deploy, secure, operate, and scale OpenSearch to search, analyze, and visualize data in real-time. With Amazon OpenSearch Service, you get easy-to-use APIs and real-time analytics capabilities to power use-cases such as log analytics, full-text search, application monitoring, and clickstream analytics, with enterprise-grade availability, scalability, and security. The service offers integrations with open-source tools such as OpenSearch Dashboards and Logstash for data ingestion and visualization. It also integrates seamlessly with other AWS services such as Amazon Virtual Private Cloud (Amazon VPC),AWS Key Management Service (AWS KMS), Amazon Data Firehose, AWS Lambda, AWS Identity and Access Management (IAM), Amazon Cognito, and Amazon CloudWatch, so that you can go from raw data to actionable insights quickly.
Amazon OpenSearch Serverless
Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without
Amazon Kinesis Data Streams
14
Overview of Amazon Web Services AWS Whitepaper
configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment.
The vector engine for Amazon OpenSearch Serverless, adds a simple, scalable, and high-performing vector storage and search capability to help developers build ML-augmented search experiences and generative AI applications without having to manage vector database infrastructure. Use cases for vector search collections include image search, document search, music retrieval, product recommendation, video search, location-based search, fraud detection, and anomaly detection.
Amazon Redshift
Amazon Redshift is the most widely used cloud data warehouse. It makes it fast, simple and cost-effective to analyze all your data using standard SQL and your existing Business Intelligence (BI) tools. It allows you to run complex analytic queries against terabytes to petabytes of structured and semi-structured data, using sophisticated query optimization, columnar storage on high-performance storage, and massively parallel query completion. Most results come back in seconds. You can start small for just $0.25 per hour with no commitments and scale out to petabytes of data for $1,000 per terabyte per year, less than a tenth the cost of traditional on-premises solutions.
Amazon Redshift Serverless
Amazon Redshift Serverless makes it easier to run and scale analytics without having to manage your data warehouse infrastructure. Developers, data scientists, and analysts can work across databases, data warehouses, and data lakes to build reporting and dashboarding applications, perform near real-time analytics, share and collaborate on data, and build and train machine learning (ML) models. Go from large amounts of data to insights in seconds. Amazon Redshift Serverless automatically provisions and intelligently scales data warehouse capacity to deliver fast performance for even the most demanding and unpredictable workloads, and you pay only for what you use. Just load data and start querying right away in Amazon Redshift Query Editor or in your favorite business intelligence (BI) tool and continue to enjoy the best price performance and familiar SQL features in an easy-to-use, zero administration environment.
QuickSight
QuickSight is a fast, cloud-powered business intelligence (BI) service that makes it easy for you to deliver insights to everyone in your organization. QuickSight lets you create and publish interactive dashboards that can be accessed from browsers or mobile devices. You can embed dashboards
Amazon Redshift
15
Overview of Amazon Web Services AWS Whitepaper
into your applications, providing your customers with powerful self-service analytics. QuickSight easily scales to tens of thousands of users without any software to install, servers to deploy, or infrastructure to manage.
AWS Clean Rooms
AWS Clean Rooms helps companies and their partners more easily and securely analyze and collaborate on their collective datasets–without sharing or copying one another's underlying data. With AWS Clean Rooms, customers can create a secure data clean room in minutes, and collaborate with any other company on the AWS Cloud to generate unique insights about advertising campaigns, investment decisions, and research and development.
AWS Data Exchange
AWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud. Qualified data providers include category-leading brands such as Reuters, who curate data from over 2.2 million unique news stories per year in multiple languages; Change Healthcare, who process and anonymize more than 14 billion healthcare transactions and $1 trillion in claims annually; Dun & Bradstreet, who maintain a database of more than 330 million global business records; and Foursquare, whose location data is derived from 220 million unique consumers and includes more than 60 million global commercial venues.
Once subscribed to a data product, you can use the AWS Data Exchange API to load data directly into Amazon S3 and then analyze it with a wide variety of AWS analytics and ML services. For example, property insurers can subscribe to data to analyze historical weather patterns to calibrate insurance coverage requirements in different geographies; restaurants can subscribe to population and location data to identify optimal regions for expansion; academic researchers can conduct studies on climate change by subscribing to data on carbon dioxide emissions; and healthcare professionals can subscribe to aggregated data from historical clinical trials to accelerate their research activities.
For data providers, AWS Data Exchange makes it easy to reach the millions of AWS customers migrating to the cloud by removing the need to build and maintain infrastructure for data storage, delivery, billing, and entitling.
AWS Data Pipeline
AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premises data sources, at specified
AWS Clean Rooms
16
Overview of Amazon Web Services AWS Whitepaper
intervals. With AWS Data Pipeline, you can regularly access your data where it’s stored, transform and process it at scale, and efficiently transfer the results to AWS services such as Amazon S3,Amazon RDS, Amazon DynamoDB, and Amazon EMR.
AWS Data Pipeline helps you easily create complex data processing workloads that are fault tolerant, repeatable, and highly available. You don’t have to worry about ensuring resource availability, managing inter-task dependencies, retrying transient failures or timeouts in individual tasks, or creating a failure notification system. AWS Data Pipeline also allows you to move and process data that was previously locked up in on-premises data silos.
AWS Entity Resolution
AWS Entity Resolution is a service that helps you match and link related records stored across multiple applications, channels, and data stores without building a custom solution. Using flexible, configurable ML and rule-based techniques, AWS Entity Resolution can remove duplicate records, create customer profiles by connecting different customer interactions, and personalize experiences across advertising and marketing campaigns, loyalty programs, and e-commerce. For example, you can create a unified view of customer interactions by linking recent events, such as ad clicks, cart abandonment, and purchases, into a unique match ID.
AWS Glue
AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You can create and run an ETL job with a few clicks in the AWS Management Console. You simply point AWS Glue to your data stored in AWS, and AWS Glue discovers your data and stores the associated metadata (such as table definition and schema) in the AWS Glue Data Catalog. Once cataloged, your data is immediately searchable, queryable, and available for ETL.
AWS Glue Data Integration Engines provide access to data using Apache Spark, PySpark, and Python. With the addition of AWS Glue for Ray, you can further scale your workloads using Ray, an open-source unified compute framework.
AWS Glue Data Quality can measure and monitor the data quality of Amazon S3 based data lakes, data warehouses, and other data repositories. It automatically computes statistics, recommends quality rules, and can monitor and alert you when it detects missing, stale, or bad data. You can access it in the AWS Glue Data Catalog and in AWS Glue Data Catalog ETL jobs.
AWS Entity Resolution
17
Overview of Amazon Web Services AWS Whitepaper
AWS Lake Formation
AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis. A data lake enables you to break down data silos and combine different types of analytics to gain insights and guide better business decisions.
However, setting up and managing data lakes today involves a lot of manual, complicated, and time-consuming tasks. This work includes loading data from diverse sources, monitoring those data flows, setting up partitions, turning on encryption and managing keys, defining transformation jobs and monitoring their operation, re-organizing data into a columnar format, configuring access control settings, deduplicating redundant data, matching linked records, granting access to data sets, and auditing access over time.
Creating a data lake with Lake Formation is as simple as defining where your data resides and what data access and security policies you want to apply. Lake Formation then collects and catalogs data from databases and object storage, moves the data into your new Amazon S3 data lake, cleans and classifies data using ML algorithms, and secures access to your sensitive data. Your users can then access a centralized catalog of data which describes available data sets and their appropriate usage. Your users then leverage these data sets with their choice of analytics and ML services, such as Amazon EMR for Apache Spark, Amazon Redshift, Amazon Athena, SageMaker AI, and QuickSight.
Amazon Managed Streaming for Apache Kafka (Amazon MSK)
Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data. Apache Kafka is an open-source platform for building real-time streaming data pipelines and applications. With Amazon MSK, you can use Apache Kafka APIs to populate data lakes, stream changes to and from databases, and power ML and analytics applications.
Apache Kafka clusters are challenging to setup, scale, and manage in production. When you run Apache Kafka on your own, you need to provision servers, configure Apache Kafka manually, replace servers when they fail, orchestrate server patches and upgrades, architect the cluster for high availability, ensure data is durably stored and secured, setup monitoring and alarms, and carefully plan scaling events to support load changes. Amazon MSK makes it easy for you to build and run production applications on Apache Kafka without needing Apache Kafka infrastructure management expertise. That means you spend less time managing infrastructure and more time building applications.
AWS Lake Formation
18
Overview of Amazon Web Services AWS Whitepaper
With a few clicks in the Amazon MSK console you can create highly available Apache Kafka clusters with settings and configuration based on Apache Kafka’s deployment best practices. Amazon MSK automatically provisions and runs your Apache Kafka clusters. Amazon MSK continuously monitors cluster health and automatically replaces unhealthy nodes with no downtime to your application. In addition, Amazon MSK secures your Apache Kafka cluster by encrypting data at rest.
Application integration
Application integration on AWS is a suite of services that enable communication between decoupled components within microservices, distributed systems, and serverless applications. You don’t need to refactor your entire architecture to benefit—decoupling applications at any scale can reduce the impact of changes, making it easier to update and faster to release new features.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing an AWS application integration service or Amazon SQS, Amazon SNS, or Amazon EventBridge?. For general information, see Application Integration on AWS.
Application integration
19
Overview of Amazon Web Services AWS Whitepaper
Services
•
AWS Step Functions
•
Amazon AppFlow
•
AWS B2B Data Interchange
•
Amazon EventBridge
•
Amazon Managed Workflows for Apache Airflow (MWAA)
•
Amazon MQ
•
Amazon Simple Notification Service
•
Amazon Simple Queue Service
•
Amazon Simple Workflow Service
Application integration
20
Overview of Amazon Web Services AWS Whitepaper
AWS Step Functions
AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. Building applications from individual components that each perform a discrete function lets you scale easily and change applications quickly. Step Functions is a reliable way to coordinate components and step through the functions of your application. Step Functions provides a graphical console to arrange and visualize the components of your application as a series of steps. This makes it simple to build and run multi-step applications. Step Functions automatically initiates and tracks each step, and retries when there are errors, so your application runs in order and as expected. Step Functions logs the state of each step, so when things do go wrong, you can diagnose and debug problems quickly. You can change and add steps without even writing code, so you can easily evolve your application and innovate faster.
Amazon AppFlow
Amazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications such as Salesforce, Zendesk, Slack, and ServiceNow, and AWS services such as Amazon S3 and Amazon Redshift, in just a few clicks. With Amazon AppFlow, you can run data flows at enterprise scale at the frequency you choose - on a schedule, in response to a business event, or on demand. You can configure data transformation capabilities such as filtering and validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. Amazon AppFlow; automatically encrypts data in motion, and allows users to restrict data from flowing over the public internet for SaaS applications that are integrated with AWS PrivateLink, reducing exposure to security threats.
AWS B2B Data Interchange
AWS B2B Data Interchange (B2Bi) automates the transformation of Electronic Data Interchange (EDI) documents into JSON and XML formats to simplify your downstream data integrations. Businesses use EDI documents to exchange transactional data with trading partners, such as suppliers and end customers, using standardized formats such as X12.
With B2Bi, you can onboard and manage your trading partners and automate the transformation of EDI documents into common data representations such as JSON and XML using a low-code interface. This approach reduces the time, complexity, and cost associated with preparing and integrating EDI data into their business applications and purpose-built data lakes. As a result,
AWS Step Functions
21
Overview of Amazon Web Services AWS Whitepaper
you can concentrate on using transactional data to drive business insights using the AWS suite of analytics, AI, and ML services.
Amazon EventBridge
Amazon EventBridge is a serverless event bus that makes it easier to build event-driven applications at scale using events generated from your applications, integrated Software-as-a-Service (SaaS) applications, and AWS services. EventBridge delivers a stream of real-time data from event sources such as Zendesk or Shopify to targets such as AWS Lambda and other SaaS applications. You can set up routing rules to determine where to send your data to build application architectures that react in real-time to your data sources with event publisher and consumer completely decoupled.
Amazon Managed Workflows for Apache Airflow (MWAA)
Amazon Managed Workflows for Apache Airflow (MWAA) is a managed orchestration service forApache Airflow that makes it easier to set up and operate end-to-end data pipelines in the cloud at scale. Apache Airflow is an open-source tool used to programmatically author, schedule, and monitor sequences of processes and tasks referred to as “workflows.” With Managed Workflows, you can use Airflow and Python to create workflows without having to manage the underlying infrastructure for scalability, availability, and security. Managed Workflows automatically scales its workflow capacity to meet your needs, and is integrated with AWS security services to help provide you with fast and secure access to data.
Amazon MQ
Amazon MQ is a managed message broker service for Apache ActiveMQ Classic and RabbitMQthat makes it easy to set up and operate message brokers in the cloud. Message brokers allow different software systems–often using different programming languages, and on different platforms–to communicate and exchange information. Amazon MQ reduces your operational load by managing the provisioning, setup, and maintenance of ActiveMQ and RabbitMQ, popular open-source message brokers. Connecting your current applications to Amazon MQ is easy because it uses industry-standard APIs and protocols for messaging, including JMS, NMS, AMQP, STOMP, MQTT, and WebSocket. Using standards means that in most cases, there’s no need to rewrite any messaging code when you migrate to AWS.
Amazon EventBridge
22
Overview of Amazon Web Services AWS Whitepaper
Amazon Simple Notification Service
Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Using Amazon SNS topics, your publisher systems can fan out messages to a large number of subscriber endpoints for parallel processing, including Amazon SQS queues, AWS Lambda functions, and HTTP/S webhooks. Additionally, SNS can be used to fan out notifications to end users using mobile push, SMS, and email.
Amazon Simple Queue Service
Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Get started with Amazon SQS in minutes using the AWS Management Console, AWS CLI, or SDK of your choice, and three simple commands.
Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.
Amazon Simple Workflow Service
Amazon Simple Workflow Service (Amazon SWF) helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully managed state tracker and task coordinator in the cloud. If your application’s steps take more than 500 milliseconds to complete, you need to track the state of processing. If you need to recover or retry if a task fails, Amazon SWF can help you.
Blockchain
Amazon Simple Notification Service
23
Overview of Amazon Web Services AWS Whitepaper
Amazon Managed Blockchain
Amazon Managed Blockchain is a fully managed service that makes it easy to create and manage scalable blockchain networks using the popular open source frameworks Hyperledger Fabric and Ethereum.
Blockchain makes it possible to build applications where multiple parties can run transactions without the need for a trusted, central authority. Today, building a scalable blockchain network with existing technologies is complex to set up and hard to manage. To create a blockchain network, each network member needs to manually provision hardware, install software, create and manage certificates for access control, and configure networking components. Once the blockchain network is running, you need to continuously monitor the infrastructure and adapt to changes, such as an increase in transaction requests, or new members joining or leaving the network.
Amazon Managed Blockchain is a fully managed service that allows you to set up and manage a scalable blockchain network with just a few clicks. Amazon Managed Blockchain eliminates the overhead required to create the network, and automatically scales to meet the demands of thousands of applications running millions of transactions. Once your network is up and running, Managed Blockchain makes it easy to manage and maintain your blockchain network. It manages your certificates, lets you easily invite new members to join the network, and tracks operational metrics such as usage of compute, memory, and storage resources. In addition, Managed Blockchain can replicate an immutable copy of your blockchain network activity intoAmazon Quantum Ledger Database (Amazon QLDB), a fully managed ledger database. This allows you to easily analyze the network activity outside the network and gain insights into trends.
Business applications
Innovative business applications with the same on-demand scalability, reliability, pay-as-you go pricing, and machine learning that drives AWS cloud infrastructure.
For general information, see AWS Business Applications.
Applications
Business applications
24
Overview of Amazon Web Services AWS Whitepaper
•
Alexa for Business
•
AWS AppFabric
•
Amazon Chime
•
Amazon Chime SDK
•
Amazon Connect
•
Amazon Pinpoint
•
Amazon SES
•
Amazon WorkDocs
•
Amazon WorkMail
Alexa for Business
Alexa for Business is a service that enables organizations and employees to use Alexa to get more work done. With Alexa for Business, employees can use Alexa as their intelligent assistant to be more productive in meeting rooms, at their desks, and even with the Alexa devices they already have at home.
AWS AppFabric
AWS AppFabric is a fully managed service that aggregates and normalizes security data across software as a service (SaaS) applications. Previously, integrating SaaS applications with existing security tools required teams to build, manage, and maintain their own point-to-point (P2P) integrations so that security teams could monitor event logs and understand activity from each application. With AppFabric, you can quickly connect multiple SaaS applications to increase observability, productivity, and security—with no coding required.
After the SaaS applications are authorized and connected, AppFabric ingests the data and normalizes it using the Open Cybersecurity Schema Framework (OCSF). OCSF allows you to set common policies, standardize security alerts, and quickly manage user access across multiple applications.
Amazon Chime
Amazon Chime is a communications service that transforms online meetings with a secure, easy-to-use application that you can trust. Amazon Chime works seamlessly across your devices so that
Alexa for Business
25
Overview of Amazon Web Services AWS Whitepaper
you can stay connected. You can use Amazon Chime for online meetings, video conferencing, calls, chat, and to share content, both inside and outside your organization.
Amazon Chime works with Alexa for Business, which means you can use Alexa to start your meetings with your voice. Alexa can start your video meetings in large conference rooms, and automatically dial into online meetings in smaller huddle rooms and from your desk.
Amazon Chime SDK
With the Amazon Chime SDK, builders can easily add real-time voice, video, and messaging powered by ML into their applications.
Amazon Connect
Amazon Connect is a self-service, omnichannel cloud contact center service that makes it easy for any business to deliver better customer service at lower cost. Amazon Connect is based on the same contact center technology used by Amazon customer service associates around the world to power millions of customer conversations. The self-service graphical interface in Amazon Connect makes it easy for non-technical users to design contact flows, manage agents, and track performance metrics – no specialized skills required. There are no up-front payments or long-term commitments and no infrastructure to manage with Amazon Connect; customers pay by the minute for Amazon Connect usage plus any associated telephony services.
Amazon Pinpoint
Amazon Pinpoint makes it easy to send targeted messages to your customers through multiple engagement channels. Examples of targeted campaigns are promotional alerts and customer retention campaigns, and transactional messages are messages such as order confirmations and password reset messages.
You can integrate Amazon Pinpoint into your mobile and web apps to capture usage data to provide you with insight into how customers interact with your apps. Amazon Pinpoint also tracks the ways that your customers respond to the messages you send—for example, by showing you the number of messages that were delivered, opened, or clicked.
You can develop custom audience segments and send them pre-scheduled targeted campaigns via email, SMS, and push notifications. Targeted campaigns are useful for sending promotional or educational content to re-engage and retain your users.
Amazon Chime SDK
26
Overview of Amazon Web Services AWS Whitepaper
You can send transactional messages using the console or the Amazon Pinpoint REST API. Transactional campaigns can be sent via email, SMS, push notifications, and voice messages. You can also use the API to build custom applications that deliver campaign and transactional messages.
Amazon SES
Amazon Simple Email Service (Amazon SES) is a cost-effective, flexible, and scalable email service that enables developers to send mail from within any application. You can configure Amazon SES quickly to support several email use cases, including transactional, marketing, or mass email communications. The Amazon SES flexible IP deployment and email authentication options help drive higher deliverability and protect sender reputation, while sending analytics measure the impact of each email. With Amazon SES, you can send email securely, globally, and at scale.
Amazon WorkDocs
Notice
New customer sign-ups and account upgrades are no longer available for Amazon WorkDocs. Learn about migration steps here: How to migrate data from Amazon WorkDocs.
Amazon WorkDocs is a fully managed, secure enterprise storage and sharing service with strong administrative controls and feedback capabilities that improve user productivity.
Users can comment on files, send them to others for feedback, and upload new versions without having to resort to emailing multiple versions of their files as attachments. Users can take advantage of these capabilities wherever they are, using the device of their choice, including PCs, Macs, tablets, and phones. Amazon WorkDocs offers IT administrators the option of integrating with existing corporate directories, flexible sharing policies and control of the location where data is stored.
Amazon WorkMail
Amazon WorkMail is a secure, managed business email and calendar service with support for existing desktop and mobile email client applications. Amazon WorkMail gives users the ability to seamlessly access their email, contacts, and calendars using the client application of their choice, including Microsoft Outlook, native iOS and Android email applications, any
Amazon SES
27
Overview of Amazon Web Services AWS Whitepaper
client application supporting the IMAP protocol, or directly through a web browser. You can integrate Amazon WorkMail with your existing corporate directory, use email journaling to meet compliance requirements, and control both the keys that encrypt your data and the location in which your data is stored. You can also set up interoperability with Microsoft Exchange Server, and programmatically manage users, groups, and resources using the Amazon WorkMail SDK.
Cloud Financial Management
Whether you were born in the cloud, or you are just starting your migration journey to the cloud, AWS has a set of solutions to help you manage and optimize your spend.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing an AWS cost management strategy. For general information, see Cloud Financial Management with AWS.
Services
•
AWS Application Cost Profiler
Cloud Financial Management
28
Overview of Amazon Web Services AWS Whitepaper
•
AWS Billing Conductor
•
AWS Cost Explorer
•
AWS Budgets
•
AWS Cost and Usage Report
•
Reserved Instance (RI) reporting
•
Savings Plans
AWS Application Cost Profiler
AWS Application Cost Profiler provides you the ability to track the consumption of shared AWS resources used by software applications and report granular cost breakdown across tenant base. You can achieve economies of scale with the shared infrastructure model, while still maintaining a clear line of sight to detailed resource consumption information across multiple dimensions.
With the proportionate cost insights of shared AWS resources, organizations running applications can establish the data foundation for accurate cost allocation model, and ISV selling applications can better understand your profitability and customize pricing strategies for your end customers.
AWS Billing Conductor
AWS Billing Conductor is a fully managed service that can support the showback and chargeback workflows of AWS Solution Providers and Enterprise customers. Using AWS Billing Conductor, you can customize your monthly billing data. The console models the billing relationship between you and your customers or business units. You can also customize a pro forma version of your billing data each month to accurately show or charge back your customers.
AWS Billing Conductor doesn't change the way that you're billed by Amazon Web Services each month. Instead, it provides you with a mechanism to configure, generate, and display rates to certain customers over a given billing period. You can also use it to analyze the difference between the rates you apply to your accounting groupings relative to your actual rates from AWS. As a result of your AWS Billing Conductor configuration, the payer account can also see the custom rate that's applied on the billing details page of the AWS Billing console, or configure a cost and usage report per billing group.
You can configure the billing groups and pricing plans using the AWS Billing Conductor or the AWS Billing Conductor API. For more information about AWS Billing Conductor service quotas, refer toQuotas and restrictions.
AWS Application Cost Profiler
29
Overview of Amazon Web Services AWS Whitepaper
AWS Cost Explorer
AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. Get started quickly by creating custom reports (including charts and tabular data) that analyze cost and usage data, both at a high level (such as total costs and usage across all accounts) and for highly-specific requests (such as m2.2xlarge costs within account Y that are tagged “project: secretProject”).
AWS Budgets
AWS Budgets gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. You can also use AWS Budgets to set RI utilization or coverage targets and receive alerts when your utilization drops below the threshold you define. RI alerts support Amazon EC2, Amazon RDS, Amazon Redshift, and Amazon ElastiCache reservations.
Budgets can be tracked at the monthly, quarterly, or yearly level, and you can customize the start and end dates. You can further refine your budget to track costs associated with multiple dimensions, such as AWS service, linked account, tag, and others. Budget alerts can be sent via email and/or Amazon Simple Notification Service (Amazon SNS) topic.
Budgets can be created and tracked from the AWS Budgets dashboard or via the AWS Budgets API.
AWS Cost and Usage Report
The AWS Cost and Usage Report is a single location for accessing comprehensive information about your AWS costs and usage.
The AWS Cost and Usage Report lists AWS usage for each service category used by an account and its IAM users in hourly or daily line items, as well as any tags that you have activated for cost allocation purposes. You can also customize the AWS Cost and Usage Report to aggregate your usage data to the daily or monthly level.
Reserved Instance (RI) reporting
AWS provides a number of RI-specific cost management solutions out-of-the-box to help you better understand and manage your RIs. Using the RI Utilization and Coverage reports available in AWS Cost Explorer, you can visualize your RI data at an aggregate level or inspect a particular
AWS Cost Explorer
30
Overview of Amazon Web Services AWS Whitepaper
RI subscription. To access the most detailed RI information available, you can leverage the AWS Cost and Usage Report. You can also set a custom RI utilization target via AWS Budgets and receive alerts when your utilization drops below the threshold you define.
Savings Plans
Savings Plans is a flexible pricing model offering lower prices compared to On-Demand pricing, in exchange for a specific usage commitment (measured in $/hour) for a one or three-year period. AWS offers three types of Savings Plans – Compute Savings Plans, Amazon EC2 Instance Savings Plans, and Amazon SageMaker AI Savings Plans. Compute Savings Plans apply to usage across Amazon EC2, AWS Lambda, and AWS Fargate. The Amazon EC2 Instance Savings Plans apply to EC2 usage, and Amazon SageMaker AI Savings Plans apply to Amazon SageMaker AI usage. You can easily sign up a one- or three-year term Savings Plans in AWS Cost Explorer and manage your plans by taking advantage of recommendations, performance reporting, and budget alerts.
Compute
Millions of organizations run diverse workloads using AWS compute services.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing an AWS compute service or Amazon Lightsail, AWS Elastic Beanstalk, or Amazon EC2?. For general information, see Compute on AWS.
Savings Plans
31
Overview of Amazon Web Services AWS Whitepaper
Topics
•
Compare AWS compute services
•
Amazon EC2
•
Amazon EC2 Auto Scaling
•
Amazon EC2 Image Builder
•
Amazon Lightsail
•
Amazon Linux 2023
•
AWS App Runner
•
AWS Batch
•
AWS Elastic Beanstalk
•
AWS Fargate
•
AWS Lambda
•
AWS Serverless Application Repository
•
AWS Outposts
•
AWS Wavelength
Compute
32
Overview of Amazon Web Services AWS Whitepaper
•
VMware Cloud on AWS
Compare AWS compute services
Category
AWS service
Instances (virtual machines)
•
Amazon Elastic Compute Cloud (Amazon EC2) — Secure and resizeable compute capacity (virtual servers) in the cloud
•
Amazon EC2 Spot Instances— Run fault-tol erant workloads for up to 90% off
•
Amazon EC2 Auto Scaling — Automatically add or remove compute capacity to meet changes in demand
•
Amazon Lightsail — Easy-to-use cloud platform that offers you everything you need to build an application or website
•
AWS Batch — Fully managed batch processing at any scale
Containers
•
Amazon Elastic Container Service (Amazon ECS) — Highly secure, reliable, and scalable way to run containers
•
Amazon ECS Anywhere — Run containers on customer managed infrastructure
•
Amazon Elastic Container Registry (Amazon ECR) — Easily store, manage, and deploy container images
•
Amazon Elastic Kubernetes Service (Amazon EKS) — Fully managed Kubernetes service
•
Amazon EKS Anywhere — Create and operate Kubernetes clusters on your own infrastructure
Compare AWS compute services
33
Overview of Amazon Web Services AWS Whitepaper
Category AWS service
•
AWS Fargate — Serverless compute for containers
•
AWS App Runner — Build and run container ized applications on a fully managed service
Serverless
•
AWS Lambda — Run code without thinking about servers. Pay only for the compute time you consume.
Edge and hybrid
•
AWS Outposts — Run AWS infrastru cture and services on premises for a truly consistent hybrid experience
•
AWS Snow Family — Collect and process data in rugged or disconnected edge environments
•
AWS Wavelength — Deliver ultra-low latency application for 5G devices
•
VMware Cloud on AWS — Preferred service for all vSphere workloads to rapidly extend and migrate to the cloud
•
AWS Local Zones — Run latency sensitive applications closer to end-users
Compare AWS compute services
34
Overview of Amazon Web Services AWS Whitepaper
Category AWS service
Cost and capacity management
•
AWS Savings Plan — Flexible pricing model that provides savings of up to 72% on AWS compute usage
•
AWS Compute Optimizer — Recommend s optimal AWS compute resources for your workloads to reduce costs and improve performance
•
AWS Elastic Beanstalk — Easy-to-use service for deploying and scaling web applications and services
•
EC2 Image Builder — Build and maintain secure Linux or Windows Server images
•
Elastic Load Balancing (ELB) — Automatic ally distribute incoming application traffic across multiple targets
Amazon EC2
Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers.
The simple web interface of Amazon EC2 allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon’s proven computing environment. Amazon EC2 reduces the time required to obtain and boot new server instances (called Amazon EC2 instances) to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change. Amazon EC2 changes the economics of computing by allowing you to pay only for capacity that you actually use. Amazon EC2 provides developers and system administrators the tools to build failure resilient applications and isolate themselves from common failure scenarios.
Amazon EC2
35
Overview of Amazon Web Services AWS Whitepaper
Instance types
Amazon EC2 passes on to you the financial benefits of Amazon scale. You pay a very low rate for the compute capacity you actually consume. For a more detailed description, refer to Amazon EC2 pricing.
Amazon EC2 instance types are named based on their family, generation, processor family, additional capabilities, and size.
•
On-Demand Instances — With On-Demand Instances, you pay for compute capacity by the hour or the second depending on which instances you run. No longer-term commitments or upfront payments are needed. You can increase or decrease your compute capacity depending on the demands of your application and only pay the specified per hourly rates for the instance you use. On-Demand Instances are recommended for:
•
Users that prefer the low cost and flexibility of Amazon EC2 without any up-front payment or long-term commitment
•
Applications with short-term, spiky, or unpredictable workloads that cannot be interrupted
•
Applications being developed or tested on Amazon EC2 for the first time
•
Spot Instances —Spot Instances are available at up to a 90% discount compared to On-Demand prices and let you take advantage of unused Amazon EC2 capacity in the AWS Cloud. You can significantly reduce the cost of running your applications, grow your application’s compute capacity and throughput for the same budget, and enable new types of cloud computing applications. Spot Instances are recommended for:
•
Applications that have flexible start and end times
•
Applications that are only feasible at very low compute prices
•
Users with urgent computing needs for large amounts of additional capacity
•
Reserved Instances — Reserved Instances provide you with a significant discount (up to 72%) compared to On-Demand Instance pricing. You have the flexibility to change families, operating system types, and tenancies while benefiting from Reserved Instance pricing when you use Convertible Reserved Instances.
•
C7g Instances — C7g Instances, powered by the latest generation AWS Graviton3 processors, provide the best price performance in Amazon EC2 for compute-intensive workloads. C7g instances are ideal for high performance computing (HPC), batch processing, electronic design automation (EDA), gaming, video encoding, scientific modeling, distributed analytics, CPU-based ML inference, and ad serving.
Amazon EC2
36
Overview of Amazon Web Services AWS Whitepaper
•
Inf2 Instances — Inf2 Instances are purpose--built for deep learning inference. They deliver high performance at the lowest cost in Amazon EC2 for generative AI models, including large language models (LLMs) and vision transformers. Inf2 instances are powered by AWS Inferentia2, the second-generation AWS Inferentia accelerator.
•
M7g Instances — M7g instances, powered by the latest generation AWS Graviton3 processors, provide the best price performance in Amazon EC2 for general purpose workloads. M7g instances are ideal for applications built on open-source software such as application servers, microservices, gaming servers, mid-size data stores, and caching fleets.
•
R7g Instances — R7g Instances, powered by the latest generation AWS Graviton3 processors, provide the best price performance in Amazon EC2 for memory-intensive workloads. R7g instances are ideal for memory-intensive workloads such as open-source databases, in-memory caches, and near real-time big data analytics.
•
Trn1 Instances — Trn1 Instances, powered by AWS Trainium accelerators, are purpose-built for high-performance deep learning training of generative AI models, including LLMs and latent diffusion models. Trn1 instances offer up to 50% cost-to-train savings over other comparable Amazon EC2 instances.
•
Savings Plans — Savings Plans are a flexible pricing model that offer low prices on EC2 and Fargate usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a one or three year term.
•
Dedicated Hosts — A Dedicated Host is a physical EC2 server dedicated for your use. Dedicated Hosts can help you reduce costs by allowing you to use your existing server-bound software licenses, including Windows Server, Microsoft SQL Server, and SUSE Linux Enterprise Server (subject to your license terms), and can also help you meet compliance requirements.
Amazon EC2 Auto Scaling
Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove EC2 instances according to conditions you define. You can use the fleet management features of Amazon EC2 Auto Scaling to maintain the health and availability of your fleet. You can also use the dynamic and predictive scaling features of Amazon EC2 Auto Scaling to add or remove EC2 instances. Dynamic scaling responds to changing demand and predictive scaling automatically schedules the right number of EC2 instances based on predicted demand. Dynamic scaling and predictive scaling can be used together to scale faster.
Amazon EC2 Auto Scaling
37
Overview of Amazon Web Services AWS Whitepaper
Amazon EC2 Image Builder
EC2 Image Builder simplifies the building, testing, and deployment of VMs and container images for use on AWS or on-premises.
Keeping virtual machine (VM) and container images up-to-date can be time consuming, resource intensive, and error-prone. Currently, customers either manually update and snapshot VMs or have teams that build automation scripts to maintain images.
EC2 Image Builder significantly reduces the effort of keeping images up-to-date and secure by providing a simple graphical interface, built-in automation, and AWS-provided security settings. With Image Builder, there are no manual steps for updating an image nor do you have to build your own automation pipeline.
Image Builder is offered at no cost, other than the cost of the underlying AWS resources used to create, store, and share the images.
Amazon Lightsail
Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server with AWS. Lightsail plans include everything you need to jumpstart your project – a VM, SSD-based storage, data transfer, DNS management, and a static IP address – for a low, predictable price.
Amazon Linux 2023
Amazon Linux 2023 (AL2023) is our new Linux-based operating system for AWS that is designed to provide a secure, stable, high-performance environment to develop and run your cloud applications. AL2023 provides seamless integration with various AWS services and development tools, and offers optimized performance for Amazon EC2 Graviton-based instances and Support at no additional licensing cost. Starting with AL2023, a new Amazon Linux major release will be available every two years. This cadence provides you with a more predictable release cycle and up to 5 years of support, making it easier for you to plan your upgrades.
AL2023 offers several improvements over Amazon Linux 2 (AL2). For example, AL2023 takes a security-by-default approach to help improve your security posture with preconfigured security policies, SELinux in permissive mode and IMDSv2 enabled by default, and the availability of kernel live patching. With deterministic upgrades through versioned repositories, you can lock to a specific version of the Amazon Linux package repository, giving you control over how and when you absorb
Amazon EC2 Image Builder
38
Overview of Amazon Web Services AWS Whitepaper
updates. With this capability, you can adhere to operational best practices more efficiently by ensuring consistency between package versions and updates across your environment. For a full comparison, refer to Comparing Amazon Linux 2 and Amazon Linux 2023.
Amazon Linux 2023 is generally available in all AWS Regions, including the AWS GovCloud (US) and the China Regions.
AWS App Runner
AWS App Runner is a fully managed service that makes it easy for developers to quickly deploy containerized web applications and APIs, at scale and with no prior infrastructure experience required. Start with your source code or a container image. AWS App Runner automatically builds and deploys the web application and load balances traffic with encryption. App Runner also scales up or down automatically to meet your traffic needs. With App Runner, rather than thinking about servers or scaling, you have more time to focus on your applications.
AWS Batch
AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (such as CPU or memory-optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. With AWS Batch, there is no need to install and manage batch computing software or server clusters that you use to run your jobs, allowing you to focus on analyzing results and solving problems. AWS Batch plans, schedules, and runs your batch computing workloads across the full range of AWS compute services and features, such as Amazon EC2 and Spot Instances.
AWS Elastic Beanstalk
AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and Internet Information Services (IIS).
You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time.
AWS App Runner
39
Overview of Amazon Web Services AWS Whitepaper
AWS Fargate
AWS Fargate is a compute engine for Amazon ECS that allows you to run containers without having to manage servers or clusters. With AWS Fargate, you no longer have to provision, configure, and scale clusters of VMs to run containers. This removes the need to choose server types, decide when to scale your clusters, or optimize cluster packing. Fargate removes the need for you to interact with or think about servers or clusters. Fargate lets you focus on designing and building your applications instead of managing the infrastructure that runs them.
Amazon ECS has two modes: Fargate launch type and EC2 launch type. With Fargate launch type, all you have to do is package your application in containers, specify the CPU and memory requirements, define networking and IAM policies, and launch the application. EC2 launch type allows you to have server-level, more granular control over the infrastructure that runs your container applications. With EC2 launch type, you can use Amazon ECS to manage a cluster of servers and schedule placement of containers on the servers. Amazon ECS keeps track of all the CPU, memory and other resources in your cluster, and also finds the best server for a container to run on based on your specified resource requirements.
You are responsible for provisioning, patching, and scaling clusters of servers. You can decide which type of server to use, which applications and how many containers to run in a cluster to optimize utilization, and when you should add or remove servers from a cluster. EC2 launch type gives you more control of your server clusters and provides a broader range of customization options, which might be required to support some specific applications or possible compliance and government requirements.
AWS Lambda
AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume—there is no charge when your code is not running. With Lambda, you can run code for virtually any type of application or backend service—all with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically run from other AWS services, or you can call it directly from any web or mobile app.
AWS Serverless Application Repository
The AWS Serverless Application Repository enables you to quickly deploy code samples, components, and complete applications for common use cases such as web and mobile backends,
AWS Fargate
40
Overview of Amazon Web Services AWS Whitepaper
event and data processing, logging, monitoring, Internet of Things (IoT), and more. Each application is packaged with an AWS Serverless Application Model (AWS SAM) template that defines the AWS resources used. Publicly shared applications also include a link to the application’s source code. There is no additional charge to use the AWS Serverless Application Repository - you only pay for the AWS resources used in the applications you deploy.
You can also use the AWS Serverless Application Repository to publish your own applications and share them within your team, across your organization, or with the community at large. To share an application you've built, publish it to the AWS Serverless Application Repository.
AWS Outposts
AWS Outposts bring native AWS services, infrastructure, and operating models to virtually any data center, co-location space, or on-premises facility. You can use the same APIs, the same tools, the same hardware, and the same functionality across on-premises and the cloud to deliver a truly consistent hybrid experience. Outposts can be used to support workloads that need to remain on-premises due to low latency or local data processing needs.
AWS Outposts come in two variants:
•
VMware Cloud on AWS Outposts allows you to use the same VMware control plane and APIs you use to run your infrastructure.
•
AWS-native variant of AWS Outposts allows you to use the same exact APIs and control plane you use to run in the AWS Cloud, but on-premises.
AWS Outposts infrastructure is fully managed, maintained, and supported by AWS to deliver access to the latest AWS services. Getting started is easy, you simply log into the AWS Management Console to order your Outposts servers, choosing from a wide range of compute and storage options. You can order one or more servers, or quarter, half, and full rack units.
AWS Wavelength
AWS Wavelength is an AWS Infrastructure offering optimized for mobile edge computing applications. Wavelength Zones are AWS infrastructure deployments that embed AWS compute and storage services within communications service providers’ (CSP) datacenters at the edge of the 5G network, so application traffic from 5G devices can reach application servers running in Wavelength Zones without leaving the telecommunications network. This avoids the latency that would result from application traffic having to traverse multiple hops across the Internet to reach
AWS Outposts
41
Overview of Amazon Web Services AWS Whitepaper
their destination, enabling customers to take full advantage of the latency and bandwidth benefits offered by modern 5G networks.
VMware Cloud on AWS
VMware Cloud on AWS is an integrated cloud offering jointly developed by AWS and VMware delivering a highly scalable, secure and innovative service that allows organizations to seamlessly migrate and extend their on-premises VMware vSphere-based environments to the AWS Cloud running on next-generation Amazon Elastic Compute Cloud (Amazon EC2) bare metal infrastructure. VMware Cloud on AWS is ideal for enterprise IT infrastructure and operations organizations looking to migrate their on-premises vSphere-based workloads to the public cloud, consolidate and extend their data center capacities, and optimize, simplify and modernize their disaster recovery solutions.
VMware Cloud on AWS is delivered, sold, and supported globally by VMware and its partners with availability in the following AWS Regions: AWS Europe (Stockholm), AWS US East (Northern Virginia), AWS US East (Ohio), AWS US West (Northern California), AWS US West (Oregon), AWS Canada (Central), AWS Europe (Frankfurt), AWS Europe (Ireland), AWS Europe (London), AWS Europe (Paris), AWS Europe (Milan), AWS Asia Pacific (Singapore), AWS Asia Pacific (Sydney), AWS Asia Pacific (Tokyo), AWS Asia Pacific (Mumbai) Region, AWS South America (Sao Paulo), AWS Asia Pacific (Seoul), and AWS GovCloud (US West). With each release, VMware Cloud on AWS availability will expand into additional global regions.
VMware Cloud on AWS brings the broad, diverse and rich innovations of AWS services natively to the enterprise applications running on VMware's compute, storage and network virtualization platforms. This allows organizations to easily and rapidly add new innovations to their enterprise applications by natively integrating AWS infrastructure and platform capabilities such as AWS Lambda, Amazon Simple Queue Service (Amazon SQS), Amazon S3, Elastic Load Balancing, Amazon RDS, Amazon DynamoDB, Amazon Kinesis, and Amazon Redshift, among many others.
With VMware Cloud on AWS, organizations can simplify their Hybrid IT operations by using the same VMware Cloud Foundation technologies including vSphere, vSAN, NSX, and vCenter Server across their on-premises data centers and on the AWS Cloud without having to purchase any new or custom hardware, rewrite applications, or modify their operating models. The service automatically provisions infrastructure and provides full VM compatibility and workload portability between your on-premises environments and the AWS Cloud. With VMware Cloud on AWS, you can use a broad range of AWS services, including compute, databases, analytics, IoT, security, mobile, deployment, application services, and more.
VMware Cloud on AWS
42
Overview of Amazon Web Services AWS Whitepaper
Customer enablement
AWS Managed Services
AWS Managed Services provides ongoing management of your AWS infrastructure so you can focus on your applications. By implementing best practices to maintain your infrastructure, AWS Managed Services helps to reduce your operational overhead and risk. AWS Managed Services automates common activities such as change requests, monitoring, patch management, security, and backup services, and provides full lifecycle services to provision, run, and support your infrastructure. Our rigor and controls help to enforce your corporate and security infrastructure policies, and enables you to develop solutions and applications using your preferred development approach. AWS Managed Services improves agility, reduces cost, and unburdens you from infrastructure operations so that you can direct resources toward differentiating your business.
AWS re:Post Private
AWS re:Post Private is a private version of AWS re:Post for enterprises with Enterprise Support or Enterprise On-Ramp Support plans. It provides access to knowledge and experts to accelerate cloud adoption and increase developer productivity. With your organization-specific re:Post Private, you can build an organization-specific developer community that drives efficiencies at scale and provides access to valuable knowledge resources. re:Post Private centralizes trusted AWS technical content and offers private discussion forums to improve how your teams collaborate internally and with AWS to remove technical obstacles, accelerate innovation, and scale more efficiently in the cloud.
Containers
AWS offers services that give you a secure place to store and manage your container images, orchestration that manages when and where your containers run, and flexible compute engines to
Customer enablement
43
Overview of Amazon Web Services AWS Whitepaper
power your containers. AWS can help manage your containers and their deployments for you, so you don't have to worry about the underlying infrastructure.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing an AWS container service or Amazon Lightsail, AWS Elastic Beanstalk, or Amazon EC2?. For general information, see Containers on AWS.
Services
•
Amazon Elastic Container Registry
•
Amazon Elastic Container Service
•
Amazon Elastic Kubernetes Service
•
AWS App2Container
•
Red Hat OpenShift Service on AWS
Amazon Elastic Container Registry
Amazon Elastic Container Registry (Amazon ECR) is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. Amazon ECR is integrated with Amazon Elastic Container Service (Amazon ECS), simplifying your development to
Amazon Elastic Container Registry
44
Overview of Amazon Web Services AWS Whitepaper
production workflow. Amazon ECR eliminates the need to operate your own container repositories or worry about scaling the underlying infrastructure. Amazon ECR hosts your images in a highly available and scalable architecture, allowing you to reliably deploy containers for your applications. Integration with AWS Identity and Access Management (IAM) provides resource-level control of each repository. With Amazon ECR, there are no upfront fees or commitments. You pay only for the amount of data you store in your repositories and data transferred to the internet.
Amazon Elastic Container Service
Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers and allows you to easily run and scale containerized applications on AWS. Amazon ECS eliminates the need for you to install and operate your own container orchestration software, manage and scale a cluster of virtual machines (VMs), or schedule containers on those VMs.
With simple API calls, you can launch and stop Docker-enabled applications, query the complete state of your application, and access many familiar features such as IAM roles, security groups, load balancers, Amazon CloudWatch Events, AWS CloudFormation templates, and AWS CloudTrail logs.
Amazon Elastic Kubernetes Service
Amazon Elastic Kubernetes Service (Amazon EKS) makes it easy to deploy, manage, and scale containerized applications using Kubernetes on AWS.
Amazon EKS runs the Kubernetes management infrastructure for you across multiple AWS Availability Zones to eliminate a single point of failure. Amazon EKS is certified Kubernetes conformant so you can use existing tooling and plugins from partners and the Kubernetes community. Applications running on any standard Kubernetes environment are fully compatible and can be easily migrated to Amazon EKS.
AWS App2Container
AWS App2Container (A2C) is a command-line tool for modernizing .NET and Java applications into containerized applications. A2C analyzes and builds an inventory of all applications running in VMs, on-premises or in the cloud. You simply select the application you want to containerize, and A2C packages the application artifact and identified dependencies into container images, configures the network ports, and generates the ECS task and Kubernetes pod definitions. A2C provisions, through AWS CloudFormation, the cloud infrastructure and CI/CD pipelines required to deploy the
Amazon Elastic Container Service
45
Overview of Amazon Web Services AWS Whitepaper
containerized .NET or Java application into production. With A2C, you can easily modernize your existing applications and standardize the deployment and operations through containers.
Red Hat OpenShift Service on AWS
Red Hat OpenShift Service on AWS (ROSA) provides an integrated experience to use OpenShift. If you are already familiar with OpenShift, you can accelerate your application development process by leveraging familiar OpenShift APIs and tools for deployments on AWS. With ROSA, you can use the wide range of AWS compute, database, analytics, machine learning (ML), networking, mobile, and other services to build secure and scalable applications faster. ROSA comes with pay-as-you-go hourly and annual billing, a 99.95% SLA, and joint support from AWS and Red Hat.
ROSA makes it easier for you to focus on deploying applications and accelerating innovation by moving the cluster lifecycle management to Red Hat and AWS. With ROSA, you can run containerized applications with your existing OpenShift workflows and reduce the complexity of management.
Databases
AWS databases offer a high-performance, secure, and reliable foundation to power generative AI solutions and data-driven applications that drive value for your business and customers.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing an AWS database service. For general information, see AWS Cloud Databases.
Red Hat OpenShift Service on AWS
46
Overview of Amazon Web Services AWS Whitepaper
Topics
•
Compare AWS database services
•
Amazon Aurora
•
Amazon DynamoDB
•
Amazon ElastiCache
•
Amazon Keyspaces (for Apache Cassandra)
•
Amazon MemoryDB
•
Amazon Neptune
•
Amazon Relational Database Service
•
Amazon RDS for Db2
•
Amazon RDS on VMware
•
Amazon Quantum Ledger Database (Amazon QLDB)
•
Amazon Timestream
•
Amazon DocumentDB (with MongoDB compatibility)
•
Amazon Lightsail managed databases
Databases
47
Overview of Amazon Web Services AWS Whitepaper
Compare AWS database services
Database
Use cases
AWS services
Relational
Traditional applications, enterprise resource planning (ERP), customer relations hip management (CRM), e-commerce
•
Amazon Aurora — Designed for unparalle led high performance and availability at global scale with full MySQL and PostgreSQL compatibility
•
Amazon RDS — Set up, operate, and scale a relational database in the cloud with just a few clicks
•
Amazon Redshift — Accelerate your time to insights with fast, easy, and secure cloud data warehousing at scale
Key-value
High-traffic web applications, e-commerce systems, gaming applications
•
Amazon DynamoDB— Fast, flexible NoSQL database service for single-digit millisecond performan ce at any scale
In-memory
Caching, session managemen t, gaming leaderboards, geospatial applications
•
Amazon ElastiCache — Unlock microsecond latency and scale with in-memory caching
•
Amazon MemoryDB — Redis-compatible, durable, in-memory database service for ultra-fast performance
Compare AWS database services
48
Overview of Amazon Web Services AWS Whitepaper
Database Use cases AWS services
Document
Content management, catalogs, user profiles
•
Amazon DocumentDB (with MongoDB compatibi lity) — Scale JSON workloads with ease using a fully managed document database service
Wide column
High-scale industrial apps for equipment maintenance, fleet management, and route optimization
•
Amazon Keyspaces — A scalable, highly available , and managed Apache Cassandra–compatible database service
Graph
Fraud detection, social networking, recommendation engines
•
Amazon Neptune — Build and run graph applicati ons with highly connected datasets
Time series
Internet of Things (IoT) applications, DevOps, industrial telemetry
•
Amazon Timestream — Fast, scalable, serverless time series database
Ledger
Systems of record, supply chain, registrations, banking transactions
•
Amazon Ledger Database Service (QLDB) — Maintain an immutable, cryptogra phically verifiable log of data changes
Amazon Aurora
Amazon Aurora is a MySQL and PostgreSQL compatible relational database engine that combines the speed and availability of high-end commercial databases with the simplicity and cost-effectiveness of open source databases.
Amazon Aurora is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases. It provides the security, availability, and reliability of
Amazon Aurora
49
Overview of Amazon Web Services AWS Whitepaper
commercial databases at 1/10th the cost. Amazon Aurora is fully managed by Amazon Relational Database Service (Amazon RDS), which automates time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups.
Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).
Amazon Aurora I/O-Optimized is a cluster configuration that offers improved price performance and predictable pricing for customers with I/O-intensive applications, such as e-commerce applications, payment processing systems, and financial applications. Aurora-Optimized offers improved performance, increasing throughput and reducing latency to support your most demanding workloads, with up to 40 percent cost savings when your I/O spend exceeds 25 percent of your current Aurora database spend.
Amazon Aurora MySQL zero-ETL integration with Amazon Redshift, now available in public preview, enables near real-time analytics and machine learning of data stored in Aurora MySQL-Compatible Edition. Transactional data written to Aurora is available to you in Amazon Redshift within seconds, without building and maintaining complex data pipelines.
Amazon DynamoDB
Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and support peaks of more than 20 million requests per second.
Many of the world's fastest growing businesses such as Lyft, Airbnb, and Redfin, as well as enterprises such as Samsung, Toyota, and Capital One, depend on the scale and performance of DynamoDB to support their mission-critical workloads.
Hundreds of thousands of AWS customers have chosen DynamoDB as their key-value and document database for mobile, web, gaming, ad tech, Internet of Things (IoT), and other applications that need low-latency data access at any scale. Create a new table for your application and let DynamoDB handle the rest.
Amazon DynamoDB
50
Overview of Amazon Web Services AWS Whitepaper
Amazon ElastiCache
Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases.
ElastiCache supports two open-source in-memory caching engines:
•
Redis — a fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. Amazon ElastiCache (Redis OSS) is a Redis-compatible in-memory service that delivers the ease-of-use and power of Redis along with the availability, reliability, and performance suitable for the most demanding applications. Both single-node and up to 15-shard clusters are available, enabling scalability to up to 3.55 TiB of in-memory data. Amazon ElastiCache (Redis OSS) is fully managed, scalable, and secure. This makes it an ideal candidate to power high-performance use cases such as web, mobile apps, gaming, ad-tech, and IoT.
•
Memcached — a widely adopted memory object caching system. Amazon ElastiCache (Memcached) is protocol compliant with Memcached, so popular tools that you use today with existing Memcached environments will work seamlessly with the service.
Amazon ElastiCache Serverless is a serverless option for Amazon ElastiCache that simplifies cache management and instantly scales to support the most demanding applications. With ElastiCache Serverless, you can create a highly available and scalable cache in less than a minute, eliminating the need to plan for, provision, and manage cache cluster capacity. ElastiCache Serverless automatically stores data redundantly across multiple Availability Zones (AZs) and provides a 99.99% availability Service Level Agreement (SLA). With ElastiCache Serverless, you pay for data stored and compute consumed by your workload, with no upfront commitments or additional costs.
Amazon Keyspaces (for Apache Cassandra)
Amazon Keyspaces (for Apache Cassandra) is a scalable, highly available, and managed Apache Cassandra–compatible database service. With Amazon Keyspaces, you can run your Cassandra workloads on AWS using the same Cassandra application code and developer tools that you use today. You don’t have to provision, patch, or manage servers, and you don’t have to install, maintain, or operate software. Amazon Keyspaces is serverless, so you pay for only the resources you use and the service can automatically scale tables up and down in response to application
Amazon ElastiCache
51
Overview of Amazon Web Services AWS Whitepaper
traffic. You can build applications that serve thousands of requests per second with virtually unlimited throughput and storage. Data is encrypted by default and Amazon Keyspaces enables you to back up your table data continuously using point-in-time recovery. Amazon Keyspaces gives you the performance, elasticity, and enterprise features you need to operate business-critical Cassandra workloads at scale.
Amazon MemoryDB
Amazon MemoryDB is a Redis-compatible, durable, in-memory database service that delivers ultra-fast performance. It is purpose-built for modern applications with microservices architectures.
MemoryDB is compatible with Redis, a popular open source data store, enabling customers to quickly build applications using the same flexible and friendly Redis data structures, APIs, and commands that they already use today. With MemoryDB, all of your data is stored in memory, which enables you to achieve microsecond read and single-digit millisecond write latency and high throughput. MemoryDB also stores data durably across multiple Availability Zones using a distributed transactional log to allow fast failover, database recovery, and node restarts. Delivering both in-memory performance and Multi-AZ durability, MemoryDB can be used as a high-performance primary database for your microservices applications eliminating the need to separately manage both a cache and durable database.
Amazon Neptune
Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Amazon Neptune supports popular graph models Property Graph and W3C's RDF, and their respective query languages Apache TinkerPop Gremlin and SPARQL, allowing you to easily build queries that efficiently navigate highly connected datasets. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.
Amazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure with support for encryption at rest. Neptune is fully managed, so you no longer need to worry about database management tasks such as hardware provisioning, software patching, setup, configuration, or backups.
Amazon MemoryDB
52
Overview of Amazon Web Services AWS Whitepaper
Amazon Neptune Analytics is an analytics database engine for quickly analyzing large volumes of graph data to get insights and find trends from data stored in Amazon S3 buckets or a Neptune database. Neptune Analytics uses built-in algorithms, vector search, and in-memory computing to run queries on data with tens of billions of relationships in seconds.
Amazon Relational Database Service
Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. It frees you to focus on your applications so you can give them the fast performance, high availability, security and compatibility they need.
Amazon RDS is available on several database instance types—optimized for memory, performance or I/O—and provides you with six familiar database engines to choose from, including MySQL,MariaDB, PostgreSQL, Oracle Database, Microsoft SQL Server, and Amazon RDS on AWS Outposts. You can use the AWS Database Migration Service to easily migrate or replicate your existing databases to Amazon RDS.
Amazon RDS for Db2
Amazon RDS for Db2 makes it easy to set up, operate, and scale Db2 deployments in the cloud.Amazon RDS automates time-consuming database administration tasks, such as provisioning, backups, software patching, monitoring, and more, to free up time to innovate and drive business value. It also offers high availability with Multi-AZ deployment, disaster recovery solutions with cross-Region backups, and security features to support your business-critical workloads. In addition, you can integrate with other IBM and AWS services to gain new insights and scale your analytics workloads.
Amazon RDS on VMware
Amazon Relational Database Service (Amazon RDS) on VMware lets you deploy managed databases in on-premises VMware environments using the Amazon RDS technology enjoyed by hundreds of thousands of AWS customers. Amazon RDS provides cost-efficient and resizable capacity while automating time-consuming administration tasks including hardware provisioning, database setup, patching, and backups, freeing you to focus on your applications. Amazon RDS on VMware brings these same benefits to your on-premises deployments, making it easy to set up, operate, and scale databases in VMware vSphere private data centers, or to migrate them to AWS.
Amazon Relational Database Service
53
Overview of Amazon Web Services AWS Whitepaper
Amazon RDS on VMware allows you to utilize the same simple interface for managing databases in on-premises VMware environments as you would use in AWS. You can easily replicate Amazon RDS on VMware databases to Amazon RDS instances in AWS, enabling low-cost hybrid deployments for disaster recovery, read replica bursting, and optional long-term backup retention in Amazon Simple Storage Service (Amazon S3).
Amazon Quantum Ledger Database (Amazon QLDB)
Amazon QLDB is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log owned by a central trusted authority. Amazon QLDB tracks each and every application data change and maintains a complete and verifiable history of changes over time.
Ledgers are typically used to record a history of economic and financial activity in an organization. Many organizations build applications with ledger-like functionality because they want to maintain an accurate history of their applications' data, for example, tracking the history of credits and debits in banking transactions, verifying the data lineage of an insurance claim, or tracing movement of an item in a supply chain network. Ledger applications are often implemented using custom audit tables or audit trails created in relational databases. However, building audit functionality with relational databases is time-consuming and prone to human error. It requires custom development, and since relational databases are not inherently immutable, any unintended changes to the data are hard to track and verify. Alternatively, blockchain frameworks, such as Hyperledger Fabric and Ethereum, can also be used as a ledger. However, this adds complexity as you need to set-up an entire blockchain network with multiple nodes, manage its infrastructure, and require the nodes to validate each transaction before it can be added to the ledger.
Amazon QLDB is a new class of database that eliminates the need to engage in the complex development effort of building your own ledger-like applications. With QLDB, your data’s change history is immutable – it cannot be altered or deleted – and using cryptography, you can easily verify that there have been no unintended modifications to your application’s data. QLDB uses an immutable transactional log, known as a journal, that tracks each application data change and maintains a complete and verifiable history of changes over time. QLDB is easy to use because it provides developers with a familiar SQL-like API, a flexible document data model, and full support for transactions. QLDB is also serverless, so it automatically scales to support the demands of your application. There are no servers to manage and no read or write limits to configure. With QLDB, you only pay for what you use.
Amazon Quantum Ledger Database (Amazon QLDB)
54
Overview of Amazon Web Services AWS Whitepaper
Amazon Timestream
Amazon Timestream is a fast, scalable, fully managed time series database service for IoT and operational applications that makes it easy to store and analyze trillions of events per day at 1/10th the cost of relational databases. Driven by the rise of IoT devices, IT systems, and smart industrial machines, time series data — data that measures how things change over time — is one of the fastest growing data types. Time-series data has specific characteristics such as typically arriving in time order form, data is append-only, and queries are always over a time interval. While relational databases can store this data, they are inefficient at processing this data as they lack optimizations such as storing and retrieving data by time intervals.
Timestream is a purpose-built time series database that efficiently stores and processes this data by time intervals. With Timestream, you can easily store and analyze log data for DevOps, sensor data for IoT applications, and industrial telemetry data for equipment maintenance. As your data grows over time, the Timestream adaptive query processing engine understands its location and format, making your data simpler and faster to analyze. Timestream also automates rollups, retention, tiering, and compression of data, so you can manage your data at the lowest possible cost. Timestream is serverless, so there are no servers to manage. It manages time-consuming tasks such as server provisioning, software patching, setup, configuration, or data retention and tiering, freeing you to focus on building your applications.
Amazon DocumentDB (with MongoDB compatibility)
Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads.
Amazon DocumentDB is designed from the ground-up to give you the performance, scalability, and availability you need when operating mission-critical MongoDB workloads at scale. Amazon DocumentDB implements the Apache 2.0 open source MongoDB 3.6 and 4.0 APIs by emulating the responses that a MongoDB client expects from a MongoDB server, allowing you to use your existing MongoDB drivers and tools with Amazon DocumentDB (with MongoDB compatibility).
Amazon Lightsail managed databases
Amazon Lightsail managed databases are separate from compute workloads, so you can build applications and websites on Lightsail instances without interruption. Lightsail supports MySQL and PostgreSQL databases , and you can configure them for standard availability for regular workloads or high availability for critical workloads. Lightsail-managed databases bundle the
Amazon Timestream
55
Overview of Amazon Web Services AWS Whitepaper
underlying compute, SSD-based storage, and data transfer bandwidth into a fixed monthly price. You can manage your Lightsail-managed database by using the Lightsail console, the AWS Command Line Interface (AWS CLI), the Lightsail API, or an AWS SDK.
Developer tools
Topics
•
AWS Infrastructure Composer
•
AWS Cloud9
•
AWS CloudShell
•
AWS CodeArtifact
•
AWS CodeBuild
•
Amazon CodeCatalyst
•
AWS CodeCommit
•
AWS CodeDeploy
•
AWS CodePipeline
•
Amazon Corretto
•
AWS Fault Injection Service
•
Amazon Q Developer
•
AWS X-Ray
AWS Infrastructure Composer
AWS Infrastructure Composer helps you visually compose and configure serverless applications from AWS services backed by deployment-ready infrastructure as code (IaC). Infrastructure Composer helps you drag and drop serverless resources onto a visual, browser-based canvas. You can connect them to quickly create your serverless application architecture. The canvas also supports grouping of resources into larger architectural components to simplify editing and configuration. AWS Infrastructure Composer can generate deployment-ready configuration with default settings based on the services that make up your application architecture. Infrastructure
Developer tools
56
Overview of Amazon Web Services AWS Whitepaper
Composer supports generating both AWS CloudFormation and AWS Serverless Application Model (SAM) artifacts.
AWS Cloud9
AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. AWS Cloud9 comes prepackaged with essential tools for popular programming languages, including JavaScript, Python, PHP, and more, so you don’t need to install files or configure your development machine to start new projects. Since your AWS Cloud9 IDE is cloud-based, you can work on your projects from your office, home, or anywhere using an internet-connected machine. AWS Cloud9 also provides a seamless experience for developing serverless applications enabling you to easily define resources, debug, and switch between local and remote running of serverless applications. With AWS Cloud9, you can quickly share your development environment with your team, enabling you to pair program and track each other's inputs in real time.
AWS CloudShell
AWS CloudShell is a browser-based shell that makes it easy to securely manage, explore, and interact with your AWS resources. CloudShell is pre-authenticated with your console credentials. Common development and operations tools are pre-installed, so no local installation or configuration is required. With CloudShell, you can quickly run scripts with the AWS Command Line Interface (AWS CLI), experiment with AWS service APIs using the AWS SDKs, or use a range of other tools to be productive. You can use CloudShell right from your browser and at no additional cost.
AWS CodeArtifact
AWS CodeArtifact is a fully managed artifact repository service that makes it easy for organizations of any size to securely store, publish, and share software packages used in their software development process. CodeArtifact can be configured to automatically fetch software packages and dependencies from public artifact repositories so developers have access to the latest versions. CodeArtifact works with commonly used package managers and build tools such as Apache Maven, Gradle, npm, yarn, twine, pip, and NuGet making it easy to integrate into existing development workflows.
AWS CodeBuild
AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage,
AWS Cloud9
57
Overview of Amazon Web Services AWS Whitepaper
and scale your own build servers. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue. You can get started quickly by using prepackaged build environments, or you can create custom build environments that use your own build tools.
Amazon CodeCatalyst
Amazon CodeCatalyst is an integrated service for software development teams adopting continuous integration/continuous deployment (CI/CD) practices into their software development process. CodeCatalyst is fully managed by AWS and puts the tools you need all in one place. You can plan work, collaborate on code, and build, test, and deploy applications. You can also integrate AWS resources with your projects by connecting your AWS accounts to your CodeCatalyst space. By managing all of the stages and aspects of your application lifecycle in one tool, you can deliver software quickly and confidently.
AWS CodeCommit
AWS CodeCommit is a fully managed source control service that makes it easy for companies to host secure and highly scalable private Git repositories. AWS CodeCommit eliminates the need to operate your own source control system or worry about scaling its infrastructure. You can use AWS CodeCommit to securely store anything from source code to binaries, and it works seamlessly with your existing Git tools.
AWS CodeDeploy
AWS CodeDeploy is a service that automates code deployments to any instance, including EC2 instances and instances running on premises. CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. You can use CodeDeploy to automate software deployments, eliminating the need for error-prone manual operations. The service scales with your infrastructure so you can easily deploy to one instance or thousands.
AWS CodePipeline
AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define. This enables you to rapidly and reliably deliver features and updates. You can easily integrate CodePipeline with third-party services such as
Amazon CodeCatalyst
58
Overview of Amazon Web Services AWS Whitepaper
GitHub or with your own custom plugin. With AWS CodePipeline, you only pay for what you use. There are no upfront fees or long-term commitments.
Amazon Corretto
Amazon Corretto is a no-cost, multiplatform, production-ready distribution of the Open Java Development Kit (OpenJDK). Corretto comes with long-term support that will include performance enhancements and security fixes. Amazon runs Corretto internally on thousands of production services, and Corretto is certified as compatible with the Java SE standard. With Corretto, you can develop and run Java applications on popular operating systems, including Amazon Linux 2, Windows, and macOS.
AWS Fault Injection Service
AWS Fault Injection Service is a fully managed service for running fault injection experiments on AWS that makes it easier to improve an application’s performance, observability, and resiliency. Fault injection experiments are used in chaos engineering, which is the practice of stressing an application in testing or production environments by creating disruptive events, such as sudden increase in CPU or memory consumption, observing how the system responds, and implementing improvements. Fault injection experiment helps teams create the real-world conditions needed to uncover the hidden bugs, monitoring blind spots, and performance bottlenecks that are difficult to find in distributed systems.
AWS Fault Injection Service simplifies the process of setting up and running controlled fault injection experiments across a range of AWS services so teams can build confidence in their application behavior. With Fault Injection Simulator, teams can quickly set up experiments using pre-built templates that generate the desired disruptions. AWS Fault Injection Service provides the controls and guardrails that teams need to run experiments in production, such as automatically rolling back or stopping the experiment if specific conditions are met. With a few clicks in the console, teams can run complex scenarios with common distributed system failures happening in parallel or building sequentially over time, enabling them to create the real world conditions necessary to find hidden weaknesses.
Amazon Q Developer
Amazon Q Developer (formerly Amazon CodeWhisperer) assists developers and IT professionals with their tasks—from coding, testing, and upgrading applications, to diagnosing errors, performing security scanning and fixes, and optimizing AWS resources. Amazon Q has advanced,
Amazon Corretto
59
Overview of Amazon Web Services AWS Whitepaper
multistep planning and reasoning capabilities that can transform existing code (for example, perform Java version upgrades) and implement new features generated from developer requests.
AWS X-Ray
AWS X-Ray helps developers analyze and debug distributed applications in production or under development, such as those built using a microservices architecture. X-Ray, you can understand how your application and its underlying services are performing so you can identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.
End user computing
Amazon AppStream 2.0
Amazon AppStream 2.0 is a fully managed application streaming service. You centrally manage your desktop applications on AppStream 2.0 and securely deliver them to any computer. You can easily scale to any number of users across the globe without acquiring, provisioning, and operating hardware or infrastructure. AppStream 2.0 is built on AWS, so you benefit from a data center and network architecture designed for the most security-sensitive organizations. Each user has a fluid and responsive experience with your applications, including GPU-intensive3D design and engineering ones, because your applications run on virtual machines (VMs) optimized for specific use cases and each streaming session automatically adjusts to network conditions.
Enterprises can use AppStream 2.0 to simplify application delivery and complete their migration to the cloud. Educational institutions can provide every student access to the applications they need for class on any computer. Software vendors can use AppStream 2.0 to deliver trials, demos, and training for their applications with no downloads or installations. They can also develop a full software-as-a-service (SaaS) solution without rewriting their application.
Amazon WorkSpaces
Amazon WorkSpaces is a fully managed, secure cloud desktop service. You can use WorkSpaces to provision either Windows or Linux desktops in just a few minutes and quickly scale to provide thousands of desktops to workers across the globe. You can pay either monthly or hourly, just
AWS X-Ray
60
Overview of Amazon Web Services AWS Whitepaper
for the WorkSpaces you launch, which helps you save money when compared to traditional desktops and on-premises VDI solutions. WorkSpaces helps you eliminate the complexity in managing hardware inventory, OS versions and patches, and Virtual Desktop Infrastructure (VDI), which helps simplify your desktop delivery strategy. With WorkSpaces, your users get a fast, responsive desktop of their choice that they can access anywhere, anytime, from any supported device.
Amazon WorkSpaces Core
Amazon WorkSpaces Core provides cloud-based, fully managed virtual desktop infrastructure (VDI) accessible to third-party VDI management solutions.
•
Simplify VDI migration and combine your current VDI software with the security and reliability of AWS.
•
Maximize productivity and business continuity with a financially backed 99.9% uptime SLA.
•
Scale on demand with fixed-rate hourly billing, no overprovisioning, and no upfront costs.
•
Improve user experience and performance with virtual desktops located closer to your global workforce.
Amazon WorkSpaces Thin Client
Amazon WorkSpaces Thin Client is a cost-effective thin client device that is built to work with AWS End User Computing (EUC) virtual desktops to provide users with a complete cloud desktop solution. WorkSpaces Thin Client is a compact device designed to connect two monitors and multiple USB devices such as a keyboard, mouse, headset, and webcam. To maximize endpoint security, WorkSpaces Thin Client devices do not allow local data storage or installation of unapproved applications. The WorkSpaces Thin Client device ships directly to end users or to your company's locations preloaded with device management software.
Amazon Workspaces Web
Amazon WorkSpaces Web is a low-cost, fully managed workspace built specifically to facilitate secure access to internal websites and software-as-a-service (SaaS) applications from existing web browsers, without the administrative burden of appliances or specialized client software. Protect internal content with enterprise controls, while providing access to all the web-based productivity tools users need from any browser.
WorkSpaces Web makes it easy for customers to safely provide their employees with access to internal websites and SaaS web applications without the administrative burden of appliances
End user computing
61
Overview of Amazon Web Services AWS Whitepaper
or specialized client software. WorkSpaces Web provides simple policy tools tailored for user interactions, while offloading common tasks like capacity management, scaling, and maintaining browser images.
Frontend web and mobile services
AWS offers a broad set of tools and services to support development workflows for native iOS, Android, React Native, and JavaScript developers. Discover how easy it is to develop, deploy, and operate your app, even if you are new to AWS.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing AWS frontend web and mobile services. For general information, see Frontend Web and Mobile on AWS.
Services
•
AWS Amplify
•
AWS AppSync
•
AWS Device Farm
Frontend web and mobile services
62
Overview of Amazon Web Services AWS Whitepaper
•
Amazon Location Service
AWS Amplify
AWS Amplify makes it easy to create, configure, and implement scalable mobile applications powered by AWS. Amplify seamlessly provisions and manages your mobile backend and provides a simple framework to easily integrate your backend with your iOS, Android, Web, and React Native frontends. Amplify also automates the application release process of both your front-end and back-end allowing you to deliver features faster.
Mobile applications require cloud services for actions that can’t be done directly on the device, such as offline data synchronization, storage, or data sharing across multiple users. You often have to configure, set up, and manage multiple services to power the backend. You also have to integrate each of those services into your application by writing multiple lines of code. However, as the number of application features grow, your code and release process becomes more complex and managing the backend requires more time.
Amplify provisions and manages backends for your mobile applications. You just select the capabilities you need such as authentication, analytics, or offline data sync, and Amplify will automatically provision and manage the AWS service that powers each of the capabilities. You can then integrate those capabilities into your application through the Amplify libraries and UI components.
AWS AppSync
AWS AppSync is a serverless back-end for mobile, web, and enterprise applications.
AWS AppSync makes it easy to build data driven mobile and web applications by handling securely all the application data management tasks such as online and offline data access, data synchronization, and data manipulation across multiple data sources. AWS AppSync uses GraphQL, an API query language designed to build client applications by providing an intuitive and flexible syntax for describing their data requirement.
AWS Device Farm
AWS Device Farm is an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real time. View video, screenshots, logs, and performance data to pinpoint and fix issues before shipping your app.
AWS Amplify
63
Overview of Amazon Web Services AWS Whitepaper
Amazon Location Service
Amazon Location Service makes it easy for developers to add location functionality to applications without compromising data security and user privacy.
Location data is a vital ingredient in today’s applications, enabling capabilities ranging from asset tracking to location-based marketing. However, developers face significant barriers when integrating location functionality into their applications. This includes cost, privacy and security compromises, and tedious and slow integration work.
Amazon Location Service provides affordable data, tracking and geofencing capabilities, and native integrations with AWS services, so you can create sophisticated location-enabled applications quickly, without the high cost of custom development. You retain control of your location data with Amazon Location, and you can combine proprietary data with data from the service. Amazon Location provides cost-effective location-based services (LBS) using high-quality data from global, trusted providers Esri and HERE.
Game tech
Amazon GameLift Servers
Amazon GameLift Servers is a managed service for deploying, operating, and scaling dedicated game servers for session-based multiplayer games. Amazon GameLift Servers makes it easy to manage server infrastructure, scale capacity to lower latency and cost, match players into available game sessions, and defend from distributed denial-of-service (DDoS) attacks. You pay for the compute resources and bandwidth your games actually use, without monthly or annual contracts.
Internet of Things (IoT)
Amazon Location Service
64
Overview of Amazon Web Services AWS Whitepaper
AWS offers Internet of Things (IoT) services and solutions to connect and manage billions of devices. Collect, store, and analyze IoT data for industrial, consumer, commercial, and automotive workloads.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing an AWS IoT service. For general information, see AWS IoT.
Services
•
AWS IoT Analytics
•
AWS IoT Button
•
AWS IoT Core
•
AWS IoT Device Defender
•
AWS IoT Device Management
•
AWS IoT Events
•
AWS IoT ExpressLink
•
AWS IoT FleetWise
•
AWS IoT Greengrass
•
AWS IoT SiteWise
IoT
65
Overview of Amazon Web Services AWS Whitepaper
•
AWS IoT TwinMaker
•
AWS Partner Device Catalog
•
FreeRTOS
AWS IoT Analytics
AWS IoT Analytics is a fully managed service that makes it easy to run and operationalize sophisticated analytics on massive volumes of IoT data without having to worry about the cost and complexity typically required to build an IoT analytics platform. It is the easiest way to run analytics on IoT data and get insights to make better and more accurate decisions for IoT applications and machine learning use cases.
IoT data is highly unstructured which makes it difficult to analyze with traditional analytics and business intelligence tools that are designed to process structured data. IoT data comes from devices that often record fairly noisy processes (such as temperature, motion, or sound). The data from these devices can frequently have significant gaps, corrupted messages, and false readings that must be cleaned up before analysis can occur. Also, IoT data is often only meaningful in the context of additional, third party data inputs. For example, to help farmers determine when to water their crops, vineyard irrigation systems often enrich moisture sensor data with rainfall data from the vineyard, allowing for more efficient water usage while maximizing harvest yield.
AWS IoT Analytics automates each of the difficult steps that are required to analyze data from IoT devices. AWS IoT Analytics filters, transforms, and enriches IoT data before storing it in a time series data store for analysis. You can setup the service to collect only the data you need from your devices, apply mathematical transforms to process the data, and enrich the data with device-specific metadata such as device type and location before storing the processed data. Then, you can analyze your data by running ad hoc or scheduled queries using the built-in SQL query engine, or perform more complex analytics and machine learning inference. AWS IoT Analytics makes it easy to get started with machine learning by including pre-built models for common IoT use cases.
You can also use your own custom analysis, packaged in a container, to run AWS IoT Analytics. AWS IoT Analytics automates the running of your custom analyses created in Jupyter Notebook or your own tools (such as Matlab, Octave, and so on) to be run on your schedule.
AWS IoT Analytics is a fully managed service that operationalizes analyses and scales automatically to support up to petabytes of IoT data. With AWS IoT Analytics, you can analyze data from millions of devices and build fast, responsive IoT applications without managing hardware or infrastructure.
AWS IoT Analytics
66
Overview of Amazon Web Services AWS Whitepaper
AWS IoT Button
The AWS IoT Button is a programmable button based on the Amazon Dash Button hardware. This simple Wi-Fi device is easy to configure, and it’s designed for developers to get started with AWS IoT Core, AWS Lambda, Amazon DynamoDB, Amazon SNS, and many other Amazon Web Services without writing device-specific code.
You can code the button's logic in the cloud to configure button clicks to count or track items, call or alert someone, start or stop something, order services, or even provide feedback. For example, you can click the button to unlock or start a car, open your garage door, call a cab, call your spouse or a customer service representative, track the use of common household chores, medications or products, or remotely control your home appliances.
The button can be used as a remote control for Netflix, a switch for your Philips Hue light bulb, a check-in/check-out device for Airbnb guests, or a way to order your favorite pizza for delivery. You can integrate it with third-party APIs such as Twitter, Facebook, Twilio, Slack or even your own company's applications. Connect it to things we haven’t even thought of yet.
AWS IoT Core
AWS IoT Core is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core can support billions of devices and trillions of messages, and can process and route those messages to AWS endpoints and to other devices reliably and securely. With AWS IoT Core, your applications can keep track of and communicate with all your devices, all the time, even when they aren’t connected.
AWS IoT Core makes it easy to use AWS services such as AWS Lambda, Amazon Kinesis, Amazon S3, Amazon SageMaker AI, Amazon DynamoDB, Amazon CloudWatch, AWS CloudTrail, and Amazon QuickSight to build Internet of IoT applications that gather, process, analyze and act on data generated by connected devices, without having to manage any infrastructure.
AWS IoT Device Defender
AWS IoT Device Defender is a fully managed service that helps you secure your fleet of IoT devices. AWS IoT Device Defender continuously audits your IoT configurations to make sure that they aren’t deviating from security best practices. A configuration is a set of technical controls you set to help keep information secure when devices are communicating with each other and the cloud. AWS IoT Device Defender makes it easy to maintain and enforce IoT configurations, such as ensuring device identity, authenticating and authorizing devices, and encrypting device data. AWS IoT Device
AWS IoT Button
67
Overview of Amazon Web Services AWS Whitepaper
Defender continuously audits the IoT configurations on your devices against a set of predefined security best practices. AWS IoT Device Defender sends an alert if there are any gaps in your IoT configuration that might create a security risk, such as identity certificates being shared across multiple devices or a device with a revoked identity certificate trying to connect to AWS IoT Core.
AWS IoT Device Defender also lets you continuously monitor security metrics from devices and AWS IoT Core for deviations from what you have defined as appropriate behavior for each device. If something doesn’t look right, AWS IoT Device Defender sends out an alert so you can take action to remediate the issue. For example, traffic spikes in outbound traffic might indicate that a device is participating in a DDoS attack. AWS IoT Greengrass and FreeRTOS automatically integrate with AWS IoT Device Defender to provide security metrics from the devices for evaluation.
AWS IoT Device Defender can send alerts to the AWS IoT Console, Amazon CloudWatch, and Amazon SNS. If you determine that you need to take an action based on an alert, you can use AWS IoT Device Management to take mitigating actions such as pushing security fixes.
AWS IoT Device Management
As many IoT deployments consist of hundreds of thousands to millions of devices, it is essential to track, monitor, and manage connected device fleets. You need to ensure your IoT devices work properly and securely after they have been deployed. You also need to secure access to your devices, monitor health, detect and remotely troubleshoot problems, and manage software and firmware updates.
AWS IoT Device Management makes it easy to securely onboard, organize, monitor, and remotely manage IoT devices at scale. With AWS IoT Device Management, you can register your connected devices individually or in bulk, and easily manage permissions so that devices remain secure. You can also organize your devices, monitor and troubleshoot device functionality, query the state of any IoT device in your fleet, and send firmware updates over-the-air (OTA). AWS IoT Device Management is agnostic to device type and OS, so you can manage devices from constrained microcontrollers to connected cars all with the same service. AWS IoT Device Management allows you to scale your fleets and reduce the cost and effort of managing large and diverse IoT device deployments.
AWS IoT Events
AWS IoT Events is a fully managed IoT service that makes it easy to detect and respond to events from IoT sensors and applications. Events are patterns of data identifying more complicated circumstances than expected, such as changes in equipment when a belt is stuck or connected
AWS IoT Device Management
68
Overview of Amazon Web Services AWS Whitepaper
motion detectors using movement signals to activate lights and security cameras. To detect events before AWS IoT Events, you had to build costly, custom applications to collect data, apply decision logic to detect an event, and then start another application to react to the event. Using AWS IoT Events, it’s simple to detect events across thousands of IoT sensors sending different telemetry data, such as temperature from a freezer, humidity from respiratory equipment, and belt speed on a motor, and hundreds of equipment management applications. You simply select the relevant data sources to ingest, define the logic for each event using simple ‘if-then-else’ statements, and select the alert or custom action to run when an event occurs. AWS IoT Events continuously monitors data from multiple IoT sensors and applications, and it integrates with other services, such as AWS IoT Core and AWS IoT Analytics, to enable early detection and unique insights into events. AWS IoT Events automatically initiates alerts and actions in response to events based on the logic you define. This helps resolve issues quickly, reduce maintenance costs, and increase operational efficiency.
AWS IoT ExpressLink
AWS IoT ExpressLink powers a range of hardware modules developed and offered by AWS Partners, such as Espressif, Infineon, Realtek, and u-blox. Connectivity modules available from the AWS Partner Device Catalog include software implementing AWS mandated security requirements, making it faster and easier for you to securely connect devices to the cloud and seamlessly integrate with a range of AWS services. AWS IoT ExpressLink modules come pre-provisioned with security credentials set by qualified AWS Partners. This enables you to offload the complex work of integrating the networking and cryptography layers to the hardware modules, and develop secure IoT products in a fraction of the time.
Devices with AWS IoT ExpressLink establish a two-way connection with AWS IoT Core through native support of the MQTT (publish/subscribe) communication mechanism, and can create and update AWS IoT Device Shadow documents. With AWS IoT ExpressLink, it’s easy to make over-the-air (OTA) updates to both the module and host processor from the AWS IoT Device Managementconsole. You can then remotely deploy security updates, bug fixes, and new firmware updates to add features and keep your device fleet always up to date. Moreover, partner modules with AWS IoT ExpressLink can also connect to the AWS IoT Device Defender to report a number of device metrics that can help detect anomalies and generate alerts.
AWS IoT FleetWise
With AWS IoT FleetWise, you can collect and organize vehicle data and store that data in a standardized way for data analysis in the cloud. AWS IoT FleetWise helps you efficiently transfer
AWS IoT ExpressLink
69
Overview of Amazon Web Services AWS Whitepaper
data to the cloud in near real time using intelligent data collection capabilities. These capabilities allow you to reduce the amount of data transferred by defining rules for when to collect and transfer data based on configurable parameters (for instance, vehicle temperature, speed, or make and model). Once the data is in the cloud, you can use it for applications that analyze vehicle fleet health. This analysis can help you to more quickly identify potential maintenance issues or make in-vehicle infotainment systems smarter. You can also feed the data into machine learning (ML) models that improve advanced technologies, such as autonomous driving and advanced driver assistance systems (ADAS).
AWS IoT Greengrass
AWS IoT Greengrass seamlessly extends AWS to devices so they can act locally on the data they generate, while still using the cloud for management, analytics, and durable storage. With AWS IoT Greengrass, connected devices can run AWS Lambda functions, run predictions based on machine learning models, keep device data in sync, and communicate with other devices securely – even when not connected to the internet.
With AWS IoT Greengrass, you can use familiar languages and programming models to create and test your device software in the cloud, and then deploy it to your devices. AWS IoT Greengrass can be programmed to filter device data and only transmit necessary information back to the cloud. You can also connect to third-party applications, on-premises software, and AWS services out-of-the-box with AWS IoT Greengrass Connectors. Connectors also jumpstart device onboarding with pre-built protocol adapter integrations and allow you to streamline authentication via integration with AWS Secrets Manager.
AWS IoT SiteWise
AWS IoT SiteWise is a managed service that makes it easy to collect, store, organize and monitor data from industrial equipment at scale to help you make better, data-driven decisions. You can use AWS IoT SiteWise to monitor operations across facilities, quickly compute common industrial performance metrics, and create applications that analyze industrial equipment data to prevent costly equipment issues and reduce gaps in production. This allows you to collect data consistently across devices, identify issues with remote monitoring more quickly, and improve multi-site processes with centralized data.
Today, getting performance metrics from industrial equipment is challenging because data is often locked into proprietary on-premises data stores and typically requires specialized expertise to retrieve and place in a format that is useful for analysis. AWS IoT SiteWise simplifies this process by
AWS IoT Greengrass
70
Overview of Amazon Web Services AWS Whitepaper
providing software running on a gateway that resides in your facilities and automates the process of collecting and organizing industrial equipment data. This gateway securely connects to your on-premises data servers, collects data, and sends the data to the AWS Cloud. AWS IoT SiteWise also provides interfaces for collecting data from modern industrial applications through MQTT messages or APIs.
You can use AWS IoT SiteWise to model your physical assets, processes and facilities, quickly compute common industrial performance metrics, and create fully managed web applications to help analyze industrial equipment data, reduce costs and make faster decisions. With AWS IoT SiteWise, you can focus on understanding and optimizing your operations, rather than building costly in-house data collection and management applications.
AWS IoT TwinMaker
AWS IoT TwinMaker makes it easier for developers to create digital twins of real-world systems such as buildings, factories, industrial equipment, and production lines. AWS IoT TwinMaker provides the tools you need to build digital twins to help you optimize building operations, increase production output, and improve equipment performance. With the ability to use existing data from multiple sources, create virtual representations of any physical environment, and combine existing 3D models with real-world data, you can now harness digital twins to create a holistic view of your operations faster and with less effort.
AWS Partner Device Catalog
The AWS Partner Device Catalog helps you find devices and hardware to help you explore, build, and go to market with your IoT solutions. Search for and find hardware that works with AWS, including development kits and embedded systems to build new devices, as well as off-the-shelf-devices such as gateways, edge servers, sensors, and cameras for immediate IoT project integration. The choice of AWS enabled hardware from our curated catalog of devices from APN partners can help make the rollout of your IoT projects easier. All devices listed in the AWS Partner Device Catalog are also available for purchase from our partners to get you started quickly.
FreeRTOS
FreeRTOS is an operating system for microcontrollers that makes small, low-power edge devices easy to program, deploy, secure, connect, and manage. FreeRTOS extends the FreeRTOS kernel, a popular open source operating system for microcontrollers, with software libraries that make it easy to securely connect your small, low-power devices to AWS Cloud services such as AWS IoT Core or to more powerful edge devices running AWS IoT Greengrass.
AWS IoT TwinMaker
71
Overview of Amazon Web Services AWS Whitepaper
A microcontroller (MCU) is a single chip containing a simple processor that can be found in many devices, including appliances, sensors, fitness trackers, industrial automation, and automobiles. Many of these small devices could benefit from connecting to the cloud or locally to other devices. For example, smart electricity meters need to connect to the cloud to report on usage, and building security systems need to communicate locally so that a door will unlock when you badge in. Microcontrollers have limited compute power and memory capacity and typically perform simple, functional tasks. Microcontrollers frequently run operating systems that do not have built-in functionality to connect to local networks or the cloud, making IoT applications a challenge. FreeRTOS helps solve this problem by providing both the core operating system (to run the edge device) as well as software libraries that make it easy to securely connect to the cloud (or other edge devices) so you can collect data from them for IoT applications and take action.
Machine Learning (ML) and Artificial Intelligence (AI)
AWS helps you at every stage of your ML adoption journey with the most comprehensive set ML services and purpose-built infrastructure. Our pretrained AI services provide ready-made intelligence for your applications and workflows.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing an AWS machine learning service, Choosing a generative AI service, andAmazon Bedrock or Amazon SageMaker AI?. For general information, see Build and scale the next wave of AI innovation on AWS.
ML and AI
72
Overview of Amazon Web Services AWS Whitepaper
Services
•
Amazon Augmented AI
•
Amazon Bedrock
•
Amazon CodeGuru
•
Amazon Comprehend
•
Amazon DevOps Guru
•
Amazon Forecast
•
Amazon Fraud Detector
•
Amazon Comprehend Medical
•
Amazon Kendra
•
Amazon Lex
•
Amazon Lookout for Equipment
•
Amazon Lookout for Metrics
•
Amazon Lookout for Vision
•
Amazon Monitron
ML and AI
73
Overview of Amazon Web Services AWS Whitepaper
•
Amazon PartyRock
•
Amazon Personalize
•
Amazon Polly
•
Amazon Q
•
Amazon Rekognition
•
Amazon SageMaker AI
•
Amazon Textract
•
Amazon Transcribe
•
Amazon Translate
•
AWS DeepComposer
•
AWS DeepRacer
•
AWS HealthLake
•
AWS HealthScribe
•
AWS Panorama
Amazon Augmented AI
Amazon Augmented AI (Amazon A2I) is a ML service which makes it easy to build the workflows required for human review. Amazon A2I brings human review to all developers, removing the undifferentiated heavy lifting associated with building human review systems or managing large numbers of human reviewers, whether it runs on AWS or not.
Amazon Bedrock
Amazon Bedrock is a fully managed service that makes foundational models (FMs) from Amazon and leading AI companies available through an API. With the Amazon Bedrock serverless experience, you can quickly get started, experiment with FMs, privately customize them with your own data, and seamlessly integrate and deploy FMs into your AWS applications.
You can choose from a variety of foundation models from leading AI companies, such as AI21 Labs, Anthropic, Cohere, DeepSeek, Luma, Meta, Mistral AI, and Stability AI. Or you can use the Amazon Nova foundation models available exclusively in Amazon Bedrock.
Amazon Augmented AI
74
Overview of Amazon Web Services AWS Whitepaper
Amazon CodeGuru
Amazon CodeGuru is a developer tool that provides intelligent recommendations to improve code quality and identify an application’s most expensive lines of code. Integrate CodeGuru into your existing software development workflow to automate code reviews during application development and continuously monitor application's performance in production and provide recommendations and visual clues on how to improve code quality, application performance, and reduce overall cost.
Amazon CodeGuru Reviewer uses ML and automated reasoning to identify critical issues, security vulnerabilities, and hard-to-find bugs during application development and provides recommendations to improve code quality.
Amazon CodeGuru Profiler helps developers find an application’s most expensive lines of code by helping them understand the runtime behavior of their applications, identify and remove code inefficiencies, improve performance, and significantly decrease compute costs.
Amazon Comprehend
Amazon Comprehend uses ML and natural language processing (NLP) to help you uncover the insights and relationships in your unstructured data. The service identifies the language of the text; extracts key phrases, places, people, brands, or events; understands how positive or negative the text is; analyzes text using tokenization and parts of speech; and automatically organizes a collection of text files by topic. You can also use AutoML capabilities in Amazon Comprehend to build a custom set of entities or text classification models that are tailored uniquely to your organization’s needs.
For extracting complex medical information from unstructured text, you can use Amazon Comprehend Medical. The service can identify medical information, such as medical conditions, medications, dosages, strengths, and frequencies from a variety of sources like doctor’s notes, clinical trial reports, and patient health records. Amazon Comprehend Medical also identifies the relationship among the extracted medication and test, treatment and procedure information for easier analysis. For example, the service identifies a particular dosage, strength, and frequency related to a specific medication from unstructured clinical notes.
Amazon DevOps Guru
Amazon DevOps Guru is an ML-powered service that makes it easy to improve an application’s operational performance and availability. Amazon DevOps Guru detects behaviors that deviate
Amazon CodeGuru
75
Overview of Amazon Web Services AWS Whitepaper
from normal operating patterns so you can identify operational issues long before they impact your customers.
Amazon DevOps Guru uses ML models informed by years of Amazon.com and AWS operational excellence to identify anomalous application behavior (such as increased latency, error rates, resource constraints, etc.) and surface critical issues that could cause potential outages or service disruptions. When Amazon DevOps Guru identifies a critical issue, it automatically sends an alert and provides a summary of related anomalies, the likely root cause, and context about when and where the issue occurred. When possible, Amazon DevOps Guru also provides recommendations on how to remediate the issue.
Amazon DevOps Guru automatically ingests operational data from your AWS applications and provides a single dashboard to visualize issues in your operational data. You can get started by enabling Amazon DevOps Guru for all resources in your AWS account, resources in your AWS CloudFormation Stacks, or resources grouped together by AWS tags, with no manual setup or ML expertise required.
Amazon Forecast
Amazon Forecast is a fully managed service that uses ML to deliver highly accurate forecasts.
Companies today use everything from simple spreadsheets to complex financial planning software to attempt to accurately forecast future business outcomes such as product demand, resource needs, or financial performance. These tools build forecasts by looking at a historical series of data, which is called time series data. For example, such tools may try to predict the future sales of a raincoat by looking only at its previous sales data with the underlying assumption that the future is determined by the past. This approach can struggle to produce accurate forecasts for large sets of data that have irregular trends. Also, it fails to easily combine data series that change over time (such as price, discounts, web traffic, and number of employees) with relevant independent variables such as product features and store locations.
Based on the same technology used at Amazon.com, Amazon Forecast uses ML to combine time series data with additional variables to build forecasts. Amazon Forecast requires no ML experience to get started. You only need to provide historical data, plus any additional data that you believe may impact your forecasts. For example, the demand for a particular color of a shirt may change with the seasons and store location. This complex relationship is hard to determine on its own, but ML is ideally suited to recognize it. Once you provide your data, Amazon Forecast will automatically examine it, identify what is meaningful, and produce a forecasting model capable of making predictions that are up to 50% more accurate than looking at time series data alone.
Amazon Forecast
76
Overview of Amazon Web Services AWS Whitepaper
Amazon Forecast is a fully managed service, so there are no servers to provision, and no ML models to build, train, or deploy. You pay only for what you use, and there are no minimum fees and no upfront commitments.
Amazon Fraud Detector
Amazon Fraud Detector is a fully managed service that uses ML and more than 20 years of fraud detection expertise from Amazon, to identify potentially fraudulent activity so customers can catch more online fraud faster. Amazon Fraud Detector automates the time consuming and expensive steps to build, train, and deploy an ML model for fraud detection, making it easier for customers to leverage the technology. Amazon Fraud Detector customizes each model it creates to a customer’s own dataset, making the accuracy of models higher than current one-size fits all ML solutions. And, because you pay only for what you use, you avoid large upfront expenses.
Amazon Comprehend Medical
Over the past decade, AWS has witnessed a digital transformation in health, with organizations capturing huge volumes of patient information every day. But this data is often unstructured and the process to extract this information is labor-intensive and error-prone. Amazon Comprehend Medical is a HIPAA-eligible natural language processing (NLP) service that uses machine learning that has been pre-trained to understand and extract health data from medical text, such as prescriptions, procedures, or diagnoses. Amazon Comprehend Medical can help you extract information from unstructured medical text accurately and quickly with medical ontologies like ICD-10-CM, RxNorm, and SNOMED CT and in turn accelerate insurance claim processing, improve population health, and accelerate pharmacovigilance.
Amazon Kendra
Amazon Kendra is an intelligent search service powered by ML. Amazon Kendra reimagines enterprise search for your websites and applications so your employees and customers can easily find the content they are looking for, even when it’s scattered across multiple locations and content repositories within your organization.
Using Amazon Kendra, you can stop searching through troves of unstructured data and discover the right answers to your questions, when you need them. Amazon Kendra is a fully managed service, so there are no servers to provision, and no ML models to build, train, or deploy.
Amazon Fraud Detector
77
Overview of Amazon Web Services AWS Whitepaper
Amazon Lex
Amazon Lex is a fully managed artificial intelligence (AI) service to design, build, test, and deploy conversational interfaces into any application using voice and text. Lex provides the advanced deep learning functionalities of automatic speech recognition (ASR) for converting speech to text, and natural language understanding (NLU) to recognize the intent of the text, to enable you to build applications with highly engaging user experiences and lifelike conversational interactions, and create new categories of products. With Amazon Lex, the same deep learning technologies that power Amazon Alexa are now available to any developer, enabling you to quickly and easily build sophisticated, natural language, conversational bots (“chatbots”) and voice enabled interactive voice response (IVR) systems.
Amazon Lex enables developers to build conversational chatbots quickly. With Amazon Lex, no deep learning expertise is necessary—to create a bot, you just specify the basic conversation flow in the Amazon Lex console. Amazon Lex manages the dialogue and dynamically adjusts the responses in the conversation. Using the console, you can build, test, and publish your text or voice chatbot. You can then add the conversational interfaces to bots on mobile devices, web applications, and chat platforms (for example, Facebook Messenger). There are no upfront costs or minimum fees to use Amazon Lex - you are charged only for the text or speech requests that are made. The pay-as-you-go pricing and the low cost per request make the service a cost-effective way to build conversational interfaces. With the Amazon Lex free tier, you can easily try Amazon Lex without any initial investment.
Amazon Lookout for Equipment
Amazon Lookout for Equipment analyzes the data from the sensors on your equipment (such as pressure in a generator, flow rate of a compressor, revolutions per minute of fans), to automatically train an ML model based on just your data, for your equipment – with no ML expertise required. Lookout for Equipment uses your unique ML model to analyze incoming sensor data in real-time and accurately identify early warning signs that could lead to machine failures. This means you can detect equipment abnormalities with speed and precision, quickly diagnose issues, take action to reduce expensive downtime, and reduce false alerts.
Amazon Lex
78
Overview of Amazon Web Services AWS Whitepaper
Amazon Lookout for Metrics
Note
On October 10, 2025, AWS will discontinue support for Amazon Lookout for Metrics. For more information, see Transitioning off Amazon Lookout for Metrics.
Amazon Lookout for Metrics uses ML to automatically detect and diagnose anomalies (outliers from the norm) in business and operational data, such as a sudden dip in sales revenue or customer acquisition rates. In a couple of clicks, you can connect Amazon Lookout for Metrics to popular data stores such as Amazon S3, Amazon Redshift, and Amazon Relational Database Service (Amazon RDS), as well as third-party Software as a Service (SaaS) applications, such as Salesforce, Servicenow, Zendesk, and Marketo, and start monitoring metrics that are important to your business. Lookout for Metrics automatically inspects and prepares the data from these sources to detect anomalies with greater speed and accuracy than traditional methods used for anomaly detection. You can also provide feedback on detected anomalies to tune the results and improve accuracy over time. Lookout for Metrics makes it easy to diagnose detected anomalies by grouping together anomalies that are related to the same event and sending an alert that includes a summary of the potential root cause. It also ranks anomalies in order of severity so that you can prioritize your attention to what matters the most to your business.
Amazon Lookout for Vision
Amazon Lookout for Vision is an ML service that spots defects and anomalies in visual representations using computer vision (CV). With Amazon Lookout for Vision, manufacturing companies can increase quality and reduce operational costs by quickly identifying differences in images of objects at scale. For example, Lookout for Vision can be used to identify missing components in products, damage to vehicles or structures, irregularities in production lines, miniscule defects in silicon wafers, and other similar problems. Amazon Lookout for Vision uses ML to see and understand images from any camera as a person would, but with an even higher degree of accuracy and at a much larger scale. Lookout for Vision allows customers to eliminate the need for costly and inconsistent manual inspection, while improving quality control, defect and damage assessment, and compliance. In minutes, you can begin using Lookout for Vision to automate inspection of images and objects – with no ML expertise required.
Amazon Lookout for Metrics
79
Overview of Amazon Web Services AWS Whitepaper
Amazon Monitron
Amazon Monitron is an end-to-end system that uses ML to detect abnormal behavior in industrial machinery, enabling you to implement predictive maintenance and reduce unplanned downtime.
Installing sensors and the necessary infrastructure for data connectivity, storage, analytics, and alerting are foundational elements for enabling predictive maintenance. However, to make it work, companies have historically needed skilled technicians and data scientists to piece together a complex solution from scratch. This included identifying and procuring the right type of sensors for their use cases and connecting them together with an IoT gateway (a device that aggregates and transmits data). As a result, few companies have been able to successfully implement predictive maintenance.
Amazon Monitron includes sensors to capture vibration and temperature data from equipment, a gateway device to securely transfer data to AWS, the Amazon Monitron service that analyzes the data for abnormal machine patterns using ML, and a companion mobile app to set up the devices and receive reports on operating behavior and alerts to potential failures in your machinery. You can start monitoring equipment health in minutes without any development work or ML experience required, and enable predictive maintenance with the same technology used to monitor equipment in Amazon Fulfillment Centers.
Amazon PartyRock
Amazon PartyRock makes learning generative AI easy with a hands-on, code-free app builder. Experiment with prompt engineering techniques, review generated responses, and develop intuition for generative AI while creating and exploring fun apps. PartyRock provides access to foundation models (FMs) from Amazon and leading AI companies through Amazon Bedrock, a fully managed serviced service.
Amazon Personalize
Amazon Personalize is an ML service that makes it easy for developers to create individualized recommendations for customers using their applications.
ML is increasingly used to improve customer engagement by powering personalized product and content recommendations, tailored search results, and targeted marketing promotions. However, developing the ML capabilities necessary to produce these sophisticated recommendation systems has been beyond the reach of most organizations today due to the complexity of developing ML
Amazon Monitron
80
Overview of Amazon Web Services AWS Whitepaper
functionality. Amazon Personalize allows developers with no prior ML experience to easily build sophisticated personalization capabilities into their applications, using ML technology perfected from years of use on Amazon.com.
With Amazon Personalize, you provide an activity stream from your application – page views, signups, purchases, and so forth – as well as an inventory of the items you want to recommend, such as articles, products, videos, or music. You can also choose to provide Amazon Personalize with additional demographic information from your users such as age, or geographic location. Amazon Personalize processes and examines the data, identifies what is meaningful, selects the right algorithms, and trains and optimizes a personalization model that is customized for your data.
Amazon Personalize offers optimized recommenders for retail and media and entertainment that make it faster and easier to deliver high-performing personalized user experiences. Amazon Personalize also offers intelligent user segmentation so you can run more effective prospecting campaigns through your marketing channels. With our two new recipes, you can automatically segment your users based on their interest in different product categories, brands, and more.
All data analyzed by Amazon Personalize is kept private and secure, and only used for your customized recommendations. You can start serving your personalized predictions via a simple API call from inside the virtual private cloud that the service maintains. You pay only for what you use, and there are no minimum fees and no upfront commitments.
Amazon Personalize is like having your own Amazon.com ML personalization team at your disposal, 24 hours a day.
Amazon Polly
Amazon Polly is a service that turns text into lifelike speech. Amazon Polly lets you create applications that talk, enabling you to build entirely new categories of speech-enabled products. Amazon Polly is an Amazon artificial intelligence (AI) service that uses advanced deep learning technologies to synthesize speech that sounds like a human voice. Amazon Polly includes a wide selection of lifelike voices spread across dozens of languages, so you can select the ideal voice and build speech-enabled applications that work in many different countries.
Amazon Polly delivers the consistently fast response times required to support real-time, interactive dialog. You can cache and save Amazon Polly speech audio to replay offline or redistribute. And Amazon Polly is easy to use. You simply send the text you want converted into speech to the Amazon Polly API, and Amazon Polly immediately returns the audio stream to your
Amazon Polly
81
Overview of Amazon Web Services AWS Whitepaper
application so your application can play it directly or store it in a standard audio file format, such as MP3.
In addition to Standard TTS voices, Amazon Polly offers Neural Text-to-Speech (NTTS) voices that deliver advanced improvements in speech quality through a new machine learning approach. Polly’s Neural TTS technology also supports a Newscaster speaking style that is tailored to news narration use cases. Finally, Amazon Polly Brand Voice can create a custom voice for your organization. This is a custom engagement where you will work with the Amazon Polly team to build an NTTS voice for the exclusive use of your organization.
With Amazon Polly, you pay only for the number of characters you convert to speech, and you can save and replay Amazon Polly generated speech. The Amazon Polly low cost per character converted, and lack of restrictions on storage and reuse of voice output, make it a cost-effective way to enable Text-to-Speech everywhere.
Amazon Q
Amazon Q is a generative AI-powered assistant for accelerating software development and leveraging your internal data.
Amazon Q Business
Amazon Q Business can answer questions, provide summaries, generate content, and securely complete tasks based on data and information in your enterprise systems. It empowers employees to be more creative, data-driven, efficient, prepared, and productive.
Amazon Q Developer
Amazon Q Developer (formerly Amazon CodeWhisperer) assists developers and IT professionals with their tasks—from coding, testing, and upgrading applications, to diagnosing errors, performing security scanning and fixes, and optimizing AWS resources. Amazon Q has advanced, multistep planning and reasoning capabilities that can transform existing code (for example, perform Java version upgrades) and implement new features generated from developer requests.
Amazon Rekognition
Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no ML expertise to use. With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and
Amazon Q
82
Overview of Amazon Web Services AWS Whitepaper
videos, as well as detect any inappropriate content. Amazon Rekognition also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.
With Amazon Rekognition Custom Labels, you can identify the objects and scenes in images that are specific to your business needs. For example, you can build a model to classify specific machine parts on your assembly line or to detect unhealthy plants. Amazon Rekognition Custom Labels takes care of the heavy lifting of model development for you, so no ML experience is required. You simply need to supply images of objects or scenes you want to identify, and the service handles the rest.
Amazon SageMaker AI
With Amazon SageMaker AI, you can build, train, and deploy ML models for any use case with fully managed infrastructure, tools, and workflows. SageMaker AI removes the heavy lifting from each step of the ML process to make it easier to develop high-quality models. SageMaker AI provides all of the components used for ML in a single toolset so models get to production faster with much less effort and at lower cost.
Amazon SageMaker AI Autopilot
Amazon SageMaker AI Autopilot automatically builds, trains, and tunes the best ML models based on your data, while allowing you to maintain full control and visibility. With SageMaker AI Autopilot, you simply provide a tabular dataset and select the target column to predict, which can be a number (such as a house price, called regression), or a category (such as spam/not spam, called classification). SageMaker AI Autopilot will automatically explore different solutions to find the best model. You then can directly deploy the model to production with just one click, or iterate on the recommended solutions with Amazon SageMaker AI Studio to further improve the model quality.
Amazon SageMaker AI Canvas
Amazon SageMaker AI Canvas expands access to ML by providing business analysts with a visual point-and-click interface that allows them to generate accurate ML predictions on their own — without requiring any ML experience or having to write a single line of code.
Amazon SageMaker AI Clarify
Amazon SageMaker AI Clarify provides machine learning developers with greater visibility into their training data and models so they can identify and limit bias and explain predictions. Amazon
Amazon SageMaker AI
83
Overview of Amazon Web Services AWS Whitepaper
SageMaker AI Clarify detects potential bias during data preparation, after model training, and in your deployed model by examining attributes you specify. SageMaker AI Clarify also includes feature importance graphs that help you explain model predictions and produces reports which can be used to support internal presentations or to identify issues with your model that you can take steps to correct.
Amazon SageMaker AI Data Labeling
Amazon SageMaker AI provides data labeling offerings to identify raw data, such as images, text files, and videos, and add informative labels to create high-quality training datasets for your ML models.
Amazon SageMaker AI Data Wrangler
Amazon SageMaker AI Data Wrangler reduces the time it takes to aggregate and prepare data for ML from weeks to minutes. With SageMaker AI Data Wrangler, you can simplify the process of data preparation and feature engineering, and complete each step of the data preparation workflow, including data selection, cleansing, exploration, and visualization from a single visual interface.
Amazon SageMaker AI Edge
Amazon SageMaker AI Edge enables machine learning on edge devices by optimizing, securing, and deploying models to the edge, and then monitoring these models on your fleet of devices, such as smart cameras, robots, and other smart-electronics, to reduce ongoing operational costs. SageMaker AI Edge Compiler optimizes the trained model to be runnable on an edge device. SageMaker AI Edge includes an over-the-air (OTA) deployment mechanism that helps you deploy models on the fleet independent of the application or device firmware. SageMaker AI Edge Agent allows you to run multiple models on the same device. The Agent collects prediction data based on the logic that you control, such as intervals, and uploads it to the cloud so that you can periodically retrain your models over time.
Amazon SageMaker AI Feature Store
Amazon SageMaker AI Feature Store is a purpose-built repository where you can store and access features so it’s much easier to name, organize, and reuse them across teams. SageMaker AI Feature Store provides a unified store for features during training and real-time inference without the need to write additional code or create manual processes to keep features consistent. SageMaker AI Feature Store keeps track of the metadata of stored features (such as feature name or version number) so that you can query the features for the right attributes in batches or in real time using
Amazon SageMaker AI
84
Overview of Amazon Web Services AWS Whitepaper
Amazon Athena, an interactive query service. SageMaker AI Feature Store also keeps features updated, because as new data is generated during inference, the single repository is updated so new features are always available for models to use during training and inference.
Amazon SageMaker AI geospatial capabilities
Amazon SageMaker AI geospatial capabilities make it easier for data scientists and machine learning (ML) engineers to build, train, and deploy ML models faster using geospatial data. You have access to data (open-source and third-party), processing, and visualization tools to make it more efficient to prepare geospatial data for ML. You can increase your productivity by using purpose-built algorithms and pre-trained ML models to speed up model building and training, and use built-in visualization tools to explore prediction outputs on an interactive map and then collaborate across teams on insights and results.
Amazon SageMaker AI HyperPod
Amazon SageMaker AI HyperPod removes the undifferentiated heavy lifting involved in building and optimizing machine learning (ML) infrastructure for large language models (LLMs), diffusion models, and foundation models (FMs). SageMaker AI HyperPod is pre-configured with distributed training libraries that enable customers to automatically split training workloads across thousands of accelerators, such as AWS Trainium, and NVIDIA A100 and H100 Graphical Processing Units (GPUs).
SageMaker AI HyperPod also helps ensure that you can continue training uninterrupted by periodically saving checkpoints. When a hardware failure occurs, self-healing clusters automatically detect the failure, repair or replace the faulty instance, and resume the training from the last saved checkpoint, removing the need for you to manually manage this process and helping you train for weeks or months in a distributed setting without disruption. You can customize your computing environment to best suit your needs and configure it with the Amazon SageMaker AI distributed training libraries to achieve optimal performance on AWS.
Amazon SageMaker AI JumpStart
Amazon SageMaker AI JumpStart helps you quickly and easily get started with ML. To make it easier to get started, SageMaker AI JumpStart provides a set of solutions for the most common use cases that can be deployed readily with just a few clicks. The solutions are fully customizable and showcase the use of AWS CloudFormation templates and reference architectures so you can accelerate your ML journey. Amazon SageMaker AI JumpStart also supports one-click
Amazon SageMaker AI
85
Overview of Amazon Web Services AWS Whitepaper
deployment and fine-tuning of more than 150 popular open-source models such as natural language processing, object detection, and image classification models.
Amazon SageMaker AI Model Building
Amazon SageMaker AI provides all the tools and libraries you need to build ML models, the process of iteratively trying different algorithms and evaluating their accuracy to find the best one for your use case. In Amazon SageMaker AI you can pick different algorithms, including over 15 that are built-in and optimized for SageMaker AI, and use over 750 pre-built models from popular model zoos available with a few clicks. SageMaker AI also offers a variety of model building tools, including Amazon SageMaker AI Studio Notebooks, JupyterLab, RStudio, and Code Editor based on Code-OSS (Virtual Studio Code Open Source), where you can run ML models on a small scale to see results and view reports on their performance so you can come up with high-quality working prototypes.
Amazon SageMaker AI Model Training
Amazon SageMaker AI reduces the time and cost to train and tune ML models at scale without the need to manage infrastructure. You can take advantage of the highest-performing ML compute infrastructure currently available, and SageMaker AI can automatically scale infrastructure up or down, from one to thousands of GPUs. Since you pay only for what you use, you can manage your training costs more effectively. To train deep learning models faster, you can use the Amazon SageMaker AI distributed training libraries for better performance or use third-party libraries such as DeepSpeed, Horovod, or Megatron.
Amazon SageMaker AI Model Deployment
Amazon SageMaker AI makes it easy to deploy ML models to make predictions (also known as inference) at the best price-performance for any use case. It provides a broad selection of ML infrastructure and model deployment options to help meet all your ML inference needs. It is a fully managed service and integrates with MLOps tools, so you can scale your model deployment, reduce inference costs, manage models more effectively in production, and reduce operational burden.
Amazon SageMaker AI Pipelines
Amazon SageMaker AI Pipelines is the first purpose-built, easy-to-use continuous integration and continuous delivery (CI/CD) service for ML. With SageMaker AI Pipelines, you can create, automate, and manage end-to-end ML workflows at scale.
Amazon SageMaker AI
86
Overview of Amazon Web Services AWS Whitepaper
Amazon SageMaker AI Studio Lab
Amazon SageMaker AI Studio Lab is a free ML development environment that provides the compute, storage (up to 15GB), and security—all at no cost—for anyone to learn and experiment with ML. All you need to get started is a valid email address—you don’t need to configure infrastructure or manage identity and access or even sign up for an AWS account. SageMaker AI Studio Lab accelerates model building through GitHub integration, and it comes preconfigured with the most popular ML tools, frameworks, and libraries to get you started immediately. SageMaker AI Studio Lab automatically saves your work so you don’t need to restart in between sessions. It’s as easy as closing your laptop and coming back later.
Apache MXNet on AWS
Apache MXNet is a fast and scalable training and inference framework with an easy-to-use, conciseAPI for ML. MXNet includes the Gluon interface that allows developers of all skill levels to get started with deep learning on the cloud, on edge devices, and on mobile apps. In just a few lines of Gluon code, you can build linear regression, convolutional networks and recurrent LSTMs for object detection, speech recognition, recommendation, and personalization. You can get started with MxNet on AWS with a fully managed experience using Amazon SageMaker AI, a platform to build, train, and deploy ML models at scale. Or, you can use the AWS Deep Learning AMIs to build custom environments and workflows with MxNet as well as other frameworks includingTensorFlow, PyTorch, Chainer, Keras, Caffe, Caffe2, and Microsoft Cognitive Toolkit.
AWS Deep Learning AMIs
The AWS Deep Learning AMIs provide ML practitioners and researchers with the infrastructure and tools to accelerate deep learning in the cloud, at any scale. You can quickly launch Amazon EC2 instances pre-installed with popular deep learning frameworks and interfaces such as TensorFlow, PyTorch, Apache MXNet, Chainer, Gluon, Horovod, and Keras to train sophisticated, custom AI models, experiment with new algorithms, or to learn new skills and techniques. Whether you need Amazon EC2 GPU or CPU instances, there is no additional charge for the Deep Learning AMIs – you only pay for the AWS resources needed to store and run your applications.
AWS Deep Learning Containers
AWS Deep Learning Containers (AWS DL Containers) are Docker images pre-installed with deep learning frameworks to make it easy to deploy custom machine learning (ML) environments quickly by letting you skip the complicated process of building and optimizing your environments from scratch. AWS DL Containers support TensorFlow, PyTorch, Apache MXNet. You can deploy
Amazon SageMaker AI
87
Overview of Amazon Web Services AWS Whitepaper
AWS DL Containers on Amazon SageMaker AI, Amazon Elastic Kubernetes Service (Amazon EKS), self-managed Kubernetes on Amazon EC2, Amazon Elastic Container Service (Amazon ECS). The containers are available through Amazon Elastic Container Registry (Amazon ECR) and AWS Marketplace at no cost—you pay only for the resources that you use.
Geospatial ML with Amazon SageMaker AI
Amazon SageMaker AI geospatial capabilities allow data scientists and ML engineers to build, train, and deploy ML models using geospatial data faster and at scale. You can access readily available geospatial data sources, efficiently transform or enrich large-scale geospatial datasets with purpose-built operations, and accelerate model building by selecting pretrained ML models. You can also analyze geospatial data and explore model predictions on an interactive map using 3D accelerated graphics with built-in visualization tools. SageMaker Runtime geospatial capabilities can be used for a wide range of use cases, such as maximizing harvest yield and food security, assessing risk and insurance claims, supporting sustainable urban development, and forecasting retail site utilization.
Hugging Face on AWS
With Hugging Face on Amazon SageMaker AI, you can deploy and fine-tune pre-trained models from Hugging Face, an open-source provider of natural language processing (NLP) models known as Transformers, reducing the time it takes to set up and use these NLP models from weeks to minutes. NLP refers to ML algorithms that help computers understand human language. They help with translation, intelligent search, text analysis, and more. However, NLP models can be large and complex (sometimes consisting of hundreds of millions of model parameters), and training and optimizing them requires time, resources, and skill. AWS collaborated with Hugging Face to create Hugging Face AWS Deep Learning Containers (DLCs), which provide data scientists and ML developers a fully managed experience for building, training, and deploying state-of-the-art NLP models on Amazon SageMaker AI.
PyTorch on AWS
PyTorch is an open-source deep learning framework that makes it easy to develop machine learning models and deploy them to production. Using TorchServe, PyTorch's model serving library built and maintained by AWS in partnership with Facebook, PyTorch developers can quickly and easily deploy models to production. PyTorch also provides dynamic computation graphs and libraries for distributed training, which are tuned for high performance on AWS. You can get started with PyTorch on AWS using Amazon SageMaker, a fully managed ML service that makes it
Amazon SageMaker AI
88
Overview of Amazon Web Services AWS Whitepaper
easy and cost-effective to build, train, and deploy PyTorch models at scale. If you prefer to manage the infrastructure yourself, you can use the AWS Deep Learning AMIs or the AWS Deep Learning Containers, which come built from source and optimized for performance with the latest version of PyTorch to quickly deploy custom machine learning environments.
TensorFlow on AWS
TensorFlow is one of many deep learning frameworks available to researchers and developers to enhance their applications with machine learning. AWS provides broad support for TensorFlow, enabling customers to develop and serve their own models across computer vision, natural language processing, speech translation, and more. You can get started with TensorFlow on AWS using Amazon SageMaker AI, a fully managed ML service that makes it easy and cost-effective to build, train, and deploy TensorFlow models at scale. If you prefer to manage the infrastructure yourself, you can use the AWS Deep Learning AMIs or the AWS Deep Learning Containers, which come built from source and optimized for performance with the latest version of TensorFlow to quickly deploy custom ML environments.
Amazon Textract
Amazon Textract is a service that automatically extracts text and data from scanned documents. Amazon Textract goes beyond simple optical character recognition (OCR) to also identify the contents of fields in forms and information stored in tables.
Today, many companies manually extract data from scanned documents such as PDFs, images, tables, and forms, or through simple OCR software that requires manual configuration (which often must be updated when the form changes). To overcome these manual and expensive processes, Amazon Textract uses ML to read and process any type of document, accurately extracting text, handwriting, tables, and other data with no manual effort. Amazon Textract provides you with the flexibility to specify the data you need to extract from documents using queries. You can specify the information you need in the form of natural language questions (such as “What is the customer name”). You do not need to know the data structure in the document (table, form, implied field, nested data) or worry about variations across document versions and formats. Amazon Textract Queries are pre-trained on a large variety of documents including paystubs, bank statements, W-2s, loan application forms, mortgage notes, claims documents, and insurance cards.
With Amazon Textract, you can quickly automate document processing and act on the information extracted, whether you’re automating loans processing or extracting information from invoices and
Amazon Textract
89
Overview of Amazon Web Services AWS Whitepaper
receipts. Amazon Textract can extract the data in minutes instead of hours or days. Additionally, you can add human reviews with Amazon Augmented AI to provide oversight of your models and check sensitive data.
Amazon Transcribe
Amazon Transcribe is an automatic speech recognition (ASR) service that makes it easy for customers to automatically convert speech to text. The service can transcribe audio files stored in common formats, like WAV and MP3, with time stamps for every word so that you can easily locate the audio in the original source by searching for the text. You can also send a live audio stream to Amazon Transcribe and receive a stream of transcripts in real time. Amazon Transcribe is designed to handle a wide range of speech and acoustic characteristics, including variations in volume, pitch, and speaking rate. The quality and content of the audio signal (including but not limited to factors such as background noise, overlapping speakers, accented speech, or switches between languages within a single audio file) may affect the accuracy of service output. Customers can choose to use Amazon Transcribe for a variety of business applications, including transcription of voice-based customer service calls, generation of subtitles on audio/video content, and conduct (text based) content analysis on audio/video content.
Two very important services derived from Amazon Transcribe include Amazon Transcribe Medicaland Amazon Transcribe Call Analytics.
Amazon Transcribe Medical uses advanced ML models to accurately transcribe medical speech into text. Amazon Transcribe Medical can generate text transcripts that can be used to support a variety of use cases, spanning clinical documentation workflow and drug safety monitoring (pharmacovigilance) to subtitling for telemedicine and even contact center analytics in the healthcare and life sciences domains.
Amazon Transcribe Call Analytics is an AI-powered API that provides rich call transcripts and actionable conversation insights that you can add into their call applications to improve customer experience and agent productivity. It combines powerful speech-to-text and custom natural language processing (NLP) models that are trained specifically to understand customer care and outbound sales calls. As a part of AWS Contact Center Intelligence (CCI) solutions, this API is contact center agnostic and makes it easy for customers and ISVs to add call analytics capabilities into their applications.
The easiest way to get started with Amazon Transcribe is to submit a job using the console to transcribe an audio file. You can also call the service directly from the AWS Command Line Interface, or use one of the supported SDKs of your choice to integrate with your applications.
Amazon Transcribe
90
Overview of Amazon Web Services AWS Whitepaper
Amazon Translate
Amazon Translate is a neural machine translation service that delivers fast, high-quality, and affordable language translation. Neural machine translation is a form of language translation automation that uses deep learning models to deliver more accurate and more natural sounding translation than traditional statistical and rule-based translation algorithms. Amazon Translate allows you to localize content such as websites and applications for your diverse users, easily translate large volumes of text for analysis, and efficiently enable cross-lingual communication between users.
AWS DeepComposer
AWS DeepComposer is the world’s first musical keyboard powered by ML to enable developers of all skill levels to learn Generative AI while creating original music outputs. DeepComposer consists of a USB keyboard that connects to the developer’s computer, and the DeepComposer service, accessed through the AWS Management Console. DeepComposer includes tutorials, sample code, and training data that can be used to start building generative models.
AWS DeepRacer
AWS DeepRacer is a 1/18th scale race car which gives you an interesting and fun way to get started with reinforcement learning (RL). RL is an advanced ML technique which takes a very different approach to training models than other ML methods. Its superpower is that it learns very complex behaviors without requiring any labeled training data, and can make short term decisions while optimizing for a longer term goal.
With AWS DeepRacer, you now have a way to get hands-on with RL, experiment, and learn through autonomous driving. You can get started with the virtual car and tracks in the cloud-based 3D racing simulator, and for a real-world experience, you can deploy your trained models onto AWS DeepRacer and race your friends, or take part in the global AWS DeepRacer League. Developers, the race is on.
AWS HealthLake
AWS HealthLake is a HIPAA-eligible service that healthcare providers, health insurance companies, and pharmaceutical companies can use to store, transform, query, and analyze large-scale health data.
Amazon Translate
91
Overview of Amazon Web Services AWS Whitepaper
Health data is frequently incomplete and inconsistent. It's also often unstructured, with information contained in clinical notes, lab reports, insurance claims, medical images, recorded conversations, and time-series data (for example, heart ECG or brain EEG traces).
Healthcare providers can use HealthLake to store, transform, query, and analyze data in the AWS Cloud. Using the HealthLake integrated medical natural language processing (NLP) capabilities, you can analyze unstructured clinical text from diverse sources. HealthLake transforms unstructured data using natural language processing models, and provides powerful query and search capabilities. You can use HealthLake to organize, index, and structure patient information in a secure, compliant, and auditable manner.
AWS HealthScribe
AWS HealthScribe is a HIPAA-eligible service that allows healthcare software vendors to automatically generate clinical notes by analyzing patient-clinician conversations. AWS HealthScribe combines speech recognition with generative AI to reduce the burden of clinical documentation by transcribing conversations and quickly producing clinical notes. Conversations are segmented to identify the speaker roles for patients and clinicians, extract medical terms, and generate preliminary clinical notes. To protect sensitive patient data, security and privacy are built-in to ensure that the input audio and the output text are not retained in AWS HealthScribe.
AWS Panorama
AWS Panorama is a collection of ML devices and software development kit (SDK) that brings computer vision (CV) to on-premises internet protocol (IP) cameras. With AWS Panorama, you can automate tasks that have traditionally required human inspection to improve visibility into potential issues.
Computer vision can automate visual inspection for tasks such as tracking assets to optimize supply chain operations, monitoring traffic lanes to optimize traffic management, or detecting anomalies to evaluate manufacturing quality. In environments with limited network bandwidth however, or for companies with data governance rules that require on-premises processing and storage of video, computer vision in the cloud can be difficult or impossible to implement. AWS Panorama is an ML service that allows organizations to bring computer vision to on-premises cameras to make predictions locally with high accuracy and low latency.
The AWS Panorama Appliance is a hardware device that adds computer vision to your existing IP cameras and analyzes the video feeds of multiple cameras from a single management interface.
AWS HealthScribe
92
Overview of Amazon Web Services AWS Whitepaper
It generates predictions at the edge in milliseconds, meaning you can be notified about potential issues such as when damaged products are detected on a fast-moving production line, or when a vehicle has strayed into a dangerous off-limits zone in a warehouse. And, third-party manufacturers are building new AWS Panorama-enabled cameras and devices to provide even more form factors for your unique use cases. With AWS Panorama you can use ML models from AWS to build your own computer vision applications, or work with a partner from the AWS Partner Network to build CV applications quickly.
Management and governance
With AWS Management and Governance services, you don't have to choose between innovating faster and maintaining control over cost, compliance, and security—you can do both.
For general information, see Management and Governance on AWS.
Services
•
AWS Auto Scaling
•
AWS CloudFormation
•
AWS CloudTrail
•
Amazon CloudWatch
•
AWS Compute Optimizer
•
AWS Console Mobile Application
•
AWS Control Tower
•
AWS Config
•
AWS Health
•
AWS Launch Wizard
•
AWS License Manager
•
Amazon Managed Grafana
•
Amazon Managed Service for Prometheus
•
AWS Organizations
Management and governance
93
Overview of Amazon Web Services AWS Whitepaper
•
AWS OpsWorks
•
AWS Proton
•
Amazon Q Developer in chat applications (formerly AWS Chatbot)
•
AWS Service Catalog
•
AWS Systems Manager
•
AWS Trusted Advisor
•
AWS User Notifications
•
AWS Well-Architected Tool
AWS Auto Scaling
AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it’s easy to setup application scaling for multiple resources across multiple services in minutes. The service provides a simple, powerful user interface that lets you build scaling plans for resources includingAmazon EC2 instances and Spot Fleets, Amazon ECS tasks, Amazon DynamoDB tables and indexes, and Amazon Aurora Replicas. AWS Auto Scaling makes scaling simple with recommendations that allow you to optimize performance, costs, or balance between them. If you’re already usingAmazon EC2 Auto Scaling to dynamically scale your Amazon EC2 instances, you can now combine it with AWS Auto Scaling to scale additional resources for other AWS services. With AWS Auto Scaling, your applications always have the right resources at the right time.
AWS CloudFormation
AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.
You can use the AWS CloudFormation sample templates or create your own templates to describe your AWS resources, and any associated dependencies or runtime parameters, required to run your application. You don’t need to figure out the order for provisioning AWS services or the subtleties of making those dependencies work. CloudFormation takes care of this for you. After the AWS resources are deployed, you can modify and update them in a controlled and predictable way, in effect applying version control to your AWS infrastructure the same way you do with your software. You can also visualize your templates as diagrams and edit them using a drag-and-drop interface with AWS Infrastructure Composer.
AWS Auto Scaling
94
Overview of Amazon Web Services AWS Whitepaper
AWS CloudTrail
AWS CloudTrail is a web service that records AWS API calls for your account and delivers log files to you. The recorded information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, the request parameters, and the response elements returned by the AWS service.
With CloudTrail, you can get a history of AWS API calls for your account, including API calls made using the AWS Management Console, AWS SDKs, command line tools, and higher-level AWS services (such as AWS CloudFormation). The AWS API call history produced by CloudTrail enables security analysis, resource change tracking, and compliance auditing.
Amazon CloudWatch
Amazon CloudWatch is a monitoring and management service built for developers, system operators, site reliability engineers (SRE), and IT managers. CloudWatch provides you with data and actionable insights to monitor your applications, understand and respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, providing you with a unified view of AWS resources, applications and services that run on AWS, and on-premises servers. You can use CloudWatch to set high resolution alarms, visualize logs and metrics side by side, take automated actions, troubleshoot issues, and discover insights to optimize your applications, and ensure they are running smoothly.
AWS Compute Optimizer
AWS Compute Optimizer recommends optimal AWS resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Over-provisioning resources can lead to unnecessary infrastructure cost, and under-provisioning resources can lead to poor application performance. Compute Optimizer helps you choose optimal configurations for three types of AWS resources: Amazon EC2 instances, Amazon EBS volumes, and AWS Lambda functions, based on your utilization data.
By applying the knowledge drawn from Amazon’s own experience running diverse workloads in the cloud, Compute Optimizer identifies workload patterns and recommends optimal AWS resources. Compute Optimizer analyzes the configuration and resource utilization of your workload to identify dozens of defining characteristics, for example, if a workload is CPU-intensive, if it exhibits a daily pattern, or if a workload accesses local storage frequently. The service processes
AWS CloudTrail
95
Overview of Amazon Web Services AWS Whitepaper
these characteristics and identifies the hardware resource required by the workload. Compute Optimizer infers how the workload would have performed on various hardware platforms (such as Amazon EC2 instances types) or using different configurations (such as Amazon EBS volume IOPS settings, and AWS Lambda function memory sizes) to offer recommendations.
Compute Optimizer is available to you at no additional charge. To get started, you can opt in to the service in the AWS Compute Optimizer Console.
AWS Console Mobile Application
The AWS Console Mobile Application lets customers view and manage a select set of resources to support incident response while on-the-go.
The AWS Console Mobile Application allows AWS customers to monitor resources through a dedicated dashboard and view configuration details, metrics, and alarms for select AWS services. The Dashboard provides permitted users with a single view a resource's status, with real-time data on Amazon CloudWatch, AWS Health Dashboard, and AWS Billing and Cost Management. Customers can view ongoing issues and follow through to the relevant CloudWatch alarm screen for a detailed view with graphs and configuration options. In addition, customers can check on the status of specific AWS services, view detailed resource screens, and perform select actions.
AWS Control Tower
AWS Control Tower automates the set-up of a baseline environment, or landing zone, that is a secure, well-architected multi-account AWS environment. The configuration of the landing zone is based on best practices that have been established by working with thousands of enterprise customers to create a secure environment that makes it easier to govern AWS workloads with rules for security, operations, and compliance.
As enterprises migrate to AWS, they typically have a large number of applications and distributed teams. They often want to create multiple accounts to allow their teams to work independently, while still maintaining a consistent level of security and compliance. In addition, they use AWS management and security services, such as AWS Organizations, Service Catalog and AWS Config, that provide very granular controls over their workloads. They want to maintain this control, but they also want a way to centrally govern and enforce the best use of AWS services across all the accounts in their environment.
AWS Control Tower automates the set-up of their landing zone and configures AWS management and security services based on established best practices in a secure, compliant, multi-account
AWS Console Mobile Application
96
Overview of Amazon Web Services AWS Whitepaper
environment. Distributed teams are able to provision new AWS accounts quickly, while central teams have the peace of mind knowing that new accounts are aligned with centrally established, company-wide compliance policies. This gives you control over your environment, without sacrificing the speed and agility AWS provides your development teams.
AWS Config
AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. The AWS Config Rules feature enables you to create rules that automatically check the configuration of AWS resources recorded by AWS Config.
With AWS Config, you can discover existing and deleted AWS resources, determine your overall compliance against rules, and dive into configuration details of a resource at any point in time. These capabilities enable compliance auditing, security analysis, resource change tracking, and troubleshooting.
AWS Health
AWS Health provides alerts and remediation guidance when AWS is experiencing events that might affect you. While the Service Health Dashboard displays the general status of AWS services, AWS Health Dashboard gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources. The dashboard displays relevant and timely information to help you manage events in progress, and provides proactive notification to help you plan for scheduled activities. With AWS Health, alerts are automatically initiated by changes in the health of AWS resources, giving you event visibility and guidance to help quickly diagnose and resolve issues.
AWS Launch Wizard
AWS Launch Wizard offers a guided way of sizing, configuring, and deploying AWS resources for third party applications, such as Microsoft SQL Server Always On and HANA based SAP systems, without the need to manually identify and provision individual AWS resources. To start, you input your application requirements, including performance, number of nodes, and connectivity on the service console. Launch Wizard then identifies the right AWS resources, such as EC2 instances and EBS volumes, to deploy and run your application. Launch Wizard provides an estimated cost of deployment, and lets you modify your resources to instantly view an updated cost assessment. Once you approve the AWS resources, Launch Wizard automatically provisions and configures the selected resources to create a fully functioning, production-ready application.
AWS Config
97
Overview of Amazon Web Services AWS Whitepaper
AWS Launch Wizard also creates CloudFormation templates that can serve as a baseline to accelerate subsequent deployments. Launch Wizard is available to you at no additional charge. You only pay for the AWS resources that are provisioned for running your solution.
AWS License Manager
AWS License Manager makes it easier to manage licenses in AWS and on-premises servers from software vendors such as Microsoft, SAP, Oracle, and IBM. AWS License Manager lets administrators create customized licensing rules that emulate the terms of their licensing agreements, and then enforces these rules when an instance of Amazon EC2 gets launched. Administrators can use these rules to limit licensing violations, such as using more licenses than an agreement stipulates or reassigning licenses to different servers on a short-term basis. The rules in AWS License Manager enable you to limit a licensing breach by physically stopping the instance from launching or by notifying administrators about the infringement. Administrators gain control and visibility of all their licenses with the AWS License Manager dashboard and reduce the risk of non-compliance, misreporting, and additional costs due to licensing overages.
AWS License Manager integrates with AWS services to simplify the management of licenses across multiple AWS accounts, IT catalogs, and on-premises, through a single AWS account. License administrators can add rules in Service Catalog, which allows them to create and manage catalogs of IT services that are approved for use on all their AWS accounts. Through seamless integration with AWS Systems Manager and AWS Organizations, administrators can manage licenses across all the AWS accounts in an organization and on-premises environments. AWS Marketplace buyers can also use AWS License Manager to track bring your own license (BYOL) software obtained from the Marketplace and keep a consolidated view of all their licenses.
Amazon Managed Grafana
Amazon Managed Grafana is a fully managed and secure data visualization service that you can use to instantly query, correlate, and visualize operational metrics, logs, and traces from multiple sources. Amazon Managed Grafana makes it easy to deploy, operate, and scale Grafana, a widely deployed open-source data visualization tool that is popular for its extensible data support.
Amazon Managed Grafana provides built-in security features for compliance with corporate governance requirements, including single sign-on, data access control, and audit reporting. Amazon Managed Grafana integrates with AWS data sources, such as Amazon CloudWatch, Amazon OpenSearch Service, AWS X-Ray, AWS IoT SiteWise, Amazon Timestream, and Amazon Managed Service for Prometheus. Amazon Managed Grafana also supports many popular open-source, third party, and other cloud data sources.
AWS License Manager
98
Overview of Amazon Web Services AWS Whitepaper
Amazon Managed Service for Prometheus
Amazon Managed Service for Prometheus is a serverless, Prometheus-compatible monitoring service for container metrics that makes it easier to securely monitor container environments at scale. With Amazon Managed Service for Prometheus, you can use the same open-source Prometheus data model and query language that you use today to monitor the performance of your containerized workloads, and also enjoy improved scalability, availability, and security without having to manage the underlying infrastructure.
Amazon Managed Service for Prometheus automatically scales the ingestion, storage, and querying of operational metrics as workloads scale up and down. It integrates with AWS security services to enable fast and secure access to data. Designed to be highly available, data ingested into a workspace is replicated across three Availability Zones in the same AWS Region.
AWS Organizations
AWS Organizations helps you centrally manage and govern your environment as you grow and scale your AWS resources. Using AWS Organizations, you can programmatically create new AWS accounts and allocate resources, group accounts to organize your workflows, apply policies to accounts or groups for governance, and simplify billing by using a single payment method for all of your accounts.
In addition, AWS Organizations is integrated with other AWS services so you can define central configurations, security mechanisms, audit requirements, and resource sharing across accounts in your organization. AWS Organizations is available to all AWS customers at no additional charge.
AWS OpsWorks
AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. AWS OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. AWS OpsWorks has three offerings, AWS OpsWorks for Chef Automate,AWS OpsWorks for Puppet Enterprise, and AWS OpsWorks Stacks.
Amazon Managed Service for Prometheus
99
Overview of Amazon Web Services AWS Whitepaper
AWS Proton
AWS Proton is the first fully managed delivery service for container and serverless applications. Platform engineering teams can use AWS Proton to connect and coordinate all the different tools needed for infrastructure provisioning, code deployments, monitoring, and updates.
Maintaining hundreds – or sometimes thousands – of microservices with constantly changing infrastructure resources and continuous integration/continuous delivery (CI/CD) configurations is a nearly impossible task for even the most capable platform teams.
AWS Proton solves this by giving platform teams the tools they need to manage this complexity and enforce consistent standards, while making it easy for developers to deploy their code using containers and serverless technologies.
Amazon Q Developer in chat applications (formerly AWS Chatbot)
Amazon Q Developer in chat applications is an interactive agent that makes it easy to monitor and interact with your AWS resources in your Slack, Microsoft Teams, and Amazon Chime chat rooms. With Amazon Q Developer in chat applications, you can receive alerts, run commands to return diagnostic information, invoke AWS Lambda functions, and create AWS support cases.
Amazon Q Developer in chat applications manages the integration between AWS services and your Slack channels, Microsoft Teams, and Amazon Chime chat rooms helping you to get started with ChatOps fast. With just a few clicks, you can start receiving notifications and issuing commands in your chosen channels or chat rooms, so your team doesn’t have to switch contexts to collaborate. Amazon Q Developer in chat applications makes it easier for your team to stay updated, collaborate, and respond faster to operational events, security findings, CI/CD workflows, budget, and other alerts for applications running in your AWS accounts.
AWS Service Catalog
AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application architectures. Service Catalog allows you to centrally manage commonly deployed IT services and helps you achieve consistent governance and meet your compliance requirements, while enabling users to quickly deploy only the approved IT services they need.
AWS Proton
100
Overview of Amazon Web Services AWS Whitepaper
AWS Systems Manager
AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. With Systems Manager, you can group resources, such as Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. Systems Manager simplifies resource and application management, shortens the time to detect and resolve operational problems, and makes it easy to operate and manage your infrastructure securely at scale.
AWS Systems Manager contains the following tools:
•
Resource groups — Lets you create a logical group of resources associated with a particular workload such as different layers of an application stack, or production versus development environments. For example, you can group different layers of an application, such as the frontend web layer and the backend data layer. Resource groups can be created, updated, or removed programmatically through the API.
•
Insights dashboard — Displays operational data that the AWS Systems Manager automatically aggregates for each resource group. Systems Manager eliminates the need for you to navigate across multiple AWS consoles to view your operational data. With Systems Manager you can view API call logs from AWS CloudTrail, resource configuration changes from AWS Config, software inventory, and patch compliance status by resource group. You can also easily integrate yourAmazon CloudWatch dashboards, AWS Trusted Advisor notifications, and AWS Health Dashboardperformance and availability alerts into your Systems Manager dashboard. Systems Manager centralizes all relevant operational data, so you can have a clear view of your infrastructure compliance and performance.
•
Run command — Provides a simple way of automating common administrative tasks such as remotely running shell scripts or PowerShell commands, installing software updates, or making changes to the configuration of OS, software, EC2 and instances and servers in your on-premises data center.
•
State Manager — Helps you define and maintain consistent OS configurations such as firewall settings and anti-malware definitions to comply with your policies. You can monitor the configuration of a large set of instances, specify a configuration policy for the instances, and automatically apply updates or configuration changes.
•
Inventory — Helps you collect and query configuration and inventory information about your instances and the software installed on them. You can gather details about your instances such
AWS Systems Manager
101
Overview of Amazon Web Services AWS Whitepaper
as installed applications, DHCP settings, agent detail, and custom items. You can run queries to track and audit your system configurations.
•
Maintenance Window — Lets you define a recurring window of time to run administrative and maintenance tasks across your instances. This ensures that installing patches and updates, or making other configuration changes does not disrupt business-critical operations. This helps improve your application availability.
•
Patch Manager — Helps you select and deploy operating system and software patches automatically across large groups of instances. You can define a maintenance window so that patches are applied only during set times that fit your needs. These capabilities help ensure that your software is always up to date and meets your compliance policies.
•
Automation — Simplifies common maintenance and deployment tasks, such as updating Amazon Machine Images (AMIs). Use the Automation feature to apply patches, update drivers and agents, or bake applications into your AMI using a streamlined, repeatable, and auditable process.
•
Parameter Store — Provides an encrypted location to store important administrative information such as passwords and database strings. The Parameter Store integrates with AWS Key Management Service (AWS KMS) to make it easy to encrypt the information you keep in the Parameter Store.
•
Distributor — Helps you securely distribute and install software packages, such as software agents. Systems Manager Distributor allows you to centrally store and systematically distribute software packages while you maintain control over versioning. You can use Distributor to create and distribute software packages and then install them using Systems Manager Run Command and State Manager. Distributor can also use AWS Identity and Access Management (IAM) policies to control who can create or update packages in your account. You can use the existing IAM policy support for Systems Manager Run Command and State Manager to define who can install packages on your hosts.
•
Session Manager — Provides a browser-based interactive shell and CLI for managing Windows and Linux EC2 instances, without the need to open inbound ports, manage SSH keys, or use bastion hosts. Administrators can grant and revoke access to instances through a central location by using AWS Identity and Access Management (IAM) policies. This allows you to control which users can access each instance, including the option to provide non-root access to specified users. Once access is provided, you can audit which user accessed an instance and log each command toAmazon S3 or Amazon CloudWatch Logs using AWS CloudTrail.
AWS Systems Manager
102
Overview of Amazon Web Services AWS Whitepaper
AWS Trusted Advisor
AWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices.
AWS User Notifications
AWS User Notifications provides a central location for managing your AWS notifications. You can receive notifications from AWS services, such as AWS Health events, Amazon CloudWatch alarms, or EC2 instance state changes, in a consistent, human-friendly format. These notifications can be delivered in multiple ways including the Console Notification Center (default), email, Amazon Q Developer in chat applications, AWS Console Mobile Application push notifications, or through theUser Notifications API.
AWS Well-Architected Tool
The AWS Well-Architected Tool (AWS WA Tool) helps you review the state of your workloads and compares them to the latest AWS architectural best practices. A workload is defined as any set of components that deliver business value, which could be an application or website. The tool is based on the AWS Well-Architected Framework, developed to help cloud architects build secure, high-performing, resilient, efficient, and sustainable application infrastructure.
The Framework provides a consistent approach for customers and partners to evaluate architectures. It has been used in tens of thousands of workload reviews conducted by the AWS Solutions Architecture team and by customers, and provides guidance to help implement designs that scale with application needs over time.
To use the AWS WA Tool, available in the AWS Management Console at no charge, just define your workload and answer a set of questions regarding operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. The AWS WA Tool then provides a plan on how to architect for the cloud using established best practices.
Media
AWS Trusted Advisor
103
Overview of Amazon Web Services AWS Whitepaper
AWS offers the most purpose-built media services, software, and appliances of any cloud to make creating, transforming, and delivering digital content fast and easy.
For general information, see Media Services on AWS.
Services
•
Amazon Elastic Transcoder
•
Amazon Interactive Video Service
•
Amazon Nimble Studio
•
AWS Elemental Appliances and Software
•
AWS Elemental MediaConnect
•
AWS Elemental MediaConvert
•
AWS Elemental MediaLive
•
AWS Elemental MediaPackage
•
AWS Elemental MediaStore
•
AWS Elemental MediaTailor
Amazon Elastic Transcoder
Amazon Elastic Transcoder is media transcoding in the cloud. It is designed to be a highly scalable, easy-to-use, and cost-effective way for developers and businesses to convert (or transcode) media files from their source format into versions that will play back on devices such as smartphones, tablets, and PCs.
Amazon Interactive Video Service
Amazon Interactive Video Service (Amazon IVS) is a managed live streaming solution that is quick and easy to set up, and ideal for creating interactive video experiences. Send your live streams to Amazon IVS using streaming software and the service does everything you need to make low-latency live video available to any viewer around the world, letting you focus on building interactive experiences alongside the live video. You can easily customize and enhance the audience experience through the Amazon IVS player SDK and timed metadata APIs, allowing you to build a more valuable relationship with your viewers on your own websites and applications.
Amazon Elastic Transcoder
104
Overview of Amazon Web Services AWS Whitepaper
Amazon Nimble Studio
Amazon Nimble Studio empowers creative studios to produce visual effects, animation, and interactive content entirely in the cloud, from storyboard sketch to final deliverable. Rapidly onboard and collaborate with artists globally and create content faster with access to virtual workstations, high-speed storage, and scalable rendering across the AWS global infrastructure.
AWS Elemental Appliances and Software
AWS Elemental Appliances and Software solutions bring advanced video processing and delivery technologies into your data center, co-location space, or on-premises facility. You can deploy AWS Elemental Appliances and Software to encode, package, and deliver video assets on-premises and seamlessly connect with cloud-based video infrastructure. Designed for easy integration with AWS Cloud media solutions, AWS Elemental Appliances and Software support video workloads that need to remain on-premises to accommodate physical camera and router interfaces, managed network delivery, or network bandwidth constraints.
AWS Elemental Live, AWS Elemental Server, and AWS Elemental Conductor come in two variants: ready-to-deploy appliances, or AWS-licensed software that you install on your own hardware. AWS Elemental Link is a compact hardware device that sends live video to the cloud for encoding and delivery to viewers.
AWS Elemental MediaConnect
AWS Elemental MediaConnect is a high-quality transport service for live video. Today, broadcasters and content owners rely on satellite networks or fiber connections to send their high-value content into the cloud or to transmit it to partners for distribution. Both satellite and fiber approaches are expensive, require long lead times to set up, and lack the flexibility to adapt to changing requirements. To be more nimble, some customers have tried to use solutions that transmit live video on top of IP infrastructure, but have struggled with reliability and security.
Now you can get the reliability and security of satellite and fiber combined with the flexibility, agility, and economics of IP-based networks using AWS Elemental MediaConnect. MediaConnect enables you to build mission-critical live video workflows in a fraction of the time and cost of satellite or fiber services. You can use MediaConnect to ingest live video from a remote event site (such as a stadium), share video with a partner (such as a cable TV distributor), or replicate a video stream for processing (such as an over-the-top service). MediaConnect combines reliable video transport, highly secure stream sharing, and real-time network traffic and video monitoring that allow you to focus on your content, not your transport infrastructure.
Amazon Nimble Studio
105
Overview of Amazon Web Services AWS Whitepaper
AWS Elemental MediaConvert
AWS Elemental MediaConvert is a file-based video transcoding service with broadcast-grade features. It allows you to easily create video-on-demand (VOD) content for broadcast and multiscreen delivery at scale. The service combines advanced video and audio capabilities with a simple web services interface and pay-as-you-go pricing. With AWS Elemental MediaConvert, you can focus on delivering compelling media experiences without having to worry about the complexity of building and operating your own video processing infrastructure.
AWS Elemental MediaLive
AWS Elemental MediaLive is a broadcast-grade live video processing service. It lets you create high-quality video streams for delivery to broadcast televisions and internet-connected multiscreen devices, such as connected TVs, tablets, smart phones, and set-top boxes. The service works by encoding your live video streams in real-time, taking a larger-sized live video source and compressing it into smaller versions for distribution to your viewers. With AWS Elemental MediaLive, you can easily set up streams for both live events and 24x7 channels with advanced broadcasting features, high availability, and pay-as-you-go pricing. AWS Elemental MediaLive lets you focus on creating compelling live video experiences for your viewers without the complexity of building and operating broadcast-grade video processing infrastructure.
AWS Elemental MediaPackage
AWS Elemental MediaPackage reliably prepares and protects your video for delivery over the Internet. From a single video input, AWS Elemental MediaPackage creates video streams formatted to play on connected TVs, mobile phones, computers, tablets, and game consoles. It makes it easy to implement popular video features for viewers (start-over, pause, rewind, and so on), such as those commonly found on DVRs. AWS Elemental MediaPackage can also protect your content using Digital Rights Management (DRM). AWS Elemental MediaPackage scales automatically in response to load, so your viewers will always get a great experience without you having to accurately predict in advance the capacity you’ll need.
AWS Elemental MediaStore
AWS Elemental MediaStore is an AWS storage service optimized for media. It gives you the performance, consistency, and low latency required to deliver live streaming video content. AWS Elemental MediaStore acts as the origin store in your video workflow. Its high performance
AWS Elemental MediaConvert
106
Overview of Amazon Web Services AWS Whitepaper
capabilities meet the needs of the most demanding media delivery workloads, combined with long-term, cost-effective storage.
AWS Elemental MediaTailor
AWS Elemental MediaTailor lets video providers insert individually targeted advertising into their video streams without sacrificing broadcast-level quality-of-service. With AWS Elemental MediaTailor, viewers of your live or on-demand video each receive a stream that combines your content with ads personalized to them. But unlike other personalized ad solutions, with AWS Elemental MediaTailor your entire stream – video and ads – is delivered with broadcast-grade video quality to improve the experience for your viewers. AWS Elemental MediaTailor delivers automated reporting based on both client and server-side ad delivery metrics, making it easy to accurately measure ad impressions and viewer behavior. You can easily monetize unexpected high-demand viewing events with no up-front costs using AWS Elemental MediaTailor. It also improves ad delivery rates, helping you make more money from every video, and it works with a wider variety of content delivery networks, ad decision servers, and client devices.
Also refer to Amazon Kinesis Video Streams
Migration and transfer
AWS offers a wide range of migration tools, guidance, services, and programs to help you assess, migrate and modernize applications and data from building the business case to leveraging AWS services to deliver new experiences.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing AWS migration services and tools. For general information, see Migrate and Modernize on AWS.
AWS Elemental MediaTailor
107
Overview of Amazon Web Services AWS Whitepaper
Services and tools
•
AWS Application Discovery Service
•
AWS Application Migration Service
•
AWS Database Migration Service
•
AWS Mainframe Modernization Service
•
AWS Migration Hub
•
AWS Snow Family
•
AWS DataSync
•
AWS Transfer Family
AWS Application Discovery Service
AWS Application Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers.
Planning data center migrations can involve thousands of workloads that are often deeply interdependent. Server utilization data and dependency mapping are important early first steps in the migration process. AWS Application Discovery Service collects and presents configuration, usage, and behavior data from your servers to help you better understand your workloads.
The collected data is retained in encrypted format in an AWS Application Discovery Service data store. You can export this data as a CSV file and use it to estimate the Total Cost of Ownership (TCO) of running on AWS and to plan your migration to AWS. In addition, this data is also available
AWS Application Discovery Service
108
Overview of Amazon Web Services AWS Whitepaper
in AWS Migration Hub, where you can migrate the discovered servers and track their progress as they get migrated to AWS.
AWS Application Migration Service
AWS Application Migration Service (AWS MGN) allows you to quickly realize the benefits of migrating applications to the cloud without changes and with minimal downtime.
AWS Application Migration Service minimizes time-intensive, error-prone manual processes by automatically converting your source servers from physical, virtual, or cloud infrastructure to run natively on AWS. It further simplifies your migration by enabling you to use the same automated process for a wide range of applications.
And by launching non-disruptive tests before migrating, you can be confident that your most critical applications such as SAP, Oracle, and SQL Server will work seamlessly on AWS.
AWS Database Migration Service
AWS Database Migration Service (AWS DMS) helps you migrate databases to AWS easily and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. The AWS Database Migration Service can migrate your data to and from most widely used commercial and open-source databases. The service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle to Amazon Aurora or Microsoft SQL Server to MySQL. It also allows you to stream data to Amazon Redshift from any of the supported sources including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, SAP ASE, and SQL Server, enabling consolidation and easy analysis of data in the petabyte-scale data warehouse. AWS Database Migration Service can also be used for continuous data replication with high availability.
AWS DMS Serverless offers the flexibility to migrate data without needing to provision replication instances, manually monitoring use, and adjusting capacity. AWS DMS Serverless supports popular use cases including continuous data replication, database consolidation, and migrations, even if the source and target database engines differ. For like-to-like or compatible database engines, you can use built-in tools with automatic scaling for a seamless database migration.
AWS Mainframe Modernization Service
AWS Mainframe Modernization Service is a unique service that allows you to migrate your on-premises mainframe workloads to a managed runtime environment on AWS. AWS Mainframe
AWS Application Migration Service
109
Overview of Amazon Web Services AWS Whitepaper
Modernization Service is a set of managed tools providing infrastructure and software for migrating, modernizing, and running mainframe applications.
•
Migrate and modernize your applications to remove the hardware and staffing costs of traditional mainframes.
•
Break up and manage your complete migration with infrastructure, software, and tools to refactor and transform legacy applications.
•
Deploy, run, and operate migrated applications in the Mainframe Modernization environment with no upfront costs.
AWS Migration Hub
AWS Migration Hub provides a single location to track the progress of application migrations across multiple AWS and partner solutions. Using Migration Hub allows you to choose the AWS and partner migration tools that best fit your needs, while providing visibility into the status of migrations across your portfolio of applications. Migration Hub also provides key metrics and progress for individual applications, regardless of which tools are being used to migrate them. For example, you might use AWS Database Migration Service, AWS Application Migration Service, and partner migration tools such as ATADATA ATAmotion, CloudEndure Live Migration, or RiverMeadow Server Migration Saas to migrate an application comprised of a database, virtualized web servers, and a bare metal server. Using Migration Hub, you can view the migration progress of all the resources in the application. This allows you to quickly get progress updates across all of your migrations, easily identify and troubleshoot any issues, and reduce the overall time and effort spent on your migration projects.
AWS Snow Family
The AWS Snow Family helps customers that need to run operations in austere, non-data center environments, and in locations where there's lack of consistent network connectivity. The Snow Family comprises AWS Snowball and AWS Snowball Edge, and offers a number of physical devices and capacity points, most with built-in computing capabilities. These services help physically transport up to exabytes of data into and out of AWS. Snow Family devices are owned and managed by AWS and integrate with AWS security, monitoring, storage management, and computing capabilities.
AWS Migration Hub
110
Overview of Amazon Web Services AWS Whitepaper
AWS Snowball
AWS Snowball is the smallest member of the AWS Snow Family of edge computing, edge storage, and data transfer devices, weighing in at 4.5 pounds (2.1 kg) with 8 terabytes of usable storage. The Snowball appliance is ruggedized, secure, and purpose-built for use outside of a traditional data center. Its small form factor makes it a perfect fit for tight spaces or where portability is a necessity and network connectivity is unreliable. You can use Snowball in backpacks on first responders, or for Internet of Things (IoT), vehicular, and drone use cases. You can run compute applications at the edge, and you can ship the device with data to AWS for offline data transfer, or you can transfer data online with AWS DataSync from edge locations.
Like AWS Snowball Edge, AWS Snowball has multiple layers of security and encryption. You can use either of these services to run edge computing workloads, or to collect, process, and transfer data to AWS. Snowball is designed for data migration needs up to 8 terabytes per device and from space-constrained environments where Snowball Edge devices will not fit.
AWS Snowball Edge
AWS Snowball Edge is an edge computing, data migration, and edge storage device. Snowball Edge can do local processing and run edge-computing workloads in addition to transferring data between your local environment and the AWS Cloud. Each Snowball Edge device can transport data at speeds faster than the internet. This transport is done by shipping the data in the devices through a regional carrier.
Snowball Edge devices have five options for device configurations:
•
Storage-optimized for data transfer, with up to 80 TB of usable storage capacity. They are well suited for local storage and large scale data transfer.
•
Storage-optimized 210 TB, with 210 TB of usable storage capacity
•
Storage-optimized with EC2-compatible compute functionality, with up to 80 TB of usable storage capacity, 40 vCPUs, and 80 GB of memory for compute functionality
•
Compute-optimized, with the AMD EPYC Gen2 having the most compute functionality with up to 104 vCPUs, 416 GB of memory, and 28 TB of dedicated NVMe SSD for compute instances. The AMD EPYC Gen1 has up to 52 vCPUs, 208 GB of memory, 39.5 TB of usable storage capacity, and 7.68 TB of dedicated NVMe SSD for compute instances.
You can use these devices for data collection, machine learning (ML) and processing, and storage in environments with intermittent connectivity (such as manufacturing, industrial, and
AWS Snow Family
111
Overview of Amazon Web Services AWS Whitepaper
transportation) or in extremely remote locations (such as military or maritime operations) before shipping them back to AWS.
•
Compute-optimized with GPU is identical to the compute-optimized AMD EPYC Gen1 option, but also includes an installed graphics processing unit (GPU). The GPU is equivalent to the one available in the P3 Amazon EC2-compatible instance type. You can use these devices for advanced ML workloads and full motion video analysis in disconnected environments.
These devices can also be rack mounted and clustered together to build larger temporary installations.
Snowball supports specific Amazon EC2 instance types and AWS Lambda functions, so you can develop and test in the AWS Cloud, then deploy applications on devices in remote locations to collect, pre-process, and ship the data to AWS. Common use cases include data migration, data transport, image collation, IoT sensor stream capture, and ML.
AWS DataSync
AWS DataSync is a data transfer service that makes it easy for you to automate moving data between on-premises storage and Amazon S3 or Amazon Elastic File System (Amazon EFS). DataSync automatically handles many of the tasks related to data transfers that can slow down migrations or burden your IT operations, including running your own instances, handling encryption, managing scripts, network optimization, and data integrity validation. You can use DataSync to transfer data at speeds up to 10 times faster than open-source tools. DataSync uses an on-premises software agent to connect to your existing storage or file systems using the Network File System (NFS) protocol, so you don’t have write scripts or modify your applications to work with AWS APIs. You can use DataSync to copy data over AWS Direct Connect or internet links to AWS. The service enables one-time data migrations, recurring data processing workflows, and automated replication for data protection and recovery. Getting started with DataSync is easy: Deploy the DataSync agent on premises, connect it to a file system or storage array, select Amazon EFS or Amazon S3 as your AWS storage, and start moving data. You pay only for the data you copy.
AWS Transfer Family
AWS Transfer Family provides fully managed support for file transfers directly into and out of Amazon S3 or Amazon EFS. With support for Secure File Transfer Protocol (SFTP), File Transfer Protocol over SSL (FTPS), and File Transfer Protocol (FTP), the AWS Transfer Family helps you seamlessly migrate your file transfer workflows to AWS by integrating with existing authentication
AWS DataSync
112
Overview of Amazon Web Services AWS Whitepaper
systems, and providing DNS routing with Amazon Route 53 so nothing changes for your customers and partners, or their applications. With your data in Amazon S3 or Amazon EFS, you can use it with AWS services for processing, analytics, ML, archiving, as well as home directories and developer tools. Getting started with the AWS Transfer Family is easy; there is no infrastructure to buy and set up.
Networking and content delivery
AWS offers a broad set of networking and content delivery services that provide the highest level of reliability, security, and performance in the cloud.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing an AWS networking and content delivery service. For general information, seeAWS Networking and Content Delivery.
Services
•
Amazon API Gateway
•
Amazon CloudFront
•
Amazon Route 53
Networking and content delivery
113
Overview of Amazon Web Services AWS Whitepaper
•
AWS Verified Access
•
Amazon VPC
•
Amazon VPC Lattice
•
AWS App Mesh
•
AWS Cloud Map
•
AWS Direct Connect
•
AWS Global Accelerator
•
AWS PrivateLink
•
AWS Private 5G
•
AWS Transit Gateway
•
AWS VPN
•
Elastic Load Balancing
•
Integrated Private Wireless on AWS
Amazon API Gateway
Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. With a few clicks in the AWS Management Console, you can create an API that acts as a “front door” for applications to access data, business logic, or functionality from your back-end services, such as workloads running on Amazon EC2, code running on AWS Lambda, or any web application. Amazon API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, authorization and access control, monitoring, and API version management.
Amazon CloudFront
Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront is integrated with AWS – both physical locations that are directly connected to the AWS global infrastructure, as well as other AWS services. CloudFront works seamlessly with services including AWS Shield for DDoS mitigation, Amazon S3, Elastic Load Balancing or Amazon EC2 as origins for your applications, and Lambda@Edge to run custom code closer to customers’ users and to customize the user experience.
Amazon API Gateway
114
Overview of Amazon Web Services AWS Whitepaper
You can get started with the content delivery network in minutes, using the same AWS tools that you're already familiar with: APIs, AWS Management Console, AWS CloudFormation, CLIs, and SDKs. Amazon CDN offers a simple, pay-as-you-go pricing model with no upfront fees or required long-term contracts, and support for the CDN is included in your existing Support subscription.
Amazon Route 53
Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route users to internet applications by translating human-readable names, such aswww.example.com, into the numeric IP addresses, such as 192.0.2.1, that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well.
Amazon Route 53 effectively connects user requests to infrastructure running in AWS—such as EC2 instances, elastic load balancers, or Amazon S3 buckets—and can also be used to route users to infrastructure outside of AWS. You can use Amazon Route 53 to configure DNS health checks to route traffic to healthy endpoints or to independently monitor the health of your application and its endpoints.
Amazon Route 53 traffic flow makes it easy for you to manage traffic globally through a variety of routing types, including latency-based routing, Geo DNS, and weighted round robin—all of which can be combined with DNS Failover in order to enable a variety of low-latency, fault-tolerant architectures. Using Amazon Route 53 traffic flow’s simple visual editor, you can easily manage how your end users are routed to your application’s endpoints—whether in a single AWS Region or distributed around the globe. Amazon Route 53 also offers Domain Name Registration—you can purchase and manage domain names such as example.com and Amazon Route 53 will automatically configure DNS settings for your domains.
AWS Verified Access
AWS Verified Access provides corporate users secure access to your applications without using a virtual private network (VPN). Based on AWS Zero Trust principles, Verified Access evaluates each application request in real time to help ensure that users can only access your applications after they meet specified security requirements. You can group applications, or define unique access policies for each application, with conditions based on user identity and device posture data.
Amazon Route 53
115
Overview of Amazon Web Services AWS Whitepaper
Amazon VPC
Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. You can use both IPv4 and IPv6 in your VPC for secure and easy access to resources and applications.
You can easily customize the network configuration for your VPC. For example, you can create a public-facing subnet for your web servers that has access to the Internet, and place your backend systems, such as databases or application servers, in a private-facing subnet with no Internet access. You can leverage multiple layers of security (including security groups and network access control lists) to help control access to EC2 instances in each subnet.
Additionally, you can create a hardware virtual private network (VPN) connection between your corporate data center and your VPC and leverage the AWS Cloud as an extension of your corporate data center.
Amazon VPC Lattice
Amazon VPC Lattice provides fully managed support for service-to-service connectivity and communication. With VPC Lattice, you can use policies to define network traffic management, access, and monitoring to connect compute services in a simplified and secure way across instances, containers, and serverless applications.
AWS App Mesh
AWS App Mesh makes it easy to monitor and control microservices running on AWS. App Mesh standardizes how your microservices communicate, giving you end-to-end visibility and helping to ensure high-availability for your applications.
Modern applications are often composed of multiple microservices that each perform a specific function. This architecture helps to increase the availability and scalability of the application by allowing each component to scale independently based on demand, and automatically degrading functionality when a component fails instead of going offline. Each microservice interacts with all the other microservices through an API. As the number of microservices grows within an application, it becomes increasingly difficult to pinpoint the exact location of errors, re-route traffic after failures, and safely deploy code changes. Previously, this has required you to build monitoring
Amazon VPC
116
Overview of Amazon Web Services AWS Whitepaper
and control logic directly into your code and redeploy your microservices every time there are changes.
AWS App Mesh makes it easy to run microservices by providing consistent visibility and network traffic controls for every microservice in an application. App Mesh removes the need to update application code to change how monitoring data is collected or traffic is routed between microservices. App Mesh configures each microservice to export monitoring data and implements consistent communications control logic across your application. This makes it easy to quickly pinpoint the exact location of errors and automatically re-route network traffic when there are failures or when code changes need to be deployed.
You can use App Mesh with Amazon ECS and Amazon EKS to better run containerized microservices at scale. App Mesh uses the open source Envoy proxy, making it compatible with a wide range of AWS partner and open source tools for monitoring microservices.
AWS Cloud Map
AWS Cloud Map is a cloud resource discovery service. With AWS Cloud Map, you can define custom names for your application resources, and it maintains the updated location of these dynamically changing resources. This increases your application availability because your web service always discovers the most up-to-date locations of its resources.
Modern applications are typically composed of multiple services that are accessible over an API and perform a specific function. Each service interacts with a variety of other resources such as databases, queues, object stores, and customer-defined microservices, and they also need to be able to find the location of all the infrastructure resources on which it depends, in order to function. You typically manually manage all these resource names and their locations within the application code. However, manual resource management becomes time consuming and error-prone as the number of dependent infrastructure resources increases or the number of microservices dynamically scale up and down based on traffic. You can also use third-party service discovery products, but this requires installing and managing additional software and infrastructure.
AWS Cloud Map allows you to register any application resources such as databases, queues, microservices, and other cloud resources with custom names. AWS Cloud Map then constantly checks the health of resources to make sure the location is up-to-date. The application can then query the registry for the location of the resources needed based on the application version and deployment environment.
AWS Cloud Map
117
Overview of Amazon Web Services AWS Whitepaper
AWS Direct Connect
AWS Direct Connect makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your data center, office, or co-location environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.
AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry standard 802.1Q virtual LANs (VLANs), this dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources, such as objects stored in Amazon S3 using public IP address space, and private resources such as EC2 instances running within a VPC using private IP address space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs.
AWS Global Accelerator
AWS Global Accelerator is a networking service that improves the availability and performance of the applications that you offer to your global users.
Today, if you deliver applications to your global users over the public internet, your users might face inconsistent availability and performance as they traverse through multiple public networks to reach your application. These public networks are often congested and each hop can introduce availability and performance risk. AWS Global Accelerator uses the highly available and congestion-free AWS global network to direct internet traffic from your users to your applications on AWS, making your users’ experience more consistent.
To improve the availability of your application, you must monitor the health of your application endpoints and route traffic only to healthy endpoints. AWS Global Accelerator improves application availability by continuously monitoring the health of your application endpoints and routing traffic to the closest healthy endpoints.
AWS Global Accelerator also makes it easier to manage your global applications by providing static IP addresses that act as a fixed entry point to your application hosted on AWS which eliminates the complexity of managing specific IP addresses for different AWS Regions and Availability Zones. AWS Global Accelerator is easy to set up, configure and manage.
AWS Direct Connect
118
Overview of Amazon Web Services AWS Whitepaper
AWS PrivateLink
AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network. AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly simplify the network architecture.
AWS Private 5G
AWS Private 5G offers an easy way to use cellular technology to augment your current network. This can help you increase reliability, extend coverage, or allow a new class of workloads, such as factory automation, autonomous robotics, and advanced augmented and virtual reality (AR/VR). You will receive all the Private 5G hardware (including SIM cards) and software you need to deploy your private cellular network and connect devices to your applications.
With a few clicks in the AWS Management Console, deploy a private cellular network that meets your connectivity requirements. Start by specifying the connectivity requirements for the desired location, the number of devices you want to connect, and the geographic area they will cover. AWS will deliver pre-integrated hardware and software components (from both AWS and our AWS Partners) that meet the enterprise connectivity requirements of your private network. AWS delivers and maintains the small cell radio units, servers, 5G core, radio access network (RAN) software, and SIM cards required to set up a private 5G network and connect devices. Once the equipment is powered on, AWS automatically configures and deploys the cellular network. All you need to do is insert the SIM cards into your devices.
AWS Private 5G is also integrated with AWS Identity and Access Management (IAM), which helps you securely access and manage AWS services and resources, including all devices connected to your Private 5G network. Private 5G manages and maintains all the software and hardware components to deliver reliable, predictable network behavior and on-demand scaling to accommodate any number of devices and sensors.
AWS Transit Gateway
AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. As you grow the number of workloads running on AWS, you need to be able to scale your networks across multiple accounts and Amazon VPCs to keep up with the growth. Today, you can connect pairs of Amazon VPCs using
AWS PrivateLink
119
Overview of Amazon Web Services AWS Whitepaper
peering. However, managing point-to-point connectivity across many Amazon VPCs, without the ability to centrally manage the connectivity policies, can be operationally costly and cumbersome. For on-premises connectivity, you need to attach your AWS VPN to each individual Amazon VPC. This solution can be time consuming to build and hard to manage when the number of VPCs grows into the hundreds.
With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway in to each Amazon VPC, on-premises data center, or remote office across your network. Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks which act like spokes. This hub and spoke model significantly simplifies management and reduces operational costs because each network only has to connect to the Transit Gateway and not to every other network. Any new VPC is simply connected to the Transit Gateway and is then automatically available to every other network that is connected to the Transit Gateway. This ease of connectivity makes it easy to scale your network as you grow.
AWS VPN
AWS Virtual Private Network (AWS VPN) solutions establish secure connections between your on-premises networks, remote offices, client devices, and the AWS global network. AWS VPN is comprised of two services: AWS Site-to-Site VPN and AWS Client VPN. Each service provides a highly-available, managed, and elastic cloud VPN solution to protect your network traffic.
AWS Site-to-Site VPN creates encrypted tunnels between your network and your Amazon Virtual Private Clouds or AWS Transit Gateways. For managing remote access, AWS Client VPN connects your users to AWS or on-premises resources using a VPN software client.
Elastic Load Balancing
Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancing offers four types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant.
•
Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Operating at the individual request level (Layer seven), Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request.
AWS VPN
120
Overview of Amazon Web Services AWS Whitepaper
•
Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. Operating at the connection level (Layer four), Network Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) and is capable of handling millions of requests per second while maintaining ultra-low latencies. Network Load Balancer is also optimized to handle sudden and volatile traffic patterns.
•
Gateway Load Balancer makes it easy to deploy, scale, and run third-party virtual networking appliances. Providing load balancing and auto scaling for fleets of third-party appliances, Gateway Load Balancer is transparent to the source and destination of traffic. This capability makes it well suited for working with third-party appliances for security, network analytics, and other use cases.
•
Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network. EC2-Classic was retired on August 15, 2022.
Integrated Private Wireless on AWS
The Integrated Private Wireless on AWS program is designed to provide enterprises with managed and validated private wireless offerings from leading Communications Service Providers (CSPs). The offerings integrate CSPs’ private 5G and 4G LTE wireless networks with AWS services across AWS Regions, AWS Local Zones, AWS Outposts, and AWS Snow Family. AWS Telco Solutions Architects technically validate the offerings for their sound architecture, and adherence to AWS best practices. Telecom companies deliver, operate, and support the offerings.
The program also uses the rich expertise of validated global AWS Independent Software Vendor (ISV) partners to accelerate the time-to-value for private wireless deployment. Integrated Private Wireless on AWS removes the long planning cycles and complex integrations usually required to set up and scale a private wireless network. You can now deploy a secure, reliable, and low-latency private wireless network to power AI/ML and IoT workloads at the edge and at scale.
Quantum technologies
Integrated Private Wireless on AWS
121
Overview of Amazon Web Services AWS Whitepaper
Amazon Braket
Amazon Braket is a fully managed quantum computing service that helps researchers and developers get started with the technology to accelerate research and discovery. Amazon Braket provides a development environment for you to explore and build quantum algorithms, test them on quantum circuit simulators, and run them on different quantum hardware technologies.
Quantum computing has the potential to solve computational problems that are beyond the reach of classical computers by harnessing the laws of quantum mechanics to process information in new ways. This approach to computing could transform areas such as chemical engineering, material science, drug discovery, financial portfolio optimization, and machine learning. But defining those problems and programming quantum computers to solve them requires new skills, which are difficult to acquire without easy access to quantum computing hardware.
Amazon Braket overcomes these challenges so you can explore quantum computing. With Amazon Braket, you can design and build your own quantum algorithms from scratch or choose from a set of pre-built algorithms. Once you have built your algorithm, Amazon Braket provides a choice of simulators to test, troubleshoot and run your algorithms. When you are ready, you can run your algorithm on your choice of different quantum computers, and gate-based computers from Rigetti and IonQ. With Amazon Braket, you can now evaluate the potential of quantum computing for your organization, and build expertise.
Robotics
AWS RoboMaker
AWS RoboMaker is a service that makes it easy to develop, test, and deploy intelligent robotics applications at scale. AWS RoboMaker extends the most widely used open-source robotics software framework, Robot Operating System (ROS), with connectivity to cloud services. This includes AWS machine learning services, monitoring services, and analytics services that enable a robot to stream data, navigate, communicate, comprehend, and learn. AWS RoboMaker provides a robotics development environment for application development, a
Robotics
122
Overview of Amazon Web Services AWS Whitepaper
robotics simulation service to accelerate application testing, and a robotics fleet management service for remote application deployment, update, and management.
Robots are machines that sense, compute, and take action. Robots need instructions to accomplish tasks, and these instructions come in the form of applications that developers code to determine how the robot will behave. Receiving and processing sensor data, controlling actuators for movement, and performing a specific task are all functions that are typically automated by these intelligent robotics applications. Intelligent robots are being increasingly used in warehouses to distribute inventory, in homes to carry out tedious housework, and in retail stores to provide customer service. Robotics applications use machine learning in order to perform more complex tasks like recognizing an object or face, having a conversation with a person, following a spoken command, or navigating autonomously.
Until now, developing, testing, and deploying intelligent robotics applications was difficult and time consuming. Building intelligent robotics functionality using machine learning is complex and requires specialized skills. Setting up a development environment can take each developer days and building a realistic simulation system to test an application can take months due to the underlying infrastructure needed. Once an application has been developed and tested, a developer needs to build a deployment system to deploy the application into the robot and later update the application while the robot is in use.
AWS RoboMaker provides you with the tools to make building intelligent robotics applications more accessible, a fully managed simulation service for quick and easy testing, and a deployment service for lifecycle management. AWS RoboMaker removes the heavy lifting from each step of robotics development so you can focus on creating innovative robotics applications.
Satellite
AWS Ground Station
AWS Ground Station is a fully managed service that lets you control satellite communications, downlink and process satellite data, and scale your satellite operations quickly, easily and cost-effectively without having to worry about building or managing your own ground station
Satellite
123
Overview of Amazon Web Services AWS Whitepaper
infrastructure. Satellites are used for a wide variety of use cases, including weather forecasting, surface imaging, communications, and video broadcasts. Ground stations are at the core of global satellite networks, which are facilities that provide communications between the ground and the satellites by using antennas to receive data and control systems to send radio signals to command and control the satellite. Today, you must either build your own ground stations and antennas, or obtain long-term leases with ground station providers, often in multiple countries to provide enough opportunities to contact the satellites as they orbit the globe. Once all this data is downloaded, you need servers, storage, and networking in close proximity to the antennas to process, store, and transport the data from the satellites.
AWS Ground Station eliminates these problems by delivering a global ground station as a service. We provide direct access to AWS services and the AWS Global Infrastructure including our low-latency global fiber network right where your data is downloaded into our AWS Ground Station. This enables you to easily control satellite communications, quickly ingest and process your satellite data, and rapidly integrate that data with your applications and other services running in the AWS Cloud. For example, you can use Amazon S3 to store the downloaded data, Amazon Kinesis Data Streams for managing data ingestion from satellites, SageMaker AI for building custom machine learning applications that apply to your data sets, and Amazon EC2 to command and download data from satellites. AWS Ground Station can help you save up to 80% on the cost of your ground station operations by allowing you to pay only for the actual antenna time used, and relying on our global footprint of ground stations to download data when and where you need it, instead of building and operating your own global ground station infrastructure. There are no long-term commitments, and you gain the ability to rapidly scale your satellite communications on-demand when your business needs it.
Security, identity, and compliance
AWS is architected to be the most secure global cloud infrastructure on which to build, migrate, and manage applications and workloads.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing AWS security, identity, and governance services. For general information, seeSecurity, Identity, and Compliance on AWS.
Security, identity, and compliance
124
Overview of Amazon Web Services AWS Whitepaper
Services
•
Amazon Cognito
•
Amazon Detective
•
Amazon GuardDuty
•
Amazon Inspector
•
Amazon Macie
•
Amazon Security Lake
•
Amazon Verified Permissions
•
AWS Artifact
•
AWS Audit Manager
•
AWS Certificate Manager
•
AWS CloudHSM
•
AWS Directory Service
•
AWS Firewall Manager
•
AWS Identity and Access Management
•
AWS Key Management Service
Security, identity, and compliance
125
Overview of Amazon Web Services AWS Whitepaper
•
AWS Network Firewall
•
AWS Resource Access Manager
•
AWS Secrets Manager
•
AWS Security Hub
•
AWS Shield
•
AWS IAM Identity Center
•
AWS WAF
•
AWS WAF Captcha
Amazon Cognito
Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. With Amazon Cognito, you can scale to millions of users and supports sign-in with social identity providers such as Apple, Facebook, Twitter, or Amazon, with SAML 2.0 identity solutions, or by using your own identity system.
In addition, Amazon Cognito enables you to save data locally on users’ devices, allowing your applications to work even when the devices are offline. You can then synchronize data across users’ devices so that their app experience remains consistent regardless of the device they use.
With Amazon Cognito, you can focus on creating great app experiences instead of worrying about building, securing, and scaling a solution to handle user management, authentication, and sync across devices.
Amazon Detective
Amazon Detective makes it easy to analyze, investigate, and quickly identify the root cause of potential security issues or suspicious activities. Amazon Detective automatically collects log data from your AWS resources and uses machine learning, statistical analysis, and graph theory to build a linked set of data that enables you to easily conduct faster and more efficient security investigations. Amazon Detective further simplifies account management for security operations and investigations across all existing and future accounts in an organization using AWS Organizations for up to 1,200 AWS accounts.
AWS security services such as Amazon GuardDuty, Amazon Macie, and AWS Security Hub, as well as partner security products, can be used to identify potential security issues, or findings. These
Amazon Cognito
126
Overview of Amazon Web Services AWS Whitepaper
services are really helpful in alerting you when and where there is possible unauthorized access or suspicious behavior in your AWS deployment. However, sometimes there are security findings that you would like to perform deeper investigations of the events that led to the findings to remediate the root cause. Determining the root cause of security findings can be a complex process for security analysts that often involves collecting and combining logs from many data sources, using extract, transform, and load (ETL) tools, and custom scripting to organize the data.
Amazon Detective simplifies this process by enabling your security teams to easily investigate and quickly get to the root cause of a finding. Detective can analyze trillions of events from multiple data sources such as Amazon Virtual Private Cloud (VPC) Flow Logs, AWS CloudTrail, and Amazon GuardDuty. Detective uses these events to automatically create a unified, interactive view of your resources, users, and the interactions between them over time. With this unified view, you can visualize all the details and context in one place to identify the underlying reasons for the findings, drill down into relevant historical activities, and quickly determine the root cause.
You can get started with Amazon Detective in just a few clicks in the AWS Management Console. There is no software to deploy, or data sources to enable and maintain. You can try Detective at no additional charge with a 30-day free trial that is available to new accounts.
Amazon GuardDuty
Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and anomalous behavior to protect your AWS accounts, workloads, Kubernetes clusters, and data stored in Amazon Simple Storage Service (Amazon S3). The GuardDuty service monitors for activity such as unusual API calls, unauthorized deployments, and exfiltrated credentials that indicate a possible account reconnaissance or compromise.
Enabled with a few clicks in the AWS Management Console and easily administrated organization-wide with its support of AWS Organizations, Amazon GuardDuty can immediately begin analyzing billions of events across your AWS accounts for signs of unauthorized use. GuardDuty identifies suspected attackers through integrated threat intelligence feeds and machine learning anomaly detection to detect anomalies in account and workload activity. When potential unauthorized use is detected, the service delivers a detailed finding to the GuardDuty console, Amazon CloudWatch Events, and AWS Security Hub. This makes findings actionable and easy to integrate into existing event management and workflow systems. Further investigation to determine the root cause of a finding is easily accomplished by using Amazon Detective directly from the GuardDuty console.
Amazon GuardDuty is cost effective and easy to operate. It does not require you to deploy and maintain software or security infrastructure, meaning it can be enabled quickly with no risk of
Amazon GuardDuty
127
Overview of Amazon Web Services AWS Whitepaper
negatively impacting existing application and container workloads. There are no upfront costs with GuardDuty, no software to deploy, and no threat intelligence feeds to enable. Furthermore, GuardDuty optimizes costs by applying smart filters and analyzing only a subset of logs relevant to threat detection, and new Amazon GuardDuty accounts are free for 30 days.
Amazon Inspector
Amazon Inspector is a new automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. With a few clicks in the AWS Management Console and AWS Organizations, Amazon Inspector can be used across all accounts in your organization. Once started, Amazon Inspector automatically discovers running Amazon Elastic Compute Cloud (Amazon EC2) instances and container images residing in Amazon Elastic Container Registry (Amazon ECR), at any scale, and immediately starts assessing them for known vulnerabilities.
Amazon Inspector has many improvements over Amazon Inspector Classic. For example, the new Amazon Inspector calculates a highly contextualized risk score for each finding by correlating common vulnerabilities and exposures (CVE) information with factors such as network access and exploitability. This score is used to prioritize the most critical vulnerabilities to improve remediation response efficiency. Additionally, Amazon Inspector now uses the widely deployed AWS Systems Manager Agent (SSM Agent) to eliminate the need for you to deploy and maintain a standalone agent to run Amazon EC2 instance assessments. For container workloads, Amazon Inspector is now integrated with Amazon Elastic Container Registry (Amazon ECR) to support intelligent, cost-efficient, and continual vulnerability assessments of container images. All findings are aggregated in the Amazon Inspector console, routed to AWS Security Hub, and pushed through Amazon EventBridge to automate workflows such as ticketing.
All accounts new to Amazon Inspector are eligible for a 15-day free trial to evaluate the service and estimate its cost. During the trial, all eligible Amazon EC2 instances and container images pushed to Amazon ECR are continually scanned at no cost.
Amazon Macie
Amazon Macie is a fully managed data security and data privacy service that uses inventory evaluations, machine learning, and pattern matching to discover sensitive data and accessibility in your Amazon S3 environment. Macie supports scalable on-demand and automated sensitive data discovery jobs that automatically tracks changes to the bucket and only evaluates new or modified objects over time. Using Macie, you can detect a large and growing list of sensitive data
Amazon Inspector
128
Overview of Amazon Web Services AWS Whitepaper
types for many countries and Regions, including multiple types of financial data, personal health information (PHI), and personally identifiable information (PII), as well as custom types. Macie also continually evaluates your Amazon S3 environment to provide an S3 resource summary and security evaluation across all of your accounts. You can search, filter, and sort S3 buckets by metadata variables, such as bucket names, tags, and security controls like encryption status or public accessibility. For any unencrypted buckets, publicly accessible buckets, or buckets shared with AWS accounts outside those you have defined in AWS Organizations, you can be alerted to act.
In the multi-account configuration, a single Macie administrator account can manage all member accounts, including the creation and administration of sensitive data discovery jobs across accounts with AWS Organizations. Security and sensitive data discovery findings are aggregated in the Macie administrator account and sent to Amazon CloudWatch Events and AWS Security Hub. Now using one account, you can integrate with event management, workflow, and ticketing systems or use Macie findings with AWS Step Functions to automate remediation actions. You can quickly get started with Macie using the 30-day trial available to new accounts for S3 bucket inventory and bucket-level evaluation at no charge. Sensitive data discovery is not included in the 30-day trial for bucket evaluation.
Amazon Security Lake
Amazon Security Lake centralizes security data from AWS environments, SaaS providers, on premises, and cloud sources, into a purpose-built data lake that's stored in your AWS account. Security Lake automates the collection and management of security data across accounts and AWS Regions so that you can use your preferred analytics tools while retaining control and ownership over your security data. With Security Lake, you can also improve the protection of your workloads, applications, and data.
Security Lake automates the collection of security-related log and event data from integrated AWS services and third-party services. It also helps you manage the lifecycle of data with customizable retention settings. The data lake is backed by Amazon S3 buckets, and you retain ownership over your data. Security Lake converts ingested data into Apache Parquet format and a standard open-source schema called the Open Cybersecurity Schema Framework (OCSF). With OCSF support, Security Lake normalizes and combines security data from AWS and a broad range of enterprise security data sources.
Other AWS services and third-party services can subscribe to the data that's stored in Security Lake for incident response and security data analytics.
Amazon Security Lake
129
Overview of Amazon Web Services AWS Whitepaper
Amazon Verified Permissions
Amazon Verified Permissions is a scalable, fine-grained permissions management and authorization service for custom applications you've built. Verified Permissions enables your developers to build secure applications faster by externalizing authorization and centralizing policy management and administration.
Verified Permissions uses Cedar, an open-source policy language and SDK, to define fine-grained permissions for application users. Your authorization model is defined using principal types, resource types, and valid actions, to control who can take what actions on which resources in a given application context. Policy changes are audited so that you can see who made the changes and when.
AWS Artifact
AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS security and compliance reports and select online agreements. Reports available in AWS Artifact include our Service Organization Control (SOC) reports, Payment Card Industry (PCI) reports, and certifications from accreditation bodies across geographies and compliance verticals that validate the implementation and operating effectiveness of AWS security controls. Agreements available in AWS Artifact include the Business Associate Addendum (BAA) and the Nondisclosure Agreement (NDA).
AWS Audit Manager
AWS Audit Manager helps you continuously audit your AWS usage to simplify how you assess risk and compliance with regulations and industry standards. Audit Manager automates evidence collection to reduce the “all hands on deck” manual effort that often happens for audits and enable you to scale your audit capability in the cloud as your business grows. With Audit Manager, it is easy to assess if your policies, procedures, and activities – also known as controls – are operating effectively. When it is time for an audit, AWS Audit Manager helps you manage stakeholder reviews of your controls and enables you to build audit-ready reports with much less manual effort.
The AWS Audit Manager prebuilt frameworks help translate evidence from cloud services into auditor-friendly reports by mapping your AWS resources to the requirements in industry standards or regulations, such as CIS AWS Foundations Benchmark, the General Data Protection Regulation (GDPR), and the Payment Card Industry Data Security Standard (PCI DSS). You can also fully customize a framework and its controls for your unique business requirements. Based on the
Amazon Verified Permissions
130
Overview of Amazon Web Services AWS Whitepaper
framework you select, Audit Manager launches an assessment that continuously collects and organizes relevant evidence from your AWS accounts and resources, such as resource configuration snapshots, user activity, and compliance check results.
You can get started quickly in the AWS Management Console. Just select a prebuilt framework to launch an assessment and begin automatically collecting and organizing evidence.
AWS Certificate Manager
AWS Certificate Manager is a service that lets you easily provision, manage, and deploy Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates.
With AWS Certificate Manager, you can quickly request a certificate, deploy it on ACM-integrated AWS resources, such as Elastic Load Balancing, Amazon CloudFront distributions, and APIs on API Gateway, and let AWS Certificate Manager handle certificate renewals. It also enables you to create private certificates for your internal resources and manage the certificate lifecycle centrally. Public and private certificates provisioned through AWS Certificate Manager for use with ACM-integrated services are free. You pay only for the AWS resources you create to run your application.
With AWS Private Certificate Authority, you pay monthly for the operation of the private certificate authority (CA) and for the private certificates you issue. you have a highly available private CA service without the upfront investment and ongoing maintenance costs of operating your own private CA.
AWS CloudHSM
The AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. With AWS CloudHSM, you can manage your own encryption keys using dedicated FIPS 140-2 Level 3 validated HSMs. AWS CloudHSM offers you the flexibility to integrate with your applications using industry-standard APIs, such as PKCS#11, Java Cryptography Extensions (JCE), and Microsoft CryptoNG (CNG) libraries.
AWS CloudHSM is standards-compliant and enables you to export all of your keys to most other commercially-available HSMs, subject to your configurations. It is a fully managed service that
AWS Certificate Manager
131
Overview of Amazon Web Services AWS Whitepaper
automates time-consuming administrative tasks for you, such as hardware provisioning, software patching, high-availability, and backups. AWS CloudHSM also enables you to scale quickly by adding and removing HSM capacity on-demand, with no up-front costs.
AWS Directory Service
AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud. AWS Managed Microsoft AD is built on actual Microsoft Active Directory and does not require you to synchronize or replicate data from your existing Active Directory to the cloud. You can use standard Active Directory administration tools and take advantage of built-in Active Directory features such as Group Policy and single sign-on (SSO). With AWS Managed Microsoft AD, you can easily join Amazon EC2 and Amazon RDS for SQL Server instances to a domain, and use AWS Enterprise IT applications such as Amazon WorkSpaces with Active Directory users and groups.
AWS Firewall Manager
AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure, from a central administrator account.
AWS Identity and Access Management
AWS Identity and Access Management (IAM) enables you to securely control access to AWS services and resources for your AWS users, groups, and roles. Using IAM, you can create and manage fine-grained access controls with permissions, specify who can access which services and resources, and under which conditions. IAM allows you to do the following:
•
You manage AWS permissions for your workforce users and workloads in AWS IAM Identity Center (IAM Identity Center). IAM Identity Center allows you to manage user access across multiple AWS accounts. With just a few clicks, you can enable a highly available service, easily manage multi-account access and the permissions to all of your accounts in AWS Organizationscentrally. IAM Identity Center includes built-in SAML integrations to many business applications, such as Salesforce, Box, and Microsoft Office 365. Further, you can create Security Assertion Markup Language (SAML) 2.0 integrations and extend single sign-on access to any of your SAML-
AWS Directory Service
132
Overview of Amazon Web Services AWS Whitepaper
enabled applications. Your users simply sign in to a user portal with credentials they configure or using their existing corporate credentials to access all their assigned accounts and applications from one place.
•
Manage single-account IAM permissions: You can specify access to AWS resources using permissions. Your IAM entities (users, groups, and roles) by default start with no permissions. These identities can be granted permissions by attaching an IAM policy that specifies the type of access, the actions that can be performed, and the resources on which actions can be performed. You can also specify conditions that must be set for access to be allowed or denied.
•
Manage single-account IAM roles: IAM roles allows you to delegate access to users or services that normally don’t have access to your organization’s AWS resources. IAM users or AWS services can assume a role to obtain a temporary security credential that be used to make AWS API calls. You don’t have to share long-term credentials or define permissions for each identity.
AWS Key Management Service
AWS Key Management Service (AWS KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. AWS KMS uses hardware security modules (HSM) to protect and validate your AWS KMS keys under the FIPS 140-2 Cryptographic Module Validation Program. AWS KMS is integrated with AWS CloudTrail to provide you with logs of all key usage to help meet your regulatory and compliance needs.
AWS Network Firewall
AWS Network Firewall is a managed service that makes it easy to deploy essential network protections for all of your Amazon Virtual Private Clouds (VPCs). The service can be setup with just a few clicks and scales automatically with your network traffic, so you don't have to worry about deploying and managing any infrastructure. The AWS Network Firewall flexible rules engine lets you define firewall rules that give you fine-grained control over network traffic, such as blocking outbound Server Message Block (SMB) requests to prevent the spread of malicious activity. You can also import rules you’ve already written in common open source rule formats as well as enable integrations with managed intelligence feeds sourced by AWS Partners. AWS Network Firewall works together with AWS Firewall Manager so you can build policies based on AWS Network Firewall rules and then centrally apply those policies across your VPCs and accounts.
AWS Network Firewall includes features that provide protections from common network threats. The AWS Network Firewall stateful firewall can incorporate context from traffic flows, such as
AWS Key Management Service
133
Overview of Amazon Web Services AWS Whitepaper
tracking connections and protocol identification, to enforce policies such as preventing your VPCs from accessing domains using an unauthorized protocol. The AWS Network Firewall intrusion prevention system (IPS) provides active traffic flow inspection so you can identify and block vulnerability exploits using signature-based detection. AWS Network Firewall also offers web filtering that can stop traffic to known bad URLs and monitor fully qualified domain names.
It’s easy to get started with AWS Network Firewall by visiting the Amazon VPC Console to create or import your firewall rules, group them into policies, and apply them to the VPCs you want to protect. AWS Network Firewall pricing is based on the number of firewalls deployed and the amount of traffic inspected. There are no upfront commitments and you pay only for what you use.
AWS Resource Access Manager
AWS Resource Access Manager (AWS RAM) helps you securely share your resources across AWS accounts, within your organization or organizational units (OUs) in AWS Organizations, and with IAM roles and IAM users for supported resource types. You can use AWS RAM to share transit gateways, subnets, AWS License Manager license configurations, Amazon Route 53 Resolver rules, and more resource types.
Many organizations use multiple accounts to create administrative or billing isolation, and to limit the impact of errors. With AWS RAM, you don’t need to create duplicate resources in multiple AWS accounts. This reduces the operational overhead of managing resources in every account that you own. Instead, in your multi-account environment, you can create a resource once, and use AWS RAM to share that resource across accounts by creating a resource share. When you create a resource share, you select the resources to share, choose an AWS RAM managed permission per resource type, and specify whom you want to have access to the resources. AWS RAM is available to you at no additional charge.
AWS Secrets Manager
AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call toSecrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB. The service is also extensible to other types of secrets, including API keys and OAuth tokens. In addition, Secrets Manager enables you to control access to secrets using fine-grained permissions and audit secret rotation centrally for resources in the AWS Cloud, third-party services, and on-premises.
AWS Resource Access Manager
134
Overview of Amazon Web Services AWS Whitepaper
AWS Security Hub
AWS Security Hub is a cloud security posture management service that performs automated, continuous security best practice checks against your AWS resources. Security Hub aggregates your security alerts (i.e. findings) from various AWS services and partner products in a standardized format so that you can more easily take action on them. To maintain a complete view of your security posture in AWS, you need to integrate multiple tools and services including threat detections from Amazon GuardDuty, vulnerabilities from Amazon Inspector, sensitive data classifications from Amazon Macie, resource configuration issues from AWS Config, and AWS Partner Network products. Security Hub simplifies how you understand and improve your security posture with automated security best practice checks powered by AWS Config rules and automated integrations with dozens of AWS services and partner products.
Security Hub enables you to understand your overall security posture via a consolidated security score across all of your AWS accounts, automatically assesses the security of your AWS accounts resources via the AWS Foundational Security Best Practices (FSBP) standard and other compliance frameworks. It also aggregates all of your security findings from dozens of AWS security services and APN products in a single place and format via the AWS Security Finding Format (ASFF), and reduces your Mean Time To Remediation (MTTR) with automated response and remediationsupport. Security Hub has out-of-the-box integrations with ticketing, chat, Security Information and Event Management (SIEM), Security Orchestration Automation and Response (SOAR), threat investigation, Governance Risk and Compliance (GRC), and incident management tools to provide your users with a complete security operations workflow.
Getting started with Security Hub requires just a few clicks from the AWS Management Console to begin aggregating findings and conducting security checks using our 30-day free trial. You can integrate Security Hub with AWS Organizations to automatically enable the service in all accounts in your organization.
AWS Shield
AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards web applications running on AWS. AWS Shield provides you with always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage Support to benefit from DDoS protection. There are two tiers of AWS Shield: Standard and Advanced.
All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against most common, frequently occurring
AWS Security Hub
135
Overview of Amazon Web Services AWS Whitepaper
network and transport layer DDoS attacks that target your website or applications. When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks.
For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (Amazon EC2), Elastic Load Balancing (ELB), Amazon CloudFront, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall. AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS related spikes in your Amazon Elastic Compute Cloud (Amazon EC2), Elastic Load Balancing (ELB), Amazon CloudFront, and Amazon Route 53 charges.
AWS Shield Advanced is available globally on all Amazon CloudFront and Amazon Route 53 edge locations. You can protect your web applications hosted anywhere in the world by deploying Amazon CloudFront in front of your application. Your origin servers can be Amazon S3, Amazon Elastic Compute Cloud (Amazon EC2), Elastic Load Balancing (ELB), or a custom server outside of AWS. You can also enable AWS Shield Advanced directly on an Elastic IP or Elastic Load Balancing (ELB) in the following AWS Regions: Northern Virginia, Ohio, Oregon, Northern California, Montreal, São Paulo, Ireland, Frankfurt, London, Paris, Stockholm, Singapore, Tokyo, Sydney, Seoul, Mumbai, Milan, and Cape Town.
AWS IAM Identity Center
AWS IAM Identity Center (SSO) is a cloud SSO service that makes it easy to centrally manage SSO access to multiple AWS accounts and business applications. With just a few clicks, you can enable a highly available SSO service without the upfront investment and on-going maintenance costs of operating your own SSO infrastructure. With IAM Identity Center, you can easily manage SSO access and user permissions to all of your accounts in AWS Organizations centrally. IAM Identity Center also includes built-in SAML integrations to many business applications, such as Salesforce, Box, and Microsoft Office 365. Further, by using the IAM Identity Center application configuration wizard, you can create Security Assertion Markup Language (SAML) 2.0 integrations and extend SSO access to any of your SAML-enabled applications. Your users simply sign in to a user portal with credentials they configure in IAM Identity Center or using their existing corporate credentials to access all their assigned accounts and applications from one place.
AWS IAM Identity Center
136
Overview of Amazon Web Services AWS Whitepaper
AWS WAF
AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. You can also customize rules that filter out specific traffic patterns. You can get started quickly using Managed Rules for AWS WAF, a pre-configured set of rules managed by AWS or AWS Marketplace sellers to address issues like the OWASP Top 10 security risks and automated bots that consume excess resources, skew metrics, or can cause downtime. These rules are regularly updated as new issues emerge. AWS WAF includes a full-featured API that you can use to automate the creation, deployment, and maintenance of security rules.
AWS WAF Captcha
AWS WAF Captcha helps block unwanted bot traffic by requiring users to successfully complete challenges before their web request are allowed to reach AWS WAF protected resources. You can configure AWS WAF rules to require WAF Captcha challenges to be solved for specific resources that are frequently targeted by bots such as login, search, and form submissions. You can also require WAF Captcha challenges for suspicious requests based on the rate, attributes, or labels generated from AWS Managed Rules, such as AWS WAF Bot Control or the Amazon IP Reputation list. WAF Captcha challenges are simple for humans while remaining effective against bots. WAF Captcha includes an audio version and is designed to meet Web Content Accessability Guidelines (WCAG) accessibility requirements.
Storage
AWS provides a broad portfolio of storage services with deep functionality for storing, accessing, protecting, and analyzing your data.
Each service is described after the diagram. To help you decide which service best meets your needs, see Choosing an AWS storage service. For general information, see Cloud Storage on AWS.
AWS WAF
137
Overview of Amazon Web Services AWS Whitepaper
Services
•
AWS Backup
•
Amazon Elastic Block Store
•
AWS Elastic Disaster Recovery
•
Amazon Elastic File System
•
Amazon File Cache
•
Amazon FSx for Lustre
•
Amazon FSx for NetApp ONTAP
•
Amazon FSx for OpenZFS
•
Amazon FSx for Windows File Server
•
Amazon Simple Storage Service
•
AWS Storage Gateway
Storage
138
Overview of Amazon Web Services AWS Whitepaper
AWS Backup
AWS Backup enables you to centralize and automate data protection across AWS services. AWS Backup offers a cost-effective, fully managed, policy-based service that further simplifies data protection at scale. AWS Backup also helps you support your regulatory compliance or business policies for data protection. Together with AWS Organizations, AWS Backup enables you to centrally deploy data protection policies to configure, manage, and govern your backup activity across your organization’s AWS accounts and resources, including Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon Elastic Block Store (Amazon EBS) volumes, Amazon Relational Database Service (Amazon RDS) databases (including Amazon Aurora clusters), Amazon DynamoDB tables, Amazon Elastic File System (Amazon EFS) file systems, Amazon FSx for Lustre file systems, Amazon FSx for Windows File Server file systems, and AWS Storage Gateway volumes.
Amazon Elastic Block Store
Amazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure, offering high availability and durability. Amazon EBS volumes offer the consistent and low-latency performance needed to run your workloads. With Amazon EBS, you can scale your usage up or down within minutes—all while paying a low price for only what you provision.
AWS Elastic Disaster Recovery
AWS Elastic Disaster Recovery (Elastic Disaster Recovery) minimizes downtime and data loss with fast, reliable recovery of on-premises and cloud-based applications using affordable storage, minimal compute, and point-in-time recovery. You can configure replication and launch settings, monitor data replication, and launch instances for drills or recovery.
Set up Elastic Disaster Recovery on your source servers to initiate secure data replication. Your data is replicated to a staging area subnet in your AWS account, in the AWS Region that you select. You can perform non-disruptive tests to confirm that implementation is complete. During normal operation, maintain readiness by monitoring replication and periodically performing non-disruptive recovery and failback drills.
If you must replicate to the AWS China Regions or perform replication and recovery into AWS Outposts, use CloudEndure Disaster Recovery available in the AWS Marketplace.
AWS Backup
139
Overview of Amazon Web Services AWS Whitepaper
Amazon Elastic File System
Amazon Elastic File System (Amazon EFS) provides a simple, scalable, elastic file system for Linux-based workloads for use with AWS Cloud services and on-premises resources. It is built to scale on demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, so your applications have the storage they need – when they need it. It is designed to provide massively parallel shared access to thousands of Amazon EC2 instances, enabling your applications to achieve high levels of aggregate throughput and IOPS with consistent low latencies. Amazon EFS is a fully managed service that requires no changes to your existing applications and tools, providing access through a standard file system interface for seamless integration. Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability. You can access your file systems across Availability Zones and AWS Regions and share files between thousands of Amazon EC2 instances and on-premises servers via AWS Direct Connect or AWS VPN.
Amazon EFS is well suited to support a broad spectrum of use cases from highly parallelized, scale-out workloads that require the highest possible throughput to single-threaded, latency-sensitive workloads. Use cases such as lift-and-shift enterprise applications, big data analytics, web serving and content management, application development and testing, media and entertainment workflows, database backups, and container storage.
For long-lived data that is accessed only a few times a year or less, consider Amazon EFS Archive, a cost-effective way to retain even your coldest data so that it's always available to power new business insights. Amazon EFS Archive supports the same intelligent tiering experience as existing EFS storage classes. This means that you can combine the sub-millisecond SSD latencies of Amazon EFS Standard for your active frequently-accessed data with the lower costs of Amazon EFS IA and Amazon EFS Archive for your colder data.
Amazon File Cache
Amazon File Cache is a fully managed high-speed cache on AWS that makes it easier to process file data, regardless of where the data is stored. Amazon File Cache serves as temporary, high-performance storage for data in on-premises file systems, or in file systems or object stores on AWS. The service allows you to make dispersed datasets available to file-based applications on AWS with a unified view and high speeds. You can link the cache to multiple NFS—including on-premises and in-cloud—or Amazon Simple Storage Service (Amazon S3) buckets, providing a unified view of and fast access to your data spanning on-premises and multiple AWS Regions. The
Amazon Elastic File System
140
Overview of Amazon Web Services AWS Whitepaper
cache provides read and write data access to compute workloads on AWS with sub-millisecond latencies, up to hundreds of GB/s of throughput, and up to millions of IOPS.
Amazon FSx for Lustre
Amazon FSx for Lustre is a fully managed file system that is optimized for compute-intensive workloads, such as high performance computing, machine learning, and media data processing workflows. Many of these applications require the high-performance and low latencies of scale-out, parallel file systems. Operating these file systems typically requires specialized expertise and administrative overhead, requiring you to provision storage servers and tune complex performance parameters. With Amazon FSx, you can launch and run a Lustre file system that can process massive data sets at up to hundreds of gigabytes per second of throughput, millions of IOPS, and sub-millisecond latencies.
Amazon FSx for Lustre is seamlessly integrated with Amazon S3, making it easy to link your long-term data sets with your high performance file systems to run compute-intensive workloads. You can automatically copy data from S3 to Amazon FSx for Lustre, run your workloads, and then write results back to S3. Amazon FSx for Lustre also enables you to burst your compute-intensive workloads from on-premises to AWS by allowing you to access your FSx file system over Amazon Direct Connect or VPN. Amazon FSx for Lustre helps you cost-optimize your storage for compute-intensive workloads: It provides cheap and performant non-replicated storage for processing data, with your long-term data stored durably in Amazon S3 or other low-cost data stores. With Amazon FSx, you pay for only the resources you use. There are no minimum commitments, upfront hardware or software costs, or additional fees.
Amazon FSx for NetApp ONTAP
Amazon FSx for NetApp ONTAP offers the first complete, fully managed NetApp file system available in the cloud making it easy for you to migrate or extend existing applications to AWS without changing code or how you manage your data . Built on NetApp ONTAP, Amazon FSx for NetApp ONTAP provides the familiar features, performance, capabilities, and APIs of NetApp file systems with the agility, scalability, and simplicity of a fully managed AWS service.
Amazon FSx for NetApp ONTAP offers high-performance file storage that is broadly accessible from Linux, Windows, and macOS compute instances via the industry-standard NFS, SMB, and iSCSI protocols. With Amazon FSx for NetApp ONTAP, you get low-cost, fully elastic storage capacity with support for compression and deduplication to help you further reduce storage costs. Amazon FSx for NetApp ONTAP file systems can be deployed and managed using the AWS Management Console or NetApp Cloud Manager for seamless set up and administration.
Amazon FSx for Lustre
141
Overview of Amazon Web Services AWS Whitepaper
Amazon FSx for OpenZFS
Amazon FSx for OpenZFS is a fully managed file storage service that lets you launch, run, and scale fully managed file systems built on the open-source OpenZFS file system. Amazon FSx for OpenZFS makes it easy to migrate your on-premises file servers—without changing your applications or how you manage data—and build new high-performance, data-driven applications in the cloud.
Amazon FSx for OpenZFS offers the familiar features, performance, and capabilities of OpenZFS file systems with the agility, scalability, and simplicity of a fully managed AWS service.
Amazon FSx for Windows File Server
Amazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system so you can easily move your Windows-based applications that require file storage to AWS. Built on Windows Server, Amazon FSx provides shared file storage with the compatibility and features that your Windows-based applications rely on, including full support for the SMB protocol and Windows NTFS, Active Directory (AD) integration, and Distributed File System (DFS). Amazon FSx uses SSD storage to provide the fast performance your Windows applications and users expect, with high levels of throughput and IOPS, and consistent sub-millisecond latencies. This compatibility and performance is particularly important when moving workloads that require Windows shared file storage, such as CRM, ERP, and .NET applications, as well as home directories.
With Amazon FSx, you can launch highly durable and available Windows file systems that can be accessed from up to thousands of compute instances using the industry-standard SMB protocol. Amazon FSx eliminates the typical administrative overhead of managing Windows file servers. You pay for only the resources used, with no upfront costs, minimum commitments, or additional fees.
Amazon Simple Storage Service
Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides easy-to-use management features so you can organize your data and configure finely-tuned access controls to meet your specific business, organizational, and compliance requirements. Amazon S3 is designed for 99.999999999% (11 9s) of durability, and stores data for millions of applications for companies all around the world.
Amazon FSx for OpenZFS
142
Overview of Amazon Web Services AWS Whitepaper
Amazon S3 storage classes are a range of storage classes that you can choose from based on the data access, resiliency, and cost requirements of your workloads. S3 storage classes are purpose-built to provide the lowest cost storage for different access patterns. S3 storage classes are ideal for virtually any use case, including those with demanding performance needs, data residency requirements, unknown or changing access patterns, or archival storage.
The S3 storage classes include:
•
S3 Intelligent-Tiering for automatic cost savings for data with unknown or changing access patterns
•
S3 Standard for frequently accessed data
•
S3 Express One Zone for your most frequently accessed data
•
S3 Standard-Infrequent Access (S3 Standard-IA) and S3 One Zone-Infrequent Access (S3 One Zone-IA) for less frequently accessed data
•
S3 Glacier Instant Retrieval for archive data that needs immediate access
•
S3 Glacier Flexible Retrieval (formerly S3 Glacier) for rarely accessed long-term data that does not require immediate access
•
Amazon S3 Glacier Deep Archive (S3 Glacier Deep Archive) for long-term archive and digital preservation with retrieval in hours at the lowest cost storage in the cloud
If you have data residency requirements that can’t be met by an existing AWS Region, you can use the S3 Outposts storage class to store your S3 data on premises. Amazon S3 also offers capabilities to manage your data throughout its lifecycle. Once an S3 Lifecycle policy is set, your data will automatically transfer to a different storage class without any changes to your application. For more information, refer to the Amazon S3 storage classes overview info graphic.
You can use S3 Object Lock to help prevent S3 objects from being deleted or overwritten for a fixed amount of time, or indefinitely. Object Lock can help you to meet regulatory requirements that require WORM (write-once-read-many) storage, or to simply add another layer of protection against object changes or deletion.
AWS Storage Gateway
The AWS Storage Gateway is a hybrid storage service that allows your on-premises applications to seamlessly use AWS cloud storage. You can use the service for backup and archiving, disaster recovery, cloud data processing, storage tiering, and migration. Your applications connect to the service through a virtual machine or hardware gateway appliance using standard storage
AWS Storage Gateway
143
Overview of Amazon Web Services AWS Whitepaper
protocols, such as NFS, SMB and iSCSI. The gateway connects to AWS storage services, such as Amazon S3, S3 Glacier, and Amazon EBS, and Amazon FSx for Windows File Server, providing storage for files, volumes, and virtual tapes in AWS. The service includes a highly-optimized data transfer mechanism, with bandwidth management, automated network resilience, and efficient data transfer, along with a local cache for low-latency on-premises access to your most active data.
AWS Storage Gateway
144
Overview of Amazon Web Services AWS Whitepaper
Next steps
Reinvent how you work with IT by signing up for the AWS Free Tier, which allows you to gain hands-on experience with a broad selection of AWS products and services. Within the AWS Free Tier, you can test workloads and run applications to learn more and build the right solution for your organization. You can also contact AWS Sales and Business Development.
By signing up for AWS, you have access to Amazon cloud computing services.
Note
The sign-up process requires a credit card, which will not be charged until you start using services. There are no long-term commitments and you can stop using AWS at any time.
To help familiarize yourself with AWS, check out AWS Skill Builder to explore free, on-demand courses developed by the experts at AWS.
Learn about the breadth and depth of AWS on our general AWS Channel and AWS Online Tech Talks.
Get hands-on experience from our self-paced labs.
Are you Well-Architected?
Explore the AWS Well-Architected Framework, which helps you understand the pros and cons of the decisions you make when building systems on AWS. Using the six pillars of the AWS Well-Architected Framework, you can learn architectural best practices for designing and operating reliable, secure, efficient, cost-effective, and sustainable systems in the cloud.
You can use the AWS Well-Architected Tool, available at no charge in the AWS Management Console, to review your workloads against these best practices by answering a set of questions for each pillar. In addition to the Framework and the AWS WA Tool, specialized guidance is provided for various types of applications.
•
In the Serverless Application Lens, we focus on best practices for architecting your serverless applications on AWS.
Are you Well-Architected?
145
Overview of Amazon Web Services AWS Whitepaper
•
In the Container Build Lens, we provide cloud-agnostic best practices for building and managing containers and container images. In addition, implementation guidance and examples are provided specific to the AWS Cloud.
•
In the Machine Learning Lens, we focus on how to design, deploy, and architect your machine learning workloads in the AWS Cloud.
•
In the Data Analytics Lens, we describe a collection of customer-proven best practices for designing well-architected analytics workloads.
•
In the Hybrid Networking Lens, we focus on how to design, deploy, and architect hybrid networking for workloads in the AWS Cloud.
•
In the IoT Lens and IoT Lens Checklist, we focus on best practices for architecting your IoT applications on AWS.
•
In the SAP Lens, we describe a collection of customer-proven design principles and best practices for ensuring SAP workloads on AWS are well-architected.
•
In the Games Industry Lens, we focus on designing, architecting, and deploying your games workloads on AWS.
•
In the Streaming Media Lens, we focus on the best practices for architecting and improving your streaming media workloads on AWS.
•
In the Healthcare Industry Lens, we focus on how to design, deploy, and manage your healthcare workloads.
•
In the Financial Services Industry Lens, we focus on best practices for architecting your Financial Services Industry workloads on AWS.
•
In the HPC Lens, we focus on best practices for architecting your High Performance Computing (HPC) workloads on AWS.
•
In the SaaS Lens, we focus on best practices for architecting your software as a service (SaaS) workloads on AWS.
•
In the Government Lens, we focus on best practices for designing and delivering government services on AWS.
•
In the Connected Mobility Lens, we focus on best practices for integrating technology into transportation systems and enhancing the overall mobility experience.
•
In the Migration Lens, we provide best practices for how to migrate to the AWS Cloud.
For more expert guidance and best practices for your cloud architecture—reference architecture deployments, diagrams, and whitepapers—refer to the AWS Architecture Center.
Are you Well-Architected?
146
Overview of Amazon Web Services AWS Whitepaper
Conclusion
AWS provides building blocks that you can assemble quickly to support virtually any workload. With AWS, you’ll find a complete set of highly available services that are designed to work together to build sophisticated scalable applications.
You have access to highly durable storage, low-cost compute, high-performance databases, management tools, and more. All this is available without up-front cost, and you pay for only what you use. These services help organizations move faster, lower IT costs, and scale. AWS is trusted by the largest enterprises and the hottest start-ups to power a wide variety of workloads, including web and mobile applications, game development, data processing and warehousing, storage, archive, and many others.
147
Overview of Amazon Web Services AWS Whitepaper
Resources
•
AWS Decision Guides
•
AWS Architecture Center
•
This Is My Architecture videos
•
AWS Documentation
•
AWS Blog
•
AWS Well-Architected Framework
•
AWS Whitepapers & Guides
148
Overview of Amazon Web Services AWS Whitepaper
Document history
To be notified about updates to this whitepaper, subscribe to the RSS feed.
Change
Description
Date
Whitepaper updated
Added AWS User Notificat ions and updated AWS Service Catalog.
June 9, 2025
Whitepaper updated
Added links to decision guideswhere appropriate.
August 27, 2024
Whitepaper updated
Amazon Q added. Amazon CodeWhisperer is now Amazon Q Developer. Amazon WorkDocs notice added.
May 3, 2024
Whitepaper updated
AWS B2B Data Interchan ge, AWS re:Post Private, Amazon ElastiCache Serverles s, Amazon Neptune Analytics , Amazon RDS for Db2, Amazon PartyRock, Amazon SageMaker AI HyperPod, and Amazon WorkSpaces Thin Client added.
March 1, 2024
Whitepaper updated
AWS Snowball Edge informati on updated.
February 22, 2024
Whitepaper updated
AWS Elastic Disaster Recovery added, other minor updates.
February 15, 2024
149
Overview of Amazon Web Services AWS Whitepaper
Whitepaper updated
Amazon Managed Grafana and Amazon Managed Service for Prometheus added.
February 5, 2024
Whitepaper updated
New Connected Mobility Lens and Migration Lens added to the Well-Architected section.
February 2, 2024
Whitepaper updated
Amazon Lumberyard is no longer offered. Use Open 3D Engine (O3DE), the Apache-licensed successor to Lumberyard.
December 1, 2023
Whitepaper updated
New services added: Amazon CodeCatalyst, AWS Verified Access, Amazon Aurora I/O-Optimized, Amazon SageMaker AI geospatial capabilities, Amazon Security Lake, AWS DMS Serverless, AWS Glue for Ray, AWS Glue Data Quality, Amazon Verified Permissions, AWS AppFabric , AWS Bedrock, vector engine for Amazon OpenSearch Serverless, AWS HealthScr ibe, AWS Entity Resolutio n, and Amazon VPC Lattice. Removed Amazon Sumerian. Numerous editorial changes throughout.
September 28, 2023
150
Overview of Amazon Web Services AWS Whitepaper
Whitepaper updated
New services added: Amazon CodeWhisperer, Amazon DataZone, Amazon Linux 2023, AWS Infrastructure Composer, AWS Clean Rooms, AWS Modular Data Center. New subservices added: Amazon OpenSearc h Serverless, Geospacial ML with Amazon Sagemaker, Amazon EC2 C7g Instances, Amazon EC2 Inf2 Instances, Amazon EC2 M7g instances , Amazon EC2 R7g Instances , Amazon EC2 Trn1 Instances . New program added: Integrated Private Wireless on AWS.
April 15, 2023
Whitepaper updated
New services added: Amazon File Cache, AWS IoT ExpressLink, AWS Mainframe Modernization Service. New subservices added: Amazon Connect Cases, Amazon Redshift Serverless, Amazon WorkSpaces Core, AWS WAF Captcha.
December 30, 2022
Whitepaper updated
New Container Build Lens and Healthcare Industry Lens added to the Well-Architected section.
December 23, 2022
151
Overview of Amazon Web Services AWS Whitepaper
Whitepaper updated
New service AWS Billing Conductor added, Global Infrastructure section updated, category icons added, and minor corrections throughout.
June 3, 2022
Whitepaper updated
Added note that EC2-Classic is being retired on August 15, 2022
February 17, 2022
Whitepaper updated
Added new services and compute services comparison table.
January 12, 2022
Whitepaper updated
Amazon Elasticsearch Service renamed Amazon OpenSearc h Service.
September 8, 2021
Whitepaper updated
Added new services and updated information throughout.
August 5, 2021
Minor update
Minor text updates to improve accuracy and fix links.
April 12, 2021
Minor update
Minor text updates to improve accuracy.
November 20, 2020
Minor update
Fixed incorrect link.
November 19, 2020
Minor update
Fixed incorrect link.
August 11, 2020
Minor update
Fixed incorrect link.
July 17, 2020
Minor updates
Minor text updates to improve accuracy.
January 1, 2020
152
Overview of Amazon Web Services AWS Whitepaper
Minor updates
Minor text updates to improve accuracy.
October 1, 2019
Whitepaper updated
Added new services and updated information throughout.
December 1, 2018
Whitepaper updated
Added new services and updated information throughout.
April 1, 2017
Initial publication
Overview of Amazon Web Services published.
January 1, 2014
Note
To subscribe to RSS updates, you must have an RSS plug-in enabled for the browser you are using.
153
Overview of Amazon Web Services AWS Whitepaper
AWS Glossary
For the latest AWS terminology, see the AWS glossary in the AWS Glossary Reference.
154


---- Introduction to AWS Cloud Practitioner Essentials ----

What is a client-server model?

You just learned more about AWS and how almost all of modern computing uses a basic client-server model. Let’s recap what a client-server model is.

Arrows pointing from client to server and from server to client to indicate a transaction.
In computing, a client can be a web browser or desktop application that a person interacts with to make requests to computer servers. A server can be services, such as Amazon Elastic Compute Cloud (Amazon EC2) – a type of virtual server.

For example, suppose that a client makes a request for a news article, the score in an online game, or a funny video. The server evaluates the details of this request and fulfills it by returning the information to the client.


Deployment models for cloud computing

When selecting a cloud strategy, a company must consider factors such as required cloud application components, preferred resource management tools, and any legacy IT infrastructure requirements.

The three cloud computing deployment models are cloud-based, on-premises, and hybrid. 

To learn more about deployment models, choose each of the following three tabs.

-- Cloud Based Development --

Run all parts of the application in the cloud.
Migrate existing applications to the cloud.
Design and build new applications in the cloud.
In a cloud-based deployment model, you can migrate existing applications to the cloud, or you can design and build new applications in the cloud. You can build those applications on low-level infrastructure that requires your IT staff to manage them. Alternatively, you can build them using higher-level services that reduce the management, architecting, and scaling requirements of the core infrastructure.


For example, a company might create an application consisting of virtual servers, databases, and networking components that are fully based in the cloud.


On-Premises Development

Deploy resources by using virtualization and resource management tools.
Increase resource utilization by using application management and virtualization technologies.
On-premises deployment is also known as a private cloud deployment. In this model, resources are deployed on premises by using virtualization and resource management tools.


For example, you might have applications that run on technology that is fully kept in your on-premises data center. Though this model is much like legacy IT infrastructure, its incorporation of application management and virtualization technologies helps to increase resource utilization.


Hybrid Development

Connect cloud-based resources to on-premises infrastructure.
Integrate cloud-based resources with legacy IT applications.
In a hybrid deployment, cloud-based resources are connected to on-premises infrastructure. You might want to use this approach in a number of situations. For example, you have legacy applications that are better maintained on premises, or government regulations require your business to keep certain records on premises.


For example, suppose that a company wants to use cloud services that can automate batch data processing and analytics. However, the company has several legacy applications that are more suitable on premises and will not be migrated to the cloud. With a hybrid deployment, the company would be able to keep the legacy applications on premises while benefiting from the data and analytics services that run in the cloud.


AWS Glossary

Reference
AWS Glossary
Copyright © 2025 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.
AWS Glossary Reference
AWS Glossary: Reference
Copyright © 2025 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.
Amazon's trademarks and trade dress may not be used in connection with any product or service
that is not Amazon's, in any manner that is likely to cause confusion among customers, or in any
manner that disparages or discredits Amazon. All other trademarks not owned by Amazon are
the property of their respective owners, who may or may not be affiliated with, connected to, or
sponsored by Amazon.
AWS Glossary Reference
Table of Contents
AWS Glossary ...................................................................................................................................................... 1
iii
AWS Glossary Reference
AWS Glossary
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
Numbers and symbols
100-continue A method that gives a client the ability to see whether a server can
accept a request before actually sending it. For large PUT requests, this
method can save both time and bandwidth charges.
A
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
AAD See additional authenticated data.
access control list
(ACL)
A document that defines who can access a particular bucket or object.
Each bucket and object in Amazon S3 has an ACL. This document defines
what each type of user can do, such as write and read permissions.
access identifiers See credentials.
access key The combination of an access key ID (for example,
AKIAIOSFODNN7EXAMPLE) and a secret access key (for example,
wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY). You use access
keys to sign API requests that you make to AWS.
access key ID A unique identifier that's associated with a secret access key; the access
key ID and secret access key are used together to sign programmatic
AWS requests cryptographically.
access key rotation A method to increase security by changing the AWS access key ID. You
can use this method to retire an old key at your discretion.
access policy language A language for writing documents (specifically, policies) that specify who
can access a particular AWS resource and under what conditions.
1
AWS Glossary Reference
account A formal relationship with AWS that's associated with all of the
following:
• The owner email address and password
• The control of resources created under its umbrella
• Payment for the AWS activity related to those resources
The AWS account has permission to do anything and everything with
all the AWS account resources. This is in contrast to a user, which is an
entity contained within the account.
account activity A webpage showing your month-to-date AWS usage and costs. The
account activity page is located at https://aws.amazon.com/accountactivity/.
AWS Account
Management
AWS Account Management is a tool that you can use to update the
contact information for each of your AWS accounts.
See Also https://aws.amazon.com/organizations.
ACL See access control list (ACL).
ACM AWS Certificate Manager is a web service for provisioning, managing,
and deploying Secure Sockets Layer/Transport Layer Security (SSL/TLS)
certificates for use with AWS services.
See Also https://aws.amazon.com/certificate-manager/.
action An API function. Also called operation or call. The activity the principal
has permission to perform. The action is B in the statement "A has
permission to do B to C where D applies." For example, Jane sends a
request to Amazon SQS with Action=ReceiveMessage.
CloudWatch: The response initiated by the change in an alarm's state
(for example, from OK to ALARM). The state change might be caused
by a metric reaching the alarm threshold, or by a SetAlarmState
request. Each alarm can have one or more actions assigned to each
state. Actions are performed once each time the alarm changes to a
state that has an action assigned. Example actions include an Amazon
SNS notification, running an Amazon EC2 Auto Scaling policy, and an
Amazon EC2 instance stop/terminate action.
2
AWS Glossary Reference
active trusted key
groups
A list that shows each of the trusted key groups, and the IDs of the
public keys in each key group, that are active for a distribution in
Amazon CloudFront. CloudFront can use the public keys in these key
groups to verify the signatures of CloudFront signed URLs and signed
cookies.
active trusted signers See active trusted key groups.
active-active A class of high availability strategies in which a workload exists
simultaneously in multiple Regions, uses multiple primary resources, and
serves traffic from all of the Regions to which it's deployed. Sometimes
referred to as active/active.
See Also , , .
active-passive A class of disaster recovery strategies that involve a primary Region and
a standby Region in a back up and restore, hot standby, pilot light, or
warm standby configuration. Sometimes referred to as active/passive.
additional
authenticated data
Information that's checked for integrity but not encrypted, such as
headers or other contextual metadata.
administrative
suspension
Amazon EC2 Auto Scaling might suspend processes for Auto Scaling
group that repeatedly fail to launch instances. Auto Scaling groups that
most commonly experience administrative suspension have zero running
instances, have been trying to launch instances for more than 24 hours,
and have not succeeded in that time.
alarm An item that watches a single metric over a specified time period and
starts an Amazon SNS topic or an Amazon EC2 Auto Scaling policy.
These actions are started if the value of the metric crosses a threshold
value over a predetermined number of time periods.
allow One of two possible outcomes (the other is deny) when an IAM access
policy is evaluated. When a user makes a request to AWS, AWS evaluates
the request based on all permissions that apply to the user and then
returns either allow or deny.
Amazon Machine
Image (AMI)
An Amazon Machine Image (AMI) is an encrypted machine image stored
in Amazon EBS or Amazon S3. AMIs function similarly to a template
of a computer's root drive. They contain the operating system and can
3
AWS Glossary Reference
also include software and layers of your application, such as database
servers, middleware, and web servers.
Amazon Web Services
(AWS)
An infrastructure web services platform in the cloud for companies of all
sizes.
See Also https://aws.amazon.com/what-is-cloud-computing/.
AMI See Amazon Machine Image (AMI).
Amplify AWS Amplify is a complete solution that frontend web and mobile
developers can use to build and deploy secure, scalable full-stack
applications powered by AWS. Amplify provides two services: Amplify
Hosting and Amplify Studio.
See Also https://aws.amazon.com/amplify/ .
Amplify Android Amplify Android is a collection of open-source client libraries that
provides interfaces for specific use cases across many AWS services.
Amplify Android is the recommended way to build native Android
applications powered by AWS.
See Also https://aws.amazon.com/amplify/ .
Amplify Hosting AWS Amplify Hosting is a fully managed continuous integration and
continuous delivery (CI/CD) and hosting service for fast, secure, and
reliable static and server-side rendered apps. Amplify Hosting provides
a Git-based workflow for hosting full-stack serverless web apps with
continuous deployment.
See Also https://aws.amazon.com/amplify/hosting/ .
Amplify iOS Amplify iOS is a collection of open-source client libraries that provides
interfaces for specific use cases across many AWS services. Amplify iOS is
the recommended way to build native iOS applications powered by AWS.
See Also https://aws.amazon.com/amplify/ .
Amplify Studio AWS Amplify Studio is a visual development environment that web and
mobile developers can use to build the frontend UI components and the
backend environment for a full-stack application.
See Also https://aws.amazon.com/amplify/studio/ .
analysis rules AWS Clean Rooms: The query restrictions that authorize a specific type
of query.
4
AWS Glossary Reference
analysis scheme CloudSearch: Language-specific text analysis options that are applied to
a text field to control stemming and configure stopwords and synonyms.
API Gateway Amazon API Gateway is a fully managed service that developers can use
to create, publish, maintain, monitor, and secure APIs at any scale.
See Also https://aws.amazon.com/api-gateway.
AWS App2Container AWS App2Container is a transformation tool that modernizes .NET and
Java applications by migrating them into containerized applications.
See Also https://aws.amazon.com/app2container.
AWS AppConfig AWS AppConfig is a service used to update software at runtime without
deploying new code. With AWS AppConfig, you can configure, validate,
and deploy feature flags and application configurations.
See Also https://aws.amazon.com/systems-manager/features/
appconfig.
Amazon AppFlow Amazon AppFlow is a fully managed integration service that you can
use to transfer data securely between software as a service (SaaS)
applications and AWS services.
See Also https://aws.amazon.com/appflow.
application Elastic Beanstalk: A logical collection of components, including
environments, versions, and environment configurations. An application
is conceptually similar to a folder.
CodeDeploy: A name that uniquely identifies the application to be
deployed. AWS CodeDeploy uses this name to ensure the correct
combination of revision, deployment configuration, and deployment
group are referenced during a deployment.
Application Auto
Scaling
AWS Application Auto Scaling is a web service that you can use to
configure automatic scaling for AWS resources beyond Amazon EC2,
such as Amazon ECS services, Amazon EMR clusters, and DynamoDB
tables.
See Also https://aws.amazon.com/autoscaling/.
Application Billing The location where your customers manage the Amazon
DevPay products they've purchased. The web address is http://
www.amazon.com/dp-applications.
5
AWS Glossary Reference
Application Composer AWS Application Composer is a visual designer that you can use to build
serverless applications from multiple AWS services. As you design an
application, Application Composer automatically generates a YAML
template with CloudFormation and AWS SAM template resources.
See Also https://aws.amazon.com/application-composer/ .
Application Cost
Profiler
AWS Application Cost Profiler is a solution to track the consumption of
shared AWS resources used by software applications and report granular
cost breakdown across tenant base.
See Also https://aws.amazon.com/aws-cost-management/awsapplication-cost-profiler/.
Application Discovery
Service
AWS Application Discovery Service is a web service that helps you plan
to migrate to AWS by identifying IT assets in a data center—including
servers, virtual machines, applications, application dependencies, and
network infrastructure.
See Also https://aws.amazon.com/application-discovery/.
application revision CodeDeploy: An archive file containing source content—such as source
code, webpages, executable files, and deployment scripts—along with
an application specification file. Revisions are stored in Amazon S3
buckets or GitHub repositories. For Amazon S3, a revision is uniquely
identified by its Amazon S3 object key and its ETag, version, or both. For
GitHub, a revision is uniquely identified by its commit ID.
application
specification file
CodeDeploy: A YAML-formatted file used to map the source files in an
application revision to destinations on the instance. The file is also used
to specify custom permissions for deployed files and specify scripts to be
run on each instance at various stages of the deployment process.
application version Elastic Beanstalk: A specific, labeled iteration of an application that
represents a functionally consistent set of deployable application code.
A version points to an Amazon S3 object (a JAVA WAR file) that contains
the application code.
AppSpec file See application specification file.
AppStream 2.0 Amazon AppStream 2.0 is a fully managed, secure service for streaming
desktop applications to users without rewriting those applications.
6
AWS Glossary Reference
See Also https://aws.amazon.com/appstream/.
AWS AppSync AWS AppSync is an enterprise-level, fully managed GraphQL service with
real-time data synchronization and offline programming features.
See Also https://aws.amazon.com/appsync/.
ARN See Amazon Resource Name (ARN).
artifact CodePipeline: A copy of the files or changes that are worked on by the
pipeline.
asymmetric encryption Encryption that uses both a public key and a private key.
asynchronous bounce A type of bounce that occurs when a receiver initially accepts an email
message for delivery and then subsequently fails to deliver it.
Athena Amazon Athena is an interactive query service that you can use to
analyze data in Amazon S3 using ANSI SQL. Athena is serverless, so
there's no infrastructure to manage. Athena scales automatically and is
simple to use, so you can start analyzing your datasets within seconds.
See Also https://aws.amazon.com/athena/.
atomic counter DynamoDB: A method of incrementing or decrementing the value of an
existing attribute without interfering with other write requests.
attribute A fundamental data element, something that doesn't need to be broken
down any further. In DynamoDB, attributes are similar in many ways to
fields or columns in other database systems.
Amazon Machine Learning: A unique, named property within an
observation in a dataset. In tabular data, such as spreadsheets or
comma-separated values (.csv) files, the column headings represent the
attributes, and the rows contain values for each attribute.
AUC Area Under a Curve. An industry-standard metric to evaluate the quality
of a binary classification machine learning model. AUC measures the
ability of the model to predict a higher score for positive examples,
those that are “correct,” than for negative examples, those that are
“incorrect.” The AUC metric returns a decimal value from 0 to 1. AUC
values near 1 indicate an ML model that's highly accurate.
7
AWS Glossary Reference
Aurora Amazon Aurora is a fully managed MySQL-compatible relational
database engine that combines the speed and availability of commercial
databases with the simplicity and cost-effectiveness of open-source
databases.
See Also https://aws.amazon.com/rds/aurora/.
authenticated
encryption
Encryption that provides confidentiality, data integrity, and authenticity
assurances of the encrypted data.
authentication The process of proving your identity to a system.
AWS Auto Scaling AWS Auto Scaling is a fully managed service that you can use to quickly
discover the scalable AWS resources that are part of your application and
to configure dynamic scaling.
See Also https://aws.amazon.com/autoscaling/.
Auto Scaling group A representation of multiple EC2 instances that share similar
characteristics, and that are treated as a logical grouping for the
purposes of instance scaling and management.
Availability Zone A distinct location within a Region that's insulated from failures in
other Availability Zones, and provides inexpensive, low-latency network
connectivity to other Availability Zones in the same Region.
AWS See Amazon Web Services (AWS).
B
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
back up and restore A disaster recovery strategy in which backups of data in the primary
Region are copied to a standby Region and can be restored from the
standby Region. You must provision the infrastructure and other
resources, such as compute, as part of a failover process.
See Also , , , .
Backint Agent AWS Backint Agent for SAP HANA is an SAP-certified backup and restore
solution for SAP HANA workloads running on Amazon EC2 instances in
the cloud.
8
AWS Glossary Reference
See Also https://aws.amazon.com/backint-agent.
AWS Backup AWS Backup is a managed backup service that you can use to centralize
and automate the backup of data across AWS services in the cloud and
on premises.
See Also https://aws.amazon.com/backup/.
basic monitoring Monitoring of AWS provided metrics derived at a 5-minute frequency.
batch See document batch.
batch prediction Amazon Machine Learning: An operation that processes multiple input
data observations at one time (asynchronously). Unlike real-time
predictions, batch predictions aren't available until all predictions have
been processed.
See Also real-time predictions.
BGP ASN Border Gateway Protocol Autonomous System Number is a unique
identifier for a network, for use in BGP routing. Amazon EC2 supports
all 2-byte ASN numbers in the range of 1 – 65335, with the exception of
7224, which is reserved.
billing See Billing and Cost Management.
Billing and Cost
Management
AWS Billing and Cost Management is the AWS Cloud computing model
where you pay for services on demand and use as much or as little as
you need. While resources are active under your account, you pay for
the cost of allocating those resources. You also pay for any incidental
usage associated with those resources, such as data transfer or allocated
storage.
See Also https://aws.amazon.com/billing/new-user-faqs/.
binary attribute Amazon Machine Learning: An attribute for which one of two possible
values is possible. Valid positive values are 1, y, yes, t, and true answers.
Valid negative values are 0, n, no, f, and false. Amazon Machine Learning
outputs 1 for positive values and 0 for negative values.
See Also attribute.
binary classification
model
Amazon Machine Learning: A machine learning model that predicts the
answer to questions where the answer can be expressed as a binary
9
AWS Glossary Reference
variable. For example, questions with answers of “1” or “0”, “yes” or “no”,
“will click” or “will not click” are questions that have binary answers. The
result for a binary classification model is always either a “1” (for a “true”
or affirmative answers) or a “0” (for a “false” or negative answers).
block A dataset. Amazon EMR breaks large amounts of data into subsets. Each
subset is called a data block. Amazon EMR assigns an ID to each block
and uses a hash table to keep track of block processing.
block device A storage device that supports reading and (optionally) writing data in
fixed-size blocks, sectors, or clusters.
block device mapping A mapping structure for every AMI and instance that specifies the block
devices attached to the instance.
AWS Blockchain
Templates
See Managed Blockchain.
blue/green
deployment
CodeDeploy: A deployment method where the instances in a
deployment group (the original environment) are replaced by a different
set of instances (the replacement environment).
bootstrap action A user-specified default or custom action that runs a script or an
application on all nodes of a job flow before Hadoop starts.
Border Gateway
Protocol Autonomous
System Number
See BGP ASN.
bounce A failed email delivery attempt.
Braket Amazon Braket is a fully managed quantum computing service that
helps you run quantum algorithms to accelerate your research and
discovery.
See Also https://aws.amazon.com/braket.
breach Amazon EC2 Auto Scaling: The condition where a user-set threshold
(upper or lower boundary) is passed. If the duration of the breach is
significant, as set by a breach duration parameter, it can possibly start a
scaling activity.
10
AWS Glossary Reference
bucket Amazon S3: A container for stored objects. Every object is contained
in a bucket. For example, if the object named photos/puppy.jpg
is stored in the amzn-s3-demo-bucket bucket, then authorized
users can access the object with the URL https://amzn-s3-demobucket.s3.region-code.amazonaws.com/photos/puppy.jpg.
bucket owner The person or organization that owns a bucket in Amazon S3. In
the same way that Amazon is the only owner of the domain name
Amazon.com, only one person or organization can own a bucket.
bundling A commonly used term for creating an Amazon Machine Image (AMI). It
specifically refers to creating instance store-backed AMIs.
C
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
cache cluster A logical cache distributed over multiple cache nodes. A cache cluster
can be set up with a specific number of cache nodes.
cache cluster identifier Customer-supplied identifier for the cache cluster that must be unique
for that customer in an AWS Region.
cache engine version The version of the Memcached service that's running on the cache node.
cache node A fixed-size chunk of secure, network-attached RAM. Each cache node
runs an instance of the Memcached service, and has its own DNS name
and port. Multiple types of cache nodes are supported, each with
varying amounts of associated memory.
cache node type An EC2 instance type used to run the cache node.
cache parameter
group
A container for cache engine parameter values that can be applied to
one or more cache clusters.
cache security group A group maintained by ElastiCache that combines inbound
authorizations to cache nodes for hosts belonging to Amazon EC2
security groups that are specified through the console or the API or
command line tools.
11
AWS Glossary Reference
campaign Amazon Personalize: A deployed solution version (trained model)
with provisioned dedicated transaction capacity for creating
real-time recommendations for your application users. After
you create a campaign, you use the getRecommendations or
getPersonalizedRanking personalization operations to get
recommendations.
See Also recommendations, solution version.
canned access policy A standard access control policy that you can apply to a bucket or
object. Options include: private, public-read, public-read-write, and
authenticated-read.
canonicalization The process of converting data into a standard format that a service
such as Amazon S3 can recognize.
capacity The amount of available compute size at a given time. Each Auto Scaling
group is defined with a minimum and maximum compute size. A scaling
activity increases or decreases the capacity within the defined minimum
and maximum values.
Cartesian product A mathematical operation that returns a product from multiple sets.
Cartesian product
processor
A processor that calculates a Cartesian product. Also known as a
Cartesian data processor.
AWS CDK AWS Cloud Development Kit (AWS CDK) is an open-source software
development framework for defining your cloud infrastructure in code
and provisioning it through AWS CloudFormation.
See Also https://aws.amazon.com/cdk/.
CDN See content delivery network (CDN).
certificate A credential that some AWS products use to authenticate AWS accounts
and users. Also known as an X.509 certificate. The certificate is paired
with a private key.
chargeable resources Features or services whose use incurs fees. Although some AWS products
are free, others include charges. For example, in an CloudFormation
stack, AWS resources that have been created incur charges. The amount
charged depends on the usage load. Use the Amazon Web Services
12
AWS Glossary Reference
Simple Monthly Calculator to estimate your cost prior to creating
instances, stacks, or other resources.
Amazon Q Developer
in chat applications
Amazon Q Developer in chat applications is an interactive agent that
makes it easier to monitor, troubleshoot, and operate AWS resources in
your Slack channels and Amazon Chime chat rooms.
See Also https://aws.amazon.com/chatbot.
Amazon Chime Amazon Chime is a secure, real-time, unified communications service
that transforms meetings by making them more efficient and easier to
conduct.
See Also https://aws.amazon.com/chime/.
CIDR block Classless Inter-Domain Routing. An internet protocol address allocation
and route aggregation methodology.
See Also Classless Inter-Domain Routing on Wikipedia.
ciphertext Information that has been encrypted, as opposed to plaintext, which is
information that has not.
classification In machine learning, a type of problem that seeks to place (classify)
a data sample into a single category or “class.” Often, classification
problems are modeled to choose one category (class) out of two.
These are binary classification problems. Problems with more than
two available categories (classes) are called "multiclass classification"
problems.
See Also binary classification model, multiclass classification model.
AWS Clean Rooms AWS Clean Rooms is an AWS service that helps multiple parties to join
their data together in a secure collaboration workspace.
See Also https://aws.amazon.com/clean-rooms/.
Client VPN AWS Client VPN is a client-based, managed VPN service that remote
clients can use to securely access your AWS resources using an Open
VPN-based software client.
See Also https://aws.amazon.com/vpn/client-vpn.
AWS Cloud Control API AWS Cloud Control API is a set of standardized application programming
interfaces (APIs) that developers can use to create, read, update, delete,
and list supported cloud infrastructure.
13
AWS Glossary Reference
See Also https://aws.amazon.com/cloudcontrolapi.
Cloud Directory Amazon Cloud Directory is a service that provides a highly scalable
directory store for your application's multihierarchical data.
See Also https://aws.amazon.com/cloud-directory/.
AWS Cloud Map AWS Cloud Map is a service that you use to create and maintain a map
of the backend services and resources that your applications depend
on. With AWS Cloud Map, you can name and discover your AWS Cloud
resources.
See Also https://aws.amazon.com/cloud-map.
cloud service provider
(CSP)
A cloud service provider is a company that provides subscribers with
access to internet-hosted computing, storage, and software services.
AWS Cloud WAN AWS Cloud WAN is a managed wide-area networking service used to
build, manage, and monitor a unified global network.
See Also https://aws.amazon.com/cloud-wan.
AWS Cloud9 AWS Cloud9 is a cloud-based integrated development environment (IDE)
that you use to write, run, and debug code.
See Also https://aws.amazon.com/cloud9/.
CloudFormation AWS CloudFormation is a service for writing or changing templates that
create and delete related AWS resources together as a unit.
See Also https://aws.amazon.com/cloudformation.
CloudFront Amazon CloudFront is an AWS content delivery service that helps you
improve the performance, reliability, and availability of your websites
and applications.
See Also https://aws.amazon.com/cloudfront.
CloudHSM AWS CloudHSM is a web service that helps you meet corporate,
contractual, and regulatory compliance requirements for data security
by using dedicated hardware security module (HSM) appliances within
the AWS Cloud.
See Also https://aws.amazon.com/cloudhsm/.
CloudSearch Amazon CloudSearch is a fully managed service in the AWS Cloud that
you can use to set up, manage, and scale a search solution for your
website or application.
14
AWS Glossary Reference
See Also https://aws.amazon.com/cloudsearch/.
CloudTrail AWS CloudTrail is a web service that records AWS API calls for your
account and delivers log files to you. The recorded information includes
the identity of the API caller, the time of the API call, the source IP
address of the API caller, the request parameters, and the response
elements that the AWS service returns.
See Also https://aws.amazon.com/cloudtrail/.
CloudWatch Amazon CloudWatch is a web service that you can use to monitor and
manage various metrics, and configure alarm actions based on data from
those metrics.
See Also https://aws.amazon.com/cloudwatch.
CloudWatch Events Amazon CloudWatch Events is a web service that you can use to deliver a
timely stream of system events that describe changes in AWS resources
to Lambda functions, streams in Kinesis Data Streams, Amazon SNS
topics, or built-in targets.
See Also https://aws.amazon.com/cloudwatch.
CloudWatch Logs Amazon CloudWatch Logs is a web service for monitoring and
troubleshooting your systems and applications from your existing
system, application, and custom log files. You can send your existing log
files to CloudWatch Logs and monitor these logs in near-real time.
See Also https://aws.amazon.com/cloudwatch.
cluster A logical grouping of container instances that you can place tasks on.
OpenSearch Service: A logical grouping of one or more data nodes,
optional dedicated master nodes, and storage required to run Amazon
OpenSearch Service (OpenSearch Service) and operate your OpenSearch
Service domain.
See Also data node, dedicated master node, node.
cluster compute
instance
A type of instance that provides a great amount of CPU power coupled
with increased networking performance, making it well suited for
High Performance Compute (HPC) applications and other demanding
network-bound applications.
15
AWS Glossary Reference
cluster placement
group
A logical cluster compute instance grouping to provide lower latency
and high-bandwidth connectivity between the instances.
cluster status OpenSearch Service: An indicator of the health of a cluster. A status can
be green, yellow, or red. At the shard level, green means that all shards
are allocated to nodes in a cluster, yellow means that the primary shard
is allocated but the replica shards aren't, and red means that the primary
and replica shards of at least one index aren't allocated. The shard status
determines the index status, and the index status determines the cluster
status.
CNAME Canonical Name Record. A type of resource record in the Domain Name
System (DNS) that specifies that the domain name is an alias of another,
canonical domain name. Specifically, it's an entry in a DNS table that you
can use to alias one fully qualified domain name to another.
Code Signing for AWS
IoT
A service for signing code that you create for any IoT device that's
supported by Amazon Web Services (AWS).
CodeBuild AWS CodeBuild is a fully managed continuous integration service that
compiles source code, runs tests, and produces software packages that
are ready to deploy.
See Also https://aws.amazon.com/codebuild.
CodeCommit AWS CodeCommit is a fully managed source control service that
companies can use to host secure and highly scalable private Git
repositories.
See Also https://aws.amazon.com/codecommit.
CodeDeploy AWS CodeDeploy is a service that automates code deployments to any
instance, including EC2 instances and instances running on-premises.
See Also https://aws.amazon.com/codedeploy.
AWS CodeDeploy
agent
AWS CodeDeploy agent is a software package that, when installed
and configured on an instance, enables that instance to be used in
CodeDeploy deployments.
CodeGuru Amazon CodeGuru is a collection of developer tools that automate
code reviews and provide intelligent recommendations to optimize
application performance.
16
AWS Glossary Reference
See Also https://aws.amazon.com/codeguru.
CodePipeline AWS CodePipeline is a continuous delivery service for fast and reliable
application updates.
See Also https://aws.amazon.com/codepipeline.
Amazon Cognito Amazon Cognito is a web service that you can use to save mobile user
data in the AWS Cloud without writing any backend code or managing
any infrastructure. Examples of mobile user data that you can save
include app preferences and game states. Amazon Cognito offers mobile
identity management and data synchronization across devices.
See Also https://aws.amazon.com/cognito/.
collaboration AWS Clean Rooms: A secure logical boundary in AWS Clean Rooms in
which members can perform SQL queries on configured tables.
AWS CLI AWS Command Line Interface is a unified downloadable and
configurable tool for managing AWS services. Control multiple AWS
services from the command line and automate them through scripts.
See Also https://aws.amazon.com/cli/.
complaint The event where a recipient who doesn't want to receive an email
message chooses "Mark as Spam" within the email client, and the
internet service provider (ISP) sends a notification to Amazon SES.
compound query CloudSearch: A search request that specifies multiple search criteria
using the Amazon CloudSearch structured search syntax.
Amazon Comprehend Amazon Comprehend is a natural language processing (NLP) service that
uses machine learning to find insights and relationships in text.
See Also https://aws.amazon.com/comprehend/.
Amazon Comprehend
Medical
Amazon Comprehend Medical is a HIPAA-eligible natural language
processing (NLP) service that uses machine learning (ML), and has been
pre-trained to understand and extract health data from medical text,
such as prescriptions, procedures, or diagnoses.
See Also https://aws.amazon.com/comprehend/medical.
condition IAM: Any restriction or detail about a permission. The condition is D in
the statement "A has permission to do B to C where D applies."
17
AWS Glossary Reference
AWS WAF: A set of attributes that AWS WAF searches for in web requests
to AWS resources such as Amazon CloudFront distributions. Conditions
can include values such as the IP addresses that web requests originate
from or values in request headers. Based on the specified conditions,
you can configure AWS WAF to allow or block web requests to AWS
resources.
conditional parameter See mapping.
AWS Config AWS Config is a fully managed service that provides an AWS resource
inventory, configuration history, and configuration change notifications
for better security and governance. You can create rules that
automatically check the configuration of AWS resources that AWS Config
records.
See Also https://aws.amazon.com/config/.
configuration API CloudSearch: The API call that you use to create, configure, and manage
search domains.
configuration template A series of key–value pairs that define parameters for various
AWS products so that Elastic Beanstalk can provision them for an
environment.
Amazon Connect Amazon Connect is a service solution that offers self-service
configuration and provides dynamic, personal, and natural customer
engagement at any scale.
See Also https://aws.amazon.com/connect/.
consistency model The method a service uses to achieve high availability. For example, it
could involve replicating data across multiple servers in a data center.
See Also eventual consistency.
console See AWS Management Console.
Console Mobile
Application
AWS Console Mobile Application lets AWS customers monitor and
manage a select set of resources to stay informed and connected with
their AWS resources while on the go.
See Also https://aws.amazon.com/console/mobile.
18
AWS Glossary Reference
consolidated billing A feature of the AWS Organizations service for consolidating payment
for multiple AWS accounts. You create an organization that contains
your AWS accounts, and you use the management account of your
organization to pay for all member accounts. You can see a combined
view of AWS costs that are incurred by all accounts in your organization,
and you can get detailed cost reports for individual accounts.
container A container is a standard unit of software that contains application code
and all relevant dependencies.
container definition A container definition specifies the details that are associated with
running a container on Amazon ECS. More specifically, a container
definition specifies details such as the container image to use and
how much CPU and memory the container is allocated. The container
definition is included as part of an Amazon ECS task definition.
container instance A container instance is a self-managed EC2 instance or an on-premises
server or virtual machine (VM) that's running the Amazon Elastic
Container Service (Amazon ECS) container agent and has been registered
into a cluster. A container instance serves as the infrastructure that your
Amazon ECS workloads are run on.
container registry A container registry is a collection of repositories that store container
images. One example is Amazon Elastic Container Registry (Amazon
ECR).
content delivery
network (CDN)
A web service that speeds up distribution of your static and dynamic
web content—such as .html, .css, .js, media files, and image files—to
your users by using a worldwide network of data centers. When a user
requests your content, the request is routed to the data center that
provides the lowest latency (time delay). If the content is already in the
location with the lowest latency, the CDN delivers it immediately. If
not, the CDN retrieves it from an origin that you specify (for example,
a web server or an Amazon S3 bucket). With some CDNs, you can help
secure your content by configuring an HTTPS connection between users
and data centers, and between data centers and your origin. Amazon
CloudFront is an example of a CDN.
19
AWS Glossary Reference
contextual metadata Amazon Personalize: Interactions data that you collect about a user's
browsing context (such as device used or location) when an event (such
as a click) occurs. Contextual metadata can improve recommendation
relevance for new and existing users.
See Also Interactions dataset, event.
continuous delivery A software development practice where code changes are automatically
built, tested, and prepared for a release to production.
See Also https://aws.amazon.com/devops/continuous-delivery/.
continuous integration A software development practice where developers regularly merge
code changes into a central repository, after which automated builds
and tests are run.
See Also https://aws.amazon.com/devops/continuous-integration/.
AWS Control Tower AWS Control Tower is a service used to set up and govern a secure,
multi-account AWS environment.
See Also https://aws.amazon.com/controltower.
cooldown period Amount of time that Amazon EC2 Auto Scaling doesn't allow the desired
size of the Auto Scaling group to be changed by any other notification
from an CloudWatch alarm.
core node An EC2 instance that runs Hadoop map and reduce tasks and stores
data using the Hadoop Distributed File System (HDFS). Core nodes are
managed by the master node, which assigns Hadoop tasks to nodes and
monitors their status. The EC2 instances you assign as core nodes are
capacity that must be allotted for the entire job flow run. Because core
nodes store data, you can't remove them from a job flow. However, you
can add more core nodes to a running job flow.
Core nodes run both the DataNodes and TaskTracker Hadoop daemons.
corpus CloudSearch: A collection of data that you want to search.
Corretto Amazon Corretto is a no-cost, multiplatform, production-ready
distribution of the Open Java Development Kit (OpenJDK).
See Also https://aws.amazon.com/corretto/.
coverage Amazon Personalize: An evaluation metric that tells you the proportion
of unique items that Amazon Personalize might recommend using your
20
AWS Glossary Reference
model out of the total number of unique items in Interactions and Items
datasets. To make sure Amazon Personalize recommends more of your
items, use a model with a higher coverage score. Recipes that feature
item exploration, such as user-personalization, have higher coverage
than those that don’t, such as popularity-count.
See Also metrics, Items dataset, Interactions dataset, item exploration,
user-personalization recipe, popularity-count recipe.
credential helper CodeCommit: A program that stores credentials for repositories and
supplies them to Git when making connections to those repositories.
The AWS CLI includes a credential helper that you can use with Git when
connecting to CodeCommit repositories.
credentials Also called access credentials or security credentials. In authentication
and authorization, a system uses credentials to identify who is making a
call and whether to allow the requested access. In AWS, these credentials
are typically the access key ID and the secret access key.
cross-account access The process of permitting limited, controlled use of resources in
one AWS account by a user in another AWS account. For example, in
CodeCommit and CodeDeploy you can configure cross-account access
so that a user in AWS account A can access an CodeCommit repository
created by account B. Or a pipeline in CodePipeline created by account
A can use CodeDeploy resources created by account B. In IAM you use a
role to delegate temporary access to a user in one account to resources
in another.
cross-Region
replication
A solution for replicating data across different AWS Regions, in near-real
time.
Cryptographic
Computing for Clean
Rooms (C3R)
AWS Clean Rooms: A capability in AWS Clean Rooms that organizations
can use to bring sensitive data together to derive new insights from data
analytics while cryptographically limiting what any party in the process
can learn.
customer gateway A router or software application on your side of a VPN tunnel that's
managed by Amazon VPC. The internal interfaces of the customer
gateway are attached to one or more devices in your home network.
21
AWS Glossary Reference
The external interface is attached to the virtual private gateway (VGW)
across the VPN tunnel.
customer managed
policy
An IAM managed policy that you create and manage in your AWS
account.
customer master key
(CMK)
We no longer use customer master key or CMK. These terms are replaced
by AWS KMS key (first mention) and KMS key (subsequent mention). For
more information, see KMS key.
D
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
dashboard See service health dashboard.
data consistency A concept that describes when data is written or updated successfully
and all copies of the data are updated in all AWS Regions. However, it
takes time for the data to propagate to all storage locations. To support
varied application requirements, DynamoDB supports both eventually
consistent and strongly consistent reads.
See Also eventual consistency, eventually consistent read, strongly
consistent read.
AWS Data Exchange AWS Data Exchange is a service that helps you find, subscribe to, and use
third-party data in the cloud.
See Also https://aws.amazon.com/data-exchange.
Amazon Data Lifecycle
Manager
Amazon Data Lifecycle Manager is an Amazon service that automates
and manages the lifecycle of Amazon EBS snapshots and Amazon EBSbacked AMIs.
data node OpenSearch Service: An OpenSearch instance that holds data and
responds to data upload requests.
See Also dedicated master node, node.
Data Pipeline AWS Data Pipeline is a web service for processing and moving data
between different AWS compute and storage services, as well as onpremises data sources, at specified intervals.
22
AWS Glossary Reference
See Also https://aws.amazon.com/datapipeline.
data schema See schema.
data source The database, file, or repository that provides information required by
an application or database. For example, in OpsWorks, valid data sources
include an instance for a stack's MySQL layer or a stack's Amazon RDS
service layer. In Amazon Redshift , valid data sources include text files in
an Amazon S3 bucket, in an Amazon EMR cluster, or on a remote host
that a cluster can access through an SSH connection.
See Also datasource.
database engine The database software and version running on the DB instance.
database name The name of a database hosted in a DB instance. A DB instance can host
multiple databases, but databases hosted by the same DB instance must
each have a unique name within that instance.
dataset Amazon Personalize: A container for the data used by Amazon
Personalize. There are three types of Amazon Personalize datasets:
Users, Items, and Interactions.
See Also Interactions dataset, Users dataset, Items dataset.
dataset group Amazon Personalize: A container for Amazon Personalize components,
including datasets, event trackers, solutions, filters, campaigns, and
batch inference jobs. A dataset group organizes your resources into
independent collections, so resources from one dataset group can’t
influence resources in any other dataset group.
See Also dataset, event tracker, solution, campaign.
datasource Amazon ML: An object that contains metadata about the input data.
Amazon ML reads the input data, computes descriptive statistics on
its attributes, and stores the statistics—along with a schema and
other information—as part of the datasource object. Amazon ML
uses datasources to train and evaluate a machine learning model and
generate batch predictions.
See Also data source.
23
AWS Glossary Reference
DataSync AWS DataSync is an online data transfer service that simplifies,
automates, and accelerates moving data between storage systems and
services.
See Also https://aws.amazon.com/datasync.
DB compute class The size of the database compute platform used to run the instance.
DB instance An isolated database environment running in the cloud. A DB instance
can contain multiple user-created databases.
DB instance identifier User-supplied identifier for the DB instance. The identifier must be
unique for that user in an AWS Region.
DB parameter group A container for database engine parameter values that apply to one or
more DB instances.
DB security group A method that controls access to the DB instance. By default, network
access is turned off to DB instances. After inbound traffic is configured
for a security group, the same rules apply to all DB instances associated
with that group.
DB snapshot A user-initiated point backup of a DB instance.
Dedicated Host A physical server with EC2 instance capacity fully dedicated to a user.
Dedicated Instance An instance that's physically isolated at the host hardware level and
launched within a Amazon VPC.
dedicated master node OpenSearch Service: An OpenSearch instance that performs cluster
management tasks, but doesn't hold data or respond to data upload
requests. Amazon OpenSearch Service (OpenSearch Service) uses
dedicated master nodes to increase cluster stability.
See Also data node, node.
Dedicated Reserved
Instance
An option that you purchase to guarantee that sufficient capacity will be
available to launch Dedicated Instances into a Amazon VPC.
AWS DeepComposer AWS DeepComposer is a web service designed specifically to educate
developers through tutorials, sample code, and training data.
See Also https://aws.amazon.com/deepcomposer.
24
AWS Glossary Reference
AWS DeepLens AWS DeepLens is a tool that provides AWS customers with a centralized
place to search, discover, and connect with trusted APN Technology and
Consulting Partners, based on customers' business needs.
See Also https://aws.amazon.com/deeplens.
AWS DeepRacer AWS DeepRacer is a cloud-based 3D racing simulator, global racing
league, and fully autonomous 1/18th-scale race car driven by
reinforcement learning.
See Also https://aws.amazon.com/deepracer.
delegation Within a single AWS account: Giving AWS users access to resources your
AWS account.
Between two AWS accounts: Setting up a trust between the account that
owns the resource (the trusting account), and the account that contains
the users that need to access the resource (the trusted account).
See Also trust policy.
delete marker An object with a key and version ID, but without content. Amazon S3
inserts delete markers automatically into versioned buckets when an
object is deleted.
deliverability The likelihood that an email message arrives at its intended destination.
deliveries The number of email messages, sent through Amazon SES, that were
accepted by an internet service provider (ISP) for delivery to recipients
over a period of time.
deny The result of a policy statement that includes deny as the effect, so that
a specific action or actions are expressly forbidden for a user, group, or
role. Explicit deny take precedence over explicit allow.
deployment
configuration
CodeDeploy: A set of deployment rules and success and failure
conditions used by the service during a deployment.
deployment group CodeDeploy: A set of individually tagged instances or EC2 instances in
Auto Scaling groups, or both.
Description property A property added to parameters, resources, resource properties,
mappings, and outputs to help you to document CloudFormation
template elements.
25
AWS Glossary Reference
detailed monitoring Monitoring of AWS provided metrics derived at a 1-minute frequency.
Detective Amazon Detective is a service that collects log data from your AWS
resources to analyze and identify the root cause of security findings
or suspicious activities. The Detective behavior graph provides
visualizations to help you to determine the nature and extent of possible
security issues and conduct an efficient investigation.
See Also https://aws.amazon.com/detective/.
Device Farm AWS Device Farm is an app testing service that you can use to test
Android, iOS, and web apps on real, physical phones and tablets that are
hosted by AWS.
See Also https://aws.amazon.com/device-farm/.
Amazon DevOps Guru Amazon DevOps Guru is a fully managed operations service powered by
machine learning (ML), designed to improve an application's operational
performance and availability.
See Also https://aws.amazon.com/devops-guru/.
dimension A name–value pair (for example, InstanceType=m1.small, or
EngineName=mysql), that contains additional information to identify a
metric.
Direct Connect AWS Direct Connect is a web service that simplifies establishing a
dedicated network connection from your premises to AWS. Using AWS
Direct Connect, you can establish private connectivity between AWS and
your data center, office, or colocation environment.
See Also https://aws.amazon.com/directconnect.
Directory Service AWS Directory Service is a managed service for connecting your AWS
resources to an existing on-premises Microsoft Active Directory or to set
up and operate a new, standalone directory in the AWS Cloud.
See Also https://aws.amazon.com/directoryservice.
discussion forums A place where AWS users can post technical questions and feedback
to help accelerate their development efforts and to engage with the
AWS community. For more information, see the Amazon Web Services
Discussion Forums.
26
AWS Glossary Reference
distribution A link between an origin server (such as an Amazon S3 bucket) and a
domain name, which CloudFront automatically assigns. Through this
link, CloudFront identifies the object you have stored in your origin
server.
DKIM DomainKeys Identified Mail is a standard that email senders use to
sign their messages. ISPs use those signatures to verify that messages
are legitimate. For more information, see https://tools.ietf.org/html/
rfc6376.
AWS DMS AWS Database Migration Service is a web service that can help you
migrate data to and from many widely used commercial and opensource databases.
See Also https://aws.amazon.com/dms.
DNS See Domain Name System.
Docker image A layered file system template that's the basis of a Docker container.
Docker images can comprise specific operating systems or applications.
document CloudSearch: An item that can be returned as a search result. Each
document has a collection of fields that contain the data that can be
searched or returned. The value of a field can be either a string or a
number. Each document must have a unique ID and at least one field.
document batch CloudSearch: A collection of add and delete document operations. You
use the document service API to submit batches to update the data in
your search domain.
document service API CloudSearch: The API call that you use to submit document batches to
update the data in a search domain.
document service
endpoint
CloudSearch: The URL that you connect to when sending document
updates to an Amazon CloudSearch domain. Each search domain has a
unique document service endpoint that remains the same for the life of
the domain.
Amazon DocumentDB Amazon DocumentDB (with MongoDB compatibility) is a managed
database service that you can use to set up, operate, and scale
MongoDB-compatible databases in the cloud.
27
AWS Glossary Reference
See Also https://aws.amazon.com/documentdb/.
domain OpenSearch Service: The hardware, software, and data exposed by
Amazon OpenSearch Service (OpenSearch Service) endpoints. An
OpenSearch Service domain is a service wrapper around an OpenSearch
cluster. An OpenSearch Service domain encapsulates the engine
instances that process OpenSearch Service requests, the indexed data
that you want to search, snapshots of the domain, access policies, and
metadata.
See Also cluster, Elasticsearch.
Domain Name System Domain Name System is a service that routes internet traffic to
websites by translating human-readable domain names (for example,
www.example.com) into the numeric IP addresses, such as 192.0.2.1,
which computers use to connect to each other.
Donation button An HTML-coded button to provide a simple and secure way for
US-based, IRS-certified 501(c)(3) nonprofit organizations to solicit
donations.
DynamoDB Amazon DynamoDB is a fully managed NoSQL database service that
provides fast and predictable performance with seamless scalability.
See Also https://aws.amazon.com/dynamodb/.
Amazon DynamoDB
Encryption Client
Amazon DynamoDB Encryption Client is a software library that helps
you protect your table data before you send it to DynamoDB.
Amazon DynamoDB
Storage Backend for
Titan
Amazon DynamoDB Storage Backend for Titan is a graph database
implemented on top of Amazon DynamoDB. Titan is a scalable graph
database optimized for storing and querying graphs.
See Also https://aws.amazon.com/dynamodb/.
DynamoDB Streams Amazon DynamoDB Streams is an AWS service that captures a timeordered sequence of item-level modifications in any Amazon DynamoDB
table. This service also stores this information in a log for up to 24
hours. Applications can access this log and view the data items as they
appeared before and after they were modified, in near-real time.
See Also https://aws.amazon.com/dynamodb/.
28
AWS Glossary Reference
E
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
Amazon EBS Amazon Elastic Block Store is a service that provides block level storage
volumes or use with EC2 instances.
See Also https://aws.amazon.com/ebs.
Amazon EBS-backed
AMI
An Amazon EBS-backed AMI is a type of Amazon Machine Image (AMI)
whose instances use an Amazon EBS volume as their root device.
Compare this with instances launched from instance store-backed AMIs,
which use the instance store as the root device.
Amazon EC2 Amazon Elastic Compute Cloud is a web service for launching and
managing Linux/UNIX and Windows Server instances in Amazon data
centers.
See Also https://aws.amazon.com/ec2.
Amazon EC2 Auto
Scaling
Amazon EC2 Auto Scaling is a web service that launches or terminates
instances automatically based on user-defined policies, schedules, and
health checks.
See Also https://aws.amazon.com/ec2/autoscaling.
EC2 instance A compute instance in the Amazon EC2 service. Other AWS services use
the term EC2 instance to distinguish these instances from other types of
instances they support.
Amazon ECR Amazon Elastic Container Registry (Amazon ECR) is a fully managed
Docker container registry that you can use to store, manage, and deploy
Docker container images. Amazon ECR is integrated with Amazon ECS
and IAM.
See Also https://aws.amazon.com/ecr.
Amazon ECS Amazon Elastic Container Service (Amazon ECS) is a highly scalable,
fast, container management service that you can use to run, stop, and
manage Docker containers on a cluster of EC2 instances.
See Also https://aws.amazon.com/ecs.
29
AWS Glossary Reference
edge location edge location is a data center that an AWS service uses to perform
service-specific operations. For example, CloudFront uses edge locations
to cache copies of your content, so the content is closer to your users
and can be delivered faster regardless of their location. Route 53 uses
edge locations to speed up the response to public DNS queries.
Amazon EFS Amazon Elastic File System is a file storage service for EC2 instances.
Amazon EFS provides an interface that you can use to create and
configure file systems. Amazon EFS storage capacity grows and shrinks
automatically as you add and remove files.
See Also https://aws.amazon.com/efs/.
Amazon EKS Amazon Elastic Kubernetes Service is a managed service that you can
use to run Kubernetes on AWS without needing to stand up or maintain
your own Kubernetes control plane.
See Also https://aws.amazon.com/eks/.
Elastic A company that provides open-source solutions—including OpenSearch,
Logstash, Kibana, and Beats—that take data from any source and search,
analyze, and visualize it in real time.
Amazon OpenSearch Service (OpenSearch Service) is an AWS managed
service for deploying, operating, and scaling OpenSearch in the AWS
Cloud.
See Also OpenSearch Service, Elasticsearch.
Elastic Beanstalk AWS Elastic Beanstalk is a web service for deploying and managing
applications in the AWS Cloud without worrying about the infrastructure
that runs those applications.
See Also https://aws.amazon.com/elasticbeanstalk.
Elastic Block Store See Amazon EBS.
Elastic Inference Amazon Elastic Inference is a resource that customers can use to attach
low-cost GPU-powered acceleration to Amazon EC2 and SageMaker
AI instances, or Amazon ECS tasks, to reduce the cost of running deep
learning inference by up to 75%.
See Also https://aws.amazon.com/machine-learning/elastic-inference.
30
AWS Glossary Reference
Elastic IP address A fixed (static) IP address that you have allocated in Amazon EC2 or
Amazon VPC and then attached to an instance. Elastic IP addresses are
associated with your account, not a specific instance. They are elastic
because you can easily allocate, attach, detach, and free them as your
needs change. Unlike traditional static IP addresses, Elastic IP addresses
allow you to mask instance or Availability Zone failures by rapidly
remapping your public IP addresses to another instance.
ELB Elastic Load Balancing is a web service that improves an application's
availability by distributing incoming traffic between two or more EC2
instances.
See Also https://aws.amazon.com/elasticloadbalancing.
elastic network
interface
An additional network interface that can be attached to an instance.
Elastic network interfaces include a primary private IP address, one or
more secondary private IP addresses, an Elastic IP Address (optional),
a MAC address, membership in specified security groups, a description,
and a source/destination check flag. You can create an elastic network
interface, attach it to an instance, detach it from an instance, and attach
it to another instance.
Elastic Transcoder Amazon Elastic Transcoder is a cloud-based media transcoding service.
Elastic Transcoder is a highly scalable tool for converting (or transcoding)
media files from their source format into versions that play on devices
such as smartphones, tablets, and PCs.
See Also https://aws.amazon.com/elastictranscoder/.
ElastiCache Amazon ElastiCache is a web service that simplifies deploying, operating,
and scaling an in-memory cache in the cloud. The service improves the
performance of web applications by providing information retrieval from
fast, managed, in-memory caches, instead of relying entirely on slower
disk-based databases.
See Also https://aws.amazon.com/elasticache/.
Elasticsearch An open-source, real-time distributed search and analytics engine used
for full-text search, structured search, and analytics. OpenSearch was
developed by the Elastic company.
31
AWS Glossary Reference
Amazon OpenSearch Service (OpenSearch Service) is an AWS managed
service for deploying, operating, and scaling OpenSearch in the AWS
Cloud.
See Also OpenSearch Service, Elastic.
AWS Elemental
MediaConnect
AWS Elemental MediaConnect is a fully-managed live video distribution
service that reliably and securely ingests video into the AWS Cloud and
transports it to multiple destinations within the AWS network and the
internet.
See Also https://aws.amazon.com/mediaconnect.
AWS Elemental
MediaConvert
AWS Elemental MediaConvert is a file-based media conversion service
that transforms content into formats for traditional broadcast and
internet streaming.
See Also https://aws.amazon.com/mediaconvert.
AWS Elemental
MediaLive
AWS Elemental MediaLive is a cloud-based live video encoding service
that creates high-quality streams for delivery to broadcasts and
internet-connected devices.
See Also https://aws.amazon.com/medialive.
AWS Elemental
MediaPackage
AWS Elemental MediaPackage is a highly-scalable video origination and
packaging service that delivers video securely and reliably.
See Also https://aws.amazon.com/mediapackage.
AWS Elemental
MediaStore
AWS Elemental MediaStore is a storage service optimized for media
that provides the performance, consistency, and low latency required to
deliver live and on-demand video content at scale.
See Also https://aws.amazon.com/mediastore.
AWS Elemental
MediaTailor
AWS Elemental MediaTailor is a channel assembly and personalized adinsertion service for over-the-top (OTT) video and audio applications.
See Also https://aws.amazon.com/mediatailor.
EMP The AWS End-of-Support Migration Program for Windows Server
provides the technology and guidance to migrate your applications
running on Windows Server 2003, Windows Server 2008, and Windows
Server 2008 R2 to the latest, supported versions of Windows Server
running on Amazon Web Services (AWS).
32
AWS Glossary Reference
Amazon EMR Amazon Elastic Map Reduce is a web service that you can use to process
large amounts of data efficiently. Amazon EMR uses Hadoop processing
combined with several AWS products to do such tasks as web indexing,
data mining, log file analysis, machine learning, scientific simulation, and
data warehousing.
See Also https://aws.amazon.com/elasticmapreduce.
encrypt To use a mathematical algorithm to make data unintelligible to
unauthorized users. Encryption also gives authorized users a method
(such as a key or password) to convert the altered data back to its
original state.
encryption context A set of key–value pairs that contains additional information associated
with AWS KMS–encrypted information.
AWS Encryption SDK AWS Encryption SDK is a client-side encryption library that you can use
to encrypt and decrypt data using industry standards and best practices.
See Also https://aws.amazon.com/blogs/security/tag/aws-encryptionsdk/.
endpoint A URL that identifies a host and port as the entry point for a web
service. Every web service request contains an endpoint. Most AWS
products provide endpoints for a Region to enable faster connectivity.
ElastiCache: The DNS name of a cache node.
Amazon RDS: The DNS name of a DB instance.
CloudFormation: The DNS name or IP address of the server that receives
an HTTP request.
endpoint port ElastiCache: The port number used by a cache node.
Amazon RDS: The port number used by a DB instance.
envelope encryption The use of a master key and a data key to algorithmically protect data.
The master key is used to encrypt and decrypt the data key and the data
key is used to encrypt and decrypt the data itself.
environment Elastic Beanstalk: A specific running instance of an application. The
application has a CNAME and includes an application version and
33
AWS Glossary Reference
a customizable configuration (which is inherited from the default
container type).
CodeDeploy: Instances in a deployment group in a blue/green
deployment. At the start of a blue/green deployment, the deployment
group is made up of instances in the original environment. At the end of
the deployment, the deployment group is made up of instances in the
replacement environment.
environment
configuration
A collection of parameters and settings that define how an environment
and its associated resources behave.
ephemeral store See instance store.
epoch The date from which time is measured. For most Unix environments, the
epoch is January 1, 1970.
ETL See extract, transform, and load (ETL).
evaluation Amazon Machine Learning: The process of measuring the predictive
performance of a machine learning (ML) model.
Also a machine learning object that stores the details and result of an
ML model evaluation.
evaluation datasource The data that Amazon Machine Learning uses to evaluate the predictive
accuracy of a machine learning model.
event Amazon Personalize: A user activity—such as a click, a purchase, or a
video viewing—that you record and upload to an Amazon Personalize
Interactions dataset. You record events individually in real time or record
and upload events in bulk.
See Also dataset, Interactions dataset.
event tracker Amazon Personalize: Specifies a destination dataset group for event
data that you record in real time. When you record events in real time,
you provide the ID of the event tracker so that Amazon Personalize
knows where to add the data.
See Also dataset group, event.
34
AWS Glossary Reference
EventBridge Amazon EventBridge is a serverless event bus service that you can use to
connect your applications with data from a variety of sources and routes
that data to targets such as AWS Lambda. You can set up routing rules
to determine where to send your data to build application architectures
that react in real time to all of your data sources.
See Also https://aws.amazon.com/eventbridge/.
eventual consistency The method that AWS services use to achieve high availability. This
involves replicating data across multiple servers in Amazon data centers.
When data is written or updated and Success is returned, all copies of
the data are updated. However, it takes time for the data to propagate
to all storage locations. The data will eventually be consistent, but an
immediate read might not show the change. Consistency is usually
reached within seconds.
See Also data consistency, eventually consistent read, strongly
consistent read.
eventually consistent
read
A read process that returns data from only one Region and might not
show the most recent write information. However, if you repeat your
read request after a short time, the response should eventually return
the latest data.
See Also data consistency, eventual consistency, strongly consistent
read.
eviction The deletion by CloudFront of an object from an edge location before
its expiration time. If an object in an edge location isn't frequently
requested, CloudFront might evict the object (remove the object before
its expiration date) to make room for objects that are more popular.
exbibyte (EiB) A contraction of exa binary byte. An exbibyte (EiB) is 2^60 or
1,152,921,504,606,846,976 bytes. An exabyte (EB) is 10^18 or
1,000,000,000,000,000,000 bytes. 1,024 EiB is a zebibyte (ZiB).
expiration For CloudFront caching, the time when CloudFront stops responding
to user requests with an object. If you don't use headers or CloudFront
distribution settings to specify how long you want objects to stay in an
edge location, the objects expire after 24 hours. The next time a user
requests an object that has expired, CloudFront forwards the request to
the origin.
35
AWS Glossary Reference
explicit impressions Amazon Personalize: A list of items that you manually add to an Amazon
Personalize Interactions dataset to influence future recommendations.
Unlike implicit impressions, where Amazon Personalize automatically
derives the impressions data, you choose what to include in explicit
impressions.
See Also recommendations, Interactions dataset, impressions data,
implicit impressions.
explicit launch
permission
An Amazon Machine Image (AMI) launch permission granted to a specific
AWS account.
exponential backoff A strategy that incrementally increases the wait between retry attempts
in order to reduce the load on the system and increase the likelihood
that repeated requests will succeed. For example, client applications
might wait up to 400 milliseconds before attempting the first retry, up
to 1600 milliseconds before the second, and up to 6400 milliseconds
(6.4 seconds) before the third.
expression CloudSearch: A numeric expression that you can use to control
how search hits are sorted. You can construct Amazon CloudSearch
expressions using numeric fields, other rank expressions, a document's
default relevance score, and standard numeric operators and functions.
When you use the sort option to specify an expression in a search
request, the expression is evaluated for each search hit and the hits are
listed according to their expression values.
extract, transform, and
load (ETL)
A process that's used to integrate data from multiple sources. Data is
collected from sources (extract), converted to an appropriate format
(transform), and written to a target data store (load) for purposes of
analysis and querying.
ETL tools combine these three functions to consolidate and move data
from one environment to another. AWS Glue is a fully managed ETL
service for discovering and organizing data, transforming it, and making
it available for search and analytics.
36
AWS Glossary Reference
F
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
facet CloudSearch: An index field that represents a category that you want to
use to refine and filter search results.
facet enabled CloudSearch: An index field option that enables facet information to be
calculated for the field.
AWS Fargate AWS Fargate is a serverless, pay-as-you-go compute engine that you can
use to build applications on AWS. You can use Amazon Elastic Container
Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon
EKS) to maintain container applications using AWS Fargate.
See Also https://aws.amazon.com/fargate/.
Fault Injection
Simulator (AWS FIS)
AWS Fault Injection Service is a managed service that you can use to
perform fault injection experiments on your AWS workloads.
See Also https://aws.amazon.com/fis.
FBL See feedback loop (FBL).
feature transformation Amazon Machine Learning: The machine learning process of
constructing more predictive input representations or “features” from
the raw input variables to optimize a machine learning model’s ability
to learn and generalize. Also known as data transformation or feature
engineering.
federated identity
management (FIM)
Allows individuals to sign in to different networks or services, using the
same group or personal credentials to access data across all networks.
With identity federation in AWS, external identities (federated users) are
granted secure access to resources in an AWS account without having to
create IAM users. These external identities can come from a corporate
identity store (such as LDAP or Windows Active Directory) or from a third
party (such as Login with Amazon, Facebook, or Google). AWS federation
also supports SAML 2.0.
federated user See federated identity management (FIM).
37
AWS Glossary Reference
federation See federated identity management (FIM).
feedback loop (FBL) The mechanism by which a mailbox provider (for example, an internet
service provider (ISP)) forwards a recipient's complaint back to the
sender.
field weight The relative importance of a text field in a search index. Field weights
control how much matches in particular text fields affect a document's
relevance score.
filter A criterion that you specify to limit the results when you list or describe
your Amazon EC2 resources.
filter query A way to filter search results without affecting how the results are scored
and sorted. Specified with the CloudSearch fq parameter.
FIM See federated identity management (FIM).
FinSpace Amazon FinSpace is a data management and analytics service purposebuilt for the financial services industry (FSI).
See Also https://aws.amazon.com/finspace.
Firehose See Firehose.
Firewall Manager AWS Firewall Manager is a service that you use with AWS WAF to
simplify your AWS WAF administration and maintenance tasks across
multiple accounts and resources. With AWS Firewall Manager, you set up
your firewall rules only once. The service automatically applies your rules
across your accounts and resources, even as you add new resources.
See Also https://aws.amazon.com/firewall-manager.
Forecast Amazon Forecast is a fully managed service that uses statistical and
machine learning algorithms to produce highly accurate time-series
forecasts.
See Also https://aws.amazon.com/forecast/.
format version See template format version.
forums See discussion forums.
function See intrinsic function.
38
AWS Glossary Reference
fuzzy search A simple search query that uses approximate string matching (fuzzy
matching) to correct for typographical errors and misspellings.
G
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
GameKit AWS GameKit is an open-source SDK and game engine plugin that
empowers game developers to build and deploy cloud-based features
with AWS from their game engine.
See Also https://aws.amazon.com/gamekit/.
Amazon GameLift
Servers
Amazon GameLift Servers is a managed service for deploying, operating,
and scaling session-based multiplayer games.
See Also https://aws.amazon.com/gamelift/.
GameSparks Amazon GameSparks is a fully managed AWS service that provides a
multi-service backend for game developers.
See Also https://aws.amazon.com/gamesparks/.
geospatial search A search query that uses locations specified as a latitude and longitude
to determine matches and sort the results.
gibibyte (GiB) A contraction of giga binary byte, a gibibyte is 2^30 or 1,073,741,824
bytes. A gigabyte (GB) is 10^9 or 1,000,000,000 bytes. 1,024 GiB is a
tebibyte (TiB).
GitHub A web-based repository that uses Git for version control.
Global Accelerator AWS Global Accelerator is a network layer service that you use to create
accelerators that direct traffic to optimal endpoints over the AWS global
network. This improves the availability and performance of your internet
applications that are used by a global audience.
See Also https://aws.amazon.com/global-accelerator.
global consistency An active-active strategy in which all reads and writes for a workload are
handled in the Region where the request originates and are replicated
synchronously to all other Regions in the architecture.
See Also , .
39
AWS Glossary Reference
global secondary index An index with a partition key and a sort key that can be different from
those on the table. A global secondary index is considered global
because queries on the index can span all of the data in a table, across
all partitions.
See Also local secondary index.
AWS Glue AWS Glue is a fully managed extract, transform, and load (ETL) service
that you can use to catalog data and load it for analytics. With AWS
Glue, you can discover your data, develop scripts to transform sources
into targets, and schedule and run ETL jobs in a serverless environment.
See Also https://aws.amazon.com/glue.
AWS GovCloud (US) AWS GovCloud (US) is an isolated AWS Region that hosts sensitive
workloads in the cloud, ensuring that this work meets the US
government's regulatory and compliance requirements. The AWS
GovCloud (US) Region adheres to United States International Traffic in
Arms Regulations (ITAR), Federal Risk and Authorization Management
Program (FedRAMP) requirements, Department of Defense (DOD) Cloud
Security Requirements Guide (SRG) Levels 2 and 4, and Criminal Justice
Information Services (CJIS) Security Policy requirements.
See Also https://aws.amazon.com/govcloud-us/.
grant AWS KMS: A mechanism for giving AWS principals long-term
permissions to use KMS keys.
grant token A type of identifier that allows the permissions in a grant to take effect
immediately.
ground truth The observations used in the machine learning (ML) model training
process that include the correct value for the target attribute. To train
an ML model to predict house sales prices, the input observations would
typically include prices of previous house sales in the area. The sale
prices of these houses constitute the ground truth.
group A collection of IAM users. You can use IAM groups to simplify specifying
and managing permissions for multiple users.
GuardDuty Amazon GuardDuty is a continuous security monitoring service. Amazon
GuardDuty can help to identify unexpected and potentially unauthorized
or malicious activity in your AWS environment.
40
AWS Glossary Reference
See Also https://aws.amazon.com/guardduty/.
H
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
Hadoop Software that enables distributed processing for big data by using
clusters and simple programming models. For more information, see
http://hadoop.apache.org.
hard bounce A persistent email delivery failure such as "mailbox does not exist."
hardware VPN A hardware-based IPsec VPN connection over the internet.
AWS Health AWS Health is a service that provides ongoing visibility into AWS
customers' accounts and the availability of their AWS services and
resources.
See Also https://aws.amazon.com/premiumsupport/technology/awshealth-dashboard.
health check A system call to check on the health status of each instance in an
Amazon EC2 Auto Scaling group.
HealthLake AWS HealthLake is a HIPAA-eligible service that helps customers
store, query, and generate artificial intelligence (AI) and machine
learning (ML) insights from healthcare data and enables healthcare data
interoperability.
See Also https://aws.amazon.com/healthlake.
highlight enabled CloudSearch: An index field option that enables matches within the field
to be highlighted.
highlights CloudSearch: Excerpts returned with search results that show where the
search terms appear within the text of the matching documents.
high-quality email Email that recipients find valuable and want to receive. Value means
different things to different recipients and can come in such forms as
offers, order confirmations, receipts, or newsletters.
41
AWS Glossary Reference
hit A document that matches the criteria specified in a search request. Also
referred to as a search result.
HMAC Hash-based Message Authentication Code is a specific construction
for calculating a message authentication code (MAC) involving a
cryptographic hash function in combination with a secret key. You
can use it to verify both the data integrity and the authenticity of a
message at the same time. AWS calculates the HMAC using a standard,
cryptographic hash algorithm, such as SHA-256.
hosted zone A collection of resource record sets that Route 53 hosts. Similar to
a traditional DNS zone file, a hosted zone represents a collection of
records that are managed together under a single domain name.
hot standby An active-passive disaster recovery strategy in which a workload is fully
scaled up in both the primary and standby Regions, but serves traffic
from only the primary Region.
See Also , , .
HRNN Amazon Personalize: A hierarchical recurrent neural network machine
learning algorithm that models changes in user behavior and predicts
the items that a user might interact with in personal recommendation
applications.
HTTP-Query See Query.
HVM virtualization Hardware Virtual Machine virtualization. Allows the guest VM to run
as though it's on a native hardware platform, except that it still uses
paravirtual (PV) network and storage drivers for improved performance.
See Also PV virtualization.
I
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
IAM AWS Identity and Access Management is a web service that Amazon Web
Services (AWS) customers can use to manage users and user permissions
within AWS.
42
AWS Glossary Reference
See Also https://aws.amazon.com/iam.
IAM Access Analyzer Access Management Access Analyzer is a feature of IAM that you can
use to identify the resources in your organization and accounts that are
shared with an external entity. Example resources include Amazon S3
buckets or IAM roles.
See Also https://aws.amazon.com/about-aws/whats-new/2019/12/
introducing-aws-identity-and-access-management-access-analyzer/.
IAM group See group.
IAM Identity Center AWS IAM Identity Center is a cloud-based service that brings together
administration of users and their access to AWS accounts and cloud
applications. You can control single sign-on access and user permissions
across all your AWS accounts in AWS Organizations.
See Also https://aws.amazon.com/single-sign-on/.
IAM policy simulator See policy simulator.
IAM role See role.
IAM user See user.
Identity and Access
Management
See IAM.
identity provider (IdP) An IAM entity that holds metadata about external identity providers.
IdP See identity provider (IdP) .
image See Amazon Machine Image (AMI).
Image Builder EC2 Image Builder is a service that facilitates building, maintaining, and
distributing customized server images that launch EC2 instances, or that
run in Docker containers.
See Also https://aws.amazon.com/image-builder.
implicit impressions Amazon Personalize: The recommendations that your application
shows a user. Unlike explicit impressions, where you manually record
each impression, Amazon Personalize automatically derives implicit
impressions from your recommendation data.
43
AWS Glossary Reference
See Also recommendations, impressions data, explicit impressions.
import log A report that contains details about how Import/Export processed your
data.
Import/Export AWS Import/Export is a service for transferring large amounts of data
between AWS and portable storage devices.
See Also https://aws.amazon.com/importexport.
import/export station A machine that uploads or downloads your data to or from Amazon S3.
impressions data Amazon Personalize: The list of items that you presented to a user when
they interacted with a particular item such as by clicking it, watching it,
or purchasing it. Amazon Personalize uses impressions data to calculate
the relevance of new items for a user based on how frequently users
have selected or ignored the same item.
See Also explicit impressions, implicit impressions.
index See search index.
index field A name–value pair that's included in an CloudSearch domain's index. An
index field can contain text or numeric data, dates, or a location.
indexing options Configuration settings that define an CloudSearch domain's index fields,
how document data is mapped to those index fields, and how the index
fields can be used.
inline policy An IAM policy that's embedded in a single IAM user, group, or role.
in-place deployment CodeDeploy: A deployment method where the application on each
instance in the deployment group is stopped, the latest application
revision is installed, and the new version of the application is started
and validated. You can choose to use a load balancer so each instance
is deregistered during its deployment and then restored to service after
the deployment is complete.
input data Amazon Machine Learning: The observations that you provide to
Amazon Machine Learning to train and evaluate a machine learning
model and generate predictions.
44
AWS Glossary Reference
Amazon Inspector Amazon Inspector is an automated security assessment service that
helps improve the security and compliance of applications deployed
on AWS. Amazon Inspector automatically assesses applications for
vulnerabilities or deviations from best practices. After performing
an assessment, Amazon Inspector produces a detailed report with
prioritized steps for remediation.
See Also https://aws.amazon.com/inspector.
instance A copy of an Amazon Machine Image (AMI) running as a virtual server in
the AWS Cloud.
instance family A general instance type grouping using either storage or CPU capacity.
instance group A Hadoop cluster contains one master instance group that contains
one master node, a core instance group that contains one or more core
node and an optional task node instance group, which can contain any
number of task nodes.
instance profile A container that passes IAM role information to an EC2 instance at
launch.
instance store Disk storage that's physically attached to the host computer for an EC2
instance, and therefore has the same lifespan as the instance. When the
instance is terminated, you lose any data in the instance store.
instance store-backed
AMI
A type of Amazon Machine Image (AMI) whose instances use an instance
store volume as the root device. Compare this with instances launched
from Amazon EBS-backed AMIs, which use an Amazon EBS volume as
the root device.
instance type A specification that defines the memory, CPU, storage capacity, and
usage cost for an instance. Some instance types are for standard
applications, whereas others are for CPU-intensive, memory-intensive
applications.
Interactions dataset Amazon Personalize: A container for historical and real-time data
collected from interactions between users and items (called events).
Interactions data can include impressions data and contextual metadata.
See Also dataset, event, impressions data, contextual metadata.
45
AWS Glossary Reference
internet gateway Connects a network to the internet. You can route traffic for IP addresses
outside your Amazon VPC to the internet gateway.
internet service
provider (ISP)
A company that provides subscribers with access to the internet. Many
ISPs are also mailbox providers. Mailbox providers are sometimes
referred to as ISPs, even if they only provide mailbox services.
intrinsic function A special action in a CloudFormation template that assigns values to
properties not available until runtime. These functions follow the format
Fn::Attribute, such as Fn::GetAtt. Arguments for intrinsic functions
can be parameters, pseudo parameters, or the output of other intrinsic
functions.
AWS IoT 1-Click AWS IoT 1-Click is a service that simple devices can use to launch AWS
Lambda functions.
See Also https://aws.amazon.com/iot-1-click.
AWS IoT Analytics AWS IoT Analytics is a fully managed service used to run sophisticated
analytics on massive volumes of IoT data.
See Also https://aws.amazon.com/iot-analytics.
AWS IoT Core AWS IoT Core is a managed cloud platform that lets connected devices
easily and securely interact with cloud applications and other devices.
See Also https://aws.amazon.com/iot.
AWS IoT Device
Defender
AWS IoT Device Defender is an AWS IoT security service that you can
use to audit the configuration of your devices, monitor your connected
devices to detect abnormal behavior, and to mitigate security risks.
See Also https://aws.amazon.com/iot-device-defender.
AWS IoT Device
Management
AWS IoT Device Management is a service used to securely onboard,
organize, monitor, and remotely manage IoT devices at scale.
See Also https://aws.amazon.com/iot-device-management.
AWS IoT Events AWS IoT Events is a fully managed AWS IoT service that you can use to
detect and respond to events from IoT sensors and applications.
See Also https://aws.amazon.com/iot-events.
AWS IoT FleetWise AWS IoT FleetWise is a service that you can use to collect, transform, and
transfer vehicle data to the cloud at scale.
See Also https://aws.amazon.com/iot-fleetwise.
46
AWS Glossary Reference
AWS IoT Greengrass AWS IoT Greengrass is a software that you can use to run local compute,
messaging, data caching, sync, and ML inference capabilities for
connected devices in a secure way.
See Also https://aws.amazon.com/greengrass.
AWS IoT RoboRunner AWS IoT RoboRunner is a solution that provides infrastructure for
integrating robots with work management systems and building
robotics fleet management applications.
See Also https://aws.amazon.com/roborunner.
AWS IoT SiteWise AWS IoT SiteWise is a managed service that you can use to collect,
organize, and analyze data from industrial equipment at scale.
See Also https://aws.amazon.com/iot-sitewise.
AWS IoT Things Graph AWS IoT Things Graph is a service that you can use to visually connect
different devices and web services to build IoT applications.
See Also https://aws.amazon.com/iot-things-graph.
IP address A numerical address (for example, 192.0.2.44) that networked devices
use to communicate with one another using the Internet Protocol (IP).
Each EC2 instance is assigned two IP addresses at launch, which are
directly mapped to each other through network address translation
(NAT): a private IP address (following RFC 1918) and a public IP address.
Instances launched in a VPC are assigned only a private IP address.
Instances launched in your default VPC are assigned both a private IP
address and a public IP address.
IP match condition AWS WAF: An attribute that specifies the IP addresses or IP address
ranges that web requests originate from. Based on the specified IP
addresses, you can configure AWS WAF to allow or block web requests to
AWS resources such as Amazon CloudFront distributions.
AWS IQ AWS IQ is a cloud service that AWS customers can use to find, engage,
and pay AWS Certified third-party experts for on-demand project work.
See Also ???TITLE???.
ISP See internet service provider (ISP).
issuer The person who writes a policy to grant permissions to a resource. The
issuer (by definition) is always the resource owner. AWS doesn't permit
47
AWS Glossary Reference
Amazon SQS users to create policies for resources they don't own. If
John is the resource owner, AWS authenticates John's identity when he
submits the policy he's written to grant permissions for that resource.
item A group of attributes that's uniquely identifiable among all of the other
items. Items in DynamoDB are similar in many ways to rows, records, or
tuples in other database systems.
item exploration Amazon Personalize: The process that Amazon Personalize uses to test
different item recommendations, including recommendations of new
items with no or little interaction data, and learn how users respond. You
configure item exploration at the campaign level for solution versions
created with the user-personalization recipe.
See Also recommendations, campaign, solution version, userpersonalization recipe.
Items dataset Amazon Personalize: A container for metadata about items, such as
price, genre, or availability.
See Also dataset.
item-to-item
similarities (SIMS)
recipe
Amazon Personalize: A RELATED_ITEMS recipe that uses the data from
an Interactions dataset to make recommendations for items that are
similar to a specified item. The SIMS recipe calculates similarity based on
the way users interact with items instead of matching item metadata,
such as price or age.
See Also recipe, RELATED_ITEMS recipes, Interactions dataset.
J
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
job flow Amazon EMR: One or more steps that specify all of the functions to be
performed on the data.
job ID A five-character, alphanumeric string that uniquely identifies an Import/
Export storage device in your shipment. AWS issues the job ID in
response to a CREATE JOB email command.
48
AWS Glossary Reference
job prefix An optional string that you can add to the beginning of an Import/
Export log file name to prevent collisions with objects of the same
name.
See Also key prefix.
JSON JavaScript Object Notation. A lightweight data interchange format. For
information about JSON, see http://www.json.org/.
junk folder The location where email messages that various filters determine to be
of lesser value are collected so that they don't arrive in the recipient's
inbox but are still accessible to the recipient. This is also referred to as a
spam or bulk folder.
K
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
Amazon Kendra Amazon Kendra is a search service powered by machine learning (ML)
that developers can use to add search capabilities to their applications
so their end users can discover information stored within the vast
amount of content spread across their company.
See Also https://aws.amazon.com/kendra/.
key A credential that identifies an AWS account or user to AWS (such as the
AWS secret access key).
Amazon S3, Amazon EMR: The unique identifier for an object in a
bucket. Every object in a bucket has exactly one key. Because a bucket
and key together uniquely identify each object, you can think of Amazon
S3 as a basic data map between the bucket + key, and the object itself.
You can uniquely address every object in Amazon S3 through the
combination of the web service endpoint, bucket name, and key, as
in this example: http://doc.s3.amazonaws.com/2006-03-01/
AmazonS3.wsdl, where doc is the name of the bucket, and
2006-03-01/AmazonS3.wsdl is the key.
Import/Export: The name of an object in Amazon S3. It's a sequence of
Unicode characters whose UTF-8 encoding can't exceed 1024 bytes. If
49
AWS Glossary Reference
a key (for example, logPrefix + import-log-JOBID) is longer than 1024
bytes, Elastic Beanstalk returns an InvalidManifestField error.
IAM: In a policy, a specific characteristic that's the basis for restricting
access (such as the current time or the IP address of the requester).
Tagging resources: A general tag label that acts like a category for more
specific tag values. For example, you might have EC2 instance with the
tag key of Owner and the tag value of Jan. You can tag an AWS resource
with up to 10 key–value pairs. Not all AWS resources can be tagged.
key pair A set of security credentials that you use to prove your identity
electronically. A key pair consists of a private key and a public key.
key prefix A string of characters that is a subset of an object key name, starting
with the first character. The prefix can be any length, up to the
maximum length of the object key name (1,024 bytes).
Amazon Keyspaces Amazon Keyspaces (for Apache Cassandra) is a scalable, highly available,
and managed Apache Cassandra-compatible database service.
See Also https://aws.amazon.com/keyspaces/.
kibibyte (KiB) A contraction of kilo binary byte, a kibibyte is 2^10 or 1,024 bytes. A
kilobyte (KB) is 10^3 or 1,000 bytes. 1,024 KiB is a mebibyte (MiB).
Kinesis Amazon Kinesis is a platform for streaming data on AWS. Kinesis offers
services that simplify the loading and analysis of streaming data.
See Also https://aws.amazon.com/kinesis/.
Firehose Amazon Data Firehose is a fully managed service for loading streaming
data into AWS. Firehose can capture and automatically load streaming
data into Amazon S3 and Amazon Redshift , enabling near real-time
analytics with existing business intelligence tools and dashboards.
Firehose automatically scales to match the throughput of your data and
requires no ongoing administration. It can also batch, compress, and
encrypt the data before loading it.
See Also https://aws.amazon.com/kinesis/firehose/.
Kinesis Data Streams Amazon Kinesis Data Streams is a web service for building custom
applications that process or analyze streaming data for specialized
50
AWS Glossary Reference
needs. Amazon Kinesis Data Streams can continuously capture and store
terabytes of data per hour from hundreds of thousands of sources.
See Also https://aws.amazon.com/kinesis/streams/.
AWS KMS AWS Key Management Service is a managed service that simplifies the
creation and control of encryption keys that are used to encrypt data.
See Also https://aws.amazon.com/kms.
KMS key The primary resource in AWS Key Management Service. In general, KMS
keys are created, used, and deleted entirely within KMS. KMS supports
symmetric and asymmetric KMS keys for encryption and signing. KMS
keys can be either customer managed, AWS managed, or AWS owned.
For more information, see AWS KMS keys in the AWS Key Management
Service Developer Guide.
L
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
labeled data In machine learning, data for which you already know the target or
“correct” answer.
Lake Formation AWS Lake Formation is a managed service that makes it easy to set up,
secure, and manage your data lakes. Lake Formation helps you discover
your data sources and then catalog, cleanse, and transform the data.
See Also https://aws.amazon.com/lake-formation.
Lambda AWS Lambda is a web service that you can use to run code without
provisioning or managing servers. You can run code for virtually any
type of application or backend service with zero administration. You can
set up your code to automatically start from other AWS services or call it
directly from any web or mobile app.
See Also https://aws.amazon.com/lambda/.
launch configuration A set of descriptive parameters used to create new EC2 instances in an
Amazon EC2 Auto Scaling activity.
A template that an Auto Scaling group uses to launch new EC2
instances. The launch configuration contains information such as the
51
AWS Glossary Reference
Amazon Machine Image (AMI) ID, the instance type, key pairs, security
groups, and block device mappings, among other configuration settings.
launch permission An Amazon Machine Image (AMI) attribute that allows users to launch an
AMI.
Launch Wizard AWS Launch Wizard is a cloud solution that offers a guided way of
sizing, configuring, and deploying AWS resources for third-party
applications, such as Microsoft SQL Server Always On and HANA based
SAP systems, without the need to manually identify and provision
individual AWS resources.
See Also https://aws.amazon.com/launchwizard.
Amazon Lex Amazon Lex is a fully managed artificial intelligence (AI) service with
advanced natural language models to design, build, test, and deploy
conversational interfaces in applications.
See Also https://aws.amazon.com/lex/.
lifecycle The lifecycle state of the EC2 instance contained in an Auto Scaling
group. EC2 instances progress through several states over their lifespan;
these include Pending, InService, Terminating and Terminated.
lifecycle action An action that can be paused by Auto Scaling, such as launching or
terminating an EC2 instance.
lifecycle hook A feature for pausing Auto Scaling after it launches or terminates an EC2
instance so that you can perform a custom action while the instance isn't
in service.
Lightsail Amazon Lightsail is a service used to launch and manage a virtual
private server with AWS. Lightsail offers bundled plans that include
everything you need to deploy a virtual private server, for a low monthly
rate.
See Also https://aws.amazon.com/lightsail/.
load balancer A DNS name combined with a set of ports, which together provide
a destination for all requests intended for your application. A load
balancer can distribute traffic to multiple application instances across
every Availability Zone within a Region. Load balancers can span
multiple Availability Zones within an AWS Region into which an Amazon
52
AWS Glossary Reference
EC2 instance was launched. But load balancers can't span multiple
Regions.
local secondary index An index that has the same partition key as the table, but a different sort
key. A local secondary index is local in the sense that every partition of
a local secondary index is scoped to a table partition that has the same
partition key value.
See Also local secondary index.
Amazon Location Amazon Location Service is a fully managed service that makes it easy
for a developer to add location functionality, such as maps, points
of interest, geocoding, routing, tracking, and geofencing, to their
applications, without sacrificing data security, user privacy, data quality,
or cost.
See Also https://aws.amazon.com/location/.
logical name A case-sensitive unique string within an CloudFormation template
that identifies a resource, mapping, parameter, or output. In an AWS
CloudFormation template, each parameter, resource, property, mapping,
and output must be declared with a unique logical name. You use the
logical name when dereferencing these items using the Ref function.
Lookout for
Equipment
Amazon Lookout for Equipment is a machine learning service that uses
data from sensors mounted on factory equipment to detect abnormal
behavior so you can take action before machine failures occur.
See Also https://aws.amazon.com/lookout-for-equipment/.
Lookout for Metrics Amazon Lookout for Metrics is a machine learning (ML) service that
automatically detects and diagnoses anomalies in business and
operational data, such as a sudden dip in sales revenue or customer
acquisition rates.
See Also https://aws.amazon.com/lookout-for-metrics.
Lookout for Vision Amazon Lookout for Vision is a machine learning service that uses
computer vision (CV) to find defects in industrial products. Amazon
Lookout for Vision can identify missing components in an industrial
product, damage to vehicles or structures, irregularities in production
lines, and even minuscule defects in silicon wafers—or any other
physical item where quality is important.
53
AWS Glossary Reference
See Also https://aws.amazon.com/lookout-for-vision/.
Lumberyard See O3DE.
M
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
Macie Amazon Macie is a security service that uses machine learning to
automatically discover, classify, and protect sensitive data in AWS.
See Also http://aws.amazon.com/macie/.
Mail Transfer Agent
(MTA)
Software that transports email messages from one computer to another
by using a client-server architecture.
mailbox provider An organization that provides email mailbox hosting services. Mailbox
providers are sometimes referred to as internet service providers (ISPs),
even if they only provide mailbox services.
mailbox simulator A set of email addresses that you can use to test an Amazon SESbased email-sending application without sending messages to actual
recipients. Each email address represents a specific scenario (such as a
bounce or complaint) and generates a typical response that's specific to
the scenario.
main route table The default route table that any new Amazon VPC subnet uses for
routing. You can associate a subnet with a different route table of your
choice. You can also change which route table is the main route table.
AWS Mainframe
Modernization
AWS Mainframe Modernization service is a cloud native platform for
migration, modernization, execution, and operation of mainframe
applications.
See Also https://aws.amazon.com/mainframe-modernization.
Managed Blockchain Amazon Managed Blockchain is a fully managed service for creating
and managing scalable blockchain networks using popular open source
frameworks.
See Also http://aws.amazon.com/managed-blockchain/.
54
AWS Glossary Reference
Amazon Managed
Grafana
Amazon Managed Grafana is a fully managed and secure data
visualization service that you can use to instantly query, correlate,
and visualize operational metrics, logs, and traces from multiple data
sources.
See Also https://aws.amazon.com/grafana/.
AWS managed key One type of KMS key in AWS KMS.
managed policy A standalone IAM policy that you can attach to multiple users, groups,
and roless in your IAM account. Managed policies can either be AWS
managed policies (which are created and managed by AWS) or customer
managed policies (which you create and manage in your AWS account).
AWS managed policy An IAM managed policy that's created and managed by AWS.
Amazon Managed
Service for
Prometheus
Amazon Managed Service for Prometheus is a service that provides
highly available, secure, and managed monitoring for your containers.
See Also https://aws.amazon.com/prometheus/.
AWS Management
Console
AWS Management Console is a graphical interface to manage compute,
storage, and other cloud resources.
See Also https://aws.amazon.com/console.
management portal AWS Management Portal for vCenter is a web service for managing your
AWS resources using VMware vCenter. You install the portal as a vCenter
plugin within your existing vCenter environment. After it's installed, you
can migrate VMware VMs to Amazon EC2 and manage AWS resources
from within vCenter.
See Also https://aws.amazon.com/ec2/vcenter-portal/.
manifest When sending a create job request for an import or export operation,
you describe your job in a text file called a manifest. The manifest file is
a YAML-formatted file that specifies how to transfer data between your
storage device and the AWS Cloud.
manifest file Amazon Machine Learning: The file used for describing batch
predictions. The manifest file relates each input data file with its
associated batch prediction results. It's stored in the Amazon S3 output
location.
55
AWS Glossary Reference
mapping A way to add conditional parameter values to an CloudFormation
template. You specify mappings in the template's optional Mappings
section and retrieve the desired value using the FN::FindInMap
function.
marker See pagination token.
AWS Marketplace AWS Marketplace is a web portal where qualified partners market and
sell their software to AWS customers. AWS Marketplace is an online
software store that helps customers find, buy, and immediately start
using the software and services that run on AWS.
See Also https://aws.amazon.com/partners/aws-marketplace/.
master node A process running on an Amazon Machine Image (AMI) that keeps track
of the work its core and task nodes complete.
maximum price The maximum price you pay to launch one or more Spot Instances. If
your maximum price exceeds the current Spot price and your restrictions
are met, Amazon EC2 launches instances on your behalf.
maximum send rate The maximum number of email messages that you can send per second
using Amazon SES.
mean reciprocal rank
at 25
Amazon Personalize: An evaluation metric that assesses the relevance
of a model’s highest ranked recommendation. Amazon Personalize
calculates this metric using the average accuracy of the model
when ranking the most relevant recommendation out of the top 25
recommendations over all requests for recommendations.
See Also metrics, recommendations.
mebibyte (MiB) A contraction of mega binary byte. A mebibyte (MiB) is 2^20 or
1,048,576 bytes. A megabyte (MB) is 10^6 or 1,000,000 bytes. 1,024
MiB is a gibibyte (GiB).
member resources See resource.
MemoryDB Amazon MemoryDB is a Redis-compatible, durable, in-memory database
service that's purpose-built for modern applications with microservices
architectures.
See Also https://aws.amazon.com/memorydb.
56
AWS Glossary Reference
message ID Amazon SES: A unique identifier that's assigned to every email message
that's sent.
Amazon SQS: The identifier returned when you send a message to a
queue.
metadata Information about other data or objects. In Amazon S3 and Amazon
EMR metadata takes the form of name–value pairs that describe the
object. These include default metadata such as the date last modified
and standard HTTP metadata (for example, Content-Type). Users
can also specify custom metadata at the time they store an object.
In Amazon EC2 metadata includes data about an EC2 instance that
the instance can retrieve to determine things about itself, such as the
instance type or the IP address.
metric An element of time-series data defined by a unique combination of
exactly one namespace, exactly one metric name, and between zero and
ten dimensions. Metrics and the statistics derived from them are the
basis of CloudWatch.
metric name The primary identifier of a metric, used with a namespace and optional
dimensions.
metrics Amazon Personalize: Evaluation data that Amazon Personalize generates
when you train a model. You use metrics to evaluate the performance of
the model, view the effects of modifying a solution’s configuration, and
compare results between solutions that use the same training data but
were created with different recipes.
See Also solution, recipe.
MFA See multi-factor authentication (MFA).
micro instance A type of EC2 instance that's more economical to use if you have
occasional bursts of high CPU activity.
AWS Microservice
Extractor for .NET
AWS Microservice Extractor for .NET is an assistive modernization tool
that helps to reduce the time and effort required to break down large,
monolithic applications running on the AWS Cloud or on premises into
smaller, independent services. These services can be operated and
managed independently.
57
AWS Glossary Reference
Migration Hub AWS Migration Hub is a service that provides a single location to track
migration tasks across multiple AWS tools and partner solutions.
See Also https://aws.amazon.com/migration-hub/.
MIME See Multipurpose Internet Mail Extensions (MIME).
Amazon ML Amazon Machine Learning is a cloud-based service that creates machine
learning (ML) models by finding patterns in your data, and uses these
models to process new data and generate predictions.
See Also http://aws.amazon.com/machine-learning/.
ML model In machine learning (ML), a mathematical model that generates
predictions by finding patterns in data. Amazon Machine Learning
supports three types of ML models: binary classification, multiclass
classification, and regression. Also known as a predictive model.
See Also binary classification model, multiclass classification model,
regression model.
Mobile Analytics Amazon Mobile Analytics is a service for collecting, visualizing,
understanding, and extracting mobile app usage data at scale.
See Also https://aws.amazon.com/mobileanalytics.
Mobile Hub See Amplify.
AWS Mobile SDK See Amplify.
Mobile SDK for
Android
See Amplify Android.
Mobile SDK for iOS See Amplify iOS.
Mobile SDK for Unity The AWS Mobile SDK for Unity is included in the AWS SDK for .NET.
Mobile SDK for
Xamarin
The AWS Mobile SDK for Xamarin is included in the AWS SDK for .NET.
Amazon Monitron Amazon Monitron is an end-to-end system that uses machine learning
(ML) to detect abnormal behavior in industrial machinery. Use Amazon
Monitron to implement predictive maintenance and reduce unplanned
downtime.
See Also https://aws.amazon.com/monitron/.
58
AWS Glossary Reference
Amazon MQ Amazon MQ is a managed message broker service for Apache ActiveMQ
that you can use to set up and operate message brokers in the cloud.
See Also https://aws.amazon.com/amazon-mq/.
MTA See Mail Transfer Agent (MTA).
Multi-AZ deployment A primary DB instance that has a synchronous standby replica in a
different Availability Zone. The primary DB instance is synchronously
replicated across Availability Zones to the standby replica.
multiclass
classification model
A machine learning model that predicts values that belong to a limited,
pre-defined set of permissible values. For example, "Is this product a
book, movie, or clothing?"
multi-factor
authentication (MFA)
An optional AWS account security feature. After you enable AWS
MFA, you must provide a six-digit, single-use code in addition to your
sign-in credentials whenever you access secure AWS webpages or
the AWS Management Console. You get this single-use code from an
authentication device that you keep in your physical possession.
See Also https://aws.amazon.com/mfa/.
multipart upload A feature that you can use to upload a single object as a set of parts.
Multipurpose Internet
Mail Extensions (MIME)
An internet standard that extends the email protocol to include nonASCII text and nontext elements, such as attachments.
Multitool A cascading application that provides a simple command-line interface
for managing large datasets.
multi-valued attribute An attribute with more than one value.
Amazon MWAA Amazon Managed Workflows for Apache Airflow is a managed
orchestration service for Apache Airflow to assist in setting up and
operating end-to-end data pipelines in the cloud at scale.
See Also https://aws.amazon.com/managed-workflows-for-apacheairflow.
N
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
59
AWS Glossary Reference
namespace An abstract container that provides context for the items (names,
or technical terms, or words) it holds, and allows disambiguation of
homonym items residing in different namespaces.
NAT Network address translation. A strategy of mapping one or more
IP addresses to another while data packets are in transit across a
traffic routing device. This is commonly used to restrict internet
communication to private instances while allowing outgoing traffic.
See Also Network Address Translation and Protocol Translation, NAT
gateway, NAT instance.
NAT gateway A NAT device, managed by AWS, that performs network address
translation in a private subnet, to secure inbound internet traffic. A NAT
gateway uses both NAT and port address translation.
See Also NAT instance.
NAT instance A NAT device, configured by a user, that performs network address
translation in a Amazon VPC public subnet to secure inbound internet
traffic.
See Also NAT gateway.
Neptune Amazon Neptune is a managed graph database service that you can
use to build and run applications that work with highly connected
datasets. Neptune supports the popular graph query languages Apache
TinkerPop Gremlin and W3C's SPARQL, enabling you to build queries
that efficiently navigate highly connected datasets.
See Also https://aws.amazon.com/neptune/.
network ACL An optional layer of security that acts as a firewall for controlling traffic
in and out of a subnet. You can associate multiple subnets with a single
network ACL, but a subnet can be associated with only one network ACL
at a time.
Network Address
Translation and
Protocol Translation
(NAT-PT) An internet protocol standard defined in RFC 2766.
See Also NAT instance, NAT gateway.
Network Firewall AWS Network Firewall is a managed service that deploys essential
network protections for all Amazon Virtual Private Clouds (Amazon
VPCs).
60
AWS Glossary Reference
See Also https://aws.amazon.com/network-firewall.
n-gram processor A processor that performs n-gram transformations.
See Also n-gram transformation.
n-gram transformation Amazon Machine Learning: A transformation that aids in text string
analysis. An n-gram transformation takes a text variable as input and
outputs strings by sliding a window of size n words, where n is specified
by the user, over the text, and outputting every string of words of size n
and all smaller sizes. For example, specifying the n-gram transformation
with window size =2 returns all the two-word combinations and all of
the single words.
NICE Desktop Cloud
Visualization
A remote visualization technology for securely connecting users to
graphic-intensive 3D applications hosted on a remote, high-performance
server.
Nimble Studio Amazon Nimble Studio is a managed AWS cloud service for creative
studios to produce visual effects, animation, and interactive content—
from storyboard to final deliverable.
See Also https://aws.amazon.com/nimble-studio/.
node OpenSearch Service: An OpenSearch instance. A node can be either a
data instance or a dedicated master instance.
See Also dedicated master node.
NoEcho A property of CloudFormation parameters that prevent the otherwise
default reporting of names and values of a template parameter.
Declaring the NoEcho property causes the parameter value to be
masked with asterisks in the report by the cfn-describe-stacks
command.
normalized discounted
cumulative gain
(NCDG) at K (5/10/25)
Amazon Personalize: An evaluation metric that tells you about
the relevance of your model’s highly ranked recommendations,
where K is a sample size of 5, 10, or 25 recommendations. Amazon
Personalize calculates this by assigning weight to recommendations
based on their position in a ranked list, where each recommendation
is discounted (given a lower weight) by a factor dependent on its
position. The normalized discounted cumulative gain at K assumes
61
AWS Glossary Reference
that recommendations that are lower on a list are less relevant than
recommendations higher on the list.
See Also metrics, recommendations.
NoSQL Nonrelational database systems that are highly available, scalable, and
optimized for high performance. Instead of the relational model, NoSQL
databases (for example, DynamoDB) use alternate models for data
management, such as key–value pairs or document storage.
null object A null object is one whose version ID is null. Amazon S3 adds a null
object to a bucket when versioning for that bucket is suspended. It's
possible to have only one null object for each key in a bucket.
number of passes The number of times that you allow Amazon Machine Learning to use
the same data records to train a machine learning model.
O
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
O3DE Open 3D Engine (successor to Amazon Lumberyard) is an open-source
3D development engine for creating games and simulations. O3DE
is licensed under Apache 2.0 and maintained by a community of
contributors, including Amazon.
See Also https://www.o3de.org/, https://aws.amazon.com/
lumberyard/, https://docs.aws.amazon.com/lumberyard/.
object Amazon S3: The fundamental entity type stored in Amazon S3. Objects
consist of object data and metadata. The data portion is opaque to
Amazon S3.
CloudFront: Any entity that can be served either over HTTP or a version
of RTMP.
observation Amazon Machine Learning: A single instance of data that Amazon
Machine Learning (Amazon ML) uses to either train a machine learning
model how to predict or to generate a prediction. Each row in an
Amazon ML input data file is an observation.
62
AWS Glossary Reference
On-Demand Instance An Amazon EC2 pricing option that charges you for compute capacity
by the hour or second (minimum of 60 seconds) with no long-term
commitment.
Open 3D Engine See O3DE.
OpenSearch Service Amazon OpenSearch Service is an AWS managed service for deploying,
operating, and scaling OpenSearch, an open-source search and analytics
engine, in the AWS Cloud. Amazon OpenSearch Service (OpenSearch
Service) also offers security options, high availability, data durability, and
direct access to the OpenSearch API.
See Also https://aws.amazon.com/elasticsearch-service.
operation An API function. Also called an action.
OpsWorks AWS OpsWorks is a configuration management service that helps you
use Chef to configure and operate groups of instances and applications.
You can define the application's architecture and the specification of
each component including package installation, software configuration,
and resources such as storage. You can automate tasks based on time,
load, or lifecycle events.
See Also https://aws.amazon.com/opsworks/.
optimistic locking A strategy to ensure that an item that you want to update has not been
modified by others before you perform the update. For DynamoDB,
optimistic locking support is provided by the AWS SDKs.
opt-in Region An AWS Region that is disabled by default. To use an opt-in Region,
you must enable it. Regions introduced after March 20, 2019 are opt-in
Regions. For a list of opt-in Regions, see Considerations before enabling
and disabling Regions in the AWS Account Management Guide.
See Also Region that is enabled by default.
organization Organizations: An entity that you create to consolidate and manage your
AWS accounts. An organization has one management account along with
zero or more member accounts.
organizational unit Organizations: A container for accounts within a root of an organization.
An organizational unit (OU) can contain other OUs.
63
AWS Glossary Reference
Organizations AWS Organizations is an account management service that you can use
to consolidate multiple AWS accounts into an organization that you
create and centrally manage.
See Also https://aws.amazon.com/organizations/.
origin access identity Also called OAI. When using Amazon CloudFront to serve content with
an Amazon S3 bucket as the origin, a virtual identity that you use to
require users to access your content through CloudFront URLs instead of
Amazon S3 URLs. Usually used with CloudFront private content.
origin server The Amazon S3 bucket or custom origin containing the definitive
original version of the content you deliver through CloudFront.
original environment The instances in a deployment group at the start of an CodeDeploy
blue/green deployment.
OSB transformation Orthogonal sparse bigram transformation. In machine learning, a
transformation that aids in text string analysis and that's an alternative
to the n-gram transformation. OSB transformations are generated by
sliding the window of size n words over the text, and outputting every
pair of words that includes the first word in the window.
See Also n-gram transformation.
OU See organizational unit.
Outposts AWS Outposts is a fully managed service by AWS that extends AWS
infrastructure, services, APIs, and tools to on-premises data centers
and edge locations. Use AWS Outposts for workloads and devices
requiring low latency access to on-premises systems, local data
processing, data residency, and application migration with local system
interdependencies.
See Also https://aws.amazon.com/outposts.
output location Amazon Machine Learning: An Amazon S3 location where the results of
a batch prediction are stored.
P
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
64
AWS Glossary Reference
pagination The process of responding to an API request by returning a large list of
records in small separate parts. Pagination can occur in the following
situations:
• The client sets the maximum number of returned records to a value
below the total number of records.
• The service has a default maximum number of returned records that's
lower than the total number of records.
When an API response is paginated, the service sends a subset of the
large list of records and a pagination token that indicates that more
records are available. The client includes this pagination token in a
subsequent API request, and the service responds with the next subset
of records. This continues until the service responds with a subset of
records and no pagination token, indicating that all records have been
sent.
pagination token A marker that indicates that an API response contains a subset of a
larger list of records. The client can return this marker in a subsequent
API request to retrieve the next subset of records until the service
responds with a subset of records and no pagination token, indicating
that all records have been sent.
See Also pagination.
paid AMI An Amazon Machine Image (AMI) that you sell to other Amazon EC2
users on AWS Marketplace.
AWS Panorama AWS Panorama is a machine learning (ML) Appliance and Software
Development Kit (SDK) that organizations can use to bring computer
vision (CV) to on-premises cameras to make predictions locally.
See Also https://aws.amazon.com/panorama.
AWS ParallelCluster AWS ParallelCluster is an AWS supported open source cluster
management tool that helps you to deploy and manage high
performance computing (HPC) clusters in the AWS Cloud.
paravirtual
virtualization
See PV virtualization.
65
AWS Glossary Reference
part A contiguous portion of the object's data in a multipart upload request.
partition A group of AWS Regions. Each Region is in only one partition,
and each partition contains one or more Regions. Partitions have
independent instances of the AWS Identity and Access Management
(IAM) infrastructure. In other words, a partition is comprised of Regions
that share the same authentication, account, and resource stack. Each
AWS account is scoped to one partition. You can't use IAM credentials
from one partition to interact with resources in a different partition.
Some AWS services are designed to provide cross-Region functionality.
Such cross-Region functionality is supported only between Regions in
the same partition. AWS commercial Regions are in the AWS partition,
China Regions are in the AWS-cn partition, and AWS GovCloud (US)
Regions are in the AWS-us-gov partition.
partition key A simple primary key, composed of one attribute (also known as a hash
attribute).
See Also primary key, sort key.
PAT Port address translation.
pebibyte (PiB) A contraction of peta binary byte, a pebibyte is 2^50 or
1,125,899,906,842,624 bytes. A petabyte (PB) is 10^15 or
1,000,000,000,000,000 bytes. 1,024 PiB is an exbibyte (EiB).
period See sampling period.
permission A statement within a policy that allows or denies access to a particular
resource. You can state any permission in the following way: "A has
permission to do B to C." For example, Jane (A) has permission to read
messages (B) from John's Amazon SQS queue (C). Whenever Jane sends
a request to Amazon SQS to use John's queue, the service checks to see
if she has permission. It further checks to see if the request satisfies the
conditions John set forth in the permission.
persistent storage A data storage solution where the data remains intact until it's deleted.
Options within AWS include: Amazon S3, Amazon RDS, DynamoDB, and
other services.
66
AWS Glossary Reference
Amazon Personalize Amazon Personalize is an artificial intelligence service for creating
individualized product and content recommendations.
See Also https://aws.amazon.com/personalize/.
PERSONALIZED_RANKING
recipes
Amazon Personalize: Recipes that provide item recommendations in
ranked order based on the predicted interest for a user.
See Also recipe, recommendations, personalized-ranking recipe,
popularity-count recipe.
personalized-ranking
recipe
Amazon Personalize: A PERSONALIZED_RANKING recipe that ranks a
collection of items that you provide based on the predicted interest
level for a specific user. Use the personalized-ranking recipe to create
curated lists of items or ordered search results that are personalized for
a specific user.
See Also recipe, PERSONALIZED_RANKING recipes.
physical name A unique label that CloudFormation assigns to each resource when
creating a stack. Some AWS CloudFormation commands accept the
physical name as a value with the --physical-name parameter.
pilot light An active-passive disaster recovery strategy in which you replicate
data from the primary Region as standby, then provision a replica
that contains only the core workload infrastructure. To make this
infrastructure functional and serve requests, you must provision the
remaining resources, such as compute.
See Also , , .
Amazon Pinpoint Amazon Pinpoint is a multichannel communications service that helps
organizations send timely, targeted content through SMS, email, mobile
push notifications, voice messages, and in-application channels.
See Also https://aws.amazon.com/pinpoint.
pipeline CodePipeline: A workflow construct that defines the way software
changes go through a release process.
plaintext Information that has not been encrypted, as opposed to ciphertext.
policy IAM: A document defining permissions that apply to a user, group, or
role; the permissions in turn determine what users can do in AWS. A
67
AWS Glossary Reference
policy typically allows access to specific actions, and can optionally grant
that the actions are allowed for specific resources, such as EC2 instances
or Amazon S3 buckets. Policies can also explicitly deny access.
Amazon EC2 Auto Scaling: An object that stores the information that's
needed to launch or terminate instances for an Auto Scaling group.
Running the policy causes instances to be launched or terminated. You
can configure an alarm to invoke an Auto Scaling policy.
policy generator A tool in the IAM AWS Management Console that helps you build a
policy by selecting elements from lists of available options.
policy simulator A tool in the IAM AWS Management Console that helps you test and
troubleshoot policies so you can see their effects in real-world scenarios.
policy validator A tool in the IAM AWS Management Console that examines your existing
IAM access control policies to ensure that they comply with the IAM
policy grammar.
Amazon Polly Amazon Polly is a text-to-speech (TTS) service that turns text into
natural-sounding human speech. Amazon Polly provides dozens of
lifelike voices across a broad set of languages so that you can build
speech-enabled applications that work in many different countries.
See Also https://aws.amazon.com/polly/.
popularity-count
recipe
Amazon Personalize: A USER_PERSONALIZATION recipe that
recommends the items that have had the most interactions with unique
users.
See Also recipe, USER_PERSONALIZATION recipes.
Porting Assistant
for .NET
Porting Assistant for .NET is a compatibility analyzer that reduces the
manual effort required to port Microsoft .NET Framework applications to
open source .NET Core.
precision at K
(5/10/25)
Amazon Personalize: An evaluation metric that tells you how relevant
your model’s recommendations are based on a sample size of K (5, 10,
or 25) recommendations. Amazon Personalize calculates this metric
based on the number of relevant recommendations out of the top K
recommendations, divided by K, where K is 5, 10, or 25.
68
AWS Glossary Reference
See Also metrics, recommendations.
prefix See job prefix.
Premium Support A one-on-one, fast-response support channel that AWS customers can
subscribe to for support for AWS infrastructure services.
See Also https://aws.amazon.com/premiumsupport/.
presigned URL A web address that uses query string authentication.
primary key One or two attributes that uniquely identify each item in a DynamoDB
table, so that no two items can have the same key.
See Also partition key, sort key.
primary shard See shard.
principal The user, service, or account that receives permissions that are defined in
a policy. The principal is A in the statement "A has permission to do B to
C."
AWS Private CA AWS Private Certificate Authority is a hosted private certificate authority
service for issuing and revoking private digital certificates.
See Also https://aws.amazon.com/certificate-manager/privatecertificate-authority/.
private content When using Amazon CloudFront to serve content with an Amazon S3
bucket as the origin, a method of controlling access to your content by
requiring users to use signed URLs. Signed URLs can restrict user access
based on the current date and time, the IP addresses that the requests
originate from, or both.
private IP address A private numerical address (for example, 192.0.2.44) that networked
devices use to communicate with one another using the Internet
Protocol (IP). Each EC2 instance is assigned two IP addresses at launch,
which are directly mapped to each other through network address
translation (NAT): a private address (following RFC 1918) and a public
address. Exception: Instances launched in Amazon VPC are assigned only
a private IP address.
private subnet A Amazon VPC subnet whose instances can't be reached from the
internet.
69
AWS Glossary Reference
product code An identifier provided by AWS when you submit a product to AWS
Marketplace.
properties See resource property.
property rule A JSON-compliant markup standard for declaring properties, mappings,
and output values in an CloudFormation template.
Provisioned IOPS A storage option that delivers fast, predictable, and consistent I/O
performance. When you specify an IOPS rate while creating a DB
instance, Amazon RDS provisions that IOPS rate for the lifetime of the
DB instance.
pseudo parameter A predefined setting (for example, AWS:StackName) that can be used in
CloudFormation templates without having to declare them. You can use
pseudo parameters anywhere you can use a regular parameter.
public AMI An Amazon Machine Image (AMI) that all AWS accounts have permission
to launch.
public dataset A large collection of public information that can be seamlessly
integrated into applications that are based in the AWS Cloud. Amazon
stores public datasets at no charge to the community and, similar to
other AWS services, users pay only for the compute and storage they
use for their own applications. These datasets currently include data
from the Human Genome Project, the US Census, Wikipedia, and other
sources.
See Also https://aws.amazon.com/publicdatasets.
public IP address A public numerical address (for example, 192.0.2.44) that networked
devices use to communicate with one another using the Internet
Protocol (IP). Each EC2 instance is assigned two IP addresses at launch,
which are directly mapped to each other through Network Address
Translation (NAT): a private address (following RFC 1918) and a public
address. Exception: Instances launched in Amazon VPC are assigned only
a private IP address.
public subnet A subnet whose instances can be reached from the internet.
PV virtualization Paravirtual virtualization allows guest VMs to run on host systems
that don't have special support extensions for full hardware and CPU
70
AWS Glossary Reference
virtualization. Because PV guests run a modified operating system that
doesn't use hardware emulation, they can't provide hardware-related
features, such as enhanced networking or GPU support.
See Also HVM virtualization.
Q
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
Amazon QLDB Amazon Quantum Ledger Database (Amazon QLDB) is a fully
managed ledger database that provides a transparent, immutable, and
cryptographically verifiable transaction log owned by a central trusted
authority.
See Also https://aws.amazon.com/qldb.
quartile binning
transformation
Amazon Machine Learning: A process that takes two inputs, a numerical
variable and a parameter called a bin number, and outputs a categorical
variable. Quartile binning transformations discover non-linearity in
a variable's distribution by enabling the machine learning model to
learn separate importance values for parts of the numeric variable’s
distribution.
Query A type of web service that generally uses only the GET or POST HTTP
method and a query string with parameters in the URL.
See Also REST.
query string
authentication
An AWS feature that you can use to place the authentication information
in the HTTP request query string instead of in the Authorization
header, which provides URL-based access to objects in a bucket.
queue A sequence of messages or jobs that are held in temporary storage
awaiting transmission or processing.
queue URL A web address that uniquely identifies a queue.
QuickSight Amazon QuickSight is a fast, cloud-powered business analytics service
that you can use to build visualizations, perform analysis, and quickly
get business insights from your data.
See Also https://aws.amazon.com/quicksight/.
71
AWS Glossary Reference
quota The maximum value for your resources, actions, and items in your AWS
account
R
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
AWS RAM AWS Resource Access Manager is a web service that AWS customers can
use to securely share AWS resources with any AWS account or within
your organization.
See Also https://aws.amazon.com/ram.
range GET A request that specifies a byte range of data to get for a download. If
an object is large, you can break up a download into smaller units by
sending multiple range GET requests that each specify a different byte
range to GET.
raw email A type of sendmail request with which you can specify the email headers
and MIME types.
Amazon RDS Amazon Relational Database Service is a web service that makes it
easier to set up, operate, and scale a relational database in the cloud.
It provides cost-efficient, resizable capacity for an industry-standard
relational database and manages common database administration
tasks.
See Also https://aws.amazon.com/rds.
read local/write global An active-active strategy in which all writes for a workload are sent to
one primary Region and all read traffic is served from the Region where
the request originates. Typically architected with an asynchronous data
store. Sometimes referred to as read local-write global.
See Also , .
read local/write local An active-active strategy in which all writes for a workload are sent to
one primary Region and all read traffic is served from the Region where
the request originates. Typically architected with an asynchronous data
store. Sometimes referred to as read local-write global.
See Also , .
72
AWS Glossary Reference
read replica Amazon RDS: An active copy of another DB instance. Any updates to
the data on the source DB instance are replicated to the read replica DB
instance using the built-in replication feature of MySQL 5.1.
real-time predictions Amazon Machine Learning: Synchronously generated predictions for
individual data observations.
See Also batch prediction.
receipt handle Amazon SQS: An identifier that you get when you receive a message
from the queue. This identifier is required to delete a message from the
queue or when changing a message's visibility timeout.
receiver The entity that consists of the network systems, software, and policies
that manage email delivery for a recipient.
recipe Amazon Personalize: An Amazon Personalize algorithm that's
preconfigured to predict the items that a user interacts with (for
USER_PERSONALIZATION recipes), or calculate items that are similar
to specific items that a user has shown interest in (for RELATED_ITEMS
recipes), or rank a collection of items that you provide based on the
predicted interest for a specific user (for PERSONALIZED_RANKING
recipes).
See Also USER_PERSONALIZATION recipes, RELATED_ITEMS recipes,
PERSONALIZED_RANKING recipes.
recipient Amazon SES: The person or entity receiving an email message. For
example, a person named in the "To" field of a message.
recommendations Amazon Personalize: A list of items that Amazon Personalize
predicts that a user interacts with. Depending on the Amazon
Personalize recipe used, recommendations can be either a list of
items (with USER_PERSONALIZATION recipes and RELATED_ITEMS
recipes), or a ranking of a collection of items you provided (with
PERSONALIZED_RANKING recipes).
See Also recipe, campaign, solution version, USER_PERSONALIZATION
recipes, RELATED_ITEMS recipes, PERSONALIZED_RANKING recipes.
Redis A fast, open-source, in-memory key-value data structure store. Redis
comes with a set of versatile in-memory data structures with which you
can easily create a variety of custom applications.
73
AWS Glossary Reference
Amazon Redshift Amazon Redshift is a fully managed, petabyte-scale data warehouse
service in the cloud. With Amazon Redshift, you can analyze your data
using your existing business intelligence tools.
See Also https://aws.amazon.com/redshift/.
reference A means of inserting a property from one AWS resource into another.
For example, you could insert an Amazon EC2 security group property
into an Amazon RDS resource.
Region A named set of AWS resources that's in the same geographical area. A
Region is comprised of at least three Availability Zones. AWS Regions
are divided into partitions. AWS commercial Regions are in the AWS
partition, China Regions are in the AWS-cn partition, and AWS GovCloud
(US) Regions are in the AWS-us-gov partition.
Region that is enabled
by default
An AWS Region that is enabled by default. Regions that were introduced
before March 20, 2019 are enabled by default and can’t be disabled.
For a list of Regions that aren’t enabled by default (opt-in Region),
see Considerations before enabling and disabling Regions in the AWS
Account Management Guide.
regression model Amazon Machine Learning: Preformatted instructions for common data
transformations that fine-tune machine learning model performance.
regression model A type of machine learning model that predicts a numeric value, such as
the exact purchase price of a house.
regularization A machine learning (ML) parameter that you can tune to obtain
higher-quality ML models. Regularization helps prevent ML models
from memorizing training data examples instead of learning how to
generalize the patterns it sees (called overfitting). When training data is
overfitted, the ML model performs well on the training data, but doesn't
perform well on the evaluation data or on new data.
Amazon Rekognition Amazon Rekognition is a machine learning service that identifies objects,
people, text, scenes, and activities, including inappropriate content, in
either image or video files. With Amazon Rekognition Custom Labels,
you can create a customized ML model that detects objects and scenes
specific to your business in images.
74
AWS Glossary Reference
See Also https://aws.amazon.com/rekognition/.
RELATED_ITEMS
recipes
Amazon PersonalizeRecipes that recommend items that are similar to a
specified item, such as the item-to-item (SIMS) recipe.
See Also recipe, item-to-item similarities (SIMS) recipe.
replacement
environment
The instances in a deployment group after the CodeDeploy blue/green
deployment.
replica shard See shard.
reply path The email address that an email reply is sent to. This is different from
the return path.
representational state
transfer
See REST.
reputation 1. An Amazon SES metric, based on factors that might include bounces,
complaints, and other metrics, regarding whether a customer is sending
high-quality email.
2. A measure of confidence, as judged by an internet service provider
(ISP) or other entity that an IP address that they are receiving email from
isn't the source of spam.
requester The person (or application) that sends a request to AWS to perform
a specific action. When AWS receives a request, it first evaluates the
requester's permissions to determine whether the requester is allowed
to perform the request action (if applicable, for the requested resource).
Requester Pays An Amazon S3 feature that allows a bucket owner to specify that anyone
who requests access to objects in a particular bucket must pay the data
transfer and request costs.
reservation A collection of EC2 instances started as part of the same launch request.
This is not to be confused with a Reserved Instance.
Reserved Instance A pricing option for EC2 instances that discounts the on-demand usage
charge for instances that meet the specified parameters. Customers pay
for the entire term of the instance, regardless of how they use it.
75
AWS Glossary Reference
Reserved Instance
Marketplace
An online exchange that matches sellers who have reserved capacity
that they no longer need with buyers who are looking to purchase
additional capacity. reserved instances that you purchase from thirdparty sellers have less than a full standard term remaining and can be
sold at different upfront prices. The usage or reoccurring fees remain
the same as the fees set when the Reserved Instances were originally
purchased. Full standard terms for Reserved Instances available from
AWS run for one year or three years.
Resilience Hub AWS Resilience Hub gives you a central place to define, validate, and
track the resiliency of your AWS application. It helps you to protect
your applications from disruptions, and reduce recovery costs to
optimize business continuity to help meet compliance and regulatory
requirements.
See Also https://aws.amazon.com/resilience-hub.
resource An entity that users can work with in AWS, such as an EC2 instance, an
DynamoDB table, an Amazon S3 bucket, an IAM user, or an OpsWorks
stack.
Resource Groups AWS Resource Groups is a web service that AWS customers can use to
manage and automate tasks on large numbers of resources at one time.
See Also AWS Resource Groups.
Amazon Resource
Name (ARN)
Amazon Resource Name is a standardized way to refer to an AWS
resource (for example, arn:aws:iam::123456789012:user/
division_abc/subdivision_xyz/Bob).
resource property A value required when including an AWS resource in an CloudFormation
stack. Each resource can have one or more properties associated with
it. For example, an AWS::EC2::Instance resource might have a
UserData property. In an AWS CloudFormation template, resources
must declare a properties section, even if the resource has no properties.
resource record Also called resource record set. The fundamental information elements in
the Domain Name System (DNS).
See Also Domain Name System on Wikipedia.
REST Representational state transfer. A simple stateless architecture that
generally runs over HTTPS/TLS. REST emphasizes that resources have
76
AWS Glossary Reference
unique and hierarchical identifiers (URIs), are represented by common
media types (such as HTML, XML, or JSON), and that operations on the
resources are either predefined or discoverable within the media type. In
practice, this generally results in a limited number of operations.
See Also Query, WSDL, SOAP.
RESTful web service Also known as RESTful API. A web service that follows REST architectural
constraints. The API operations must use HTTP methods explicitly,
expose hierarchical URIs, and transfer either XML, JSON, or both.
return enabled CloudSearch: An index field option that enables the field's values to be
returned in the search results.
return path The email address that bounced email is returned to. The return path is
specified in the header of the original email. This is different from the
reply path.
revision CodePipeline: A change that's made to a source that's configured in a
source action, such as a pushed commit to a GitHub repository or an
update to a file in a versioned Amazon S3 bucket.
AWS RoboMaker AWS RoboMaker is a cloud-based simulation service that robotics
developers use to run, scale, and automate simulation without
managing any infrastructure.
See Also https://aws.amazon.com/robomaker.
role A tool for giving temporary access to AWS resources in your AWS
account.
rollback A return to a previous state that follows the failure to create an object,
such as CloudFormation stack. All resources that are associated with the
failure are deleted during the rollback. For AWS CloudFormation, you
can override this behavior using the --disable-rollback option on
the command line.
root Organizations: A parent container for the accounts in your organization.
If you apply a service control policy to the root, it applies to every
organizational unit and account in the organization.
root credentials Authentication information associated with the AWS account owner.
77
AWS Glossary Reference
root device volume A volume that contains the image used to boot the instance (also known
as a root device). If you launched the instance from an AMI backed by
instance store, this is an instance store volume created from a template
stored in Amazon S3. If you launched the instance from an AMI backed
by Amazon EBS, this is an Amazon EBS volume created from an Amazon
EBS snapshot.
route table A set of routing rules that controls the traffic leaving any subnet that's
associated with the route table. You can associate multiple subnets with
a single route table, but a subnet can be associated with only one route
table at a time.
Route 53 Amazon Route 53 is a web service that you can use to create a new DNS
service or to migrate your existing DNS service to the cloud.
See Also https://aws.amazon.com/route53.
row identifier Amazon Machine Learning: An attribute in the input data that you
can include in the evaluation or prediction output to make it easier to
associate a prediction with an observation.
rule AWS WAF: A set of conditions that AWS WAF searches for in web
requests to AWS resources such as Amazon CloudFront distributions. You
add rules to a web ACL, and then specify whether you want to allow or
block web requests based on each rule.
S
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
Amazon S3 Amazon S3 is storage for the internet. You can use it to store and
retrieve any amount of data at any time, from anywhere on the web.
See Also https://aws.amazon.com/s3.
Amazon S3 Glacier Amazon S3 Glacier is a secure, durable, and low-cost storage service for
data archiving and long-term backup. You can reliably store large or
small amounts of data for significantly less than on-premises solutions.
S3 Glacier is optimized for infrequently accessed data, where a retrieval
time of several hours is suitable.
78
AWS Glossary Reference
See Also https://aws.amazon.com/glacier/.
Amazon S3-Backed
AMI
See instance store-backed AMI.
SageMaker AI Amazon SageMaker AI is a fully managed cloud service that builds,
trains, and deploys machine learning (ML) models by using AWS
infrastructure, tools, and workflows.
See Also https://aws.amazon.com/sagemaker.
AWS SAM AWS Serverless Application Model is an open-source framework for
building and running serverless applications. AWS SAM provides
a command line interface tool and a shorthand syntax template
specification that you can use to quickly iterate through your serverless
application lifecycle.
See Also https://aws.amazon.com/serverless/sam/.
sampling period A defined duration of time, such as one minute, which CloudWatch
computes a statistic over.
sandbox A testing location where you can test the functionality of your
application without affecting production, incurring charges, or
purchasing products.
Amazon SES: An environment that developers can use to test and
evaluate the service. In the sandbox, you have full access to the Amazon
SES API, but you can only send messages to verified email addresses
and the mailbox simulator. To get out of the sandbox, you must apply
for production access. Accounts in the sandbox also have lower sending
limits than production accounts.
scale in To remove EC2 instances from an Auto Scaling group.
scale out To add EC2 instances to an Auto Scaling group.
scaling activity A process that changes the size, configuration, or makeup of an Auto
Scaling group by launching or terminating instances.
scaling policy A description of how Auto Scaling automatically scales an Auto Scaling
group in response to changing demand.
79
AWS Glossary Reference
See Also scale in, scale out.
scheduler The method used for placing tasks on container instances.
schema Amazon Machine Learning: The information that's needed to interpret
the input data for a machine learning model, including attribute names
and their assigned data types, and the names of special attributes.
score cut-off value Amazon Machine Learning: A binary classification model outputs a score
that ranges from 0 to 1. To decide whether an observation is classified
as 1 or 0, you pick a classification threshold, or cut-off, and Amazon ML
compares the score against it. Observations with scores higher than the
cut-off are predicted as target equals 1, and scores lower than the cutoff are predicted as target equals 0.
SCP See service control policy.
AWS SCT AWS Schema Conversion Tool is a desktop application that automates
heterogeneous database migrations. You can use AWS SCT to convert
database schemas and code objects, SQL code in your applications, and
ETL scripts to a format compatible with the target database. Then, you
can use AWS SCT data extraction agents to migrate data to your target
database.
See Also https://aws.amazon.com/dms/schema-conversion-tool.
AWS SDK for .NET AWS SDK for .NET is a software development kit that provides .NET
API operations for AWS services including Amazon S3, Amazon EC2,
IAM, and more. You can download the SDK as multiple service-specific
packages on NuGet.
See Also https://aws.amazon.com/sdk-for-net/.
SDK for C++ AWS SDK for C++ is a software development kit that provides C++ APIs
for many AWS services including Amazon S3, Amazon EC2, DynamoDB,
and more. The single, downloadable package includes the AWS C++
library, code examples, and documentation.
See Also https://aws.amazon.com/sdk-for-cpp/.
SDK for Go AWS SDK for Go is a software development kit for integrating your Go
application with the full suite of AWS services.
See Also https://aws.amazon.com/sdk-for-go/.
80
AWS Glossary Reference
SDK for Java AWS SDK for Java is a software development kit that provides Java API
operations for many AWS services including Amazon S3, Amazon EC2,
DynamoDB, and more. The single, downloadable package includes the
AWS Java library, code examples, and documentation.
See Also https://aws.amazon.com/sdk-for-java/.
SDK for JavaScript in
Node.js
AWS SDK for JavaScript in Node.js is a software development kit for
accessing AWS services from JavaScript in Node.js. The SDK provides
JavaScript objects for AWS services, including Amazon S3, Amazon
EC2, DynamoDB, and Amazon SWF. The single, downloadable package
includes the AWS JavaScript library and documentation.
See Also https://docs.aws.amazon.com/sdk-for-javascript/v2/
developer-guide/.
SDK for JavaScript in
the Browser
AWS SDK for JavaScript in the Browser is a software development kit
for accessing AWS services from JavaScript code running in the browser.
Authenticate users through Facebook, Google, or Login with Amazon
using web identity federation. Store application data in DynamoDB, and
save user files to Amazon S3.
See Also https://docs.aws.amazon.com/sdk-for-javascript/v2/
developer-guide/.
SDK for PHP AWS SDK for PHP is a software development kit and open-source PHP
library for integrating your PHP application with AWS services such as
Amazon S3, Amazon S3 Glacier, and DynamoDB.
See Also https://aws.amazon.com/sdk-for-php/.
SDK for Python
(Boto3)
AWS SDK for Python (Boto3) is a software development kit for using
Python to access AWS services such as Amazon EC2, Amazon EMR,
Amazon EC2 Auto Scaling, Kinesis, or Lambda.
See Also http://boto.readthedocs.org/en/latest/.
SDK for Ruby AWS SDK for Ruby is a software development kit for accessing AWS
services from Ruby. The SDK provides Ruby classes for many AWS
services including Amazon S3, Amazon EC2, DynamoDB and more.
The single, downloadable package includes the AWS Ruby Library and
documentation.
See Also https://aws.amazon.com/sdk-for-ruby/.
81
AWS Glossary Reference
SDK for Rust AWS SDK for Rust is a software development kit that provides APIs and
utilities for developers. It enables Rust applications to integrate with
AWS services such as Amazon S3 and Amazon EC2.
SDK for Swift AWS SDK for Swift is a software development kit that provides support
for accessing AWS infrastructure and services using the Swift language.
search API CloudSearch: The API that you use to submit search requests to a search
domain.
search domain CloudSearch: Encapsulates your searchable data and the search
instances that handle your search requests. You typically set up a
separate Amazon CloudSearch domain for each different collection of
data that you want to search.
search domain
configuration
CloudSearch: A domain's indexing options, analysis schemes,
expressions, suggesters, access policies, and scaling and availability
options.
search enabled CloudSearch: An index field option that enables the field data to be
searched.
search endpoint CloudSearch: The URL that you connect to when sending search requests
to a search domain. Each Amazon CloudSearch domain has a unique
search endpoint that remains the same for the life of the domain.
search index CloudSearch: A representation of your searchable data that facilitates
fast and accurate data retrieval.
search instance CloudSearch: A compute resource that indexes your data and processes
search requests. An Amazon CloudSearch domain has one or more
search instances, each with a finite amount of RAM and CPU resources.
As your data volume grows, more search instances or larger search
instances are deployed to contain your indexed data. When necessary,
your index is automatically partitioned across multiple search instances.
As your request volume or complexity increases, each search partition is
automatically replicated to provide additional processing capacity.
search request CloudSearch: A request that's sent to an Amazon CloudSearch domain's
search endpoint to retrieve documents from the index that match
particular search criteria.
82
AWS Glossary Reference
search result CloudSearch: A document that matches a search request. Also referred
to as a search hit.
secret access key A key that's used with the access key ID to cryptographically sign
programmatic AWS requests. Signing a request identifies the sender and
prevents the request from being altered. You can generate secret access
keys for your AWS account, individual IAM users and temporary sessions.
Secrets Manager AWS Secrets Manager is a service for securely encrypting, storing, and
rotating credentials for databases and other services.
See Also https://aws.amazon.com/secrets-manager/.
security group A named set of allowed inbound network connections for an instance.
(Security groups in Amazon VPC also include support for outbound
connections.) Each security group consists of a list of protocols, ports,
and IP address ranges. A security group can apply to multiple instances,
and multiple groups can regulate a single instance.
Security Hub AWS Security Hub is a service that provides a comprehensive view of the
security state of your AWS resources. Security Hub collects security data
from AWS accounts and services and helps you analyze your security
trends to identify and prioritize the security issues across your AWS
environment.
See Also https://aws.amazon.com/security-hub/.
sender The person or entity sending an email message.
Sender ID A Microsoft controlled version of SPF. An email authentication and antispoofing system. For more information about Sender ID, see Sender ID
in Wikipedia.
sending limits The sending quota and maximum send rate that are associated with
every Amazon SES account.
sending quota The maximum number of email messages that you can send using
Amazon SES in a 24-hour period.
AWS Serverless
Application Repository
AWS Serverless Application Repository is a managed repository that
teams, organizations, and individual developers can use to store and
83
AWS Glossary Reference
share reusable applications, and assemble and deploy serverless
architectures in powerful new ways.
See Also https://aws.amazon.com/serverless/serverlessrepo/.
server-side encryption
(SSE)
The encrypting of data at the server level. Amazon S3 supports three
modes of server-side encryption: SSE-S3, where Amazon S3 manages
the keys; SSE-C, where the customer manages the keys; and SSE-KMS,
where AWS KMS manages keys.
Service Catalog AWS Service Catalog is a web service that helps organizations create and
manage catalogs of IT services that are approved for use on AWS. These
IT services can include everything from virtual machine images, servers,
software, and databases to complete multitier application architectures.
See Also https://aws.amazon.com/servicecatalog/.
service control policy Organizations: A policy-based control that specifies the services and
actions that users and roles can use in the accounts that the service
control policy (SCP) affects.
service endpoint See endpoint.
service health
dashboard
A webpage showing up-to-the-minute information about AWS service
availability. The dashboard is located at http://status.aws.amazon.com/.
AWS Service
Management
Connector
AWS Service Management Connector enables customers to provision,
manage, and operate AWS resources and capabilities in familiar IT
Service Management (ITSM) tooling.
See Also https://aws.amazon.com/service-management-connector.
Service Quotas A service for viewing and managing your quotas easily and at scale as
your AWS workloads grow. Quotas, also referred to as limits, are the
maximum number of resources that you can create in an AWS account.
service role An IAM role that grants permissions to an AWS service so it can access
AWS resources. The policies that you attach to the service role determine
which AWS resources the service can access and what it can do with
those resources.
Amazon SES Amazon Simple Email Service is a simple and cost-effective email
solution for applications.
84
AWS Glossary Reference
See Also https://aws.amazon.com/ses.
session The period when the temporary security credentials that are provided by
AWS STS allow access to your AWS account.
SHA Secure Hash Algorithm. SHA1 is an earlier version of the algorithm,
which AWS has replaced with SHA256.
shard OpenSearch Service: A partition of data in an index. You can split an
index into multiple shards, which can include primary shards (original
shards) and replica shards (copies of the primary shards). Replica shards
provide failover. This means that, if a cluster node that contains a
primary shard fails, a replica shard is promoted to a primary shard.
Replica shards also can handle requests.
shared AMI An Amazon Machine Image (AMI) that a developer builds and makes
available for others to use.
Shield AWS Shield is a service that helps to protect your resources—such as
Amazon EC2 instances, Elastic Load Balancing load balancers, Amazon
CloudFront distributions, and Route 53 hosted zones—against DDoS
attacks. AWS Shield is automatically included at no extra cost beyond
what you already pay for AWS WAF and your other AWS services.
For added protection against DDoS attacks, AWS offers AWS Shield
Advanced.
See Also https://aws.amazon.com/shield.
shutdown action Amazon EMR: A predefined bootstrap action that launches a script that
runs a series of commands in parallel before terminating the job flow.
signature Refers to a digital signature, which is a mathematical way to confirm the
authenticity of a digital message. AWS uses signatures to authenticate
the requests you send to our web services. For more information, to
https://aws.amazon.com/security.
SIGNATURE file Import/Export: A file that you copy to the root directory of your storage
device. The file contains a job ID, manifest file, and a signature.
Signature Version 4 Protocol for authenticating inbound API requests to AWS services in all
AWS Regions.
85
AWS Glossary Reference
Signer AWS Signer is a fully managed code-signing service used to ensure the
authenticity and integrity of an AWS customer's code.
Silk Amazon Silk is a next-generation web browser that's available only on
Fire OS tablets and phones. Built on a split architecture that divides
processing between the client and the AWS Cloud, Amazon Silk creates a
faster, more responsive mobile browsing experience.
Simple Mail Transfer
Protocol
See SMTP.
Simple Object Access
Protocol
See SOAP.
SimSpace Weaver AWS SimSpace Weaver is a managed service that you can use to build
and run large-scale spatial simulations in the AWS Cloud.
See Also https://aws.amazon.com/simspaceweaver/.
SIMS recipe See item-to-item similarities (SIMS) recipe.
single sign-on An authentication scheme that allows users to sign in one time to access
multiple applications and websites. The service name AWS Single SignOn is now AWS IAM Identity Center.
See Also IAM Identity Center.
Single-AZ DB instance A standard (non-Multi-AZ) DB instance that's deployed in one Availability
Zone, without a standby replica in another Availability Zone.
See Also Multi-AZ deployment.
Site-to-Site VPN AWS Site-to-Site VPN is a fully managed service that you can use to
establish Internet Protocol security (IPsec) VPN connections between
your AWS networks and your on-premises networks.
See Also https://aws.amazon.com/vpn/site-to-site-vpn.
sloppy phrase search A search for a phrase that specifies how close the terms must be to one
another to be considered a match.
AWS SMS AWS Server Migration Service is a service that combines data collection
tools with automated server replication to speed the migration of onpremises servers to AWS.
86
AWS Glossary Reference
See Also https://aws.amazon.com/server-migration-service.
SMTP Simple Mail Transfer Protocol. The standard that's used to exchange
email messages between internet hosts for the purpose of routing and
delivery.
snapshot Amazon EBS: A backup of your volumes that's stored in Amazon S3.
You can use these snapshots as the starting point for new Amazon EBS
volumes or to protect your data for long-term durability.
See Also DB snapshot.
Snowball AWS Snowball is a petabyte-scale data transport solution that uses
devices that are secure to transfer large amounts of data into and out of
the AWS Cloud.
See Also https://aws.amazon.com/snowball.
Amazon SNS Amazon Simple Notification Service is a web service that applications,
users, and devices can use to instantly send and receive notifications
from the cloud.
See Also https://aws.amazon.com/sns.
SOAP Simple Object Access Protocol. An XML-based protocol that you can use
to exchange information over a particular protocol (for example, HTTP
or SMTP) between applications.
See Also REST, WSDL.
soft bounce A temporary email delivery failure such as one resulting from a full
mailbox.
software VPN A software appliance-based VPN connection over the internet.
solution Amazon Personalize: The recipe, customized parameters, and
trained models (solution versions) that can be used to generate
recommendations.
See Also recipe, solution version, recommendations.
solution version Amazon Personalize: A trained model that you create as part of a
solution in Amazon Personalize. You deploy a solution version in a
campaign to generate recommendations.
See Also solution, campaign, recommendations.
87
AWS Glossary Reference
sort enabled CloudSearch: An index field option that enables a field to be used to sort
the search results.
sort key An attribute used to sort the order of partition keys in a composite
primary key (also known as a range attribute).
See Also partition key, primary key.
source/destination
checking
A security measure to verify that an EC2 instance is the origin of all
traffic that it sends and the ultimate destination of all traffic that it
receives. In other words, this measure verifies that the instance isn't
relaying traffic. By default, source/destination checking is turned on. For
instances that function as gateways, such as Amazon VPC NAT instances,
source/destination checking must be disabled.
spam Unsolicited bulk emails.
spamtrap An email address that's set up by an anti-spam entity. This email address
isn't for correspondence but rather for monitoring unsolicited emails.
This is also called a honeypot.
SPF Sender Policy Framework. A standard for authenticating email.
SPICE A robust in-memory engine that is part of Amazon QuickSight.
Engineered for the cloud, SPICE (Super-fast, Parallel, In-memory
Calculation Engine) uses a combination of storage and in-memory
technologies. It uses these to get faster results from interactive queries
and advanced calculations on large datasets. SPICE automatically
replicates data for high availability. SPICE makes it possible for Amazon
QuickSight to support hundreds of thousands of simultaneous analyses
across a variety of data sources.
Spot Instance A type of EC2 instance that you can bid on to use unused Amazon EC2
capacity.
Spot price The price for a Spot Instance at any given time. If your maximum price
exceeds the current price and your restrictions are met, Amazon EC2
launches instances on your behalf.
SQL injection match
condition
AWS WAF: An attribute that specifies the part of web requests (such as
a header or a query string) that AWS WAF inspects for malicious SQL
88
AWS Glossary Reference
code. Based on the specified conditions, you can configure AWS WAF
to allow or block web requests to an AWS resource, such as an Amazon
CloudFront distribution.
Amazon SQS Amazon Simple Queue Service is a reliable and scalable hosted queues
for storing messages as they travel between computers.
See Also https://aws.amazon.com/sqs.
Amazon SWF Amazon Simple Workflow Service is a fully managed service that helps
developers build, run, and scale background jobs that have parallel or
sequential steps. Amazon SWF functions similar to a state tracker and
task coordinator in the AWS Cloud.
See Also https://aws.amazon.com/swf/.
SSE See server-side encryption (SSE).
SSL Secure Sockets Layer
See Also Transport Layer Security (TLS).
stack CloudFormation: A collection of AWS resources that you create and
delete as a single unit.
OpsWorks: A set of instances that you manage collectively, typically
because they have a common purpose such as serving PHP applications.
A stack serves as a container and handles tasks that apply to the group
of instances as a whole, such as managing applications and cookbooks.
station CodePipeline: A portion of a pipeline workflow where one or more
actions are performed.
station A place at an AWS facility where your AWS Import/Export data is
transferred on to, or off of, your storage device.
statistic One of five functions of the values submitted for a given sampling
period. These functions are Maximum, Minimum, Sum, Average, and
SampleCount.
stem The common root or substring shared by a set of related words.
stemming The process of mapping related words to a common stem. This enables
matching on variants of a word. For example, a search for "horse" could
89
AWS Glossary Reference
return matches for horses, horseback, and horsing, as well as horse.
CloudSearch supports both dictionary based and algorithmic stemming.
step Amazon EMR: A single function applied to the data in a job flow. The
sum of all steps comprises a job flow.
Step Functions AWS Step Functions is a web service that coordinates the components of
distributed applications as a series of steps in a visual workflow.
See Also https://aws.amazon.com/step-functions/.
step type Amazon EMR: The type of work done in a step. There are a limited
number of step types, such as moving data from Amazon S3 to Amazon
EC2 or from Amazon EC2 to Amazon S3.
sticky session A feature of the ELB load balancer that binds a user's session to a
specific application instance. This is so that all requests that are coming
from the user during the session are sent to the same application
instance. By contrast, a load balancer defaults to route each request
independently to the application instance with the smallest load.
stopping The process of filtering stop words from an index or search request.
stopword A word that isn't indexed and is automatically filtered out of search
requests because it's either insignificant or so common that including
it results in too many matches to be useful. Stopwords are language
specific.
Storage Gateway AWS Storage Gateway is a hybrid cloud storage service that provides onpremises access to virtually unlimited cloud storage.
See Also AWS Storage Gateway.
streaming Amazon EMR: A utility that comes with Hadoop that you can use to
develop MapReduce executables in languages other than Java.
CloudFront: The ability to use a media file in real time—as it's
transmitted in a steady stream from a server.
streaming distribution A special kind of distribution that serves streamed media files using a
Real Time Messaging Protocol (RTMP) connection.
Streams See Kinesis Data Streams.
90
AWS Glossary Reference
string match condition AWS WAF: An attribute that specifies the strings that AWS WAF searches
for in a web request, such as a value in a header or a query string. Based
on the specified strings, you can configure AWS WAF to allow or block
web requests to an AWS resource, such as a CloudFront distribution.
string-to-sign Before you calculate an HMAC signature, you first assemble the required
components in a canonical order. The preencrypted string is the stringto-sign.
strongly consistent
read
A read process that returns a response with the most up-to-date data.
This data reflects the updates from all previous write operations that
were successful—regardless of the Region.
See Also data consistency, eventual consistency, eventually consistent
read.
structured query Search criteria that are specified using the CloudSearch structured
query language. You use the structured query language to construct
compound queries that use advanced search options and combine
multiple search criteria using Boolean operators.
AWS STS AWS Security Token Service is a web service for requesting temporary,
limited-privilege credentials for IAM users or for users that you
authenticate (federated users).
See Also https://aws.amazon.com/iam/.
subnet A segment of the IP address range of a Amazon VPC that an EC2
instance can be attached to. You can create subnets to group instances
according to security and operational needs.
Subscription button An HTML-coded button that provides a simple way to charge customers
a recurring fee.
suggester CloudSearch: Specifies an index field for getting autocomplete
suggestions and options that can enable fuzzy matches and control how
suggestions are sorted.
suggestions Documents that contain a match for the partial search string in the field
that's designated by the suggester. CloudSearch suggestions include
the document IDs and field values for each matching document. To be a
91
AWS Glossary Reference
match, the string must match the contents of the field starting from the
beginning of the field.
Sumerian Amazon Sumerian is a set of tools for creating and running high-quality
3D, augmented reality (AR), and virtual reality (VR) applications on the
web.
See Also https://aws.amazon.com/sumerian/.
supported AMI An Amazon Machine Image (AMI) similar to a paid AMI, except that the
owner charges for additional software or a service that customers use
with their own AMIs.
SWF See Amazon SWF.
symmetric encryption Encryption that uses a private key only.
See Also asymmetric encryption.
synchronous bounce A type of bounce that occurs while the email servers of the sender and
receiver are actively communicating.
synonym A word that's the same or nearly the same as an indexed word and that
likely produces the same results when specified in a search request.
For example, a search for "Rocky Four" or "Rocky 4" likely returns the
fourth Rocky movie. You can do this by designating that four and 4 are
synonyms for IV. Synonyms are language specific.
Systems Manager AWS Systems Manager is the operations hub for AWS and hybrid
cloud environments that can help achieve secure operations at scale. It
provides a unified user interface for users to view operations data from
multiple AWS services and automate tasks across their AWS resources.
See Also https://aws.amazon.com/systems-manager.
T
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
table A collection of data. Similar to other database systems, DynamoDB
stores data in tables.
92
AWS Glossary Reference
tag Metadata that you can define and assign to AWS resources, such as an
EC2 instance. Not all AWS resources can be tagged.
tagging Tagging resources: Applying a tag to an AWS resource.
Amazon SES: Also called labeling. A way to format return path
email addresses so that you can specify a different return path
for each recipient of a message. You can use tagging to support
VERP. For example, if Andrew manages a mailing list, he can use the
return paths andrew+recipient1@example.net and andrew
+recipient2@example.net so that he can determine which email
bounced.
target attribute Amazon Machine Learning (Amazon ML): The attribute in the input data
that contains the “correct” answers. Amazon ML uses the target attribute
to learn how to make predictions on new data. For example, if you were
building a model for predicting the sale price of a house, the target
attribute would be “target sale price in USD.”
target revision CodeDeploy: The most recent version of the application revision that has
been uploaded to the repository and will be deployed to the instances
in a deployment group. In other words, the application revision currently
targeted for deployment. This is also the revision that will be pulled for
automatic deployments.
task An instantiation of a task definition that's running on a container
instance.
task definition The blueprint for your task. Specifies the name of the task, revisions,
container definitions, and volume information.
task node An EC2 instance that runs Hadoop map and reduce tasks, but doesn't
store data. Task nodes are managed by the master node, which assigns
Hadoop tasks to nodes and monitors their status. While a job flow
is running, you can increase and decrease the number of task nodes.
Because they don't store data and can be added and removed from a job
flow, you can use task nodes to manage the EC2 instance capacity your
job flow uses, increasing capacity to handle peak loads and decreasing it
later.
93
AWS Glossary Reference
Task nodes only run a TaskTracker Hadoop daemon.
tebibyte (TiB) A contraction of tera binary byte. A tebibyte (TiB) is 2^40 or
1,099,511,627,776 bytes. A terabyte (TB) is 10^12 or 1,000,000,000,000
bytes. 1,024 TiB is a pebibyte (PiB).
template format
version
The version of an CloudFormation template design that determines
the available features. If you omit the AWSTemplateFormatVersion
section from your template, AWS CloudFormation assumes the most
recent format version.
template validation The process of confirming the use of JSON code in an CloudFormation
template. You can validate any AWS CloudFormation template using the
cfn-validate-template command.
temporary security
credentials
Authentication information that's provided by AWS STS when you call an
STS API action. Includes an access key ID, a secret access key, a session
token, and an expiration time.
Amazon Textract Amazon Textract is a service that automatically extracts text and data
from scanned documents. Amazon Textract goes beyond simple optical
character recognition (OCR) to also identify the contents of fields in
forms and information stored in tables.
See Also https://aws.amazon.com/textract/.
throttling The automatic restricting or slowing down of a process based on one or
more limits. For example, Kinesis Data Streams throttles operations if
an application (or group of applications operating on the same stream)
attempts to get data from a shard at a rate faster than the shard limit.
API Gateway uses throttling to limit the steady-state request rates for a
single account. Amazon SES uses throttling to reject attempts to send
email that exceeds the sending limits.
time-series data Data that's provided as part of a metric. The time value is assumed
to be when the value occurred. A metric is the fundamental concept
for CloudWatch and represents a time-ordered set of data points. You
publish metric data points into CloudWatch and later retrieve statistics
about those data points as a time-series ordered dataset.
94
AWS Glossary Reference
timestamp A date/time string in the ISO 8601 format (more specifically, in the
YYYY-MM-DD format).
Timestream Amazon Timestream is a scalable and serverless time series database
service for real-time analytics, DevOps, and IoT applications that you can
use to store and analyze trillions of events per day.
See Also https://aws.amazon.com/timestream.
TLS See Transport Layer Security (TLS).
tokenization The process of splitting a stream of text into separate tokens on
detectable boundaries such as white space and hyphens.
AWS Toolkit for Eclipse AWS Toolkit for Eclipse is an open-source plugin for the Eclipse Java
integrated development environment (IDE) that makes it easier to
develop, debug, and deploy Java applications using Amazon Web
Services.
See Also https://aws.amazon.com/eclipse/.
AWS Toolkit for
JetBrains
AWS Toolkit for JetBrains is an open-source plugin for the integrated
development environments (IDEs) from JetBrains that makes it easier to
develop, debug, and deploy serverless applications using Amazon Web
Services.
See Also https://aws.amazon.com/intellij/, https://aws.amazon.com/
pycharm/.
AWS Toolkit for
Microsoft Azure
DevOps
AWS Toolkit for Microsoft Azure DevOps provides tasks you can use in
build and release definitions in VSTS to interact with AWS services.
See Also https://aws.amazon.com/vsts/.
AWS Toolkit for Visual
Studio
AWS Toolkit for Visual Studio is an extension for Visual Studio that
helps in developing, debugging, and deploying .NET applications using
Amazon Web Services.
See Also https://aws.amazon.com/visualstudio/.
AWS Toolkit for Visual
Studio Code
AWS Toolkit for Visual Studio Code is an open-source plugin for the
Visual Studio Code (VS Code) editor that makes it easier to develop,
debug, and deploy applications using Amazon Web Services.
See Also https://aws.amazon.com/visualstudiocode/.
95
AWS Glossary Reference
AWS Tools for
PowerShell
AWS Tools for PowerShell is a set of PowerShell cmdlets to help
developers and administrators manage their AWS services from the
PowerShell scripting environment.
See Also https://aws.amazon.com/powershell/.
topic A communication channel to send messages and subscribe to
notifications. It provides an access point for publishers and subscribers
to communicate with each other.
Traffic Mirroring An Amazon VPC feature that you can use to copy network traffic from an
elastic network interface of Amazon EC2 instances. You can then send
this network traffic to out-of-band security and monitoring appliances
for content inspection, threat monitoring, and troubleshooting.
See Also https://aws.amazon.com/vpc/.
training datasource A datasource that contains the data that Amazon Machine Learning uses
to train the machine learning model to make predictions.
Amazon Transcribe Amazon Transcribe is a machine learning service that uses automatic
speech recognition (ASR) to quickly and accurately convert speech to
text.
See Also https://aws.amazon.com/transcribe/.
Amazon Transcribe
Medical
Amazon Transcribe Medical is an automatic speech recognition (ASR)
service for adding medical speech-to-text capabilities to voice-enabled
clinical documentation applications.
See Also https://aws.amazon.com/transcribe/medical/.
Transfer Family AWS Transfer Family offers fully managed support for transferring files
over SFTP, FTPS, and FTP into and out of Amazon S3 or Amazon EFS,
as well as support for the Applicability Statement 2 (AS2) protocol for
business-to-business (B2B) transfers.
See Also https://aws.amazon.com/aws-transfer-family.
transition CodePipeline: The act of a revision in a pipeline continuing from one
stage to the next in a workflow.
Amazon Translate Amazon Translate is a neural machine translation service that delivers
fast, high-quality, and affordable language translation.
See Also https://aws.amazon.com/translate/.
96
AWS Glossary Reference
Transport Layer
Security (TLS)
A cryptographic protocol that provides security for communication over
the internet. Its predecessor is Secure Sockets Layer (SSL).
trust policy An IAM policy that's an inherent part of an IAM role. The trust policy
specifies which principals are allowed to use the role.
Trusted Advisor AWS Trusted Advisor is a web service that inspects your AWS
environment and makes recommendations for saving money, improving
system availability and performance, and helping to close security gaps.
See Also https://aws.amazon.com/premiumsupport/trustedadvisor/.
trusted key groups Amazon CloudFront key groups whose public keys CloudFront can use to
verify the signatures of CloudFront signed URLs and signed cookies.
trusted signers See trusted key groups.
tuning Selecting the number and type of AMIs to run a Hadoop job flow most
efficiently.
tunnel A route for transmission of private network traffic that uses the internet
to connect nodes in the private network. The tunnel uses encryption
and secure protocols such as PPTP to prevent the traffic from being
intercepted as it passes through public routing nodes.
U
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
unbounded The number of potential occurrences isn't limited by a set number. This
value is often used when defining a data type that's a list (for example,
maxOccurs="unbounded"), in WSDL.
unit Standard measurement for the values submitted to CloudWatch as
metric data. Units include seconds, percent, bytes, bits, count, bytes/
second, bits/second, count/second, and none.
usage report An AWS record that details your usage of a particular AWS service.
You can generate and download usage reports from https://
aws.amazon.com/usage-reports/.
97
AWS Glossary Reference
user A person or application under an account that makes API calls to AWS
products. Each user has a unique name within the AWS account, and
a set of security credentials that aren't shared with other users. These
credentials are separate from the security credentials for the AWS
account. Each user is associated with one and only one AWS account.
USER_PERSONALIZATION
recipes
Amazon Personalize: Recipes that are used to build a recommendation
system that predicts the items that a user interacts with based on data
provided in Interactions, Items, and Users datasets.
See Also recipe, user-personalization recipe, popularity-count recipe,
HRNN.
user-personalization
recipe
Amazon Personalize: An HRNN-based USER_PERSONALIZATION
recipe that predicts the items that a user interacts with. The userpersonalization recipe can use item exploration and impressions data to
generate recommendations for new items.
See Also HRNN, recipe, USER_PERSONALIZATION recipes, item
exploration, impressions data, recommendations.
Users dataset Amazon Personalize: A container for metadata about your users, such as
age, gender, or loyalty membership.
See Also dataset.
V
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
validation See template validation.
value Instances of attributes for an item, such as cells in a spreadsheet. An
attribute might have multiple values.
Tagging resources: A specific tag label that acts as a descriptor within a
tag category (key). For example, you might have EC2 instance with the
tag key of Owner and the tag value of Jan. You can tag an AWS resource
with up to 10 key–value pairs. Not all AWS resources can be tagged.
Variable Envelope
Return Path
See VERP.
98
AWS Glossary Reference
verification The process of confirming that you own an email address or a domain so
that you can send email from or to it.
VERP Variable Envelope Return Path. A way that email-sending applications
can match bounced email with the undeliverable address that caused
the bounce by using a different return path for each recipient. VERP is
typically used for mailing lists. With VERP, the recipient's email address
is embedded in the address of the return path, which is where bounced
email is returned. This makes it possible to automate the processing
of bounced email without having to open the bounce messages, which
might vary in content.
versioning Every object in Amazon S3 has a key and a version ID. Objects with the
same key, but different version IDs can be stored in the same bucket.
Versioning is enabled at the bucket layer using PUT Bucket versioning.
VGW See virtual private gateway (VGW).
virtual private gateway
(VGW)
The Amazon side of a VPN connection that maintains connectivity.
The internal interfaces of the virtual private gateway connect to your
Amazon VPC through the VPN attachment. The external interfaces
connect to the VPN connection, which leads to the customer gateway.
virtualization Allows multiple guest virtual machines (VM) to run on a host operating
system. Guest VMs can run on one or more levels above the host
hardware, depending on the type of virtualization.
See Also PV virtualization, HVM virtualization.
visibility timeout The period of time that a message is invisible to the rest of your
application after an application component gets it from the queue.
During the visibility timeout, the component that received the message
usually processes it, and then deletes it from the queue. This prevents
multiple components from processing the same message.
VM Import/Export VM Import/Export is a service for importing virtual machine (VM)
images from your existing virtualization environment to Amazon EC2
and then exporting them back.
See Also https://aws.amazon.com/ec2/vm-import.
99
AWS Glossary Reference
volume A fixed amount of storage on an instance. You can share volume data
between more than one container and persist the data on the container
instance when the containers are no longer running.
Amazon VPC Amazon Virtual Private Cloud is a web service for provisioning a logically
isolated section of the AWS Cloud virtual network that you define. You
control your virtual networking environment by selecting your own
IP address range, creating subnets and configuring route tables and
network gateways.
See Also https://aws.amazon.com/vpc.
VPC endpoint A feature that you can use to create a private connection between your
Amazon VPC and another AWS service without requiring access over the
internet, through a NAT instance, a VPN connection, or Direct Connect.
VPG See virtual private gateway (VGW).
AWS VPN AWS Virtual Private Network provides functionality that establishes
encrypted connections between your network or device, and AWS. AWS
VPN is comprised of two services: AWS Client VPN and AWS Site-to-Site
VPN.
See Also https://aws.amazon.com/vpn.
AWS VPN CloudHub AWS VPN CloudHub is a feature that enables secure communication
between branch offices using a simple hub-and-spoke model, with or
without a VPN.
VPN connection Amazon Web Services (AWS): The IPsec connection that's between a
Amazon VPC and some other network, such as a corporate data center,
home network, or colocation facility.
W
Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X,
Y, Z
AWS WAF AWS WAF is a web application firewall service that controls access to
content by allowing or blocking web requests based on criteria that you
specify. For example, you can filter access based on the header values
or the IP addresses that the requests originate from. AWS WAF helps
100
AWS Glossary Reference
protect web applications from common web exploits that could affect
application availability, compromise security, or consume excessive
resources.
See Also https://aws.amazon.com/waf/.
Amazon WAM Amazon WorkSpaces Application Manager (Amazon WAM) is a web
service for deploying and managing applications for WorkSpaces.
Amazon WAM accelerates software deployment, upgrades, patching, and
retirement by packaging Windows desktop applications into virtualized
application containers.
See Also https://aws.amazon.com/workspaces/applicationmanager.
warm standby An active-passive disaster recovery strategy in which a workload is
scaled down in the passive standby Region, but is otherwise fully
functional. This is not an Amazon EC2 Auto Scaling term, but an
industry-standard resilience term.
See Also , , .
AWS Wavelength AWS Wavelength is a service by AWS that embeds AWS compute and
storage services within 5G networks to provide mobile edge computing
infrastructure. Use AWS Wavelength to develop, deploy, and scale ultralow-latency applications to mobile devices and end users.
See Also https://aws.amazon.com/wavelength.
web access control list
(web ACL)
AWS WAF: A set of rules that defines the conditions that AWS WAF
searches for in web requests to an AWS resource, such as a Amazon
CloudFront distribution. A web access control list (web ACL) specifies if
to allow, block, or count the requests.
Web Services
Description Language
See WSDL.
WorkDocs Amazon WorkDocs is a managed, secure enterprise document storage
and sharing service with administrative controls and feedback
capabilities.
See Also https://aws.amazon.com/workdocs/.
Amazon WorkLink Amazon WorkLink is a cloud-based service that provides secure access to
internal websites and web apps from mobile devices.
101
AWS Glossary Reference
See Also https://aws.amazon.com/worklink/.
WorkMail Amazon WorkMail is a managed, secure business email and calendar
service with support for existing desktop and mobile email clients.
See Also https://aws.amazon.com/workmail/.
WorkSpaces Amazon WorkSpaces is a managed, secure desktop computing service
for provisioning cloud-based desktops and providing users access to
documents, applications, and resources from supported devices.
See Also https://aws.amazon.com/workspaces/.
WSDL Web Services Description Language. A language that's used to describe
the actions that a web service can perform, along with the syntax of
action requests and responses.
See Also REST, SOAP.
X, Y, Z
X.509 certificate A digital document that uses the X.509 public key infrastructure (PKI)
standard to verify that a public key belongs to the entity that's described
in the certificate.
X-Ray AWS X-Ray is a web service that collects data about requests that your
application serves. X-Ray provides tools that you can use to view, filter,
and gain insights into that data to identify issues and opportunities for
optimization.
See Also https://aws.amazon.com/xray/.
yobibyte (YiB) A contraction of yotta binary byte. A yobibyte (YiB) is 2^80 or
1,208,925,819,614,629,174,706,176 bytes. A yottabyte (YB) is 10^24 or
1,000,000,000,000,000,000,000,000 bytes.
zebibyte (ZiB) A contraction of zetta binary byte. A zebibyte (ZiB) is 2^70 or
1,180,591,620,717,411,303,424 bytes. A zettabyte (ZB) is 10^21 or
1,000,000,000,000,000,000,000 bytes. 1,024 ZiB is a yobibyte (YiB).
zone awareness OpenSearch Service: A configuration that distributes nodes in a cluster
across two Availability Zones in the same Region. Zone awareness helps
to prevent data loss and minimizes downtime if a node and data center
102
AWS Glossary Reference
fails. If you enable zone awareness, you must have an even number of
data instances in the instance count, and you also must use the Amazon
OpenSearch Service Configuration API to replicate your data for your
OpenSearch cluster.
103




---AWS FULL COURSE FOR BEGINNERS----

Cloud computing is at the cusp
of technological advancement.
And when you talk about cloud
computing it cannot shy away
without mentioning
Amazon web services (AWS),
which is one of the leading
cloud service providers
in the market.
If you are looking for a career
in this domain you have landed
at the right place Edureka
brings you a complete course
on Amazon web services,
which not only touches
upon the fundamental
but also die.
It's deeper at
a conceptual level.
So let us take a look
at the offerings
of this session first.
We would start with the
fundamentals of cloud computing
and Amazon web services
moving on we will talk
about the core services
that Amazon web services
has to offer to you.
The first domain is
the compute domain
where we would be exploring
services like ec2.
Elastic Beanstalk
and Lambda moving on.
We'll talk about
this storage domain
where we'll be exploring
services like S3 EFS and Next
in line is the networking domain
where we'll be talking about
services like VPC Route 53 Etc.
Then could be
talking about management
and monitoring services
like Cloud watch
cloudformation load balances
Etc moving on you take a look
at Cloud security
and take a look at services.
Like I am Etc then
the database part
where we'll be exploring
services like Amazon redshift.
Once we are done
with the core Services,
we will be also
discussing develops on AWS
where we will be talking
about AWS services
like aw score pipeline,
aw score commit Etc.
Now that the devops part
and the core part
of AWS is over.
We can also switch
to the career part
where we'd be
discussing some numbers
like jobs friends salaries Etc
and would also take
a look at the roles
and responsibilities.
And what are the kind of things
that you should know
when you talk about making a
career in this particular Dome?
So before we get started,
feel free to subscribe
to our YouTube channel
to get the latest updates
on the trending Technologies.
Firstly let's understand
why Cloud to understand
this we need to understand
the situation that existed
before Cloud came
into existence.
So what happened back then
and firstly in order
to host a website you
have to buy a stack
of servers and we all know
that servers are very costly.
So that meant we ended up
paying a lot of money next was
the issue of traffic now
as we all know
if you are hosting a website
we are dealing with traffic
that is not constant
throughout the day
and that meant more pain
we would understand
that as we move further.
And the other
thing was monitoring
and maintaining your servers.
Yes.
This is a very big problem
now all these issues.
They led to
certain disadvantages.
What are those
as I mentioned servers
are very costly.
Yes.
The setup was again costly
and thus you ended up
being a lot of money
and there were other factors
contributing to this point.
Let's discuss those as well.
One troubleshooting was
a big issue since you're dealing
with a business your Prime Focus
is on taking good decisions
so that you have
Business does well,
but if you end up
troubleshooting problems
or you focus more on
infrastructure related issues,
then you cannot focus
more on your business
and that was a problem.
So either you had
to do multitasking
or you have to hire
more people to focus
on those issues thus again
you ended up being more money
as I've discussed the traffic
on a website is never constant.
And since it varies you are
not certain about its patterns.
Say, for example,
I need to host a website
and for that what I
decided is I am reserving.
To petabytes of total memory for
my usage based on the traffic
but as the traffic
where is there would be times
when the traffic is high
and my whole to petabytes
of data is consumed
or space is consumed Roger,
but what if the traffic
is very low for certain
hours of the day.
I'm actually not
utilizing these servers.
So I end up paying
more money for the servers
than I should be.
So yes upscaling was an issue.
So all these things
were an issue
because we were
paying more money.
We do not have sufficient time
to Take our decisions properly.
There was ambiguity.
There was more trouble
monitoring and maintaining
all these resources and apart
from that one important point
which we need to consider
is the amount of data
that is being generated now
and that was being generated
then then it was okay,
but nowadays if you take a look
at it the amount of data
that is generated is huge
and this is another reason why
Cloud became so important as
of mentioned the data now,
we all know that everything
is going online these days
and what that means
is we shop online.
And we buy food online.
We do almost everything
that is required as
an whatever information we need.
We get everything online
bookings and reservations.
Everything can be taken care of
that means we have a lot of data
that is being generated
these days and this
is Digital Data back
in those times.
We were communicating
through verbal discussions
and all those things so
through paperwork and that was
a different data to maintain
since everything is moving
on cloud or moving
online the amount of data
that we have is used these days.
Days, and then when you have
this huge amount of data,
you need a space
where you can actually go ahead
and maintain this data.
So yes again,
there was a need of this piece
and all these issues
that is your cost.
You're monitoring
your maintenance providing
sufficient space.
Everything was taken
care by Cloud.
So let us try to understand
what this cloud is exactly.
Well think of it as a huge space
that is available
online for your usage.
Now.
This is a very generic
definition to give you
to be more specific.
I would be seeing that.
Think of it as a collection
of data centers now
data centers again at a place
where you store your data or you
host applications basically,
so when you talk
about these data centers,
they were already existing.
So what did
Cloud do differently?
Well, what cloud
did was it made sure
that you are able
to orchestrate your various
functionings applications
managing your resources properly
by combining all
these data centers together
through a network
and then providing
you the the control
to use this resources
and to manage them properly
to make it even more simpler.
I would say there was a group
of people or organizations.
Basically that went ahead
and what these servers
these compute capacities
storage places compute services
and all those things
and they have their own
channel or Network.
All you have to do was go ahead
and rent those resources only
to the amount you need it
and also for the time
that you needed.
So yes, this is what cloud
did It let you rent the services
that you need and use
only those services
that you need.
So you ended up paying
for the services
that you rented and you ended
up saving a lot of money.
The other thing is
these service providers.
They take care of all the issues
like your security
your underlying infrastructures
and all those things.
So you can freely focus
on your business
and stop worrying
about all these issues.
So this is what cloud is
in simple words.
It's a huge space which has
all these services available
and you can just go ahead
and pick and read.
And those services
that you want to use so
what is cloud computing?
Well, I've already discussed
that just to summarize
it I would say it is
nothing but an ability
or it is a place where you
can actually store your data.
You can process it
and you can access it
from anywhere in the world.
Now.
This is an important Point
say for example,
you decide to choose
a reason for infrastructure
somewhere in u.s.
You can certain maybe China
or maybe in India
and you can still have access
to all your resources
that is there in u.s.
All you need is a good
And a connection
so that is what cloud
does it makes the world
accessible it lets
you have your applications
wherever you want to
and manage them the way you want
to next we would be discussing
different service models.
Now you need to understand
one thing you are being offered
cloud services the platform
to use your services
or your applications basically,
but then different people
have different requirements.
There are certain people
who just want to consume
a particular resource
or there's certain people
who actually want
to to go ahead and create
their own applications
great the own infrastructure
and all those things.
So based on these needs we
have particular service models
that is your Cloud
providers provide you
with a particular model
which suits your needs.
So let us try to understand
these models one by one we
have these three models
that is your iaas
your paas and your saas.
I would be discussing them
in the reverse order.
That is I would be talking
about saas first
and then I would go upwards
so let us start.
Saas, or SAS SAS is nothing
but a software-as-a-service.
Now what happens here
is basically you're just
consuming a service
which is already
being maintained and handled
by someone else to give
you a valid example.
We have a Gmail.
All you do is you send mail to
people and you receive mails
and whatever functionality you
do is you just use the service
that is there.
You do not have to maintain it.
You do not have to worry
about up scaling down
scalings security issues
and all those things.
Everything is taken care
by Google say for example,
you are Gmail is what
I'm talking about Google
manages everything here.
So all you have to worry
about is consuming that service
now this model is known
as software as a service
that is saas.
Next we have passed
that is platform as a service
now here you are provided
with a platform
where you can actually go ahead
and build your own applications
to give you an example.
We have our Google app engine.
Now when you talk
about Google app engine,
what you can do is
you can go ahead.
You can create
your own applications
and you can put it
on Google app engine so
that others can use it as well.
So in short you're using
the app platform to create
your own applications,
and lastly we have iaas
that is infrastructure
as a service.
Now.
What do I mean by this?
Well, the whole infrastructure
is provided to you
so that you can go ahead and
create your own applications.
That is an underlying structure
is given to you based on that.
You can go ahead and choose
your operating systems
the kind of Technology
on to use on that platform
the applications you want
to build an All those things
so that is what an iaas
is infrastructure-as-a-service
basically,
so these were
the different models
that I wanted to talk about.
Now.
This is the architecture
that gives you a clear depiction
as in what happens
as far as the service
models are concerned.
Now, you have something
called as your sass now here
as you see all you're doing
is you're consuming your data,
that's it or using it.
Everything else is managed
by your vendor.
That is your applications
runtime middleware OS
virtualization servers Network.
Everything as far as your past
is concerned your data
and applications are
taken care by you.
That is you can go ahead you
can build your own applications.
You can use
the existing platform
that is provided to you.
And finally you have your iaas.
Now what happens here
is only the basic part
that is your
networking storage servers
and virtualization is managed
by your vendor deciding
what middleware OS runtime
applications and data
that resides on your end.
You have to manage
all these things
that is you are just
given a box of car.
For example people
or maybe parts of car you
go ahead and you fix it.
And you use it for your own sake
that is what iaas is to give you
another example think
of it as eating a pizza.
Now there are various ways
of doing that one you order
it online you sit at home
you order the pizza.
It comes to your place
you consume it
that is more of your saas.
That is software as a service.
You just consume the service.
Next is a platform as a service.
Now when I say platform
as a service you can think
of it as going to a hotel
and eating a pizza.
Say, for example,
I go They have
the infrastructure
as in I have tables chairs.
I have to go sit
just order the pizza.
It is given to me.
I consume it and I come
back home and iaas.
Now.
This is where you go ahead
and make your own pizza.
You have the infrastructure
you buy it from somewhere
or whatever it is.
You use your pizza.
You put it in our new put spices
all those things.
Can you eat it now?
This is the difference
between these three services.
So let us move further
and discuss the next topic.
That is the different
deployment models
that are there now
when you talk about
deployment models you
can also call All them as
different types of clouds
that are there in the market
we have these three types.
That is your public Cloud
your private cloud
and your hybrid Cloud.
Let us try to understand
these one by one now
as the name suggests
the public Cloud it's available
to everyone you have
a service provider
who makes these services
or these resources available
to people worldwide
through the internet.
It is an easy and very
inexpensive way of dealing
with the situation
because all you have to do
is you have to go ahead
and rent this cloud
and you're good to you.
And it is available publicly.
Next we have the private Cloud.
Now.
This is a little different here.
You are provided
with this service
and you can actually
go ahead and create
your own applications.
And since it's a private Cloud
you are protected by a firewall
and you do not have to worry
about various other issues
that are there at hand and next.
We have our hybrid Cloud now,
it is a combination
of your private cloud
and your public Cloud say,
for example, you can go ahead
and build your applications
privately you can use them.
You can consume them you
can use them efficiently.
When you sense that peak
in your traffic.
You can actually
move it to public
that is you can move it
to the public cloud
and even others can have access
to it and they can use it.
So these are the three
basic deployment models
that are there
for your exposure
or your usage rather and you can
go ahead and use those as well.
I hope this was clear
to all of you.
So let us move further and try
to understand the next topic
that is different Cloud
providers that are there
in the market now
as I've mentioned
what happened was
since Cloud came into existence.
Quite a few people went ahead
and they bought their own
infrastructure and now they rent
the services to other people
and when you talk
about this infrastructure
the quite a few people out there
who are actually providing
these cloud services
to different people
across the globe.
Now, when you talk
about these Cloud providers,
the first thing
that should come to your mind
is Amazon web services
because it is highly popular
and it leaves other
Cloud providers way behind.
The reason I'm saying
this is the numbers
that talk about Amazon web
services to You an example
if you talk about
its compute capacity.
It is six times larger than all
the other service providers
that are there in the market
say for example,
if you talk about the other
service providers in the market,
if the compute capacity combined
was X Amazon web services alone
gives you a capacity of 6 x
which is huge apart from that.
It's flexible pricing
and various other reasons.
That is the services it provides
and all those things.
It is rightly a global
leader and the fact
that it had a head start.
It started way
before many other services
that are there in the market.
It actually gained popularity.
And now we see quite
a few organizations going ahead
and using Amazon web services
apart from that.
We have Microsoft Azure,
which is a Microsoft product
and we all know
that when Microsoft decides
to do something they expect
that they kill
all the competition
that is there in the market.
It is still not in terms
with Amazon web services
or few other service providers
that are then the market
but not very neck to neck
but it is probably
the second best
when you talk about Amazon web.
Services or the cloud service
providers in the market?
So yep.
It has a lot
of catching up to do
when you compare it
with Amazon web services,
but it is still a very
good cloud service provider
that is there in the market.
Then we have something called
as Google Cloud platform again
a very good cloud provider
in the market.
Now, why am I saying this?
We all know the infrastructure
that Google has to offer
to you it has one
of the best search engine
that is then the market
and the amount of data they deal
with every day is huge.
So they are the Pioneers
when you talk about Data
and all those things
and they know
how to actually handle
this amount of data and
how to have an infrastructure
that is very good.
That is why they have a very
good facility and that leads
to it being one of the cheapest
service providers in the market.
Yes.
There are certain features
that DCB offers
which are better.
Even than Amazon web services
when you talk about its pricing
and the reason for it is
it has various other services
that are there water does is it
helps you optimize various costs
how it uses analytics
and various other ways
by which it can optimize
the amount of power you use
and that leads
to less usage of power.
And since you are
paying less for power
that is provided as a paying
less for power you end up paying
less for your services as well.
So that is why it is
so cost efficient.
Then the other service providers
that is we have digital ocean.
We have to remark we have IBM
which is again very popular,
but that is a discussion
for some other time.
As far as these
service providers go.
These are the major ones that
as we have Amazon web services
we Microsoft Azure,
we have DCP which are
talked about a lot.
This was about the basic Cloud
providers and the basic intro
which I wanted you all to have.
I hope you all are clear
with whatever Concepts
we've discussed in time.
Let's try to understand
a little more about AWS.
Well, it is a complete
software suit or
a cloud service provider,
which is highly secure.
It provides you with various
compute storage database
and a number of other services,
which we would be discussing.
Discussing in further
slides as well.
And when you talk
about the market it is the best
and it has various
reasons to be the best
in the market one being
its flexibility its scalability
and its pricing other reasons
being its compute capacity now,
why is it so important
to compute capacity?
Well, if you talk
about the compute capacity,
you need to understand one thing
if you take all the other
cloud service providers
in the market and you
combine the compute capacity
that is your layout
AWS and you take all others
into consideration
this Is would be
somewhere equal to say x
and if you compare it
with AWS, it is 6X.
So AWS has
more compute capacity,
which is six times more than all
the other service providers
that are there in the market.
So that is a huge amount.
So these are the reasons
that make a database one
of the best in the market
and let's try to find out
what are the other reasons
about aw that make it so good.
What are the services features
and its uses basically,
so I would be discussing
some use cases now.
Now if you are talking about a
manufacturing organization now,
the main focus is
to manufacture Goods,
but most of the businesses
they focus so much
on various other services are
practices that need
to be taken care of that.
They cannot focus on the
manufacturing goal of this is
where aw steps--and it
takes care of all the it
infrastructure and management.
That means businesses are free
to focus on manufacturing
and they can actually go ahead
and expand a lot
architecture Consulting now,
the main concern is prototyping
and During a dove is takes care
of both the issues it
lets you have automated
or speed up rendering as
far as prototyping is concerned
and that is why architectural
business benefit a lot
when you talk about using AWS
or any cloud provider
but AWS being the best
in the market again,
the services are
the best media company now
as far as a media company
goes the main concern
is generating content
and the place to dump it
out to store it again,
aw takes care
of all these situations
or both these situations.
Large Enterprises when you talk
about large Enterprises
their reach is worldwide,
so they have to
reach the customers
and the employees globally
or across different places.
So AWS gives you that option
because it has
a global architecture
and your research
can be very wide as
far as these points are
concerned the advantages of AWS
as I mentioned.
I won't say advantages exactly.
I would say features
as well flexibility.
Now as far as AWS is concerned
it is highly flexible now
the The reasons to support it
and one of the major reasons
is it's very cost-effective.
Let us try to understand these
two points together other now
when you talk about flexibility,
the first concern you
should have is you are dealing
with big organizations.
They have a lot of data
that needs to be managed
deployed and taken care
of now when you talk
about a cloud provider
if it is flexible,
all these things are taken care
of the second thing is it is
highly cost-effective now
when I say cost-effective
AWS takes care
of almost every aspect.
Aspect if you are
a beginner or a learner,
they have something
called as a free tier.
That means you have sufficient
resources to use for free and
that too for one long year
stood have sufficient Hands-On
without paying anything plus
it has something called as
pay-as-you-go model now
when I say pay
as you go model
what it does is it charges
you only for the services
which are using
and only for the time being
you're using them again
that lets you scale up nicely
and hence you end
up paying very less
since you are being very less.
And since you have
so many options
when you are actually
buying it Services
what that does is
that gives you a lot of
flexibility scalability again,
the first two points
are related to this point.
Now, how is that
when I say scalability
what happens is
as I mentioned it
is very affordable.
So you're paying
on a daily basis if you're using
a particular service
for one hour you'll be paying
it only for one hour.
That is how flexible it is.
And what that does is
that gives you a freedom
to scale up and even scale down
since it Is easy to scale up?
It is always advisable
that you start with less
and then scale as
for your needs plus they're
quite a few services
that are there which can
be automatically schedule.
Now what that means is you
would be using them only
when there is an up time
and in down time
you can miss those
get automatically shut down
so you do not have to worry
about that as well.
So when you talk
about scalability scaling up
and down is very easy as
far as AWS course security
again are now security
has been a topic of debate
when you talk about
What cloud services especially
but AWS puts all
those questions to rest.
It has great security mechanism.
Plus it provides you
with various compliance programs
that again help you take care
of security and when you talk
about real-time Security even
that is taken care
of you can take care
of all the suspicious activities
that are there and not uaw's
takes care of all those things
and you're let free to focus
on your business rather.
So these are the advantages
which I feel that AWS adds value
to and apart from that
the quite a few other points
like we have
automatic scheduling
which I just mentioned you have
various integrated apis.
Now these apis
that are available in
different programming languages
and that makes it architecture
really very strong to switch
from one programming language
to another so these are some
of the features I feel
that make AWS a wonderful
wonderful service provider
in the market.
So let's move further and try
to understand other things
as far as database is concerned.
It's Global architecture
when you talk
about a double usage
of mentioned it is the best
service provider in the market.
So what X ews this popular.
One of the reasons is
its architecture now when I talk
about its architecture,
it is very widely spread
and it covers almost every area
that needs to be covered.
So let's try to understand
how it works.
Exactly.
Well if you talk
about AWS architecture now,
the architecture is divided
into two major parts
that is Regions
and availability zones.
Now when you talk
about the regions
and availability zones reasons
are nothing but different
locations across the world
where they have there.
Various data centers put up now.
As far as one region
goes it might have
more than one Data Center
and these data centers
are known as availability Zone.
You being a consumer
or an individual you
can actually access
or access these Services
by sitting anywhere in the world
to give you an example.
If I'm sitting in some part
of the world say,
for example, I am
in Japan right now.
I can actually have access
to the services or data centers
that are there in u.s.
Right now.
So that is how it works.
You can choose your region.
Accordingly you can pick your
availability zones and use those
so you do not have to worry
about anything to throw
some more light on it.
You can take a look
at this small map
which is the global map and it
shows the different places
which has its regions
and availability zones.
Now as far as this map goes,
I believe it's fairly old
and it has been upgraded
in recent times
because AWS is putting a lot
of effort to have
more data centers
or more availability zones
as far as there.
Wide reach is concerned
and we can expect some
in China as well.
So yes, they are actually
reaching for and white.
So when you talk
about these regions
and availability zones,
if you take a look at this map
what you can see is
you have your reason
which is an orange color.
And the number that is inside.
It is the number
of availability zones
that they has to be now
to give you an example.
We have São Paulo,
which says that it has
three availability zones,
so that is how it is
and the ones that are
in the green Are the ones
which are coming
soon are the regions
that are in progress and some
of these have actually gone.
I hadn't already started
or have been made
available to people.
So yes, this is
how the architecture works
and this is how the database
architecture looks like.
Okay, so let's move further
and take a look at the next
concept domains of AWS.
When you talk about its domains.
The first domain
that we are going
to discuss is compute.
And when you talk
about compute the first thing
that should come to your mind
is easy to have a nice easy
to it is elastic Cloud compute
and what it does
is it lets you have
a resizable compute capacity.
It's more of a raw server
where you can host a website
and it is a clean slate.
Now.
What do I mean by this?
Say for example,
you go ahead and buy a laptop.
It is a clean device
where you can have
your own OS you can choose
which OS you want and all
those things accordingly.
Your ec2 is again a clean slate
and you can do so
many things with it.
Now next you have
elastic Beanstalk with lets
you deploy your various
applications on AWS.
And the only thing you need
to know about this thing is
you do not have to worry about
the underlying architecture now,
it is very similar to your ec2.
And the only difference
between the two is as
far as your elastic Beanstalk
is Concern you can think
of it as something
that has predefined libraries.
Whereas your ec2 is
a clean slate when I say
predefined libraries say,
for example, you want to use
Java as far as easy to goes.
Now.
This is just an example.
Don't take it literally
will have to say for example,
install everything from
the beginning and start fresh.
But as far as your elastic
Beanstalk is concerned
it has this predefined libraries
and you can just go
ahead and use those
because there's an underlying
Sighing architecture,
which is defined.
Let me say it again.
I just give you an example
don't take these sentences
literally so next
we have migration
when you talk about migration,
you need to understand one thing
AWS has a global architecture
and there would be
a requirement for migration.
And what aw does is it lets you
have physical migration as well.
That means you can
physically move your data
to the data center.
Which you desire now,
why do we need to do that?
Say, for example,
I am sending an email.
Somebody I can do
that through internet,
but imagine if I have
to give somebody a movie.
So instead of sending it online.
I can actually go ahead
and give it to someone
if that person is means
reachable for me
and that way it would be
more better for me.
My data remains secure
and so many other things so same
is with data migration as well.
And when you talk about AWS,
it has something
called as snowball
which actually lets you move
this data physically now,
it's a storage service
and it actually helps you
in migration a lot security.
And compliance now
when you talk about security,
we have various services.
Like I have I am we have KMS now
when I say I am it is nothing
but your identification and
authentication management tool.
We have KMS which lets
you actually go ahead
and create your own public
and private keys
and that helps you keep
your system secure the quite
a few other services as well,
but I would be mentioning one
or two services from each domain
because as we move further
in future sessions,
we would be discussing
each of these services
in detail and that is
when I would be throwing a lot
more Done these topics for now.
I would be giving you
one or two examples and
because I want you
all to understand these
to some extent getting
into details of all
these things would be
too heavy for you people
because the quite a few domains
and quite a few services
that we need to cover and
as we move further definitely
we would be covering all
those services in detail.
Then we have storage now
when I talk about storage
again AWS has quite a few
services to offer to you.
We have something called
as your S3 now s38 works as
a bucket object kind of a thing.
Your storage place is called
as a bucket and your object
which you store in nothing,
but your files now
these objects have to be stored
in their food files
which act as the buckets
basically and then we
have something called
as your cloudfront
which is nothing but
your content delivery Network.
We have something
called as Glacier.
Now when you talk about Glacier
you can think of it as a place
where you can store archives
because it is
highly affordable next.
We have networking
when you talk about networking.
We have services like VPC.
Direct Connect Route 53,
which is a DNS a
when I say VPC it is
a virtual Network
which actually lets you move
or launcher resources.
That is your AWS resources.
Basically when you talk
about Direct Connect,
you can think of it as
a least internet connection
which can be used
with an AWS next on this list.
We have something
called as messaging.
Yes AWS Usher's
secured messaging
and the quite a few applications
to take care of that as well.
We have something called as
Cloud trial we have opsworks
all these things there.
Help you in messaging
or communicating with
other parties basically
databases now storage
and databases are similar,
but you have to understand
one difference when you talk
about your storage
that is where you store
your executable files.
So that is the difference
between the two and
when you talk about databases,
we have something
called as your Aurora,
which is something
which is very sql-like
and it lets you perform
various SQL options
at a very faster rate
and what Amazon claims has
it is five times faster
than What aeschylus?
So yes, when you talk
about Aurora again a great
service to have we also have
something called as Dynamo DB
which is a non relational dbms.
When you talk about
non relational dbms,
I won't be discussing that
but this helps you in dealing
with various unstructured
data sources as well.
Next on this list.
We have the last domain
that is the management tools.
Now when you talk
about management tools,
we have something
called as cloudwatch,
which is a monitoring tool
and it lets you set alarms
and all those Those
things hopefully today
when we are done with
the demo part you'd be having
at least one part
of your cloudwatch code
because we would be creating
alarms using Cloud was today.
So stay tuned for that as well.
So this is about AWS
and it's Basics as
in the points,
which we just discussed
that as what it is its use has
its advantages its domain
its Global architecture.
So you guys what I've done
is I've gone ahead
and I've switched
into my AWS account.
The first thing you
need to understand is
what AWS does is it offers
you a free tier now
while I was talking
about these things I
just rush through it
because I know
that I was going to give
you a demo on these things.
So and I wanted to discuss
this thing in detail.
Now when you talk about AWS,
if you are a beginner,
this is where you start now,
what aw does is it provides you
with its free tier
which is accessible
to you for Twelve months
and the quite a few Services
which we just discussed
which are available
to you for free.
And when I say free
the certain limitations on it
as in these many hours is
what you can use it for
and this is the amount of memory
or storage you can use in total
and all those things
and its capacity
and everything based on that you
have different instances,
which you can create
an all those things.
Now.
What aw is does is it gives
you these services for free?
And as long as you
stay in the limits
that AWS has set you
won't be charged anything.
And trust me when it is
for learning purposes
that is more than enough and
let's quickly go ahead and take
a look at these Services first
and then there are
few other points,
which I would like
to discuss as well.
But firstly the free
tier services and say this is
what it has to offer
to you 12 months
of free and always free products
when you talk about
easy to which is one
of its most popular compute
Services 750 ours
and that is per month.
Next you have Amazon quick site,
which gives you 1 GB
of spice capacity.
Now I won't get into the details
of these things as an what
spice capacity is
and all those things
when you have time,
I would suggest
that you go ahead
and explore these things as in
what do these things do today?
We are going to focus more
on the easy to part.
So for now,
let's quickly take
a look at these one
by one first Amazon RDS,
which is again,
which gives you send
50 hours of your T,
2 micro instance Amazon S3,
which is a storage
which again gives you 5 GB
of standard storage
and it w is Lambda
1 million free request.
So there's some
of the videos here actually
which would introduce
you to these things
that would help you get started
with how to creating an account
and all those things and this
is the other important point
which I would like to mention.
When you do create
an AWS account.
The first thing you
need to consider
as they would be asking you
for your credit card details.
So how does the login process
work firstly you go there
you doing your email ID
and your basic details as in
why do you want to use it
and all those things next?
What it would do is just
to verify your account.
And it would ask you
for your credit card details,
even the debit
card details work.
I've actually tried those
so you can go ahead
and give you a credit card
or debit card details.
And when you do that
what it does is it subtracts
a very small amount
from your account.
I did this in India,
and I know that I
was charged to rupees
which is fairly less
and that was again
refunded back to me
in two to three working days.
The only reason they cut
those two rupees was just
for the verification purpose
that my account is up
and running and I am
a legitimate user.
Now as long as you
stay in the limits,
you won't be charged anything.
But if you do
cross those limits,
you'll be charged.
Are you might be
worried as an what?
If I do cross the limit
would I be charged?
Yes, you would be
but the fact is you
actually won't go beyond it.
And even if you do
you'll be notified seeing
that you are going
about the limit
or about the limit.
Even when your free
subscription ends.
You are notified saying
that do you want to enter
your billing details?
And do you want to start billing
and if you say yes only
then would be charged
for the subsequent.
Months and that is
a very stringent process.
You don't have
to worry about it.
That is you won't be losing out
on any money as long as you
follow these rules.
So if you do not have
an account my suggestion
would be you go ahead.
You would log into AWS and
create your free tier account
which has a very easy
and two to three step process.
So guys, I would start
this session by talking about
what is an instance
would understand.
What is AWS ec2 service
which is core for us.
Standing instances in AWS.
Then we'll talk
about different types of
ec2 instances would understand
how instance pricing models work
and we'll take a look
at a use case
which would be followed
by a demo that walks you
through all the stuff
that we have talked about.
So it is a fairly good content
and a lot of stuff
to study today.
So as let us just
quickly move further
and take a look
at these things one by one.
So first and foremost guys,
we would be talking
about an instance.
So when you talk
about an instance,
we have this definition here.
Let's try and understand
what does this definition
has to say first
and then probably I would throw
in some light on that.
So as far as this definition
goes it says and instance is
nothing but a virtual server
for running applications
on Amazon ec2.
It can also be understood
like a tiny part
of a larger computer
a tiny part which has
its own Hardware network
connection operating system.
Cetera, but it is
actually virtual in nature.
So there are a lot
of words here and a lot
of stuff has been said,
let me try and simplify
this particular definition
for you people.
So guys when I say
a virtual server running
on your application not on
your application virtual server
that basically hosts
our application is
what I should say.
So what do I mean by this?
What do I mean by a virtual
instance a virtual presence
of a particular device?
Well guys when you talk about
software development elopement
application development.
What you do is you are supposed
to build an applications and run
those on servers right?
But at times there are a lot
of constraints like the space
that you use the resources
that you want to use
say for example,
certain applications run
on Windows certain run
on Mac OS and certain run
on your Ubuntu OS right?
So in that case,
I cannot always go ahead
and have different systems
and different operating systems
on them and then
run my applications
on top of that right
because it is time consuming.
Stu and also consumes
a lot of money
that you invest into it.
So what is
the solution for that?
What if I could have
a single device
and on top of which I could
create virtual compartments
in which I could store
my data differently
store my applications run
my applications differently.
Wouldn't that be nice?
Well, when you talk
about an instance,
that is what it exactly
does you can think of it
as a tiny part of a computer.
Well, that is what
it is time to symbolize.
I mean you have a system
on top of which.
You can run different
applications and how it works is
if you are running
an application a in part
1 and running an application B
in Part B of your server these
applications have a feeling
that they are running
individually on that system
and there is no other system
running on top of it.
So this is what
virtualization is.
It creates a virtual environment
for your application to run
and one such instance
of this virtual environment
is called as an instance.
So when you talk
about virtualization,
it is not something
that is very complicated.
As you can see
in the first image.
You can see a man surrounded
by various virtual images
something that you see
in an Iron Man movie.
When you talk
about virtualization,
it is very simple.
It can be a simple computer
which is shared by different
people and those people
are working quite
independently on that server.
That is what
virtualization is that is
what an instances in this image
the second image each All
of this individual would be
using a different instance.
So this is what an instance is
when you talk
about virtualization.
So guys, let us move further
and take a look
at some other pointers.
Now we understood
what an instances
what virtualization is
to some extent at least guys.
As far as the session goes.
I believe this
information is enough.
If you wish to know
more about virtualization,
you can visit our YouTube
channel and take a look
at VMware tutorial.
It talks about this particular
Topic in a More detail.
So let's let us move further
and try to understand easy
to now now easy to as an Amazon
web services compute service.
It stands for
elastic compute Cloud.
Now, what do I mean by this?
When you say
an elastic Cloud compute?
That means basically it is
a service with lets you
actually go ahead and Carry
Out computation practice
and when I say elastic it means
that it is fairly resizable
and fairly reusable.
Once we get into the demo part
probably you'd get
a better picture.
What do I mean by elasticity?
Because it is highly
flexible highly scalable.
It is very cost efficient
and it serves a lot of purposes.
Now.
These are some of the features
that I just mentioned right?
Let me throw in some more light
on these pointers as well.
What do I mean by scalable now
when you talk about
a cloud platform one
of its best features is it gives
you high amount of scalability?
That means your applications
can scale up.
Down depending upon the data
that you want to use
on top of it.
So if the traffic increases
more you need more performance.
So your application
should be able to scale
to those needs, right?
So that is what cloud computing
provides you with and
that is what ec2 also provides
you with when I say an instance.
Basically, what you're doing
is you're launching
a virtual machine.
It is called as instance
in terms of AWS.
So this virtual machine
should be scalable.
That means it should scale up
and scale down both
in terms of memory.
A storage and even in terms
of the computation
that it is providing.
So when you talk about easy
to it is highly scalable.
Once we get into the demo part
you would see this now
it being scalable
and it being cost-efficient
makes it highly flexible.
So that is the third Point.
Let us try and understand
the second Point as well.
What makes easy to cost
efficient when you talk
about cost optimization.
What easy to does is
it lets you scale up
and down I just mention
that right so instead
of buying Number of instances
or instead of buying a number
of services you
can actually go ahead
and scale this instance up and
down with minimal cost changes.
So you're saving money
because apart from that there
are burstable instances.
There are various pricing models
that ec2 boasts of using
which you can actually
save a lot of money
as we move further.
We'd be talking
about those models as well.
So meanwhile, just bear with me
so easy to well it is a service
which is a computation service
and it takes care of
Of following pointers.
I mean it is easily resizable.
It is cost efficient.
It is highly scalable
and all these features make
it highly flexible as well.
So guys, let us move
further and take a look
at some other pointers as well.
So what are the types
of instances now
when you talk about
easy to it is one
of the oldest AWS services.
So if you talk about
the type of instances
that are there in the market.
Well, there are
quite a few types
of instances that you can deal
with and these are some
of the popular ones
Once I move into the demo part,
I would maybe talk
about other instances
but to keep it simple
basically these instances
they have different families.
I mean, you have
the T Series you have
the M series The cseries.
Well, basically these series
consists of different
kind of instances
that serve different purposes
to simplify this process.
What AWS has done
is it has gone ahead
and categorized these instances
into following types.
The first one is
your general purpose instance.
Now it is basically
suited for applications
that require a balance
of performance and cost
that means places where you
require quick responses,
but it is still cost-effective.
I mean say for example
the example shown here
email response systems.
Now you require a quick response
and there will be n
number of responses
or n number of emails
that would pop in
but you do not want
to pay a lot of money
for this kind of service.
So in this case you need
cost optimization as well
and you need Quick
response as well.
So this is where your general
purpose instances come
into picture next on this list.
You have yard compute instance.
Now what a compute
instances these are
for applications that require
a lot of processing.
Now when you say
computation they have
better computation power.
That means if there is
a lot of data that need
quicker computation power you
can use these kind of instances.
What is an example.
You have your analyzing
streaming data now
if you know,
what stream Data
is it is the data
that continuously flows
in and flows out.
That means you
are streaming the data say
for example this session it
is being streamed, right?
I mean the information
or whatever is happening
here it is going live.
So in order to process
this kind of data,
you need systems that give
you good computation power
which are very active
and very good in nature.
So when you talk
about compute instances,
they provide you with these kind
of services and that is why
if you are dealing
with streaming data
if you wish to analyze
this kind of data,
Definitely go for
compute instances.
So next on this list.
We have memory instances.
Now, what are
these instances for?
Now?
These are the instances that
are required for applications
that require more memory
or in better terms more RAM,
right random access memory.
So these are for applications
that require good
computation power again,
like the previous one,
but when you talk about Ram,
it is something that resides
in your local system, right?
So you need instances.
Which have good memory capacity
and what kind
of application it serves?
Well, you can
think of applications
that need multitasking multi
processing say for example,
I need a single system
that does fetching data for
me as well process it for me
as well dashboard
it for me as well
and then gives it to
the End customer as well.
So these kind of applications
require memory instances
moving further guys.
We have the storage instances
as the name suggests.
These applications are
or these instances are
for applications that require.
You to store huge
amounts of data.
Say for example,
you have large size applications
like your big data applications
where the amount
of data is used number.
So you would be requiring more
storage more storage flexibility
in that case.
You can opt for instances
that are specifically
optimized for storage
kind of requirements.
And then you have
your GPU instances.
If you know what GPU is
you would understand
what it serves that means
if you are interested
in graphical kind of work
where you have basically
A heavy Graphics rendering
in that case you can opt
for GPU kind of instances
which basically help you
sir purposes like 3D modeling
and stuff like that.
So guys, this was about
the different kind of instances.
Now, let us try and understand
what are the different
instance pricing models
that are out there.
So guys when you talk
about pricing ec2
or a SS in general,
it ensures that you
can save a lot of money,
but normally what people do
is they are under the And that
if we just go ahead
and take in Cloud probably
you would go ahead
and save a lot of money.
Yes Cloud does support
applications in such a way
that you would spend very
less amount but it involves
a lot of planning guys.
So each time you use
a particular service.
It is very
important to understand.
How does that
particular service work?
And if you actually plan
in the services in that manner
you would actually end
up saving a lot of money.
So let us try and understand
how the pricing models work
when you talk about it.
See two in particular.
So Guys.
These are some
of the pricing models
that easy to has
to offer to you.
You have your on demand
dedicated on the spot
and reserved instances.
Now, let me try and simplify
what these instances are.
And what do I mean by these now
when you say an on-demand
instance as the name suggests,
it is an instance
that you demand and you get it.
Now these instances
are made available to you
for a limited time frame
say for example,
I need a particular instance
for an hourly basis.
So I would be wanting
to use that instance
for only that Eurasian.
So to use that instance
for that particular duration.
What I do is I actually go ahead
and demand this instance.
So a tub -
would give me that instance
but it would work for
an are only
so my prices for that instance
would be fixed on that manner.
I mean the fact
that I would be using
it for one instance
or for an one are basically
so I would be charged only
for that one hour.
And once that are
is complete that instance
it gets Terminated on its own
it's similar to renting a flat
for one month suppose
if I move to a new city
and I'm looking
something temporary say,
for example, I'm
looking for a hostel
or a paying guest kind
of a living system.
Right?
So in that case,
what I would do is I would
upfront go and tell the owner
that I would be staying
here for a month.
You can charge me
for a month only
if it is 1000 more
than normal charge.
It is fine.
But once the month is over,
I would like to leave
right so that kind of service
or that kind of instance.
Demand is called
as on-demand instances
basically dedicated now Guys.
These instances are
kind of given to
a particular organization
so that their security
is defined better
than other say for example,
if I need to protect my data,
I need my data to be privatized
Now understand this thing AWS
or the other Cloud platforms
are highly secure.
Your data is secure no matter
whether they are
on dedicated instance or not.
But what happens is you
normally share your Space
with someone else
data remains private
but there are companies
that deal with highly
confidential data.
And in that case they want
that extra Assurance as an okay.
I am using a space
which is not shared by anyone.
So in that case you
have dedicated instances,
which basically serve your needs
like high security
and basically an isolation
from the other vendors as well.
So that is what dedicated
instances do they are costlier.
But yeah, they give you
that isolation on spot.
Now guys, when I say
A non spot instance,
it is like bidding
say for example,
I am buying a particular share.
So I have
a particular budget right
so I might have
a budget of $300.
So what I do is I go
ahead and buy the chair
and I sat in a cap
as an okay to the max
I can bid for $300
for the share.
So if the price goes
above 300 dollars,
I'm not taking that share right?
So if there is a particular
instance you can bid
for that instance as an okay.
This is the maximum price
that I pay for this.
Ernst so if that instance
is available at that price
it is given to you
and if after a particular
duration the price
of this instance can change
so it is available to you
for a limited period of time.
So if you are dealing
with data that is volatile
and you want to work
on the data in real-time,
so you cannot for this instance
because after a while the price
of this instance might change
and this instance
might be terminated
and you might not be able
to use it for a longer while
but the thing it does
is it is available
to you at a cheaper price?
And at the pricing bit
that you put on it,
so that is why it
is more affordable.
But again, it is good
for volatile data only finally
you have the reserved instance.
It is like renting
an apartment on a lease
for a longer period right?
I mean suppose
if I am getting a flat
on an agreement will basis
where I sign
an agreement for a year.
That means I
am reserving this flat
for one complete year, right?
So nobody else
in comments say that okay,
you have to vacate this.
A flat right so
that is one benefit.
And the other thing is
you have a fixed set of rent.
So if you're taking something
for a longer duration,
there is a chance
that you might end up paying
lesser money for that as well.
Now what happens here is
when you talk about it from the
instance perspective suppose,
you know that you
would be needing
this much configuration
for this duration.
You can rent that particular
instance for that duration,
and probably you end up
saving a lot of money now
when you talk about AWS
it gives you Latif
where you can actually go ahead
and upscale downscale
your instances to your needs.
You can kinda terminate stuff
and move to the next up.
But if you are certain
about certain things as an okay,
I have to use this no matter
what happens for a longer
duration in that case.
You can offer reserved kind
of instances and those are
more affordable to you.
So Guys, these were
different types of instances
based on the pricing
that is there.
Now.
We have talked about General
cluster ization of instances,
like the general-purpose
the GPU that was based
on They're functioning,
right then we learned about
the pricing models as well.
Now.
There is one more type
that we need to understand
or one more classification
that we need to understand.
Let us try and take a look
at those as well.
So we are classifying
instances based on
that General functioning.
Now, what do I mean by this?
Well, these are the types.
Let us take a look
at those one by one first.
So when I say
burstable instance,
we've talked about general
purpose instances, right?
So what happens is there is
a category of General.
But was instances with start
with a base utilization power
available to you.
That means if you want
to utilize your CPU
for a certain amount burstable
instances are good here.
Let me throw in some more light
as in what am I talking about?
Exactly suppose.
I need a CPU utilization
of 20% And I know that
so I can go
for burstable instances.
What they do is they start
with the functioning
of 20% but in case
if I'm dealing with data
that It is not constant
that might change
with time say for example,
if my website
experiences more traffic,
so I might need
more performance.
Right?
So in that case
what burstable instances
do is they burst
out of their current performance
200% CPU utilization
so that you can get
more performance.
Now what happens here is you
are charged a particular amount
for these instances
and you have certain credits
for which you can use
the burst people performance and
if you do not use the bustable.
Performance those credits
can be used later as well.
So you are getting
optimize performance as well.
And you are saving
some money as well in case
if there is an urgent traffic
that you experience you
have something called
as EBS optimized now
when you talk about
EBS optimized now,
these are the applications
where basically you
are processing data
at a higher speed.
Say for example,
there is some application
where the data is
flowing in continuously.
So I need quick response, right?
So EBS backed up
or EBS optimized instances.
What they do is they give you
high input output processing
and that is why these are
good instances to art
for these situations
cluster networking.
Basically, they form clusters
of instances now
a particular cluster
what it does is it serves
one kind of purpose say
for example in my application.
What I want is I have
different sections
and in different sections
my first section requires
To be processing data
at a faster rate.
The other one.
I wanted to be storage
optimized so I can
Define different clusters
of instances that serve
different purposes here.
And then I have
the dedicated one.
We've already talked
about dedicated one.
It is more related
to the data security part.
So Guys, these were the
different types of instances.
I know I've talked
about a lot of stuff
once we get into the demo part
probably this would ease up
a little more for you people.
I believe you people are
with me and you are
following this session.
So guys now let us move further
and take a look at the use case
so that we can just move further
and take a look at the demo part
as well for this use case.
I've considered
a derecho itself.
Let us try and understand
what could be
the possible problems
that can be solved by
using these instances.
Now imagine that
if it is Erica used AWS
as their Cloud partner
and they used the ec2 service.
So what kind of problems could
be solved by these instances
that we just talked
about suppose we have
the first problem
where you have To analyze
the data of the customer.
So what kind of application
would you use?
Can you guess that for me?
I won't be looking
at your answers.
Let me just quickly go ahead
and give you other examples as
well so that we can discuss
these one by one suppose.
You also have an auto
responsible system now compare
these two and let
me know which one
would you believe
would be served better
by these instances that
we've just talked about.
So when you talk
about the performance here guys
when you talk about analysis
of data for the customers data,
it is never Went right at times
the data is used
at times it is less.
So in this case,
I would need burstable performs.
So my general purpose
burstable performance instances
would serve me better right
auto response email system.
I need quick response,
but I do not want
to invest a lot
of money EBS optimized instances
with iops would help me better
search engine and browsing.
I believe it is fairly clear.
I'm talking about browsing
and search engine
to different things I want
to do I would be opting
for Stud Network instances,
right and confidential data.
Well, I would be opting for
the dedicated instances here.
So guys, this was
a very simple use case.
So let us move into
the demo part and try
and understand ec2 a little
more shall we so guys what
I've done is I've gone ahead
and I've signed
into my AWS Management console.
Please forgive me guys.
I have a lot of gold today
and that is why my voice is
little Jiggly and echoing.
So I hope you people
are not offended by
that moving further.
The guys this is
the AWS Management console.
You can sign in to AWS
free tier account
and probably Avail
these Services you can practice
a lot of stuff by signing
into your free tier account.
How do you do that?
Just go ahead and look
for a SS free tier and sign in
with your credit card
or debit card.
You won't be charged
you have these services
for free for one complete year
and you can practice most
of the services that are there.
There is some free tier limit
on these services.
So check the upper cap as
in what those limits are so
that you Get charged.
So guys this is
how the console looks like.
We are going to go ahead
and learn about easy to hear.
That is the instant
service in AWS.
So let's search for ec2.
And you would be redirected
to this page guys.
Now when you talk about ec2,
there are a lot of things
that you can do.
You have Amazon Marketplace
where you have am eyes,
I will tell you.
What am I is our do not worry
you can just go ahead
and launch our instances.
You can attach volume to it.
You can detach volume storage
from these instances.
And when I say am I is those are
Amazon machine image has
that means once you
create an instance,
you can create an image
of that instance as well.
That means a template
of that instance
as Suppose you have
certain applications running
on top of that instance
certain specific settings
that you've done
for those instance
and you do not want to do
those settings again.
And again, you can create images
of that instances as well.
So let us see what all we can do
with these instances.
So let us first
launch an instance.
So guys, once you click
on that launch instance button,
you would be given a number
of options to choose
from you can launch
Linux instances Ubuntu
instances Windows instances.
And you can choose the EBS
backed up non-abs backed up.
So there are a lot of choices
when you actually go ahead
and launch these instances.
You can see this Ubuntu
Red Hat Microsoft Windows
and there are specific
instances specialized
in deep learning some
of our service specification.
You can see that there are
quite a few instances,
but ensure that
if you are practicing
choose the free tier
eligible one for now,
I'm going to go ahead and launch
a simple Windows instance.
Let's not get
into the Ubuntu one
because Request a petition
to sign for that.
So let us not do that.
So guys once you click
on launch an instance,
you can see that you
are redirected to this page.
Now if you take a look
at the information here,
it talks a lot.
Now.
This instance is
general purpose.
We've discussed the
other families, right?
This is one.
This one is T 2 micro there are
t 2 T 3 micro and medium
and bigger instances as well.
The size is very guys the Tito
micro one is free tier eligible.
You have t to Nano
you have small right?
So you have me do
Another large instances as well.
So when you say a microphone,
it has 1 V CPU and one gigabyte
of memory instant storage.
It is EBS backed up
and what kind
of network performance it
gives you low to moderate.
So I would say
configure further.
These are some configuration
details what network it
is following what subnet ID.
It is falling
that means it falls
under the cloud Network guys.
That means your Cloud
would have a network
and under that Network
lies are instance
so that it's accessible.
SS policies security policies
can be managed.
So let it be basic for now.
Let us move further.
Storage now guys,
this is the storage it
is your route storage
and 30 GB of space.
You can change it
if you want say a hundred
but let us take 2 34 now
and guys you can see
these are the types.
You have a general purpose.
You have your
provisioned magnetic now,
there is one more type
of instance guys.
That is HDD kind of an instance,
but guys when you talk
about root storage,
you cannot attach HDD to it,
right because route
storage is something
that is constantly Constant,
if you wish to have HDD kind
of storage it has
to be attached secondary.
So if I add new volume here,
you can see and if I
search for this now,
it gives me an option
of cold HDD, right?
So that is what guys I mean
in order to have this kind
of HD kind of a volume you need
to use secondary storage for it.
So let us cancel this for now
and just go ahead and say next
you can add in tags guys
for the Simplicity
of namesake say for example
sample today and let's just say
next Security Group
guys Security Group.
What do I mean by this?
Well, basically you have set
of policies as in
who gets to access.
What kind of traffic do you
want to your instance?
What kind of traffic do you want
to flow out of your instance
so you can create
a security group and you
can use customized as well
when you create one
this type is RDP.
That means it can allow
traffic from a desktop
or a remote desktop app
and through which I can log.
To my system I can add
other rules as well.
I can add PCP HTTP
kind of rules.
And these are the port ranges
you can specify those for now.
I'm allowing traffic
from everywhere through our DP
and I can say review
and launch improve
your security it says
but this is a basic one guys,
you can add in more rules
as I've already mentioned.
So let's not do that.
Let's say launch generate a key
pair now a key pair is something
that lets you log
into your instance.
It is a double security
for your Instance you
do not want your instance
to be left insecure.
Right?
So in that case,
you need to generate a key pair.
You can use an existing one
or you can create
a new one as well.
So let's just say
that I want to create
a new key pair.
So I say create and let us say
Vishal 3 4 1 2 1 and let's
just say download.
So guys once you
download this instance,
what you do is
and protects cut it from here
and I'm going to go ahead
and paste this instance
to the desktop guys and
let's just say paste.
Here it is.
So the reason I'm doing this is
because basically we
would be needing this thing is
if you lose this key there
is no other way to explain.
Is your instant so
make sure you keep it safe
and I say lunch.
So guys now this process
it takes a minute
or two to go ahead
and launch our instance.
So meanwhile you'd have
to bear with me.
So what happens is
once you do actually go ahead
and launch this instance.
It involves a couple of steps
like basically it does
some Security checks
some status checks and
while these statistics happen,
it takes a minute
or two and once the instances up
and ready we can actually go
ahead and take a look
at this instance.
So meanwhile guys what I'm going
to do is I'm going to go ahead
and take to the ec2 part Now
there are three instances
that are running guys.
Now, this is
somebody else's account.
So there are quite
a few other instances
that are running you can see
that there must be
some instance here
which basically is initializing.
So this is the one
that we are going to use.
This is the ID.
Let's not remember
that we know that this
is getting initialized.
So as these are the other
instances this one is start.
Let us take a look
at this instance as
well to understand
as an what happens.
So Guys, these are the options
that I have right?
You can actually go
ahead and get the password.
You can create a template
for your instance.
What you can also do
is you can start stop.
Now.
This instance
is already stopped.
So you do not have these options
that has stops.
He Burnett and reboot you
can start this instance
and probably you can go
ahead and do that.
Now when you stop an instance
if you want to actually
make a snapshot you
want to take snapshots
you want to create Amazon
machine image is out of it.
What you do is you
stop that instance
so that you prevent
any activity from happening.
In that instance
so that you can take
an exact snap of it.
So that is why you
stopped an instance
when you wish to do
these kind of operations.
Once you start it again,
you can make it function
normally at it was functioning.
If you are done
using an instance,
you can terminate it there
and there guys, so these are
the options instance setting.
Okay.
So as these are the options
you can add tags to it.
You can attach replace.
I am rules that is
access management policies guys.
So you have a user
access management.
Here you can attach
roles to it as well.
You can change the instance
type guys you can click on it
and you can go ahead
and do that.
You can change it
to higher versions as well.
Now, why do you need
to do this suppose?
I am experiencing
a particular traffic
and my instance
supports that need but
if I move further and future,
I need to cater more traffic.
What do I do in that case
in that case guys?
I can actually go
ahead and update it
to a larger version
unlike your other applications.
You are
on-premise infrastructure.
Where you have
to actually go ahead
and have new servers you data
on top of it here.
What you do is you just click
on this thing and it happens
in a couple of seconds.
You are instance gets optimized
or upscale to a better lever.
And that is why
it is highly scalable
because what you can also
do is you can change
termination protection of this
is for data security suppose.
If I am using
a particular instance,
and in that case,
I accidentally deleted
my data would be lost.
Right?
So what this Does is it changes
or turns my termination
protection on that means
if I have to
delete this instance?
I have to get into the instance.
I have to change the policy
and then delete it.
I mean I cannot delete
it unknowingly, right?
So that is why this service
helps now while talking
about these things guys are
instance is up and ready.
Let us just launch it.
I say connect.
And it says download
remote desktop file
the RDP path that I
talked about right
and I need to get in my password
as well guys to login.
How do I do that?
I click here.
I choose the file for that.
I'm gonna go to the desktop.
I'm going to scroll down.
There is a file
called as Vishal.
I open it and I decrypt it
and there you go guys.
My password is here.
I can just copy it.
So if this is copied
I can launch this.
Remote desktop file.
It would ask me
for the password.
I would say take this and okay.
Do you want to
login and securely?
Yes.
And guys a Windows instance
would be launched.
It is just like your
Windows operating system,
but it is running
on my existing system guys.
They can see
personalized settings.
It is setting up
personalized setting for me
and in half a minute
maybe in 10 seconds.
My Windows app would be
up and running.
So just like my Windows device.
I have one more Windows device
so I can do something
in this device
and something else in my normal
Windows device as well guys.
So this is what you are.
Instance does it basically
creates an instance
of word Sewell machine
for you to work on I Believe
by Navi one understood.
What a virtual machine is.
So guys we are done
with this part.
So let us just use it for now.
Let us see
if there is anything else
that we need to talk about now,
if I come back
here I've mentioned
that you can take
snapshots, right?
So these are am is what am I is
it is an image basically
so I can actually go ahead
and launch an Emi
for an instance
that I already have.
I can create an image of it.
There is a volume here.
So my instances are
EBS backed up right?
So there is a block storage
attached to it.
Can I add another storage to it?
Yes, I can remove
the previous storage and attach
a different storage to it.
Say for example,
this is the store is
that I have with me
if I click on it and I
will go into actions.
I can create
a A short out of it.
Once I create
a snapshot out of it.
I can attach it
to the existing instance.
So we just launched
an instance, right?
So if I want to
replace the volume
that is already attached to it.
What I do is I actually go ahead
and detach the volume
that is already attached.
So I would be stopping
my instance First
Once I stopped the instance.
I can come to the volume assume
that this volume is attached
to some instance.
So I need to detach it
from here and the snapshot
that I've already created.
Or if I have created one,
I can select
that and I can attach
that to the existing instance.
All I have to do is
I have to go ahead
and create an image here.
Once I create an image
it would ask me.
What can I do with it?
I would ask me
to actually go ahead
and given the region in which
the instance was created.
Now my instance
that I just used was created
in a particular region.
I'm working in
Ohio reason for now.
What do I mean by these regions?
Well, basically what Says
AWS has different data centers
in different regions
of the world.
So you can choose the reason
that is convenient to you that
suits your business needs right
so I can create instances
in those particular regions.
So if my instance was
in particular region,
I need to create
a snapshot in that region
and then attach that snapshot
or that volume to my instance.
So guys I Believe by now,
you've understood a lot
of things you've understood
what instances are
how to launch those
how to create those
and how to make those work.
So as far as this
is Ian goes guys.
I wanted to talk
about these pointers
one more important point
that I would like
to mention here is make sure
that you terminate
your instances so
that to avoid any charges
if there are any now this
being a free tier account.
I don't think there
would be a lot of charges
but still I would request
you to actually go ahead
and terminate the instances even
if they don't charge you a lot
because that is a good practice
because there are certain
services that might charge
you a lot more guys.
So I'm going to terminate
my instances the ones
that I have created today.
So let's just wait a minute
and in a minute or two guys,
these instances would
be terminated from end to end.
Today's session is going
to be all about AWS Lambda.
So without making
any further Ado,
let's move on to today's
agenda to understand
what all will be covered today.
So we'll start off today's
session by discussing
the main services in the AWS
compute domain after that.
We're going to see
why AWS Lambda is as
a separate service.
We're going to discuss
what aw is Lambda actually is
and then we'll move on
to the part where we'll see
how you can use a double s
Lambda using the AWS sdks
once we're done with that
I'll teach You guys
how you can integrate your SDK
with the Eclipse IDE?
And in the end
we'll be doing a demo.
So let me quickly show you guys
how we will be using AWS Lambda
in today's demonstration.
So guys, this is a website
that I created which is hosted
on the Local Host.
Now what this website
does is it applauds a file
onto the H3 file system now
once the file is uploaded.
It sends me a mail
regarding that now
that meal is generated by a SS.
I'm not now let me
quickly show you
how that mail actually looks
like so let me upload
a file over here.
So let file be this I click on
open and before uploading image.
I will show you my inbox.
So as of now,
I don't have any
As you can see, right.
So what I'll do is I'll click
on upload image now.
It is S3 upload complete.
Now.
What is this website does
is it will upload my file?
It will rename the file
according to the system time
so that there is no conflict
in the name of the object.
Right?
So whatever file
that I've uploaded right
now will be uploaded
on in this bucket.
So if I refresh this you can see
that there's a file
over here, right?
So this file has now
been renamed, right?
Right, and I also have
an email over here,
which says awacs test, right?
So if I click on this email,
I can see that I have got a mail
from this address saying
that an object has been uploaded
the name of the object.
Is this the size of the object?
Is this the bucket name?
Is this and it will slash
modified on 12/31 UTC right?
So let me quickly compare
whether this file
name is the same.
So it's seven four eight
and it's a sin
for it here as well.
Awesome.
Now, the next cool thing
that you can do over here
is you can move this file
to some other folder.
So all you have to do is
you will reply to this mail
by saying move you click
on send now when I send move
to this email address
that I have configured
in my code what it does
is it will basically move
this file from this bucket
to some other bucket.
So let me quickly.
Press it and see
whether my file has been moved.
So as you can see
my bucket is now empty now.
Let me go back.
So basically my file was
there in Erica demo now,
it will be there
in quarantine demo bucket.
So as you can see seven four
eight file has now been moved
to the quarantine demo by simply
writing a male over here.
It says move
so we'll be creating
this demo today.
Let's move on to the first topic
of today's discussion.
That is the AWS compute domain.
So the main services are
under this domain are easy
to elastic Beanstalk
and AWS Lambda.
Now among these three the most
important service is easy to so
easy to is basically just
like a raw server.
It is like a personal computer
that you're working
on remotely, right?
So it can install any kind
of improv operating system
of your choice,
which is supported by
the AWS infrastructure
and then you can use it
in any manner as you want.
You can configure it to become.
A web server.
You can configure it
to become a worker
to your environment anything.
Uh, next service
is elastic Beanstalk,
which is an automated
version of ec2.
So with the elastic Beanstalk,
you don't get the access
to the operating system,
but you still have a control
over the configuration
of your system
so you can choose what kind
of instance you want
to launch, right?
So elastic Beanstalk is used
to deploy an application.
So basically you just upload
your code and your application
is deployed on the
AWS infrastructure, right.
So this is what elastic
Beanstalk is all about.
Then we have
the AWS Lambda service.
So the Lambda service is
again an automated version
of ec2 wherein you
don't get the access
to the operating system
with the errors Lambda.
You don't even have
the choice to choose what kind
of configuration you want
with your server, right?
So with either plus Lambda you
just have to upload your code
and it executes.
It's that simple
but then why do we have?
Have an AWS Lambda service
when we have elastic Beanstalk.
So let's understand that.
So either plus Lambda
like a told you guys.
It is an automated version
of easy to just
like elastic Beanstalk,
but then with AWS Lambda,
you can only execute
background tasks, right?
You cannot deploy
an application.
So either plus Lambda is not
used to deploy an application.
It is used to execute
background tasks.
Other than that like I told
you guys you don't have
to choose the Integration
and a double s Lambda you
don't have to choose what kind
of servers you want on
depending on your workload.
Thus kind of configuration.
The server configuration
is assigned to you, right?
So this is why
we use AWS Lambda,
but then let's go on
to the definition part and see
what AWS Lambda actually is.
So according to its definition.
It's a survivalist
compute service
because you're not choosing
the server's right.
You're not choosing what kind
of Aggression you want
in your server?
It's a serverless
compute service you
just upload your code.
And the code is executed.
It's that simple right
and also like it's mentioned
in the definition
and I told you guys
again again it is used
to execute background tasks.
It is not used to deploy
an application guys.
This is the main
difference between
elastic Beanstalk news12.
So as an architect,
you should know
what the use case is
and with service
will suit it better.
So Moving on now,
you've understood what
AWS Lambda actually is
and why do we use it?
Right?
So let's move ahead to see
how you can use this service.
So you can use the service using
the software development kits
which are provided by AWS.
So before moving ahead
and understanding
how you can use the skills.
Let's understand what
these kids are all about.
So the software development kits
are basically apis
which are used by developers
to connect to the desired
service at the wound.
So it makes the life
of the developer easy
because he can now concentrate
on the logical part
of his application rather than
wasting time on understanding
how you can connect
his code to the service
which is there on AWS, right?
The other part is
that these sdks
are used with ID.
He's right.
So currently we
have only two IDs
which are supported
that is eclipse
and visual studio.
So today in this session.
I'm going to teach you guys.
Is how you can connect your SD
keys with the Eclipse IDE?
So let's do that.
So before that we
are going to configure
or we going to code ra SS
and of function in Java,
right?
And that is the reason
we're using Eclipse.
Now.
First of all,
you have to install
eclipse on a system.
Once you do that.
This is the eclipse green guys.
This is how your Eclipse
dashboard will look like.
So for installing ews SDK
on your Eclipse,
you have to click on Help
and then you'll go
to install new software.
Once you have reached here.
You will enter the website name.
That is aws.amazon.com
/ Eclipse.
Once you have entered
that just hit enter
and it will list you
guys all the SDK is
which are available
all the tools
which are available
select all the tools
and click on finish
and then it will take
some time to download the SDK,
but then it will integrate
everything into your
And then you'll have a button
like this over here.
Right?
So with this button you
can actually deploy a new server
which is configured
according to AWS.
So guys, this is
how you install sdks with IDE.
Alright guys, so it's time for
the demo now enough of theory.
So what we'll be doing is
where our aim is
to create an application
which will be uploading our
files onto the S3 file system.
And what a Lambda function
here Willy We'll be doing is so
like I told you guys
Lambda function basically
executes your background task,
right so that we
don't want to burden server
on which the website
is hosted in this task.
We want some other server
to execute this task.
What is this task?
We basically want
to get an email with all
the details of the file,
which has just been uploaded
on the S3 file system.
So that email will be sent
by the Lambda server now
once we get that email
if you reply to that email
that the file has to be moved
Lambda will Pick up that email
it will read that email
and it will perform
the necessary operation.
So if we specify move,
what will basically do is it
will pick that file move it
to some other bucket
and store it over there.
So this is the project
that will be doing right
now sounds simple right?
But let me show
you the architecture.
Let me explain you what
the architecture tells you.
So basically this
is our website.
So what our website
we'll be doing is it
will be uploading a file
onto the S3 file system.
At the same time it will also
be making an entry into the sqs,
which is nothing
but a simple queue service
which use your data, right?
So as soon as your file
is uploaded on to S
3 S 3 is bucket is configured
in a way to invoke
the Lambda function.
Now as soon as the Lambda
function is invoked now
Lambda functions are stateless.
They don't know anything about
what file you have uploaded
or what you have done.
You have to feed
them information.
And that is the reason we
have updated the entry in s us
or the file which Recently
been uploaded right?
So what ew is Lambda will do
is it will read this queue
and we'll get the file name
and we'll actually
retrieve all the properties
from that file name from S3.
Now once it has retrieved
all the file names all
the properties of that file.
It'll actually mailed me
with using the SES service in
AWS the details of that file now
once I receive the details
of that file,
I have an option to reply
to that email, right?
Now how will I reply
to that email is like this.
So I will open the email client
on my computer and I
will reply to that email
that email will actually
go to that address
which is actually pointed
to my DNS server and
that DNS server will actually
redirect that email to SES.
Now SES on receiving
that email has been configured
to invoke the Lambda function
that Lambda function
will be invoked again.
The file will be read
from the SQL.
That file will be moved
to a new bucket
and in the end that message
will be deleted by
ask U s-- now my S
us has been configured like this
that in case I don't reply
to that email within
two or three minutes
that message will automatically
be deleted from the queue.
And in that case
if you try to move that file,
you will not be allowed to do so
because that file is
no longer available
in the cube, hence.
You cannot move it, right?
So this is what our project.
It is going to be all about now.
I have already showed you
how the project works.
So let me quickly delete
the project and again show you
how it can be configured
from scratch right?
So give me a moment.
All right.
So everything is set.
Now the first thing
that I'll be doing is I'll
be configuring my S3 to interact
with my Lambda function, right?
So what I have not done is I'm
not deleted the Lambda function
because there's no point.
You just have to click next
and your function
will be created.
What matters is the code,
so I have uploaded
the code in your LMS
with the proper documentation.
If you have any doubts,
you can actually email me
regarding the doubt
and I'll clear it.
You so as an architect,
your job will be to act
take this architecture.
Not the coding.
The coding part has to be done
by the AWS developer,
but it is a good
to know knowledge, right?
So that is the reason I
have uploaded the code
for the website
and AWS Lambda to your LMS.
Okay.
So like I said,
I have to configure my is 3
so that it can interact
with AWS number.
Now.
My website's code is like this
that it will upload
the file to a bucket
in S3 called either a car.
A demo, right?
So what we'll be doing
is we will be going
to the Elder a car demo bucket.
Which is here, I click
on the dareka demo bucket.
I click on properties.
I'll click on events and let
me delete this event right now.
Right?
So I will be adding
a notification now now let
me call this notification
as AWS - Lambda right.
Now.
What I want it to do is whenever
the event is a put event
that is and upload event.
I want it.
Send a notification to
my Lambda function.
So I have to select
the Lambda function.
So my function
should be this one
and I will click on Save.
Let me check
if everything has been filled.
Yes.
It has let's click on save.
All right, so I have
one active notifications
now now you might get
an error over here saying
that you don't have
sufficient permissions.
So if you get that error
on the right hand side,
you'll have a button
called add permissions.
Mission just click
on that button
and everything will open up
an automatically basically
those permissions are
for your Lambda function.
Your Lambda function
may not have permissions
to get notifications from S3.
But once you click
on that button,
you will get the proper
notifications automatic.
Right?
So this is how you
will configure your S3 bucket.
Now, let's go back
to our slide to see
what other things we have to do.
So we have configured RS3
to invoke a Lambda function
once a file is Loaded to S3.
Now.
A Lambda has already
been configured to interact
with ses through the code,
which is so through the code
should be calling
the SES service
and we'll be living
in a meal now the next function
or the next thing is
to configure SES or
before that lets configure
our sqs, right?
So our sqs is basically
a simple queue service.
So we have to create a queue
in a COS in which our website.
It will be uploading of files,
right so let's do that.
Let's just go back
to our dashboard.
So this is our dashboard guys
and we'll go to the sqs service.
Well, click on create
new Q fifo queue and
that Q has to be named
as hello - Lambda.
And since it's a fifo queue,
you have to give
the extension as dot fifo.
All is done.
Let's click on
quick create Cube.
Okay, so my Q has now
been created and now I
have to configure this Q
so that whenever
the message comes in,
it gets automatically
deleted after 2 minutes.
All right, so let
us configure it
so Will click on configure q
and we set this to two minutes.
All right.
All is done.
Let's click on Save changes.
All right.
So my Q has
also been configured.
Let me go back to my slide.
Alright, so my sqs
has been configured now,
so let me configure
my SES now now this
might be a little tricky.
So hang on with me.
We'll go back to the dashboard.
We'll go to the ACs service.
Now first of all
in the SES service,
you actually have to add
the email addresses.
Now.
How will you
add email addresses?
You will actually have
to verify a new email address.
Now you have to verify
the recipient as well.
So since I want to receive
the email from the SES service,
I'll have to type
in my email address,
which is he meant at the rate
of the rate during car.com.
And we have to verify
this email address.
Now I'll receive
a verification email on him
and the other a cannot go.
So, let me quickly go
back click on inbox now.
I have got
a verification request right?
So I'll click
on this verification link.
Okay.
So my email address
has now been configured
has now been verified.
So it says
congratulations awesome.
So let me go back to my ACS
says pending verification.
Let me quickly refresh it.
All right.
So it says verified now
now let's go back to our slide.
All right, so guys we have
configured the recipient of SES.
But what about the sender right?
So we have to configure
the sender as well because
and why do we have
to configure the sender?
And the sender has
to be a domain name
that you own right?
Can you have to own
that domain name
so that you can send emails
via that domain name now
what I mean by that is
you may say that okay,
why not use the recipient
address only why not use
payment Authority diwaker dotco
for sending the email
but our application
also receives email
if you would have noticed right
so for receiving the emails
through a CSU have to actually
own the domain name now
since I'm an employee,
I don't own any record or go,
right?
So what I've done is I
have actually created
a domain name I can get
a free domain name.
This website it is
my dot dot dot TK.
You can go in this website
and create a domain
for yourself for free.
So basically you will be getting
this domain name free
for three months.
All right.
I am almost from
the expiry date.
So I might have to renew it.
Okay, but since this is a demo,
let me quickly show you.
All right, so I have actually
created this domain name
and I can use this domain.
Name to send or receive emails.
Now what I'll have to do
or how do I configure this
in my CSS is like this.
So you will go to your SES.
You see this tab?
It says email receiving right?
So we will click on rule sets
and you'll have to create
a new rule said before that.
You have to actually
verify a domain you
basically have to verify
that the domain is actually
owned by you now how you will do
that will click
on verify a new domain
and you You will give
your domain name here,
which is any record or TK.
Click on verify this domain
and you will get
these two records over here.
Now.
Where will you enter
these two records is actually
in the DNS server.
So the domain name
Eddie record or TK has to point
to a DNS server, right?
And in that DNS server,
you will be putting
in these two records.
Now.
How will you point any record
or TK to a DNS server?
So DNS server is
basically Route 53 8
so we'll be configuring Route
53 with any record or TJ.
Let me show you quickly
how you can do that.
Let me open my Route 53 service.
So this is my Route 53 service.
I don't have any host
of drones as of now.
So let's click on get started
now click on create hosted zone.
So my domain name is
anyway card or DK right?
Click on create.
All right, so I have created
a hosted zone now
in my Route 53.
Now what I have to do is
I'll have to connect this domain
to my Route 53 now.
How will you do that?
You will click on manage domain.
And you will click
on management tools
and you'll click
on name servers, right?
So these name servers
have to be updated
with the name servers
provided you over here, right?
So let me quickly show you
so you will copy
this paste it here.
Remember guys don't include
the dot in the end.
Otherwise, it will
give you an error.
So without the dot
copy the name server.
Right, so I first
save to and see
if it's working click
on change name server.
All right says
changes saved successfully.
All right, so it's saving
the server's now.
So, let me copy
the rest to as well.
All right.
So I've copied my name servers
I click on change name servers
and fingers crossed.
Okay, so it says
changes saved successfully.
All right.
So my domain name is now
pointing to Route 53 awesome.
So now in Route 53,
I have to include
these records now.
How will I do that?
Let me quickly show you
so you go to Route
53 and you will click
on create record set now you
don't have Prototype anything
here just in the type
click on MX and in the value.
So as you can see,
there's a value for m x
over here just copy this value
and paste it here, right?
This is it guys nothing has
to be done here.
Click on create.
Awesome, so I have an MX record
now now let's we have
to create one more record, sir.
And that name has to be
like this right?
So I'll copy
this part paste it here
as you can see now.
The name is underscore
Amazon SES dot Ed u--
a card or TK.
And as you can see the name
over here is to seem right.
So this name has to be the same
and the type of value is txt.
Select EXT from here and then
you have to enter the value.
So the value is this enter
this value over here
and click on create.
Awesome.
So my Route 53 is now
configured to actually
Sue actor receive the emails
from the Ed Eureka dot t--
k domain cool.
So we'll go back
to our SES now close it it says
pending verification refresh it.
Alright, so as you can see
my domain name is now verified.
All right, so let's just go
to the rule sets now.
So email receiving we have
to configure so I click
on view active ruleset.
There is no rule sets
while create a rule.
Now I have to
specify a recipient.
So let me specify
hello at the rate
and Eureka door TK, right?
I'll click on ADD recipient.
So my verification
status is verified
because my domain name
is verified now will click
on next step.
Now.
What action do you
want it to do right.
So if you receive email
on this email-id,
what do you want to do?
So what we want to do is we want
to invoke a Lambda function.
Now what Lambda function
do you want to invoke?
I want to invoke my function
to and will click on next step.
So everything seems fine
will click on Next Step again.
So it is ask me the rule name.
Let me give the rule name
as Lambda - demo.
click on next step
and click on create rule.
Okay, so my rule set has now
been enabled awesome.
So I have configured
my SES as well.
So let me go back to my slide.
Alright, so I've
configured my CS I
have configured my Route 53.
I've configured my ews Lambda.
I have configured my sqs.
I have configured my S3.
And my website is also
configured right so we
created a rescue SQ.
So we may have to change the url
in our code to Let's
quickly do that.
We'll go back here.
Go to the dashboard.
click on SQL s Alright,
so this is our q
and this is a URL.
So basically I have named
the queue seems so
so if you do that sometimes
a URL don't change.
So let me see
if I have to upload
the code or not.
So I'll go to my Lambda function
Handler go to the part
where my cue is saved.
All right, let me
anyway space to Q over here.
I think it is the same.
Yes.
It is the same.
Anyways, let us save it.
This is my function one.
So let me upload the code now.
So it's my function
and click on finish.
Right, so it is uploading
the function right now.
So meanwhile, let me go
to my function to and configure
in the queue address,
which is this.
Paste it here control s save it.
And once this
process is complete.
I will upload this code as well.
So while this is uploading,
let me change the address
in my index file as well.
This is my websites index file.
So I'll go to the queue URL
which is this.
I will change it save
it and close it.
Alright, so my website's
address has also been done.
Alright, so my code is uploaded
for this function.
Let me upload the code
for function to as
well because we are may change
upload function to AWS.
So it is my function
to that is my function
to in Lambda click on next.
And click on finish.
All right, so my code
is being uploaded Let's Wait
Awhile so that
my code gets uploaded
and then we can proceed
with our demonstration.
Alright, so my code
has now been uploaded
to both my Lambda functions.
Now.
What I'll do is I will go
to my local host website
click on refresh.
And I will upload a file.
So let me go back and see
what is there
in my bucket right now
so that it becomes easier
for us to verify
that a file has been uploaded.
So as of now my bucket is empty.
I there's nothing in my editor
a car demo bucket and my other
bucket is quarantine demo.
This is the place
where my other file
will go right?
Let me empty this as well so
that we are clear
that some Checked
has been added.
All right, so this bucket
has also been cleared.
So we'll go to a local host
will choose a file.
So let's upload some image.
So let it be this image,
right I click on open
and click on upload image.
All right, so it says
S3 upload complete.
All right.
So let me check
if a file has been added
in My riruka Demo bucket.
I'll click on refresh.
Awesome.
So one file has been added
and it's called one four.
Nine two, five four
six zero nine seven.
Let me check in my email
to let me check
if I got an email.
So yes, I've got an email.
Let me click on it.
All right.
So this is the name
of the file that I got,
which is the same right?
So, let me see
if there is something
in my Quarantine demo bucket,
so there's nothing there.
I'll come back now.
I'll reply to
this email as move.
So this basically means move
my file to some of the bucket
and I'm replying it
to hello either a Teddy
Ricardo TK, right?
So now we'll hit on send.
So my message has
been sent to Route 53,
which will be sent to ACS
which will invoke the Lambda
function which will move.
My file to the other bucket.
So let us check if that is done.
So first let us check
if I enter a car demo bucket
has been emptied so
will click on refresh.
Alright, so my I do take
a bucket has now been emptied.
Let's go back and check
if something has an ad
in my core and tine Emma bucket.
Alright guys, so my file
has successfully moved
to this bucket Let
Us verify the name.
So this is one phone nine to
five four six zero nine seven.
Let us check that in the email.
So the email
that we replied to had then
the object name as one phone
nine to five four six
eight or 7 so this is
the same file you guys.
All right guys,
so we have completed
our demonstration successfully.
Welcome to the session
on elastic Beanstalk
a web application hosting
platform offered by Amazon.
So without any delay,
let me give you
a brief overview
of what we will be
discussing today firstly.
We will see what elastic
Beanstalk exactly is,
and then we'll discuss certain.
In Salient features
of elastic Beanstalk moving on.
We'll try to understand elastic
Beanstalk a Little Deeper
by taking a look
at its components
and then at
its architecture in finally,
we'll try to deploy an
application on elastic Beanstalk
for practical understanding
of the concept.
So let's get started.
What is elastic Beanstalk?
If I have going
to find elastic Beanstalk
and Amazon terminology,
then it is a platform
as a service where you
can deploy your application,
which you might have developed
with programming languages
like Java dotnet PHP node.js
and many others
on familiar servers such as
Apache nginx passenger
and Tomcat the definition,
which I just mentioned
seems to have a lot
of technical terms as ended.
Well, let's try to figure
out what elastic Beanstalk is
and simple terms.
All right, let's say you Need
to build a computer tonight.
Well, you have two ways
to go at it first.
You can go to a computer
Warehouse Computer Warehouse
is a place
where you have different
components of computer laid out
in front of you
like you have CPU motherboards
router disk drive models
and many other components
you can choose
which have a component you need
and assemble them
and form a brand new computer.
This is similar to situation
when you try to
deploy an application
without using elastic Beanstalk
when you try to develop
Application by yourself you
will have a list of tasks
which you need to do.
Like you might have to decide
on how powerful you want
your ec2 instance to be
then you have to choose
a suitable storage
and infrastructure stack
for your application.
You might have to install
substrate surface for monitoring
and security purposes as
well moving on to option b,
you can always visit
an electronic retail store
which has pre-configured
computers laid out
in front of you.
Let's say you are
a graphic designer
and you want a computer
which has a modern graphical
user interface installed in it.
All you have to do
is specify this requirement
to a salesperson and walk out
with a computer of your choice.
Well, I personally
prefer this option.
This is similar to the situation
where you're trying
to deploy an application
using elastic Beanstalk
when you use elastic Beanstalk
to develop your application.
All you have
to do is concentrate
on your code list of the tasks
like installing ec2 instances
auto-scaling groups
maintaining security
and monitoring.
Etc is done by
elastic Beanstalk.
That is the beauty
of elastic Beanstalk.
So let's go back and take a look
at the definition again and see
if he'll understand
it this time.
Well elastic Beanstalk as
a platform as a service
where developers just have
to upload their application load
balancing auto-scaling
an application Health monitoring
or all and it automatically
by elastic Beanstalk.
Now, let's try to understand
how elastic Beanstalk
as a platform as
a service is beneficial.
Vishal to app developer.
I'm sure most of you know,
what platform as a service
has but let's try to refresh
what we know platform
as a service as
a cloud computing service
which provides you a platform
where you can deploy
and host your application
elastic Beanstalk
makes the process
of app development much more fun
and less complex and I
have five points to prove
that to you firstly it
offers quicker deployment
suppose you're developing
an app by yourself.
Then you'll have to do a lot
of tasks by yourself
like you might After decide
on ec2 instance choose
a suitable storage
and infrastructure stock
as well as install
auto-scaling groups as well.
And then you might have
to install substrate surface
for monitoring and
security purposes.
Well, this will take
quite a lot of time but
if you have used
platform-as-a-service to develop
your app then all you have
to do is develop a proper court
for your application rest
will be handled by
platform as a service
or elastic Beanstalk
in this case,
which makes the entire process
of app development
much more faster.
now secondly elastic
Beanstalk simplifies entire app
development process like the set
or developers have
to do is concentrate
on developing a code
for their application rest,
like monitoring servers
storage networking Etc
and managing virtualization
operating system databases
is done by elastic Beanstalk,
which simplifies
the entire process
for a developer using
platform as a service
to deploy our application make
Center app development process
more cost-effective
if you're trying
to dip By yourself,
then you might have to install
separate surface for monitoring
and security purposes
and I'm sure
for that you'll have to pay
a lot of money extra money.
But if you're using
an elastic Beanstalk
to deploy your application
it will provide you all
this additional software
such as a package
and you can avoid paying
unnecessary operating costs
also elastic Beanstalk
offers multi-tenant
architecture by that.
I mean, it makes it easy
for the users to share
their application
on different devices.
And that too with high security
when I say high
security platform as
a service will provide
you a detailed report
regarding your application usage
different people or users
who are trying to access
your application as well.
But this information
you can be sure
that your application is
not under any cyber threat
and finally platform
as a service provides you
an option where you can know
if the user who is using
your application is getting
a better experience out of it or
not with platform-as-a-service.
You can collect feedback
at Seven stages
of your app development
like during development stage
like testing Stage
production stage design stage
by doing so you will have
a report regarding
how your application
is performing at every level
and you can make
improvements if needed.
So this is how platform as
a service like a are an elastic
Beanstalk makes it easy
for developers to develop
an all-around perfect up
guys will be able
to relate to this point
when we try to deploy
an application using
elastic Beanstalk in the later
part of this session.
You'll understand.
How will a Stick
Beanstalk is beneficial
to app developer in Marquette.
There are quite a lot
of application hosting platforms
which are providing
platform as a service.
Let's have a look
at few of that.
First.
We have something
called openshift.
It is a web hosting platform
offered by Red Hat.
Then you have Google app engine
which we all know ask a lingo
at is a platform as a service
where you can deploy your
application and just do minutes
apparently will provide you
a production ready environment
where all you have to do
is deploy your application code.
Then you have python anywhere.
It doesn't online
integrated development platform
and web hosting service as well.
But based on Python language,
then you have elastic Beanstalk
offered by Amazon moving on.
We have a sure app Services
by Microsoft and many others.
But today our main focus will be
on elastic Beanstalk,
which is a web hosting platform
offered by Amazon now
that you have basic
understanding of elastic.
Stop, let's go ahead
and take a look at few
of its features.
Mostly all the features
are similar to the ones
which we discussed earlier,
like elastic Beanstalk makes
an app development process
more faster and simpler
for developer moreover.
All developer has
to do is concentrate
on developing code list
of the configuration details
and managing and monitoring
details will be handled
by elastic Beanstalk.
Also elastic Beanstalk
automatically scales up
your abs resources,
which have been
assigned to your uh,
Occasion by elastic Beanstalk
based on your application
specific needs but
there is one feature
which is specific
to elastic Beanstalk suppose.
You have deployed an application
using elastic Beanstalk,
but now you want to make changes
to the configurations
which have been already assigned
to your application by
elastic Beanstalk though.
Bienstock is a platform
as a service.
It provides you with an option
where you can change
the pre-assigned configurations
like you do and infrastructure
as a service.
Well if you remember Member
when if you're trying to use
infrastructure-as-a-service to
deploy an application,
you will have full control
over AWS resources.
Similarly Beanstalk also
provides you with full control
over your AWS resources
and you can have access
to the underlying resources
at any time.
Now, let's try to understand
elastic Beanstalk
a little deeper first.
We'll be discussing few
components of elastic Beanstalk,
then we'll have a look
at its architecture.
What we have your first we
have something called
application suppose you
have decided to do a project.
So what you do you go ahead
and create a separate folder
on your personal computer,
which is dedicated
to your project.
Let's say your project needs
Apache server SQL database
and a platforming
software like Eclipse.
So you install
all the software's
and stole them in the folder
which is dedicated
to your project.
So that will be easy
for you to access
whenever you need all
the software's similarly
when you try to do
deploy an application
on elastic Beanstalk Beanstalk
will create a separate folder
which is dedicated
to your application
and an aw storms.
This folder is
what we call an application
if I have to Define folder or
application in technical terms,
then it is a collection
of different components
like environments
your application versions
and environment configuration.
Let's try to understand each
of these components one by one.
We have something called
application version suppose you
have written a code stored.
In the file and deployed
this coat on elastic Beanstalk
and your application
has been successfully launched
but now you want to make
certain changes to the code.
So what you do you go ahead
and open the file make
changes to it save it
and then again deployed
on elastic Beanstalk
elastic Beanstalk again,
successfully launches
your application.
So you have two versions
of your application now,
it's just a copy
of your application code,
but with different changes
and elastic Beanstalk
will provide you with an option
where you can upload
different versions.
As of your application
without even deleting
the previous ones then we have
something called environment
environment is a place
where you actually
run your application
when you try to launch
and elastic Beanstalk
environment Beanstalk starts
as ining various AWS resources,
like ec2 instances
auto-scaling groups load
balancer security groups
to your application the point
which you have to remember
is at a single point
of time environment can run
only a single version
of your application.
Elastic Beanstalk will provide
you with an option
where you can create
multiple environments for
your single application suppose.
I want and different environment
for different stages of my app.
Like I want an environment
for development stage
one for production stage and one
for testing stage.
I can go ahead and do
that create a different
environment for different stages
of my application
and suppose you have same
version or different version
of your application installed
on all these environments.
It's possible to run all
this application versions
at same time.
I hope that was clear.
Well, you'll understand
them practically when we
try to deploy an application in
the later part of the session.
Then we have something
called environment Tire
when you try to launch
an elastic Beanstalk environment
elastic Beanstalk asks
you to choose amount
to environment tires,
which are web
server environment.
And then you have
worker environment.
If you want your application
to handle HTTP request,
then you choose
web server environment.
And if you want your application
to handle background task
that is where a work environment
comes into picture.
Sure, which to choose
either web server
or work environment
and how to work with them
when we'll try to deploy
an application in later part.
And lastly we have
something called environment
Health based on
how your application is running
Beanstalk reports the health
of your web server environment
and it uses different
colors to do.
So first gay indicates
that your environment
is currently being updated.
Let's say you
have installed one version
and now you're trying
to upload different version.
Well, it's taking a lot
of time so that time
it shows gray color.
It means your environment is
still under updating process.
Then you have green which means
that your environment has passed
the recent health check.
Then you have a low which means
that your environment has failed
one or more checks and red
failed three or more
checks moving on.
Let's try to understand
the architecture of
elastic Beanstalk.
Like I said early on
when you try to launch
an elastic Beanstalk environment
Beanstalk ask you to choose Was
among two different environment
tires firstly we have
web server environment
web server environment
usually handles HTTP
requests from clients
and it has different components
firstly we have something
called environment.
You know, what environment
is it's a place where we
actually run your application
and Beanstalk provide
you with an option
where you can create
multiple environments and
the main point is
at a point of time
this particular environment
can run only one version
of your application moving
on we have Something called
elastic load balancer.
Let's say your application
is receiving a lot of requests.
So what elastic load balancer
does is it distributes
all this request
among different ec2 instances
so that all the requests
are handled and no request
is being delayed.
What actually happens is
when you launch an environment
or URL is created
and this URL in the form
of C name is made to point
elastic load balancer
senior is nothing
but alternate name for your url.
So when your application
receives requests all
these requests are forwarded
to elastic load balancer
and this load
balancer distributes.
These requests among ec2
instances of Auto scaling group.
Then we have Auto
scaling Group Well,
if your web server is trying
to handle a lot of traffic
and it's having a scarcity
of ec2 instances,
then Auto scaling group
automatically installs
few easy to instances.
Similarly.
If traffic is very low,
then it automatically terminates
under use ec2 instances then
we Have ec2 instance.
So whenever you try
to launch an elastic
Beanstalk environment Beanstalk
will assign your application
with a suitable ec2 instance,
but the software stack like
the operating system the servers
and different software's
which are supposed
to be installed
on your instance are decided by
a device called container type.
For example,
let's say my environment
as Apache Tomcat container.
So what it does it installs
Amazon Linux operating
system Apache web server
and Tomcat software.
Do you see two instance
similarly depending
on your application requirements
it installs different software
stack on your ec2 instances.
Then we have a software
component called host manager
which runs on every
easy to instance
that has been assigned
to your application.
There is host managers
responsible for various tasks
firstly it will provide
your detailed report
regarding performance
of your application.
Then it provides
instant level events.
It monitors your application
log files as well
and it monitors
your Datian server,
you can view all
these metrics log files
and create various alarms on
cloudwatch monitoring dashboard.
Then you have security
groups Security Group is
like a firewall
to your instance.
Not anybody can
access your instance.
It's just for security purposes.
So elastic Beanstalk has
a default Security Group,
which allows client to access
your application using Port 80.
You can Define
more security groups
if you need and then
elastic Beanstalk also
provides you with an option
where you can define
a security group.
All your database
for security purposes moving on.
We have something
called Walker environment.
First question that comes
to our mind is what is worker.
Well suppose your web server has
received a request from client.
But on the way
while it's trying to process the
request it has come across tasks
which are consuming
a lot of resources.
I'm taking a lot
of time because of which
it's quite possible
that your web server
might deny other request.
So what it does it forwards
these requests to something
called Welcome these worker
handles all this stuff.
Us on behalf of web server.
So basically worker is a process
that handles background tasks
which are time intensive
and resource intensive.
And in addition.
If you want you can use walker
to send email notifications
to generate metric reports
and clean up databases
when needed let's try
to understand why we need Walker
with the help of you skis,
so I have a client he has made
a request to a web server
and the web server
has accepted the request
and it starts
processing the request
but While it's processing
the request it comes
across the switch
are taking a lot of time.
Meanwhile, this client
has requested or send
another request to a web server
since web server is still
processing the first request
it denies second request.
So what is the result
of this as the performance
and the number
of requests accepted by
a web server will drastically
decrease alternatively let's say
a client has made a request
and your web servers accepted it
and it starts processing
the request and again,
it comes across Stars
which are doing a lot
of time this time.
What it does it transfers
or it passes all this task
to walk our environment
and this work environment
will handle all these stars
and request one
is successfully completed.
Meanwhile, if it
receives a second request
since it has completed
processing request one,
it will accept requests to I
hope the scenario was clear.
We'll all we are doing
by installing work environment
is we are avoiding spending lot
of time on single request here.
Now, you know what
web server environment is
and work environment is and why
do we need work environment?
But there has to be some way so
that this web server environment
can pass on this task
to work environment.
Let's see how so you have
your web server environment.
It has received a request
and while processing
it as encounter tasks
which are taking a lot of time.
So what it does it creates and
sqs message sqs is a simple
to service offered by Amazon
and this message is then put
into es que es que
and the different
requests are arranged
based on priority in this qsq.
Meanwhile when you're trying
to install Walker.
Environment elastic
Beanstalk has installed
something called demon.
What is demon does it pulls
sqs message from Askew
and then it sends the Stars
to web application,
which is running
on vodka environment as
a result or as a response
to spin start application
handles all the stars
and responds with
an HTTP response option.
So this is
how the entire process
of handling tasks transferring
and then handling does goes on
so you have a client he has made
a request to a web server,
but the web servers
encounter with tasks
which are I'm consuming
and resource consuming.
So it passes this
request rescue is Cube.
And when you try
to install walking environment,
there's a demon which pulls
out all this messages art us
from your rescue.
And then this demon
sends all the stars to
our application application
results all the stars
and then it responds
with a HTTP response option.
So this is how your to
application communicate I
can read was lot of 30.
Don't worry.
We have arrived
at the fun part of session
where we'll be trying
to deploy an application
using elastic Beanstalk hear you
by doing or by creating
an application on
elastic Beanstalk practically,
you'll understand different
concepts its architecture
and different environment tires
and all this.
So let's go ahead.
So this is my area
plus Management console.
And if you want to take a look
at all the services, then you
have all the services here,
but were mainly
concerned with elastic.
Up, which have recently used.
So it shows that all recently
used resources or Services here.
So I'm going to choose
that elastic Beanstalk and this
is my Beanstalk console.
If you're trying to deploy
an application for first time,
this is the page where you land
when we scroll down it says
that I can deploy an application
and three easy steps.
All you have to do is select
a platform of my choice then
upload our application code
if I have one or use
a sample application code
and then run it.
Let's see if it's
as Easy as it says here,
so go ahead and click on create
new application option here.
It will ask you for application
name and description.
I'm going to name my application
as Tomcat app then description
as my new web app.
And then I'm going to click
on this create option C.
When I try to
create an application.
It has created a separate folder
which is dedicated
to my application.
And in that folder,
we have different components
as you can see here.
I have my environment then I
have application versions and
if I've saved any configuration,
it will show all
the saved configurations here.
Now.
Let's go ahead
and create an environment
on the right side.
You see an actions option
and you click on
that you get different choices.
You can just select
the create environment here.
So again, it's asking you
to choose among two different
web environment tires.
You have web server environment
and work environment
in web server environment.
Your application handles
HTTP requests from clients.
Then you have work environment
where your application
will process background tasks
like time intensive
and resource consuming
task in this demo.
I'm going to work only
with Observer environment.
You can go ahead explore
and create work environment.
Once you understand
how to deploy an application
on elastic Beanstalk.
So I'm going to click
on the select option here.
It will take me to a It's bad
enough to give a domain name
or in technical terms are URL
to my application.
You can give any URL
of your choice and see
if it's available.
So let's say my Tom app
and it see if it's available.
It says the domain name is
available then description.
I'm going to give
it a same as before.
So my new web app then
when I scroll down it asked me
for a platform of my choice.
There are different options.
You have go then you have
dotnet Java Ruby PHP
node.js python Tomcat.
At and if you're trying
to deploy an application
on the platform,
which is not here,
you can configure
your own platform
and deploy turn
elastic Beanstalk.
It provides an option here.
You can see there's
in custom platform here.
So I'm going to choose Tomcat
platform for my application.
And since I'm not
any kind of developer,
I'm just going to go ahead
and use the sample application
provided by Amazon.
But if you have
any application code
if you have created or develop
some code you can store
that in a file and upload
your it says you can upload
your code then you have a zip.
You need to convert
your file to zip our war file
and then upload it here.
So I'm going to just
select sample application
and then click on create
an environment here.
So it's going to take awhile
for elastic Beanstalk to launch
my environment though.
It's not as much time
as it would have
taken me to develop
entire application by myself
while elastic Beanstalk
is trying to launch environment.
Let's discuss some points or in
the earlier part of the session
with discuss some benefits of
elastic Beanstalk firstly I said
that it fast ins your process.
Of developing an entire.
So it's true.
Doesn't it?
All I did was
select the platform
of my choice dress is done
by elastic Beanstalk itself.
So thereby saving a lot
of time similarly it
simplifies the process
of app development again.
All I did was select
a platform of my choice
like installing easy
to instances security groups
Auto scaling groups
and assigning IP addresses rest
is done by elastic Beanstalk.
I even mentioned
a point where I said
that it will provide
elastic Beanstalk provides you
with an opportunity.
And now you can change
the present configuration.
We'll explore that.
Once the environment is created.
Let's go ahead and see
what elastic Beanstalk is doing.
It says that it has created
a storage for my environment.
Well S3 bucket
solar all my files
where I have my application code
are stored there then test
created a security group
as well and elastic
IP address then it says
it's launching an ec2 instance.
So you see it's as easy as that.
All you have to do is select
a platform of your choice rest
is Founded by elastic
Beanstalk and later on
if you're not satisfied.
If you want to change
some configuration,
you can go ahead
and do that here.
Look at this.
This is the IP address
which are domain name
which are assigned to my up.
It says new instance
has been added and in addition
it showing each task
while it's doing
Isn't that cool?
You'll know what your
environment is currently doing.
So it's still taking a while.
So it says it has installed
and added instance
to my application
and my environment has been We
launched it is finished
almost all the tasks.
It should have taken
to environment page now.
So this is my environment page
or you can see
our dashboard first.
You have environment healthier.
It says green.
It means that my environment
has successfully passed
the health check then
it shows the sample version
of your application
since I've used the sample
application and saying
sample application here
since I've chosen
Tomcat as my platform.
It has installed
suitable infrastructure stacked
like Amazon Linux
and you have Java
8 aiming language.
Let's go ahead
and explore this page first.
We have something
called configuration here.
Like I said, though,
it is a platform as a service.
It provides you with an option
value can change configuration.
So you will have full control
of your resources first.
We have something
called instances here.
When I click on modify option,
you can see
that elastic Beanstalk
has assigned micro instance
to our application
if I want I can go
ahead and change it
to different instance based
on my application
requirement scrolling down.
I have cloudwatch monitoring.
If I want detailed monitoring,
then I can go for one minute
if I want basic
monitoring or not.
So detailed monitoring then I
can choose five minutes here.
Then I have an option
of resigning storage
to my application as
well at says we have
magnetic storage general purpose
and provision.
Iops as well.
When we scroll down again.
We see different
security groups.
I can just click on that
and the security group
will be added to my application.
So once you've made
the changes you can click
on apply option or do I
haven't made any changes.
I'm just going to click here.
So now elastic Beanstalk is
trying to update my environment.
So it's showing gray color here.
If you recollect a mentioned
during the earlier part
that grey indicates.
My environment is being updated.
Okay, let's go back
to configurations.
We did have a look at instances.
Then you have something
called capacity apparently
elastic Beanstalk is
design a single instance
to my application.
If I want I can go ahead and
change to auto-scaling groups.
You have an option
called load balance
so you can click on that here
and you can set the minimum
and maximum number of instances
that your auto scaling.
Group can install as well then
if you have chosen
a load balancer option earlier
than a load balance
would have been enabled here.
Then we have monitoring details
which provides you with
two options enhanced monitoring
and basic monitoring
and when we scroll down
you can see a streaming
to cloudwatch logs option here.
So if you want your log files,
you can view them
on cloudwatch dashboard as well.
You can set the retention period
according to your choice
and suppose you want
your application for
some private purpose.
Then you can create a generate
a private VPC for you.
Your application similarly,
you can add or decrease
the amount of storage as well.
So by explaining all this
what I want to say is your hands
are not tied you can make
changes to configurations.
If you want.
Then we have logs option.
If you want to have a look
at the last 10 lines
of your log files,
then you have an option.
It says last hundred line.
Sorry lost a hundred lines then
if you want full log files,
then you click on that
do provide you a file
and download format.
You can just download it.
Then we have health option here
where it provides health.
You are a cc sources basically
shows ec2 instance here.
It says it's been
7 minutes or six minutes
since my ec2 instance
has been installed.
Then you have monitoring
where it shows
different monitoring details
like CPU utilization Network
in network out.
If you want you can go ahead
and create an alarm
with alarm option here suppose
you want notifications
to be sent to you
when the CPU utilization
or when the number
of ec2 instances are scarce
in your auto scaling group.
Then you have events here
events basically are nothing
but it's a list of things
which has happened
since you started launching
an environment when I go down
it says we have seen earlier
on the black screen
the same things are applied your
so it says create
an environment starting
then we saw that AC to instance
has been installed security
groups elastic IP address.
So basically it
shows all the events
that has happened from the time
elastic Beanstalk has started
to launch our environment
and till the time you
terminated the environment.
So that's it.
Then you have tag files.
You can assign different
key values as well.
Let's go back.
This is a sample application,
which I've tried to use not let
me try to upload and deploy
a new application version here.
Okay, I'm gonna go
to documentation here.
I'm interested with
elastic Beanstalk.
I'm going to select on that
and then develop a guide click
on getting started on
when you scroll down on deploy
a new application Virginia based
on your sample application.
You have different versions
of your application
since I've selected
a tomcat is my platform.
I have a tomcat zip file.
You're a boy.
Already downloaded
that so I'm gonna just going
to upload the file then
so let's go back
and it says upload
and deploy but let's go
back to our folder.
Then there's an application
versions option here.
So it gives you deploy and
upload option separately here.
I'm just going to upload
first then we'll deployed
version label new version
and upload the file.
I have it here zip file.
I'm just going to attach
the file and then click
on upload optional.
The new version of
my application has been uploaded
but it's not been deployed yet.
So when I go Can you can see
that I can still see
the same version
which was there before now?
Let's go back and deploy it.
Okay.
I'm going to select this
and then I'm going to click
on deploy option
and select employer.
Let's go back
to environment and check
so my environment
is being updated.
So again the gray color here
once it's updated as and show
the new version name here.
It is uploaded.
So as you can see it showing
the version name of my new
version application version.
Like I said all your both
my application versions.
Are there have been
deleted any you don't.
Have to delete
your application versions
when you create
a new one similarly,
you can upload multiple versions
of your application going
back actions option.
Then you have
load configuration,
which will definitely
load your configuration.
Then you have saved we can save
this configuration suppose.
You want to create
an application with
the same configurations again,
you don't have to start
from the beginning
from creating application
environment all that.
You can just save
the configuration and use
for the other application
or other environment
of your application.
Then you can clone
your environment as well rebuild
and I environment
and terminate as well.
So here I have
saved configuration.
If you have saved this
configuration the configuration
of been listed here
and like that conversation.
I can use when I'm creating
a new environment.
Okay, just let's see
if have explored all
the options environment
how well I forgot to show
you one most important thing
when I click on this URL.
It takes me to a page
where it shows
but my application
has been successfully installed.
Well, that's it.
So now you know
how to deploy an application
using elastic Beanstalk.
Do I have used
the sample application?
Are you can go ahead
and upload a code of yours
if you have any and try it out.
Well, all the options here
seems to be user-friendly
so you will know what to do.
It seems to be easier process.
You'll understand it better
when you try to reply
an application by yourself.
So first and foremost,
I would start by talking about
what cloud storage exactly is.
Then we would move further
and understand some of the myths
that surround cloud storage
but also discuss certain
cloud storage practices
and would understand
how different cloud storage
service providers work.
Finally, I would finish
things off with the demo part
where I would be talking about
how cloud storage Services work
on Amazon web services.
So I hope this agenda
is clear to you guys.
So let's not waste any time
and quickly get started then.
So what exactly is cloud storage
now first and foremost,
let me tell you what prompted
me to actually go ahead
and take this session.
Well recently.
I had been interviewing
and where I asked people
what do what did the know
about cloud computing
and they told me
that cloud computing is a place
or it is a place online
where you actually store data.
I went to some extent I agree.
Yes cloud computing
helps you store data,
but that is not the definition
on the longer run.
So that is why I thought
that we should actually go ahead
and have this session
so that we can discuss
some of the myths
that surround cloud computing
and tout store is in particular.
So guys, let's start
with a basic definition first.
Storage.
Well, it is something
that is made available
in the form of service.
Which is connected
over a network.
So guys this is
a very basic definition
and the throw some more light.
I would like to actually go
ahead and given certain examples
as well to specify
what does this definition mean?
But to some point
this definition is correct.
It says that it is
nothing but a storage
which is available as a service
which is connected
over a network now again,
you might wonder as
in this is what people told me
in the interview, right?
I mean it is a place
where you store data.
So yes cloud storage
to some extent.
Yes.
This is what it is.
But when you talk about
cloud storage it is lot more
than this basic definition.
Let's try to understand
what all this cloud storage
exactly has to offer
to you people.
Well first and foremost
as I've already mentioned
it is storage it can let
you store emails media.
Now when I say
media you can store
in a different kind of media
whether it's your images
whether it's your videos
or maybe other kind of files.
It also lets you hold
Services as well.
Yes.
We are living in the world
of internet right now and there
are various Services websites
that are online
and this data can be stored
by using Cloud platform.
and finally I'm sorry guys
finally it is nothing
but the backup now
when I say back up guys,
we are talking
about large Enterprises
that let you back up the data
and the using Cloud
platform to do that.
But again, it's to still
holds the same point right?
I mean when I say
emails Media Services backup
for large organizations,
I mean it is still
a simple storage know now,
let me tell you what it does
when I say backup
for large organizations.
We are referring to a lot
of pointers here data coming in
from different sources.
The weight is processed.
The weight is integrated and
stored into a particular storage
how it is handled
and what all can you do with it.
Now when you talk
about a cloud storage,
it actually takes care
of all these things.
That means it's not redundant
or a dead storage
where you just take
your data and put in
your data you can think
of it as smart data storage.
So to understand
that let's talk about
cloud computing a little so
what cloud computing does
is it lets you have this data
on the platform
and it is a platform
where it has
a number of services
that lets you compute
or process this data to suit
your business needs now,
it can be using machine
learning Big Data finding
out certain patterns
using power bi tools
or not power bi tools bi tools.
And also do a lot
of other things
like maybe use a cloud platform
where the data can be used
for marketing purposes, Maybe.
I think I owe to Bots
and stuff like that.
So this is what
a cloud computing platform.
Does it basically lets you use
different sources and use
this particular data to do
multiple or different
kinds of things.
So when I say a cloud storage
it basically ensures
there is a mechanism
that in first place it stores
data and lets you perform some
of the actions
that you can actually
perform on this data.
So as we move further,
I would be discussing
quite a few pointers
that support this claim or this.
Definition of mine.
So let's just move
further and try to understand
a little more pointers
or some other pointers
that talk about cloud storage
but to keep it simple.
It is a storage
that lets you do a lot of things
with the data primary reason
being storing the data
and the other reasons
being processing it
or managing it also so let's
move further and take a look
at the next pointer.
So what are the myths
that surround a cloud storage?
Well when you talk
about the myths,
this is what some people
The same that cloud computing
is suitable only
for large scale organizations
know this is not true.
Let me give you
an example recently.
What happened was one
of my friends.
He actually happen
to format his mobile phone
and he lost all the images
and other data
that was there on that phone.
So the problem was he
never backed that data
on any Drive neither
on Google Drive
or anywhere so he lost the data
so he came to us and he told us
that this is what happened.
So we told him that You
should have backed it up.
Maybe on Google Drive.
So next time he did
that and again,
he being used
to losing his data.
He lost his data again.
So he again comes up and he's
like I've lost the data
so we reminded him
that he had his data stored
on Google Drive.
So when you talk
about Google drive,
it is nothing but an online
storage where you actually
make a copy of a data,
so he made a copy of his data
and he could actually
get that data back.
So when I say cloud storage it
gives you a simple application
or a simple.
That you can actually go ahead
and just put in your data just
like Google River you can put
in your data as well.
So it is not limited to
large-scale organizations only
if even you are
a single individual
where you just need
to store your data,
you can use cloud storage.
Now, there are there are
various cloud service providers
that actually meet
or cater different cloud
computing needs So based on
that the cloud storage
is might get complicated
and might give you
more functionality.
But even if you need is
as basic as storing data,
don't worry cloud
computing or cloud.
Storage is for you as well.
Now if you talk
about small scale businesses,
yes these days
the amount of data
that is generated is huge.
And that is why
what happens is even
for small scale organizations.
You need a place
where you can store your data
and somebody can manage the data
for you so you can focus
on your business goals.
So this is
where cloud storage comes
into picture for even small
scale businesses as well.
So if you ask me,
yes last scale
organizations are suitable
for cloud computing or only
large-scale organizations.
A suitable for cloud storage.
This is a myth.
Complexity with cloud guys.
Now.
What does this term symbolize
people normally assume
that having that private
infrastructure makes it easier
for them to actually go
ahead and put in your data
that is not true.
The fact that people are used
to certain methods
or methodologies.
They feel comfortable with it.
Whether cloud is complex or not.
I would say it is not why
because if you get used
to certain Services,
you would realize that storing
or moving a data to cloud is
actually lot more easier
than Normal infrastructures are
your previous or traditional
infrastructures is
what I would say,
so whether cloud is complex,
I would say no
as we move into the demo part
probably we would be talking
about this pointer
or once I give the demo
probably you would have
a clearer picture
how easy it is to actually
move your data to Cloud.
Not eco-friendly.
Now this might sound
out of the blue.
I mean you might wonder this is
not a sociology session.
So where did this
point coming from?
I mean not eco-friendly.
Yes what people
assume is the fact
that a large amount
of data is being stored
on these platforms.
So we have use amounts
or use numbers of data centers
which are big in size
and they consume
a lot of electricity.
So there is power
wastage electricity wastage.
Well, that is a myth again first
and foremost the fact
that Getting a centralized
storage somewhere.
That means most of the data
would be stored there.
So yes, you are
automatically saving out
on your power consumption
when you talk about it from
a global or an Eco perspective.
The other thing is I
would not want to point
out a particular
cloud service provider.
But when you talk about GCB
that is Google Cloud platform,
then Amelie provide
their cloud services
at a very affordable price now,
why is that?
The reason for that
is they've actually put
in a lot of effort
into the research part.
Where the researched a lot on
how they can actually
minimize the cost
and how did they do it?
They basically ensure
that the amount of power
that is consumed
by the resources.
They tried and optimize
that amount to a minimum amount
so that they are charged less
and in a way you
are charged less.
So if they're optimizing
that particular process,
obviously you're consuming
less amount of electricity.
So whether it's eco-friendly
definitely it is
eco friendly friendly.
Zero down time again.
There's no such thing
as zero downtime.
Now the fact
that I'm talking about
cloud storage does not mean
that I tell you
that it has zero downtime
and you're completely secured
know there is a possibility
that there might be
a downtime the fact
that cloud ensures
that this downtime is very less.
Now.
That is a plus Point
what loud also
does is it ensures
that there is disaster recovery
and there is always a backup
of your data or your resources.
So even if something goes down
for a very little time
and we normally it happens
for a very less time
if it does happen
and it happens very rarely,
but even if it
happens care is taken
that nothing harms
your resources or your data.
So zero downtime.
No that is not true.
But definitely downtime
is taken care of when you talk
about Cloud storages.
There is no need
of cloud storage.
Okay, this is one
of the biggest myths
whether people agree or not.
If you go back like 10 years
from now probably people
did not know a lot
about cloud computing.
But with time people
are actually moving to cloud
and if you take a look
at recent statistics,
they would agree as well.
I mean people would be wanting
to switch to cloud
in near future.
And the reason for that
is the quite a few service
is quite a few facilities
that cloud gives you
and that is why people
are moving to And
if you do move to Cloud,
you'll be using
cloud storage inevitably.
So yes that is going to happen.
And if you think
that there is no need
for cloud storage
definitely near future.
I would assure you
that even you would
be moving to Cloud.
So Guys, these are some
of the major myths there are
some other myths as well as we
move further not worried.
We would be discussing that as
well in some other pointers.
So let's just go ahead and talk
about some of the benefits
of using a cloud storage
for data storage
or basically using Cloud
for data storage.
So what are the benefits
of the signal I purposely kept
this pointer for the later half
and I first discussed the myth
because these pointers
would definitely help
you understand some
of those myths better.
Not a cloud platform
is customer-friendly.
What do I mean by this?
Well, first and foremost
when you talk
about cloud storage,
what you're able to do
is you're able to scale
up your storage scale
down your storage keep
it secure monitor it
and you can ensure
that there is constant
backup taken of your data.
So when you talk about it
from a security perspective,
it is secure as well plus
what cloud service providers do
is they've had so many services
that In the market you talk
about any popular cloud
service provider they
have lot of services
that are made available.
What do these services
do is they ensure
that you're functioning
on cloud platform is
very smooth and same is
for cloud storage as well.
You can utilize various
Services which ensure
that you're functioning
or you're working
on cloud becomes easy again,
which I have been
reiterating for a while.
Now that I would be talking
about these in future slides.
Don't worry as we get
into the demo part you would
and how user-friendly
these Cloud platforms
are Security now again,
this is an important point
when you talk about
Cloud platforms Cloud storages
are they secure or not?
Definitely they are very secure
and there was a time
when people believed
that these platforms
when not secure
to a greater extent
and that out was understandable.
I mean if there is something
that is new in the market
you tend to doubt
that but if you talk
about Cloud platforms
these platforms are actually
more secure than your on-premise
or your traditional.
Says which people are used
to using the reason for this is
if you talk about
cloud service providers,
let's talk about AWS.
That is Amazon web services
in this case.
What it does is it gives you
a shared security model now,
what do I mean by this you
have service level agreements
where you and your customer
or maybe the customer
and the AWS providers.
They basically come to a term
where the decide as
in what kind of security
or what kind of principles
are to be implemented
on the architecture and you
can take control as a new.
You can decide what accesses do
you want to give to the vendor?
And what are the axis is
you want to keep to yourself?
So when you do
combine this approach?
It ensures that security is is
at the optimum and you get to be
or you get to take control
of your security as well.
So yes,
if you talk about cloud storage
being secure or not.
Yes.
It is very secure to name
some we have S3 and AWS.
It is highly durable
and it is highly reliable.
So when you talk
about disaster recovery
and T it is almost up to there
and as I've already
mentioned not everything
is hundred percent
when I talked about
the downtime or yeah
the downtime part so yes,
not everything is
hundred percent.
But when you talk
about security and durability
when you talk about S3
in particular it is
99 point something
six or seven times nine that is
99.999999 times durable.
So that does make
a system very secure.
Another benefit guys.
It is pocket-friendly.
Now, if you talk
about cloud service providers,
whether it's storage,
whether it's compute service
database Services all
these Services you can actually
go ahead and use these services
for rental basis.
It's just like
paying for electricity.
I mean, if you're using
a particular service you
would be paying for that service
for the duration
you use that service
and you would be paying
only for the resources
that you've used.
So it is pay-as-you-go
kind of a model
where The only for the resources
you use and only for
the time duration you use
so whether it's
pocket friendly or not.
Yes.
It is pocket friendly.
And as you move further,
I mean if you are using
more storage the cost again,
it comes down
to a greater extent.
So it is already cheaper and
if you decide to scale up,
it would be more cheaper
or it would be cheaper is
what I should say.
So yeah, these are some
of the benefits now
if you talk about cloud
computing and storage again,
there are other benefits
like as I've already
mentioned durability.
Scalability and various
other benefits but these
are some core ones.
I would not want to get
into the details
because I wish to keep everyone
on the same page for people
who have been attending
this session for the first
time and for people
who probably know a bit about
cloud computing again guys,
if some of the terms
that I'm talking
about in this session you feel
that these terms are
fairly new for you
and I'm probably going
at a faster Pace,
I would suggest
that you actually do go ahead
and check into the The sessions
that we have
on our YouTube channel
because we've talked
about a lot of stuff there.
I mean other cloud services
what cloud computing is
what cloud service providers are
what are different
service models and
quite a few other videos
and sessions to be honest.
So I would suggest
that you go through
those sessions as well.
And I'm sure
that by now many of you
might have been wondering as
in whether this session
would be recorded
and a copy of it
would be available to you.
People are not not very
most of us sessions.
They go on you.
Boop so probably a copy
of it would be there on YouTube.
And if not,
you can actually share
your email IDs as well.
If it does not go on YouTube.
Somebody would share a copy
of the session with you people.
So guys if I'm
if I'm happening to go
a little faster than
what you're expecting
do not worry you'd be having
a copy of this as well.
But for now just try to keep up
with the pace that I am going
with and I'm sure
that by the end of the session
we all would be good.
So guys what are some
of the cloud storage practices
that you should take
care of now?
These are the practices
that should concern somebody
who is planning to move
to Cloud again.
If you are a newbie
and you're just here to practice
we are not talking about you
in particular but these pointers
are important for you as
an individual as well.
But I'm talking
about it from more
business business perspective
or more industrial perspective.
So if your organization
is planning to move
to Cloud Definitely.
These are some
of the practices or pointers
that you should take care of.
So first and
foremost scrutinize SLA,
so as I've already
mentioned you have SLS
where your service providers or
vendors basically come to a term
where you actually
go ahead and decide
on particular rules as a nugget.
These are the terms and these
are the services as a vendor.
I would be providing
you people and you as
a customer you agree
to certain terms as an okay.
This is what you
would be giving us.
And this is what we
would be paying you.
So there are certain pointers
that you should consider
while you are actually
signing your essays.
That you need to understand
is when they say
that you would be this is the
base charge try to understand
how the charges would be
when you decide to scale up
and stuff like that other thing
that you need to consider
as I've talked about downtime.
Right?
So normally you have SLS
where people talk
about the stuff
that there won't be an outage
which is more than 10 minutes.
So yes, I mean this
sounds fairly good right?
So in an hour's time,
this is a hypothetical
example do not consider
that there would be
a downtime of 10.
Minutes, this is
for your understanding.
Let's assume that there's
a downtime of maybe 10 minutes
in an hour's time,
which is too high for now,
but let's assume
that so what service
provider would claim is
if there is a downtime
once probably this is
what the charge would be.
But if it goes down
after that probably you get
some more consistent discount
and those kind of things.
So if there is an SLA
where you say
that it is 10 minutes,
What if they were to down times
of nine minutes in an hour
and that is fairly close, right?
So you've been robbed
of your right?
So that is
what I'm trying to say.
I mean if you do actually
go ahead and have
particular SLS make sure
that you consider
in right points that suit
in your business as well.
Follow your business
needs again guys storage
as we move further,
we will be discussing
what are the different kinds
of storage is so when you talk
about cloud service providers,
they provide UN number
of storages or In types
of storage is what I should say.
So depending upon the business
you're dealing with the kind
of data that is generated.
You should be able
to choose a proper storage
for your requirements.
I mean, whether you're dealing
with a real time data,
whether it's stationary
data archival data based on
that you should be able
to actually go ahead and set
up your cloud storage.
Also, you need to
understand as an okay.
Um, this is the date
I would be putting in
and these are the Integrations
I would be needing
because I'm using
these kinds of tools.
So are those With
my cloud platform,
so probably you need to consider
these pointers as well.
And if you follow these rules
probably a business would end
up saving a lot of money.
Now there have been used cases
where businesses have
actually gone ahead
and saved lakhs of dollars
thousands of dollars.
So yes considering
these pointers understanding
your business also
becomes important.
You need to ensure
that the security
which you are actually
managing or monitoring
is defined properly.
I've already mentioned
that if you talk
about cloud service providers,
they let you have an SLA
where you both come
to a similar agreement.
So understand the security
what are the accesses
that you have?
What are the accesses?
You want to give?
What kind of data are you
dealing with and based on that?
Probably you can come to terms
when you're actually
moving to Cloud.
Plan your storage future
what we are trying trying
to say here is plan the future
of your storage again.
Do you need to scale
up in your future?
What are the peak times
that we can expect
and stuff like that.
So when you initially actually
set your storage up probably
you would be in a much
better position to scale up.
I'm not refraining from the fact
that cloud providers
are already scalable,
but just to be secure you can do
that when you talk
about Cloud providers
mostly the give you an option
of scaling, right?
V or instantly but still
having an understanding of
how much storage you need
where you going to move
in like two years
three years time probably
having an understanding
of all those things
would definitely hold you
in a much better position.
Be aware of hidden costs
again guys have talked
about the first SLA, right?
So it is similar
to that understand
what you're paying for.
How much are you paying for?
It is a pay-as-you-go model
but having an understanding
of which Services would cost you
how much would help you
in performing proper essays
or having proper policies
for your storage.
So these are some
of the do's and don'ts
of cloud storage guys.
Again, if you need more insights
on different Services as well.
We have a video or a session
on YouTube which is called
as Interviews best practices you
can take a look at that as well
where we talk
about different services
and how can you actually perform
certain tasks which would ensure
that you are in the
best possible position.
So guys we've talked
about quite a few things.
We wonder stood
what cloud storage is.
We were understood
what are the benefits
what are some of the myths
and what are some
of the practices
that you should take
care of now,
let's take a look at some
of the different
cloud service providers
that provide you
with the services
and once we are done with it,
then probably we would move
into the demo part.
So guys the quite
a few cloud service providers,
which also provide you
with storage Services.
We have Google cloud platform,
which is one
of the leading ones digitalocean
probably it's everywhere
whether you search
for Internet ads companies.
It's there.
Tara Mark again,
this is a popular cloud
service provider IBM.
Is there in storage or in Cloud
for a very long time guys now
if you go way back
I happen to did
like I happened
to attend a session
where I believe it was AWS
and some reinvent session
where I do not remember
the name of the speaker,
but that wasn't made
a very valid point.
He's at that in 1980s.
He remembered or he happen
to visit a facility.
I believe it.
As IBM's I'm not sure
who's I think it was IBM's
so he said
that they had this huge machine
which was for storage.
I mean, it looked very cool
in 1980s use machine
and it was very costly
it was like somewhere
around thousands of dollars
and the storage space was 4mb.
Yes for 4mb, the cost
was thousands of dollars.
So you can understand
how far storage has come
how far cloud has come
and And yes, IBM,
it has been there.
I mean it has been
there since then.
So if you talk
about IBM you talk
about Google's Cloud platform.
These are principal
cloud service providers.
Then you have Microsoft
Azure knife you talk
about current market.
I mean if you go by the stats
alone Microsoft Azure and AWS.
These are the leading
cloud service providers AWS
is way ahead of all the other
cloud service providers.
I'm so sorry,
but if you talk about Mike Soft
as your it is
actually catching up
that Amazon web services
and greeson starts show
that Microsoft Azure
is doing fairly fairly.
Well, so yes,
these are some of the popular
cloud service providers and more
or less all of them have
good storage Services as well.
But as I've already mentioned
Amazon web services is one
of the best in the market
and in today's session,
we would be understanding
some of the popular
cloud service services
that Amazon web services
has to offer to you
and when I say popular Services,
I would be focusing on
storage Services specifically.
So guys, let me switch into the
console and we can discuss some
of these Services there
and directly move
into the demo part.
So yes guys,
I hope this screen is
visible to you people.
This is how the AWS
Management console looks like.
So again for people
who are completely new
to Cloud platform.
Let me tell you
that what Amazon web services
are most of the other
cloud service providers
do is they give you
a free tier account?
What they're trying to say here
is you come you use our services
for free for a short
duration of period
And if you like then go
ahead and buy our services
so These services are actually
made available to you
for free for one complete Year.
Yes.
There are certain limits
or bounds on these services.
So if you exceed those limits
you would be charged.
But if you stay
in the bounds or limits,
you won't be charged
and if you talk
about exploring these Services,
these limits are free tier
services are more than enough.
So again guys,
if you are completely new
you should come here.
That is Amazon web services
Management console create
a free tier account.
It is a very simple process.
Put in certain details
where you work.
Why do you want to use
these services are basic details
and then probably you would have
to enter your debit card
or credit card details.
Don't worry.
They won't charge you but this
is for the verification purpose.
And again,
if you're worried about
whether you would be charged
or an amount would be -
from your credit amount that
or your credit card
that does not happen guys,
aw is gives you a notification
saying that okay,
you've been using these services
and probably you might be
over using some of your services
also you An setting alarms
where if you reach
a particular limit after that,
you can actually
go ahead and ensure
that there is an alarm
so that you do not exceed
the free tier limit.
So yes, once you do have
an account you can Avail all
the services that are here guys.
So let's just go ahead and take
a look at the console a little
and just jump into the storage
Services right away.
So when you click
on this icon here storage guys
or Services rather you get
access to all these Services
as I've already mentioned
AWS provides you
quite a few Services the same
room hundred Services guys,
and they cover
different domains.
You can see the domain names
at the top computer
Vortex analytics business
applications storage.
You have management
and governance security
identity management
and all those Services guys.
So the in number of services
whether it's migration
whether its Media
Services you Services
for almost everything so
as we would be focusing
on the storage Services
before we go there.
This is one thing probably
you can select a region
where you want to operate
from that is you want
to create your resources
in this particular region.
You can always have
this option of using it.
So what is the reason
guys your data is based
in a data center, right?
I mean your data
is copied somewhere.
So if you are
using those resources,
probably your data
would be fetched
from that particular location.
Asian so you can choose
a region probably
which is close to you
if you like if your business
is located somewhere else
probably you can choose
that region as well.
So you need to go
through the list of regions
that are available
and accordingly make a decision.
Now this being
a simple demo guys,
I'm would be sticking up
or sticking to Ohio basically.
So let's just go ahead
and jump into the cloud
services part and let's talk
about storage in particular.
So guys,
if you take a look
at the storage services
that are here you can see
that These are
the storage services
that AWS has to offer to you.
We have S3.
We have EFS you have FSX
you have S3 Glacier storage
Gateway an AWS back up.
Let me just try
and throw some light
on some of these services
and probably we would just
go ahead and get
into the demo of one or two
of these services at least.
So guys, I'm
when you talk about S3,
it is simple storage service.
So that is s now
this storage is basically
Object bucket kind of a storage.
I mean your container
where you put in your data
where you store your data
is called as bucket
and your data or your files
are basically stored
in the form of objects.
Let's just go ahead and quickly
create a small bucket.
This would be a very small
introduction to the service.
Let's just go ahead and do that.
So when you keep on click
on this icon guys,
that is S3.
It redirects you
to the S3 console guys
where you can actually go ahead
and create a bucket.
I've mentioned the pointer
that there are Don't services
that make your job very easy
with cloud service providers
and when you talk
about storage Services,
it is no different.
I mean there are
Services which ensure
that your job is fairly easy.
So let's just go ahead and see
how easy it is to work with S3.
If you wish to create
a bucket guys,
if you wish to
create a container,
it is very easy.
Just go ahead and click
on create bucket
and give it some name say Sample
for today, maybe guys.
I'm very bad
at naming conventions.
But please forgive me for that.
Again.
The names here should be unique.
I mean if the name is taken
somewhere else probably
you cannot renamed.
I mean you cannot use
that name again.
So yes, and so
that your name is unique
and probably guys you should try
and name your buckets
in such a way
that those are more
relatable say for example,
if you have a bucket
for maybe creating
a particular application,
so maybe bucket
for that application.
And or something like that
so that you have a hierarchy
and in that way you
can assign IM users
or access to those buckets
in a particular order
because you would not want
all your users to have
access to that bucket.
Right?
So naming convention
becomes very important.
So just go ahead and say next.
Keep all the virgin's
guys versioning becomes
very important again.
Let's not get into the details.
But let me give you a small idea
what happens here versions.
That means each time
of buckets get updated.
Probably I would want
to version or a copy of it
and I would want the latest one.
So when I was on it,
it maintains those copies
and if I need to go back
I can actually go back
to a particular level
or a benchmark,
which I set the previous
time in this case.
Let's stick to basic one
and I'd not want
any logging details either.
So just next.
Again, guys, there are
certain Public Access has
which have been given so
permissions and access
we would talk about
that not worry for now
just say next and I
would say create a bucket.
And guys the bucket
is already ready.
I'm in my container
is already ready
so I can just go ahead
and probably open
this bucket and put in a file
if I want and that was
very easy guys.
I say upload and if I'm
connected to my local system,
I just say add files.
Let's pick this random file,
which uses this name
and I see upload.
And there you go guys
the file is already there.
I mean, we've created a bucket
a container will put in a files.
It's as simple
as that permissions
as I've already mentioned now,
let me talk about this point.
I skip this point, right?
So let's discuss this a little
so guys security something
that you can handle.
So you would decide
or you need to decide
what are the users
that need to access
a particular bucket suppose.
Your organization has
different people working
on different different teams.
I mean you have somebody
who is a developer.
There's somebody who's working
on maybe The administrative part
on maybe on the designing part.
So for particular bucket,
you have particular data
so you can decide
who gets to access
what so setting
in policies becomes important.
You can create your own policies
as well initially.
We saw that certain
Public Access is restricted
to this bucket.
I said, let's skip it skip
that for now.
So when I say that
Public Access is restricted,
that means not any public
policy can come in
and dictate terms are saying
that use this policy why
because There is a restriction.
This is a private bucket
and not anyone can use it.
So guys when you talk
about S3 in particular,
you can create buckets
you can have backups.
You can have your EBS backups
also moved here.
You can have your you can move
your data from here to Glacier.
We would be talking
about they should not worry.
You can have your elastic
Beanstalk applications
your past applications
and the data can be stored
in your S3 buckets.
You can have
your CI CD pipelines
and the data can be moved
again to the S3 bucket.
Now, this is highly durable
and highly reliable.
It's of storing data
and it gives you fast retrieval
of data as well.
Let's go ahead and try
to understand some other
services as well guys.
So when I come back here
and I cefs elastic file storage
or system browser.
So here basically
in this storage you
can store files.
Yes.
We are talking about data
that is in the form of files.
And if you wish
to connect it better
with the network you can go
for EFS as well because then
you have something
called as S3 Glacier.
Yes.
We talked about S3 right
where data is.
Is durable and it
can be accessed very quickly S3
on the other hand lets
you store archival data.
Let me tell you what
archival data is first.
So guys when you talk
about archival data,
basically what happens
with archival data is
you're dealing with data
that you do not need
to use every day.
Let me give you an analogy.
I'm not sure
whether you'd be able
to relate to that.
So guys, I'm your
birth certificate now,
I belong to India
and we've been taking A lot
but we still have a lot of data
that is in the form of papers.
Even if you go to hospitals
attempt to request
for a birth certificate.
It might take days
for you to get
that birth certificate.
Why because there is some person
who will be going
through all those documents
and giving you that document.
This is just an example.
Do not relate it
like very seriously.
But yeah, so it might take
a couple of days,
right so and the birth
certificate thing.
I mean, I might not need
birth certificate every day.
It might be once-in-a-decade
that I might go to a hospital
and probably request
that particular birth.
Ticket, right?
So this is a kind
of data probably
which had not need
regularly or in real time.
So I can compromise
a little on the fact
that if the person is giving
me that data in two days time.
It's okay because
that does not cost me anything.
I can wait for two days maybe
but that's not the case
at times you need the data
to be retrieved very quickly.
So if that is the case you
should store it where in S3,
but if you're fine
with this delay,
probably you would want
to store it in Glacier.
Why?
These are normally
takes a longer while
to retrieve your data,
but the advantage
of Glacier is it is profitable
because it is very affordable
compared to S 3 S 3 is
already affordable.
You can check in for the prices.
But if you have archival data,
which you won't be using
everyday, you can store
it here and the fact
that it takes a longer
while it won't cost you.
I mean, it won't cost
in that perspective of accessing
your data in real time.
Right?
So if the data is something
that is not needed regularly you
can Move to S3 Glacier, right?
So what happens is S 3 you can
actually move in all your data.
And then if you realize
that there is certain data,
which would not need every day.
Just move it from S
3 to S 3 Glacier
where the data is stored
in archival form and it is
or it does not cost you a lot.
So again guys,
I won't be getting
into the demo of S3 Glacier.
We have a session on S3 Glacier
or Amazon web services
Glacier other and to do that.
What you need is you need
probably a third party tool.
That makes it easier
for you to retrieve the data.
So I won't be getting
into the stuff
where I download that to land
and show you how it works.
It's very simple.
We'll just like
we created buckets.
Are you create volts there
and you probably move
in your data and you
can retrieve that data.
But again, it takes a long while
to retrieve that data.
So it is similar to S3,
but little different so
yeah, that is S3 Glacier.
We understood what EFS is
and what S3 is then again guys,
you have some other
services as well here
if I Scroll down you have
your storage Gateway.
You have your AWS
backup as well.
So what are these things?
And what do these things
do well storage Gateway
an AWS back up basically back up
as it says you can have
backup of your data
and you can like save it
from going down and stuff like
that when you talk about storage
get with these are services
that let you move your data
from on-premise atmosphere
or your infrastructure
rather to Cloud.
So if you already have data that
is on your existing on-premise
or infrastructure rather,
you can actually move
that data to Cloud as well.
So there are services
to help you do that.
And those services are
your storage Gateway services?
So guys we've discussed some
of these Services
there is something else
which is called as
elastic block storage.
Elastic Block store is
what it does is it lets
you create volumes snapshots
and copies of the volume
that is attached
to your instances.
Let's go ahead and take
a look at how this works.
I mean there are a lot
of pointers to talk about it.
So as I move further,
I would be discussing
those pointers while I
also show you how to do it.
So guys when I say EBS
or elastic block storage
what that does is it lets
me attach some kind of volume
to my instance now instances.
Let me tell you
what instances are first.
Now when you talk
about cloud services,
they give you compute Services
where you can spawn instances
or spawn temporary
servers or servers
where you want to host
a data now each time.
I won't be going out and buying
a new machine right instead.
What cloud does is it?
What happens?
Yes, guys.
Okay, guys, I'm not sure
whether there was a lag
while you were going
to this session.
What happened is let me tell you
what happened my connection
the streaming connection
to my software,
which I'm using to stream.
This session did go down
a minute back and it shows now
that it is connected.
So I would like to know
whether I'm audible
to you people are not if yes,
then we can continue
with this session guys.
Okay, I'm guessing we're fine.
So I'm just gonna go ahead
and continue with the session.
I was talking about instances.
Let me talk a little
more about it.
So when I talk
about these servers
that are ready to use basically
these servers are something
that you can use
and you can have
some memory attached to it.
So what we're going
to do is we're going
to go ahead and launch
one instance and understand
how memory or hose
storage works with it.
So to do that we
were going to go ahead
and just launched
that particular service.
It is called as To which
is a compute service guys.
So here I can actually go ahead
and create servers
or launch instances
in simple words.
So let's just go ahead and
launch a particular instance.
Now, I have the freedom
of launching both
linux-based windows-based one
to based kind of instances.
So you have the freedom
of choosing what kind
of instance do you want
this being a simple demo guys.
I'm going to stick
with the windows instance.
I'm not going to show you
how to deal with that instance
because I've done
that in previous sessions.
You can take a look
at some of those switch
sessions as well guys.
Let's just go ahead and launch
this particular session
or this particular instance
rather now guys,
this is a Windows
instance and okay,
not this let me launch
on basic one.
This is also free tier guys.
But yeah, I would
want this make sure
that your instance
is EBS backed.
So guys, you're backing
up Works in two ways.
You can back it up on S3.
You can back it up on eBay
as that is elastic block storage
now elastic block.
Storage is important why it lets
you create images and volumes.
What are those
we'll talk about that
once we create this instance.
So ensure that
for now it is EBS.
So if I click
on this is the thing
if I click on this icon,
It would give me
details what kind
of instance I'm launching
when I say T2 micro.
It is a small instance
which has one CPU
and one gigabytes of memory
for now and I can just
go ahead and say next.
Okay, some of the other details
whether you want
to be PC or not.
Let's not discuss
that and then you get
into the storage part guys.
This is the device with two
which I am attaching
my root volume.
So this is the path rather.
So I need to focus on this.
It is SDA one guys.
That is slash Dev slash sd1.
You need to remember this
when you create new volumes
and the types of volumes
that you can attach
to your instance are these
you have general-purpose SSD
provision tie offs and magnetic.
It is take a something
that is getting outdated
probably might be replaced.
So these are the few ones you
also have some other kind
of volumes that you
can attach but the point
that you need to remember is
when you talk about having
a primary volume in that case
you have only these options
because these are bootable guys
so there are certain other
volumes that you can attach
if I attach a secondary volume,
you see the options are more.
I have SSD for traffic
optimization and then I
have cold SSD as well.
But this is a basic thing.
We not going to get
into the details of that.
You would skip
that so guys all I'm trying
to say is this is the device
this is the size
and probably this is the type
of instance or volume.
Sorry is that would be attached
to my instance.
So let's just go ahead and say
next a tax for now.
Let's not add anything
and then let me say
configure the settings.
So guys when I launched
an instance it says
that security is not Optimum.
It's okay.
I mean you can assign the port
you want to when you use it
for a higher security purpose.
And then this is important guys
for your each instance.
You need a key pair
which is a secret way
of logging in
or a secure way of logging
in not secret a secure way.
So this is a second
place authentication.
Once you're logged
into your account.
You would be needing a key pair
if you wish to
use this instance,
so make sure you create one
and you store that one
as well if you have one
which you can use probably.
can do that as you can just
create one say Nuki I
said download guys.
Once you download it.
Keep it safe somewhere.
It is stored in the form
of that p.m. File.
So do that and then I
say launch an instance.
So guys once this happens
if I just go back
to the ec2 dashboard
probably I can see
that there is an instance
which is running for now.
It is 0 why
because guys my instances
still getting launched.
It takes a couple of minutes
or 1 and 1/2 or 1 minute
probably to launch an instance.
The reason for this is probably
a lot of things happen
in the background.
I mean certain
network is associated.
If you talk about an instance,
it needs to communicate
with other instances, right?
So in that case Probably
you need to have a network
that lets all
these instances connect.
So a network is set here
basically and probably all
the storage volume is attached
in a lot of things happen.
That is why there are
certain statistics
that your instance needs
to go through and hence.
It takes a minute
or so to launch this instance.
So if you take a look
at this the status text it says
that it is initializing.
So if you refresh it
probably it happens at times.
So let's just try our luck see
whether it's No,
it's still initializing but guys
we can see the volume
that would be attached to it.
So, let me just come here
and rather go here
if I click on volumes,
there is there is a volume
that is attached to it.
So there is a 30 GB volume.
So there's a volume
that probably has
a size of 30 GB.
So it is here already
and it is in use
so it would be attached
to my instance
once it is up and running.
So the point I'm trying to make
here is what elastic block.
Storage does is it lets
you manage all these things now?
There are two ways to manage
these things either you create
a copy of this volume
disable this volume
and then attach the next one
or probably you can directly
scale your existing volume
or make changes
to it right away.
So what elastic Block store
is does is it lets
you manage the storages?
So again, let me tell
you how it works.
So when I create
an instance probably discredited
in a particle particular region,
right so in that A particular
region say for example
now I'm based in India.
So I have a data
center in Mumbai.
So my instance would be created
at that data center
and probably the storage
for it would also be there.
So there is no latency
when I try to use that storage.
So this is
what EBS does it lets you manage
that particular storage.
So how it works is I
can create a copy of it.
So what this copy does is it
serves two purposes so next time
if I wish to make In just
to that storage I can do
that if this particular storage
or volume goes down.
I have a backup copy again.
I can create snapshots as well.
Now what snapshots do is
basically they let me replicate
my instance and the volume
that is attached with it.
So instead of creating
an instance again,
and again with
if I've defined certain
properties for my instance
and not have to worry about
defining those properties again,
and again, I can just create
a snapshot or I can rather
create an Emi out of it,
which I can store
and use it next time.
If I want to spawn
a similar instance,
so this is very BS helps
in it lets you have backups
of all these storages
it lets you create copies of it.
So even if something goes
down you can work on the copy
that you have so guys by now.
Our instance would be created.
Let's just go ahead
and take a look at it.
It says it is running guys,
and we've already taken
a look at the volume.
Let us create a copy
of this volume to do that.
I'm going to go to the actions
my instances selected already.
I can just go to modify
and make changes
to this volume right away,
which is an easier way,
but I'm going to show you
how it can be done
the other way as well
how it used to work
previously so I can just say
that create a snapshot.
details Sample,
and I say create.
So guys are snapshot is created.
If I come here I can take a look
at the snapshot again.
It is spending might
take half a minute
for the snapshot to get created
so I can just come here
and replace or refresh other.
These things at times
take a little while.
So guys we would be creating
a copy of it probably viewed
by detaching the volume
that we have created and it
is attached to our instance
and we would replace
that with the copy
that we are creating now.
So once this thing is done
and created we can do that.
For some reason it's taking
longer while today.
Let's hope that it
gets done quicker.
Look, it's still processing.
Let's bear with me
or just bear with me.
Meanwhile this happens.
Again guys if I was too fast
and if I missed out
on certain things I
would like to tell you
that you can go through
our other sessions on YouTube
and probably you would be in a
much better state to understand
what has happened here again,
there was an outage
we're not out.
It's my software did not work
properly the streaming software
and probably there was a lack
of a minute or two.
So I'm hoping
that you are did not miss out
on anything that was happening.
Meanwhile.
Just hope that this Snapshot
gets created quickly.
It is still pending
and this is irritating at times
when it takes a long
while It's completed guys.
A snapshot is ready.
I can just go ahead and say
create a volume out of it,
which I wish to attach.
So guys there
are certain details
that we need to do.
So for that laces go back first.
Let's go back to the instance
that we have and let's see
where the instance
is created guys.
So as you can see
if you come here,
it would give you
the details of the place
where the instance is created.
So it is u.s.
East to see so when you
create an volume a volume,
it is necessary
that you created
in the same region guys
because as I've already
mentioned the benefits
of having it in same
reason is region is
that you can attach it
to your existing instance
and it saves you
from various Layton sees so,
yep, let's go back
to the snapshot spot and say
create a volume of it.
I say create and then
I probably let's say
I want more storage guys
that's in 90.
Okay, this is general
purpose it is to way.
So let's go to to see
if I'm not wrong.
It was to see let's
just go ahead and create it in
to see and say create volume.
Clothes so guys are instances
where our volume is created
successfully again guys.
Now you can take a look at it.
From this perspective.
I have my Snapshot here, right?
So this snapshot says 30gb
that does not mean
that the snapshot
which I took its size is 30 GB.
It says that it was created
from an instance
or size is 30 GB.
So there's a difference between
these two things guys understand
that as well.
So I have a volume
which is based in availability
Zone to see I have an instance
which is here and it again is
it availability Zone to see
so we can attach to it.
Let's just again go back
to the volume spot.
So guys, I have two volumes.
I created this one and this
is attached to my instance.
Let me just try
and remove this first.
detach volume Okay,
it's giving me an error
try to understand why
this error is there guys.
My instance is already running.
So I cannot directly remove
this volume from here for that.
I would have to select
this instance go
to instant State and say stop
so it stops working for now.
And once it does I
can attach the volume.
So for now what you can see
is there are these volumes here
it is in use right?
So once the instant
stops it would be available
and won't been used
so I can replace it
with this instance.
So it has stopping it
hasn't stopped yet.
So as do not worry,
we would be done
with the session very soon.
And once we are done probably
you all would be free to leave.
I believe that this session
has taken longer
than my normal sessions.
But yeah, there was
a lot of stuff to talk
about we talked about
the complete story services
that you have reached has
to offer to you people hence.
This session was so long.
So let's just quickly go ahead
and finish the stuff.
It has stopped.
So guys I can now go ahead
and remove the volume
or detach this volume and go
ahead and attach the other ones
if I say detach it would detach.
Yeah, see both are available.
Now.
Let's try to attach
this volume and say
attach volume search
this is the instance guys,
which I have created
and you need to give
in the device details,
which was / what
with the details.
Let's just go back
and take a look at the details
that we're supposed
to enter in here.
So as a you need to give
in the path that we talked
about which is the drive
that we've discussed, right?
So that is the part
that you need to enter.
And then you actually go
ahead and say SD a one.
Slash and probably you
would be more than four to go.
So this is the other thing I
do not remember the other part.
So you need to go ahead and put
in these details here.
If you put in
these part details guys,
you can just go ahead
and attach your volume
right away and this volume
would get attached
to your instance.
So this is how it works
and you can actually go back
and do other things as well.
So if I just come here
I have this instance.
So what you have to do is
you have to actually go ahead
and click on this thing for now.
It's not working.
But if you just come back
here or to the volume part.
So if you just go
to the volumes part
with we were at in the previous.
Slide you can actually go ahead
and attach the volumes now here
you go by just go to instances.
Probably go back
and I say ec2 again.
Yeah, if I come back
two volumes guys.
You can attach the volumes
that are there you
can delete those
and you can do a number
of changes that you wish to do.
So just go ahead
and attach these volumes
and you would be more
than good to actually go ahead
and launch our instances
or manage the storages
that are there.
Again.
The only thing that I missed out
on is the path I told
you to note the path
the device name, right?
You just have to go ahead and
enter in the device name here.
And if you enter
in the device name
while creating your volume
or attaching your
volume your instance.
Get attached to that
or your volume
would get attached
to that instance right away.
So yes guys thus pretty
much sums up today's session.
We've talked about
quite a few things here guys.
We've talked about S3 Services
we've talked about we've talked
about EBS in particular.
We've understood like
how to detach a volume
how to attach on I
just did not show you
how to attach the volume,
but you can do that.
The reason I'm not showing you
that is probably lost out
on On the device name here,
which normally comes in here.
So before you
deactivate your device,
make sure that you have
this name and when you do launch
or attach your volume
to that particular thing,
all you have to do is you
just go to the volume spot.
And probably when you
say attached to a particular
instance put in that device name
there and you are
instance would be attached
or your volume would be attached
to your instance
and you can just go
ahead and say launch
or just start
this so-called instance again,
and you'll be good to go guys.
So as far as this particular
session goes Guys,
these are the pointers
I wanted to talk about.
I hope that I've talked
about most of these pointers
and I've cleared all your mints
or doubts that were there.
So that's when you
talk about S3.
Now.
It has a simple storage service
which is simple or easy
to use in real sense.
It lets you store
and retrieve data
which can be in any amount
which can be of any type
and you can move it
from anywhere using
the web or Internet.
So it is called as
storage service of the internet.
What are the features
of this particular service?
It is highly durable guys now.
Now, why do I call it durable,
it provides you durability
of 99.999999 some 11:9 now
when you talk about
that amount of durability,
it is understandable
how durable this Services
what makes it this durable.
It uses a method
of checksum wear.
It constantly uses
checksums to analyze
whether your data was corrupted
at a particular point and if yes
that is rectified right away,
and that is why this service is
so durable, then it is.
Be flexible as well as I've
already mentioned S3 is a very
simple service and the fact
that you can store
any kind of data.
You can store it in any reason
or any available reason is
what I would mean
by the sentence.
It makes it highly flexible
to store the data
in this particular
service and the fact
that you can use so many pi
as you can and of secure
this data in so many ways
and it is so affordable.
It meets different kinds
of needs thus making it
so flexible available.
Is it available?
Yes, definitely it is
Is very much available
as we move into the demo part,
I would be showing
you which regions
basically let you create
these kind of storages
and how can you move
and store data
in different regions as well.
So if you talk
about availability, yes,
it is available in different
regions and the fact
that it is so affordable
making it available becomes all
the more easy cost-efficient.
Yes now to start
with we normally do not get
anything for free in life.
But if you talk about S3 storage
AWS has a free tier
which lets you use.
Public services for free
for one complete year
but this happens
in certain limits.
Now when you talk about S3,
you can store 5 GB of data
for free at least to get started
or get used to the service.
I believe that is more
than enough and
what it also does is
it lets you have somewhere
around 20,000 get requests
and somewhere around 2,000
put requests as well.
So these are something
that let you store
and retrieve data
apart from that.
You can move in 15 GB
of data every month outside.
Side of your S3 Service as well.
So if you are getting
this much for free,
it is definitely
very much affordable.
Also, it charges you on pay
as you go model.
Now.
What do I mean by this?
Well, when I say pay
as you go model
what we do here is we pay only
for the time duration
that we use the service
for and only for the capacity
that we use this service form.
So that is why
as you move along
if you need more services,
you would be charged more.
If you do not need more amount
of the service you
won't be charged to that.
Extent, so is it cost efficient?
Definitely it is scalable.
Yes.
That is the best thing
about AWS Services.
Most of them are scalable.
I mean you can store
huge amount of data,
you can process
huge amount of data.
You can acquire
use amount of data
if it is scalability
that is your concern you do
not have to worry about it here
because even this
service readily scales
to the increasing data that you
need to store and the fact
that it is pay
as you go model
did not have to worry
about the cost Factor as well.
Is it secure definitely?
It is now you can encrypt
your data you have
various bucket policies as well
that let you decide
who gets to access your data
who gets to write data
or gets to read data.
And when I said you
can encrypt your data
you can actually go ahead
and encrypted data
both on client side
and on your server side as well.
So is it secure I believe
that answers the question
on its own.
So Guys these were some
of the features of Amazon S3.
So guys now let
us try to understand
how does S3 storage
actually work now it works
with the Concept of objects
and buckets now bucket,
you can think
of it as a container
where as an object is a file
that you store
in your container.
These can be thought of
as AWS S3 resources.
Now when I say an object
basically object is
your data file.
I've already mentioned
that you can store any kind
of data whether it's your image,
whether it's your files blocks,
whatever it is,
these are nothing but your data
and this data
comes with metadata
when I say an object.
It is combination of your data
plus some metadata
or Or information
about the data what kind
of information basically
you have the key
that is the name of the file
that you use inversion
ID is something
that tells you
which version are you using
as we discuss versioning?
Probably I would talk
about Virgin ID a little more.
But meanwhile,
I believe this is more
than enough your objects
are nothing but your files
with the required metadata
and the buckets
as I've already mentioned.
These are nothing but containers
that hold your data.
So how does it work guys?
Well, what happens
is Sickly you go ahead
and create pockets in regions
and you store your data
in those regions.
How do you decide what buckets
to you is what reasons to use
where to create the bucket
and all those things.
Well, it depends
on quite a few factors
when I say I have
to create a bucket.
I need to decide what reason
would be more accessible
to my customers or to my users
and how much cost
does that region charge me
because depending upon
the region your cost might vary.
So that is one factor
that you need to consider
and let and see as well.
I mean if you put your data
In an S3 bucket,
that is far away
from you fetching it might cause
high amount of latency as well.
So once you
consider these factors,
you can create a bucket
and you just tore your objects
when I said version ID key,
actually a system
automatically generates
these features for you.
So for you it is very
simple create a bucket pick
up your object put it in it
or just go ahead
and retrieve the data from
the bucket whenever you want.
So I believe this gives
you some picture about
what S 3 is now let
me Weekly switch
into the demo part and let
me give you a quick idea
or quick demo as to how S3 works
so that it is not too
much theory for you people.
So guys what I've done is
I've actually gone ahead
and I've switched into
my Amazon Management console.
Now as I've already
mentioned AWS gives
you a free tier for which
you can use AWS services
for free for one complete year.
Mine is not a free tier account.
But yeah,
if you are a starter you
can create a fresh account.
You just have to go ahead
and given certain details
all You do is
you just go to your
web browser search
for AWS free tier and sign in
with the required details.
They would ask you
for a credit card
or your debit card details enter
any one of those
for the verification purpose
and you can actually go
ahead and set up alarms as well
which would tell you as in.
Okay.
This is the limit to which
you have used the services
and that way you won't be
charged for Access of data usage
or service usage having said
that guys this is
about creating an account.
I believe it is fairly simple.
You can create an account
once you create an account.
Is this is the console
that would be available to you?
What you have to do is
you have to go ahead
and search for Amazon S3.
If you search s3r,
it would kind of redirect you
to that service page.
So guys as you can see,
this is the company's
account probably somebody
uses it in the company
and they have the buckets
that are already created.
Let's not get
the that is just go ahead
and create our own bucket
and just go ahead and put
in some data into it.
It is fairly simple guys.
I've already mentioned.
It is very simple
to use kind of service.
All I have to do is click
on create bucket and enter
in name for some bucket guys.
Now this name is unique.
It is globally unique
once you enter a name
for the bucket you
Not use the same name
for some other bucket.
So make sure you put
in valid name and the fact
that I use the term Global
something reminded me
to be explained of so guys
as you can see
if I go back here.
I want you to notice this part.
So guys when you are
into the Management console
or you open any service
by default the region
is North Virginia?
Okay.
So if I create a resource,
it would go to this region.
But when I select the service
that is S 3 you can see
that this region
automatically goes to Global
that means it is
a global Service.
It does not mean
that you cannot create bucket
in particular regions
you can do that.
But the service is global is
what they're trying
to see so let us go ahead
and create the bucket
Let Us call it today.
Demo, you cannot use caps guys.
You cannot use some symbols.
So you have to follow
the naming Convention as well.
Today is demo.
Sorry.
I'm very bad
at naming conventions guys.
I hope it is.
Okay, let it be in u.s.
East you can choose
other regions as well guys,
but for now,
let it be whatever it is.
So I'm going to stick
to North Virginia.
There are 76 buckets
that are being used.
Let us just say next
bucket name already exists.
So this was already taken
guys see So you cannot use it.
Let's call it say.
vamos bucket 1 3 1 1 3 Okay.
Do you want to keep all
the versions of the object?
We will talk about
what versions are.
Okay guys.
Meanwhile, you just
bear with me.
I'm just going to go ahead
and create a bucket create
a bucket and there you go guys.
I'm sure removes bucket
should be here somewhere.
Here it is.
If I open it I can just go ahead
and create folders inside it
or I can directly upload data.
So I say upload select a file.
Let's just randomly
select this file.
It is Van Dusen
founder of python.
Basically, let's just say next.
Next next and the data
is uploaded guys.
You can see the data
being uploaded and my file
is here for usage.
So guys, this is how object
and bucket kind of stuff works.
You can see that this is
the data that I have
if I click on it,
I would get all the information.
What is the key?
What is the version
value for now?
Let's not discuss version.
But this is the key
or the name of the file
that I've uploaded.
So it is fairly
clear right guys.
So let us just
quickly switch back
to the presentation and discuss
some other stuff as well.
Well now guys
another important topic
that is to be discussed
here is S3 storage classes.
Now, we've discussed
how the data is stored
or how buckets and objects work
but apart from that
we need to discuss
some other pointers as well as
in how does AWS charge me
or what kind
of options do I have
when it comes
to storing this data.
So it provides you
with three options guys
standard infrequent and Glacier.
Let me quickly give
you an explanation to
what do these storage classes
mean and what all this?
Offer to us when I say standard
it is the standard storage
which gives you low latency.
So in case
if there is some data that needs
to be refreshed right away,
you can actually go
ahead and use standard storage
say for example,
I wish to go to a hospital
for certain kind of checkup.
So in that case my details
would be entered in and the fact
that I am getting
myself checked in a hospital
or diagnosed in the hospital.
What happens is
this data is important and
if it is needed right away,
it should be available.
So this kind of data can be
stored in your standard storage
where the latency is
very less the next we have
in frequent access.
Now, what do I mean
by that now in this case
my latency period has to be low
because I'm talking
about data that I
would actually need any time
if I want to but
when I store this data
for a little longer duration,
all I want is this data
to be retrieved quickly say,
for example, I get
a particular report
or a particular test done.
So in that case I
Actually go ahead
and submit my details
or say for example,
my blood samples,
but I need this information
maybe after three days.
So what happens is
in this scenario,
I would want to store this data
for a longer term,
but the retrieval should be
faster here in the first case
that was not the case if I
needed that data right away,
and if I wanted it to be stored
for a very short duration,
I would use standard.
But if I want to store it
for a longer duration,
and I want a quick
retrieval in that case,
I would be using
in frequent access
and finally I Glacier we have
already discussed this here.
Your retrieval speed is low
and the data needs to be put in
for a longer duration.
And that is why
it is more affordable.
If you take a look at the stats
that are there in the image
above you can see
that minimum storage
duration is nothing
for standard for infrequent.
It is 30 days and for
Glacier it is 90 days.
If you take a look at latency,
it is milliseconds
milliseconds and four hours.
So that itself explains
a lot of stuff here.
So what art This classes
and what do they do?
I believe some ideas clear
to you people again
as we move into the demo part,
we would be discussing
this part as well.
And we would also discuss
expiration and transition
that supports these
terms but let us move
further and try to understand
something else first
versioning and cross
region replication now guys
when I say virginie,
I'm actually talking
about keeping multiple copies
of my data now,
why do I need versioning?
And why do I
need multiple copies?
He's of my data.
I've already mentioned
that AWS S3 is highly
durable and secure.
How is that because you can fix
the errors that are there
and you can also have
multiple copies of your data.
You can replicate your data.
So in case
if your data center goes down
a copy of it is mentioned
or maintained somewhere
else as well.
How is this done
by creating multiple versions
of your data say for example,
an image, I store it
in my S3 bucket.
What happens here is there is
key the name is same image.
And virgin is some 3 3 3 3 3
right now take a look
at the other image.
If I actually go ahead
and create a copy of the first
image its name would remain same
but it's version
would be different.
So suppose both of these images.
They reside in one bucket.
What these images are doing
is they are having
multiple copies are giving
me multiple copies now
in case of image
not a lot would change
but if I have doc files
or data files
in that case versioning
becomes very important
because if I make changes
Changes to particular data
if I delete a particular file
a backup should always be there
with me and this is
where versioning becomes
very very important.
What are the features of
versioning by default poisoning
is disabled when you say
or when you talk about S3,
you have to go ahead
and enable this versioning
it prevents over writing
or accidental deletion.
We've already discussed
that you get non-concurrent
version by specifying
version ID as well.
What do I mean by this?
That means if I
actually go ahead
and create one more copy
of the data and store it.
So the latest copy
would be available on top
but I can go to the virgin's
option put in the ID
that belong to the previous
version and I can fetch
that version as well.
So what is cross reason
replication now guys,
we've discussed versioning.
Let us talk about
another important topic that is
cross region replication.
Now when you talk about S3,
basically what happens is you
create a bucket in a region
and you store data
in that region,
but what if I want to move
my data from one region
or from one bucket in The region
to other bucket in other region,
can we do that?
Yes cross reason replications
let you do that.
So what you do is you
basically go ahead
and create a bucket
in one region you create
another bucket in another region
and probably you give access
to the first bucket to move data
from itself to the other bucket.
So this was about versioning.
This was about
cross region replication
and I believe you've also talked
about storage classes.
Let me quickly switch
into the demo part
and discuss these topics
too little He did so
guys moving back.
What we have done is
we've actually gone
ahead and created
a bucket already right
when you talk about
what was the name of the pocket.
It was removes if I'm not wrong.
Yep.
So if you click
on the bucket name removes
what it does is it basically
shows you these details guys.
Now you can see
that your versioning
is disabled, right?
So if I click on it,
I can actually come to this page
and I can say enable virginie.
That means a copy of the data.
That I create
is always maintained.
So if I go to the most bucket,
or I just move back
get this interface
can be a little irritating
at times you have to move back
and forth every now
and then so guys there is a file
which we have stored.
You can just take a look
at this date first.
It says that it is 235
that was the time
when the object was moved.
Let me just say
that upload the same file.
This was the file
will be uploaded as
in next next next upload.
So where is this file
is getting uploaded.
You can see the name
of the file is still same.
We have only one file here.
Why because it was recently
modified at 2:45
from to 25 to 35.
It got changed to 245.
So it is fairly clear guys.
What is happening here?
Your data is getting modified.
And if you wonder as in
what happened to the previous
version, don't worry.
If you click
on this show option,
you can see that both
of your virgins are
still here guys.
This was created to 30.
And at 2:45.
So this way data replication
and data security
works much better.
So you can secure your data.
You can replicate your data.
So in case
if you lose your data,
you always have
the previous versions to deal
with how does the previous
version thing works so
as what happens is
if I delete this file
what Amazon S3 would do
is it would set a marker
on top of this file.
And once I delete it
if I search for that ID
that ID won't be available.
Why because the our car
has switched to the next ID now.
So whatever I want to do I
can do with the next ID as well.
So there is one more thing
that you also need
to understand here is
what happens to the file.
I mean, I've actually deleted
a file but a virgin is there
with me can I delete
all the versions?
Yes, you can specify the ID
and you can delete all
the versions that you want.
You can also do one thing
that is you can set a particular
life cycle for your files
when I say life cycle you
can decide as an okay now.
I have a file instead.
That storage we've discussed.
This storage is Right standard
storage infrequent and Glacier
what you can do
with your life cycle management
is you can decide as an okay
for a particular time duration.
I want this file to stay
in standard maybe after a while.
I want to move it to infrequent
and after a while.
I want to move
to Glacier say for example,
there is certain data,
which was very important for me
but having used that data,
I don't want to use it
for next few months.
So in that case I can move
to the substitutes or to
the other storage classes.
We're probably I won't
be needing to use that data
for a long while and doing that.
I won't be paying for this data
as I used to pay
for the standard
because standard is
the costliest of the three.
So let us quickly.
See can we do that or
how does it work?
At least if I just go back?
This is my file.
I can actually just go ahead and
switch to management in that.
I have the option of life cycle
if I click here.
There is no life cycle
add a life cycle.
You can add
a lifecycle rule as well.
This new let me call it new
and let me say next it asks me.
What do I want to do?
You can add rules
in life cycle configuration
to tell Amazon S3
to transition objects
to another storage class.
There are three requests fees
when using lifecycle
to transition data
to any other S3
or sa Glacier storage.
So which version do I
wish to use current?
I can say yes a transition
and I can select
transition to this tear
when after 30 days.
Days, and if I say next
it would agree expiration.
You can select other
policies as well.
So guys when I say
transition first thing
what it does is it tells
me what time to transition
to which storage
class and expiration.
It tells me when does this
expire so I can decide
when to clean up the objects
and when not to let's not do
that for now.
Let's just say next next so guys
what will happen here is
after 30 days my data would move
to a standard one a storage
so you can actually
go Then decide
whether you want to move
to Glacier in that drop-down
you had more options as well.
I did not do that,
but it is pretty understandable.
You can move to Glacier as well.
So this is about
life cycle guys.
One more thing.
You have something called as
replication you can add
replication as well.
If you wish to replicate your
data cross reason replication.
I believe guys,
I do not have access to do
that because I'm using
someone else's account for now,
but let me just give
you some idea as
to what you can do
to replicate your data.
You can just go ahead
and click on get started.
Dated so replication
to remind you people it is
nothing but a process
of moving data from bucket
in one region to add the bucket
in some other region.
So for that I need
to select the source bucket.
So let us just say
that this is the bucket that I
have next now guys in my case.
I haven't created
the second bucket.
What you can do is
you can just go ahead
and create one more bucket.
Once you create
the bucket you can select
the destination bucket for now.
Let us just say
that this is a bucket
that has been created
by someone else.
I'm not gonna transfer data
are but let's just select this
for the demo sick.
This is the bucket
that I have see it says
that bucket does not
have versioning enabled.
This is very
important Point guys.
I showed you how to
enable versioning right?
If you select the bucket there
is an option on the right
side saying virginie,
you can actually go ahead
and enable versioning there.
So once you enable
versioning you would be able
to use this bucket.
Do you want to change
the storage class
for the replicated objects
if you say yes it Would give
you the option of selecting.
What storage class do you
want to select right?
If you don't you don't have
to you can say next you have
to enter an IM role.
If you do not have any you
just say create a roll
and then the rule name
in this case.
I do not have any details
about this and I
don't want to create a role
because this account
does not belong to me.
Sorry for that inconvenience,
but you can actually go ahead
and select create a role
in just say next and I'm sure
that you can actually go ahead
and your bucket starts.
Audio our cross reason
replication starts working.
What happens after that is
once you store your object
in a particular file,
you can actually move
that object not in a particular
file in a particular bucket.
You can move the data
from that bucket
to the other bucket and a copy
of your data is maintained
in both the buckets
that you use.
So this is what cross
region replication is guys.
I believe that we have discussed
what our storage classes
we have discussed.
What is cross region replication
and we've discussed versioning
in general let Let's
quickly move back
to the presentation
and discuss the remaining
topics as well.
So guys have switched
into the presentation part
till time we've discussed
how cross region replication
Works we've discussed
how versioning works
and we have seen
how to carry out that process.
The other important topic
that we need to focus
on is we've know like
how to create versions
how to move data from one place
to the other but the thing is
what if I have to move data
from a particular location
to a location that is
very far away from me.
And still ensure
that there is not too
much latency in it.
Because if you're moving data
from one location to location
that is far away from you.
It is understandable
that it would take
a longer while why
because we are moving
data from internet.
So the amount of data
that you move and the further
you move it should take
a longer while for that.
So how do you
solve that problem?
You have S3
transfer acceleration.
You can do that by using
other services as well.
We discussed snowball
and snowmobile as well,
but they physically move.
The data and at times
it takes a number
of days to move your data
with S3 transfer acceleration
that is not the issue
because it moves at data
at a very fast pace.
So that is a good thing.
So, how can you move your data
at a faster Pace by using
S3 transfer acceleration?
Okay, let us first understand
what it is exactly.
So what it does is
it enables fast easy
and secure transfers of files
or long distances
between your client
and S3 bucket and to do that.
It uses a service call.
Cloudfront and the S locations
it provides you
as I move further
I would be talking
about what cloudfront
is do not worry about it first.
Let us take a look
at this diagram.
So normally if you
are moving your data
or directly uploading your data
to a bucket that is located
at a far away distance.
I mean suppose I'm a customer
and I wish to put my data
into an S3 bucket,
which is located maybe
a continent away from me.
So using internet it might take
a longer while instead.
What I can do is I
can use transfer.
Generation.
So how is it different now guys,
there is a service called
as AWS Cloud front what it does.
Is it basically lets
you cash your data
when I say cash or data
that means you can store
your data at a location
that is in the interim
or that is close
to your destination.
Now this service
is basically used to ensure
that data retrieval
is faster suppose.
I'm searching for
a particular URL.
What happens is when I type
that URL request is sent to
the server it fetches the data
and sends it to me.
So If it is located
at a very far location,
it might take long
while for me to fetch the data.
So what people do is
they analyzed as in
how much requests are coming
from a particular location
and if there are frequent
and a lot of requests
what they do is they set
up an age location
close to that particular region.
So you can put your data
you can cash a data
on that is location
and the data can be fetched
from that is location
at a faster rate.
So this is how is locations work
what transfer acceleration
does is it basically puts
in your data
at the edge location
so that it can be moved
to your S3 bucket
at a quicker pace.
And that is why it is fast.
So guys this was
about S3 data acceleration.
Let us quickly move
into the console part
and try to understand
how S3 acceleration works.
So guys have switched
into the console S3 acceleration
or data transfer acceleration
is very easy thing to do.
I do not remember
the bucket name.
I think it was Ram or something.
Okay, if I select this
and open it I actually go
to the Properties part less.
There are other things
that you might want to consider.
You can come here and take
a look at those as well for now.
I'm just going to say
go ahead and enable
transfer acceleration.
It is suspended.
I can enable it it gives
me the endpoint as well
and I say save So guys
what this means is
if I'm putting my data
into this bucket,
it would be
transferred very quickly
or I can use this bucket
to transfer my bit data
at a quicker Pace by using
data transfer acceleration
by S3 again guys.
I missed out on one
important point the fact
that we have been talking about
buckets and stuff like that.
There is something important
that I would like to show
to you people first.
Let us just go back
and disable this part.
I do not want it to have
the transfer acceleration.
Going and I just wanted to show
it to you people how it is done.
I just say go back to suspended
and one more thing guys,
if you once you actually unable
the transfer part and
if you upload a file,
you can see the difference
in the speed.
The problem is you need
a third party tool to do that.
So you can actually go ahead
and download a third-party tool
as well and using
that you can actually go
ahead and see how it works.
Having said that I was talking
about buckets in general.
So let us just go back
and go to removes again.
There you go.
And I'm going to copy the a RN.
I'll tell you why
I've copied the iron now
when I open this bucket guys,
we have quite a few
things permissions.
I talked about security,
right so you can decide
Public Access as in
who gets to access your bucket.
So guys, you can actually
go ahead and decide
who gets to access
what kind of buckets say,
for example here
in your blog Public Access.
You can decide
who gets to access
what data publicly for
that you have access control
lists using these ACLS.
You can actually decide who gets
to How other thing you can do is
you can just go ahead and create
a bucket policy and decide
who gets to access your bucket
or who gets to put your data
or delete your data
and do all these things.
Let us just go ahead
and create a policy.
Now, you can write
your own policy or you
can just use a policy generator
which again is
a third party tool.
So I want to create
a bucket policy forum is 3 so,
let's just say S3 bucket policy
and what kind of effect I want.
I mean do I want someone
to access my system
or do I want to deny someone
from accessing my system I can.
Decide that so let's
for now just say
that I want to deny someone
from doing something
and what I wanted someone to do
is to deny a particular thing
for that person
for all the objects.
I mean, I do not want
that person to access any
of the objects that is there.
So what I say is star
that means nobody
should able to do anything
to any of the objects
that are there in this bucket.
So it says star service
Amazon S3 what action I want.
I want to prevent someone
from deleting an object they go
and This is the AR
n that is why I copied it.
It should be followed
by a forward slash
and a star add a statement
and Ice Age ended policy.
So guys the policy
has been generated.
I just have to copy it
if I copy this thing
and I go back to the console
if I paste it here
I can say save It
saved I'll save it
again just to be safe.
So guys we have actually gone
ahead and let me just go ahead
and again go to ramose.
So there's not there is
an object here.
Let me just try
and delete this object.
If I just go
to the actions part here
and I say delete see
the file is still here.
Is it the other version?
No, it's not deleted.
See there's an error here.
If I click on it.
It says hundred percent field
why access denied
because I do not have the access
to delete the object right now.
Why because I've created
a bucket policy guys.
So that is what bucket policies
an AC else do the Let
you make your objects
or your data more secure.
And as you saw in the option,
there are quite a few options
that you have at your disposal,
which you can choose
from which you can mix
and match and decide
as an look at this is
what I want to do.
I want to probably give someone
an access to delete a bucket.
I want to give someone
an access to do this or do that.
So, where's this was about
S3 data transfer acceleration,
and we've also seen
how you create a bucket policy
how you attach it to your bucket
and stuff like that now,
let me just go back
and kind of Shove this session
or finish this session
up with a use case
so that you can probably
understand the topics
that we've discussed
a little more first.
Let us go back
to the use case guys.
So guys have switched into
my presentation console again,
and we would be discussing
IMDb media now for people
who watch movies.
They might know what IMDb
is it is a website
that gives you
details about movies.
They tell you what are
the movies that are nice
if you probably select
or type a particular He
named they would give you
details about it as a whole
where the actors
how was the movie
how was the review a short
snippet explaining you what
the movie is about its genre
and stuff like that.
Plus they have their own
ratings to kind of gauge
in the customers even better
as an IMDb being a popular site
and when they say
that this movie is
this person good or like
by these many people people
normally believe it
so they have that score as well.
So if you talk about a website
that basically deals
with movies you understand
the number of movies
that are released worldwide.
And if most of them
are present here on IMDb,
that means that database is huge
but we are talking about data
that is being processed
in great numbers great amounts.
I mean when you talk
about the data that is here.
What is happening here
is you have n number of movies
that are being released.
So if someone searches
for a particular movie,
it has to go through
the database and the data has
to be fresh to him right away.
So how do you deal
with the latency issue?
Well, this would answer
a lot of questions
or it would sum up lot of topics
that we've discussed.
Here let us go through
this use case probably.
So what happens here
is in order to get
the lowest possible latency
all the possible results
for a search our pre-calculated
with a document
for every combination
of letters in the search
what this means is probably
based on the letters.
You have a document
that is created and it
is traversed in such order
that all the data
is scanned letter wise
when you actually go
ahead and put forth a query
what happens is suppose
if there is a 20 character
Or a word that you put
in so there are somewhere
around twenty three two,
one zero three zero combinations
that are possible.
So your computer has to go
through these many combinations.
What S3 does is it
basically lets you store
the data that I am DB has
and once IMDb has told that data
they use cloudfront again,
we have discussed.
What cloudfront is they use
cloudfront to store this data
to the nearest possible
location so that
when a user fetches this data,
it is Fest from that location.
So what happens is Basically,
when these many possibilities
are combinations are to be dealt
with it becomes complicated
but in practice
what IMDb does is it basically
uses analytics in such a way
that these combinations
become lesser?
So in order to search
for a 20 character letter
they basically have to go
through one five
zero zero zero zero documents
and because of S3
and cloudfront you basically
can distribute all the data
to different Edge locations
and two buckets with in as
And since we're talking
about huge amount of data,
it is more than terabytes.
It is like hundreds
thousands of terabytes of data
so we can understand
how much data are we talking
about and S3 actually features
or serves a number of
such use cases or requirements.
So as I Believe by now,
you've understood what S3
is let me give you a quick sum
up or a quick walkthrough as
to what we've studied
because we've talked about a lot
of topics guys first we
started with the basic.
Six of different
storage Services we
were understood sorceresses
like EFS EBS storage Gateway.
We've talked about Glacier.
We've talked about
snowmobile snowball
and then we move to S 3 S
3 we talked about buckets.
We talked about objects.
We talked about versioning
we understood why
versioning is needed
so that we can basically
replicate our data prevent it
from deletion prevent
it from corruption.
We also talked about
across region replication
where you can move
data from one region
to the other we talked about
how we can Move
data Faster by using
S3 data transfer acceleration.
And then we also took a look
at the basics like what
are the storage classes?
What are the bucket policies
how to create bucket policies
and we also discussed
an important topic called
as transition and expiration
where if your data
expires it is deleted
if your data needs
to be transferred
to different stages you
can do that as well.
So all these topics are
discussed and we also discussed
some important features
and finally We finish
this session up with a use case.
So networking domain
basically offers three kind
of services the VPC
Direct Connect and out 53.
Let's discuss each.
One of them.
So vbc is
a virtual private Cloud.
So it's a virtual network.
If you include your all
your air pollution sources
that have launched
inside one VPC then
all these resources
become visible to each other
or can interact with each other.
Mine said inside the VPC
now the other use for PPC is
that when you have
a private Data Center
and you are using
AWS infrastructure as well
and you want your AWS resources
to be to be used
as if they were on your own
network in that case,
you will establish
a virtual private Network
that is a VPN connection
to your virtual private cloud
in which have included
all the services
that you want in
on your private Network.
You will connect
a private Network
through the V PC using the VPN
and then you You can access
all your AWS resources
as if they were
on your own network.
And that is what we
see is all about.
It provides you security
it makes communication
between the AWS Services easy
and it also helps you connect
your private data center
to the AWS infrastructure.
So guys, this is what
VPC is all about.
Let's go ahead on
to our next service,
which is Direct Connect so
Direct Connect is a replacement
to an internet connection.
It is a leased line.
A direct line
to the AWS infrastructure.
So if you feel
that the bandwidth
of internet is not enough
for your data requirements
or your networking requirements.
You can take at least line
to the AWS infrastructure
in the form of the
direct connect service.
So instead of using
the internet you would now
use the direct connect service
for your data stream to flow
between your own data center
to the illness infrastructure.
And that is what
Direct Connect is all about.
Nothing much further to explain.
Let's move on to a next service
which is is Route 53 be
so Route 53 is
a domain name system.
So what is the domain
name system, basically,
whatever URL you enter
has to be directed
to a domain name system
which converts the URL
to up IP address.
The IP address is
of the server on which
your website is being hosted.
The weight functions is
like this you buy a domain name
and the only setting
that you can do
in that domain name
or the setting which is required
in that domain name are
the name servers right.
Now.
These name servers
are provided to you by Route
53 these name servers that are
To provide you are to be entered
in the settings
of that domain name.
So whenever user
points to that URL,
he will be pointed
to Route 53 the work in the
domain name settings is done.
You have to configure
the Route 53.
Now another your request
has reached out 53.
It has to be pointed
to the server on which
your website is hosted.
So on a Route 53 now you
have to enter the IP address
or the Alias of the instance
on of to which you want
your traffic to be directed to
so you peed in the IP address
or you feel in the Alias
and It's done.
You're the loop is now complete
your url will now get pointed
to Route 53 and Route 53 in turn
will point to the instance
on which your application
or website is being hosted.
So this is the role
which Route 53 plays.
It's a domain name system.
So it basically redirects
your traffic from your url
to the IP address of the server
on which an application
or website is hosted.
Alright guys, so we're done
with the networking domain.
In today's session we
would be understanding
what AWS Cloud front is
but before we do go
ahead and understand
what cloudfront exactly is.
Let's start by taking a look
at today's agenda first
first and foremost.
I would be talking about
what AWS exactly is good.
Also understand.
Why do we need
it abuse cloudfront
and what it is exactly
the never talked about
how content gets delivered
using Amazon cloudfront
and what are its applications?
Finally, I would finish
things off with the demo part
where I would be talking
about AWS Cloud turn
distributions having said
that let's not waste any time
and jump into the first
topic of discussion
that has what is AWS.
Will AWS stands
for Amazon web services,
which is a leading
cloud service provider
in the market and it has
the highest market share
when you talk about
any cloud service provider.
Now what Amazon web services
does is it provides you
with 70-plus services
and these services
are Growing the name some
of these Services we
have something called
as your computation
Services your storage
Services your database services
and all these services are made
available to you through Cloud.
That means you can rent all
these services and pay
only for the services
that you use and only
for the time duration
you use these services for
if you want to know more about
how a database works exactly.
I would suggest
that you go through the videos
that we have on YouTube.
We have quite a few videos
on YouTube with talk about AWS
in particular all you
have To do is you have to go
to our YouTube channel
and type a direct iaws and
you'd be having all the videos
that are related to AWS.
But that is not the
discussion for today.
We are here to discuss
what cloudfront is
and I would like
to stick to that.
So coming back to cloudfront
when you talk about AWS
you have some Services now,
what aw does is
it offers you various
infrastructure as services
and even platform as Services
now these services are made
available to you in the form
of infrastructures or platforms
where you can actually
go ahead and host.
Applications or websites.
So when you do go ahead and host
these applications online
what your cloud provider
has to worry about is
the way data is fetched
because if you have
a website online now
that website would be visited
by quite a few people
and they would be requesting
particular content
or data, right?
So in that case
that data has to be made
available to your customers.
So how does it happen?
Exactly and how does AWS
make it happen to understand
that consider the scenario
suppose you You are
a particular user
in your trying to visit
a particular website and imagine
that that website
is based somewhere
at a very far location suppose.
You are based somewhere in USA.
And that website
its server actually hosts
or is based in Australia.
Now in that case
when you make a request
for a particular object
or particular image or maybe
content now your request
is sent to the server
that is in Australia and then
it gets delivered to you.
In this process
to there are quite a
few interrelated networks
that deal which you are
not aware about the content
directly gets delivered to you
and you have a feeling
where you feel
that you type
in a particular URL
and the content is directly
made available to you,
but that is not how it works
quite a few other things happen
in the interim and due to that.
What happens is the data
that gets delivered to you.
It does not get delivered
to you very quickly.
Why is that because
you'd be sending
in a request it would go
to the original server.
And from there.
The content is delivered.
To you now,
if you are based in USA,
the situation would be
convenient if the data
is delivered to you
from somewhere close by now
when you talk about
a traditional system
where you are sending a request
to somewhere in Australia,
this is what happens your data
or your request is sent to
the server based in Australia
and then it processes
that request and that data
is made available to you
which gets delivered to you.
But if you have
something like cloudfront
what it does is it sets
in an intermediate point where?
Data actually gets cached first
and this cache data
is made available
to you on your request.
That means the delivery
happens faster and you
save a lot of time.
So how does AWS Cloud
front exactly do it?
Let's try to understand
that but when you talk about aw,
cloudfront what it
does is first and foremost,
it speeds up
the distribution process
and you can have
a any kind of content
whether it's static
or dynamic and it is made
available to you quickly.
What cloudfront does is it?
It focuses on these three points
one is your outing to is
your Edge locations
and three is the way the content
is made available to you.
Let's try to understand
these one by one
when you talk about routing.
I just mentioned
that the data
gets delivered to you
through a series of networks.
So what cloudfront
does is it ensures
that there are quite
a few Edge locations
that are located close to you
and the data that you want
to access it gets cached
so that it can be delivered
to you quickly.
And that is why the data
that is being delivered
to you is more available
than in any other possible case.
So what happens exactly
and how does this content
gets delivered to you?
Let's try to understand
this with the help
of this diagram suppose.
You are a user.
So basically what you would do
is you would send in a request
that needs to reach
a particular server.
Now in this case
what happens is first
your request it goes
to an edge location
and from there to your server
to understand this
to you have to understand
two scenarios first
and foremost suppose
you're based in USA
and you want to fetch
a particular day.
That is based in Australia.
You would be sending
in a request.
But what AWS does is instead
of sending the request
directly to your server,
which is based in Australia.
Maybe it has these
interim as locations
which are closer to you.
So the request it goes
to the edge location
first and it checks
whether the data
that you are requesting
is already cashed their or not.
If it is not cached then
the request is sent to
your original server.
And from there the data is
delivered to the edge location
and From there it comes to you.
Now, you might wonder
as an this is a very
complex process and
if it is taking
these many steps.
How is it getting delivered
to me quicker than
in normal situation.
We'll think of it
from this perspective.
If you do send in
this request directly
to the main server again,
the data would flow
through some Network
and then it would be delivered
to you instead.
What happens here is
at your age location
the data gets cached.
So if you requested again,
it would be delivered
to you quicker
if it is requested by anyone.
It would be delivered
to them quicker plus how
as locations work is
when you do send in this request
and when there's
location Fitch's this data from
your so-called original server
in that case 2 when
the first bite it arrives
at your age location,
it directly gets delivered
to you and how does this content
exactly get stored here?
Well, first and foremost
what happens is
what your age location has is it
has some Regional cash as well.
Now this cash would basically
hold all the content
that is requested more.
More frequently in
your region suppose
a website has summon number
of content and out of it.
Some content is kind
of requested a lot
in a particular region.
So surrounding that region.
The closest is location
would have a regional cash
which would hold all the content
that is more relevant
for those users
so that it can be frequently
delivered to these users
and can be made available
to them quickly in case
if this data gets outdated
and it is no longer
being requested then this data
can be replaced with Guys
that is requested
more frequently.
So this is how cloudfront work.
What it does is it
creates a distribution
and you have some Edge locations
through which you can actually
request the data faster.
So what are the applications
that cloudfront has
to offer to you now,
I won't say
applications instead.
I would say some of the benefits
of using cloudfront.
Let's try to understand those
one by one first and foremost
what it does is it accelerates
your static website
content delivery.
We just discussed
that that means
if you are requesting
a particular image
or something like that,
it gets delivered
to you quicker.
Why because it is cashed
at your age location
and you do not have to worry
about any latency issues.
Next.
What it does is it provides
you various static
and even Dynamic content
suppose you need some video
or a live session
or something like that even
that gets delivered
to you quickly.
I just mentioned
that when you request
a particular thing When
the first bite it arrives
at your age location
your cloudfront starts streaming
that to you our start delivering
that to you same happens
with the live streaming
videos as well.
You would be getting
that streams instantly
without any Latin see
what server encryption now
when you do access this content
what AWS Cloud Trend
does is it lets you have
this so-called domain
where you put in HTTP
and you get secured data.
So you already have
one layer of security,
but it also lets
you add another.
Layer of security by giving you
something called as encryption
by encrypting your data or by
using your key value pairs,
which is the same.
You're actually ensuring
that your data is more secured
and it can be accessed
privately as well customization
at the age.
Now.
What do I mean by this now?
There is some content
that needs to be delivered
to the user or to the end user
if the customization it happens
at the server again,
it might be time consuming
and there are quite
a few drawbacks of it.
Say for example,
I need a particular content
and it needs to be processed
or Customized at
the very last moment.
So these things can be done
at the age location as well.
Thus helping you save time money
and various other
factors as well.
And finally what it does
is it uses something
called as Lambda H
which again lets you deal
with various customizations
and lets you serve
your content privately.
So these are some
of the applications
or uses of cloudfront.
What I'm going to do now
is I'm going to switch
into my AWS console
and I'm going to talk about
AWS Cloud Trend distributions.
And how can you go ahead
and create one?
So stay tuned and let
me quickly switch
into the console first.
So yes guys,
what I've done is I've gone
ahead and I've logged
into my AWS console.
Now for people who are
completely new to AWS.
What you can do is you
can actually go ahead
and create a free tier account.
You have to visit AWS website
and search for free tier
you would get this option.
Just create an account.
They would ask you
for your credit
or debit card details probably
but And charge you
a minimal amount is charged
and that is reverted
back to your account
that is for
verification purposes.
And after that what aw
is does is it offers
you certain Services
which are made available to you
for free for one complete year
that is as long as you
stay in the limits
or the specified limit
switch AWS has set
so those limits are more
than enough to practice
or to learn AWS.
So if you want to do go
ahead and get a proper hands on
on various database Services,
I would suggest
that you do visit their website
and create this free Terror.
Count once you do have
that account you have
all these services
that are made available to you
as I just mentioned.
There are 70 plus services
and these are the services
that are there
which are can actually
go ahead and use
for different purposes
our Focus today.
However is creating
a cloudfront distribution
which we just discussed
in the so-called theory part.
I would be repeating
few topics here to
while we do go ahead and create
our cloudfront distribution.
Now as I've already mentioned
we want to fetch data
or fetch a particular object
and if that is placed
A particular Edge location
that would be made
available to me.
So what we are doing
here is imagine
that our data is placed
at a particular original
server in our case.
Let's consider it
as an S3 bucket.
Now S3 is nothing
but a storage service
with AWS that is simple
storage service rather.
That is SS and that is
why we call it S 3
so what we are going to do
is we're going to go ahead
and create an S3 bucket in
that we would be putting
in certain objects,
and we'd Be accessing
that by using
our Cloud Trend distribution.
So let's just go ahead
and create a bucket
first you can see we have S3
in my recently Used Services.
You can just type S three-year
and that would made
available to you.
You can click on it
and your simple
storage service opens.
You would be required to go
ahead and create a bucket.
This is how you do it.
You click on Create
and you give it some name say
maybe bucket use small
letters bucket for AWS demo,
maybe and I would given
some number 0 0 0
I see next next next
I need a basic bucket.
So I won't be putting
in any details.
Do we have a bucket here?
There you go.
We have a bucket here.
And in this bucket,
what I'm going to do is
I'm going to put in some content
that we can actually request
for so let's just go ahead
and create an HTML file and put
in maybe an image or something.
So I have a folder
here in that folder.
I have a logo of ADA Rekha,
I would be using that logo
and I would want to go
ahead and create.
Create an HTML file
which I can refer.
So I would open my Notepad
and I would write
a simple HTML code.
I won't get into the details
of how to write an HTML code.
I assume that you all know it.
If not, you can use this code.
So let's create a head file
basically or a head tag rather.
Let's see a demo tag,
maybe and I close
this head tag.
I need somebody in here, right?
So let's say Did
the body we say?
Welcome to Eureka
and I and the body here
and I save this file and save as
where do I want to save it?
and see if it here
and I would save it as
a maybe index dot HTML.
I save it probably
got saved somewhere else.
Let me just copy it
and paste it here.
I've done that.
This is the file now.
We have these files.
Let's upload it
to our S3 bucket.
Come here.
I say upload I want
to add files.
So add files.
Where do I go?
I go to the folder I go to demo
and I select these two files
and I say upload.
There you go.
My files are here
and I say upload small files
so should not take a long time
fifty percent successful
hundred percent successful.
There you go.
You have these Two files now,
we have our S3 bucket
and we have two files.
This is our origin server.
Now.
I need to create a distribution
and use it to do that.
I would click on services
and come here and I
would search for cloudfront.
There you go.
And I say create a distribution.
So I click on this icon.
Now you have two options.
First one is something
that lets you have
your static data moved in
or moved out or if you want
to live stream your data you
should go for this option.
But that is not the case.
We would be sticking
with this thing.
I say get started.
I need to enter
in a domain name.
So it gives me suggestions
and this is the first one
which I just created
original path is something
that you can give in father.
A folders from where you
want to access the data,
but mind directly
resides in the bucket.
There are no extra folder.
So I don't need to enter
anything original ID.
This is what I have here.
Basically I can use this
or I can just go ahead
and change the name
if I want to but I would let
it stay the waiters
restrict bucket access.
Yes.
I want to keep it private.
So I say restrict
and I create a new identity
and the you I have a new user
created here apart from
that Grant read
permissions on bucket.
Update my bucket
policy according this
what I would say then I would
scroll down customer headers
and on I don't need to put
in these details.
How do I want my data to
be accessed the protocol policy?
I would say redirect
is TTP to https,
so that it is secured
if I scroll down I have
some other options as
well cast STP methods
and all those things.
Do I need to change
these object caching?
Can I customize it?
Yes, I can.
But again, I would be using
the by default one
if you want to you can
Is it smooth streaming?
No, these are some of the things
that you need to focus on
if you have some streaming data,
you can put in
details accordingly,
but we are not doing that.
What is the price class
that you want to choose?
You have some options here
which you can pick from I
would be going for the default
one and then I just scroll down
and I say create a distribution.
So your distribution
is getting created now
and this process
might take a long while
if you click on this thing
you realize that.
It is in progress
and it takes somewhere
around 10 to 12 minutes
for this distribution
to get created.
So meanwhile, I'm going
to pause this session
and I would come back
with the remaining part.
Once this distribution
is completed.
So bear with me for that while
so there you go.
The distribution
has been deployed.
The status is deployed here
so we can actually go
ahead and use this thing.
Now, we have a domain name here,
which I can use and I
can just enter it here
and we would be redirected
to the page.
And what happens here is
you would be actually given
access to this page
through the age location.
That means you're not going
to the server instead.
The data has been cast away
from your distribution
or your eyes location other so
you enter this website
and you hit the enter button.
As an error it
shouldn't have been.
Oh, I know what just happened.
When you do go ahead and create
your so-called distribution in
that you actually have an option
of selecting a by default file,
which I did not so
I will have to give an extension
here saying slash index dot HTML
and if I hit
the enter button now,
it should redirect you
to the demo tag with says
welcome to edu Rica, right?
So this was the HTML file
that we created and we
also had a PNG file
which we wanted.
Access the name
was logo dot PNG.
Okay, this is funny.
This should not happen.
Why is this happening?
Let's take a look at it.
Whether we have that file there
because if it was there we
should be able to access it.
And what was my bucket
this was the one
oh, this has happened
when I uploaded
that file it got saved
with this extension
dot PNG dot PNG.
So if I come here
and I type dot PNG here,
there you go.
You have that object delivered
to you through your Or so-called
distribution in this session.
We will be discussing
about Amazon cloudwatch.
So without any delay,
I'll walk you
through the topics
which we will be
discussing today firstly.
We will see
what Amazon cloudwatch has
and why do we need it?
Then?
We'll discuss certain Amazon
cloudwatch Concepts moving on.
We'll take a look
at two most important segments
of Amazon cloudwatch.
What chart Amazon cloudwatch?
Events and Amazon
cloudwatch locks and finally
to make the soil more fun
and interesting for you
of included Adam as well.
So let's get started first.
Let us try to understand why
we need cloud based monitoring
with couple of scenarios
in our first scenario consider
that you have hosted
a messenger app on cloud
and your app has
gained a lot of Fame
but clearly the number
of people using an application
has gone down tremendously
and you have no idea
what the issue is.
Well, it could be due
to two reasons firstly
since your application has
complex multi-tier architecture
monitoring the functionality
of every layer by yourself
will be a difficult task.
Don't you think and secondly,
since you're not using any kind
of monitoring tool here,
you wouldn't know
how your application
is performing on cloud.
Well one solution for that is
to employ a monitoring tool
this monitoring tool
will provide you insights
regarding have your application
is performing on cloud
and with the state.
You can make
necessary improvements
and you can also make sure
that your application is in part
with today's customer needs
and definitely after
a while you'll notice
that the number of people using
your application has increased
moving on to our next scenario.
Let's say your manager
as assigned you with a project
and he wants you
to make this project
as cost effective as possible.
So as you can see
in this project you using
five virtual servers
which perform highly
complex computations
and all these Servers are
highly active during data.
That is the and most
traffic during data.
But during nighttime,
the servers are idle by that
I mean the CPU utilization
of these servers
during night time is
less than 15% and yet
as you notice here
in both the cases you
are paying same amount of money.
You have to notice
two points here firstly
all your virtual servers
are underused during night time
and secondly you're paying
for the resources
which are not using
and this definitely
Is not cost-effective.
So one solution is
to employ a monitoring tool
this monitoring tool
will send you a notification
when they serve as our Idol
and you could schedule
to stop the servers on time.
So guys, this is one way to make
your project most cost-effective
and avoid paying
unnecessary operating costs.
Let's consider another scenario
for better understanding.
So let's say I have o stood
an e-commerce website on cloud
and during sale season
many customers are trying
to access my website
which Which is
definitely a good thing,
but for some unfortunate
reason application downtime
has occurred and you
guys have to remember
that I'm not using any kind
of monitoring tool here.
So little bit difficult
for me to identify
the error and troubleshoot
that in reasonable amount
of time and it's quite possible
that in this period
my customer might have moved on
to different website.
So you see that I've lost
a potential customer here.
So if I have had
a monitoring tool
in this situation,
it would have identified
the error in all yours.
Just itself and
rectify the problem.
Well at could have easily
avoided losing my customer.
So I hope guys with help
of these use cases you were able
to understand as to why we
need cloud-based monitoring.
So let me just summarize
what we have learnt till now.
We need monitoring firstly
because it provides a detailed
report regarding performance
of your applications
on cloud and secondly,
it helps us to reduce
unnecessary operating costs,
which we are paying
to the cloud provider
moreover it did.
Ex problems at all your stage
itself so that you
can prevent disasters later
and finally it monitors
the users experience
and provides us inside
so that we
can make improvements.
So while guys in this session,
we will be discussing
about one such versatile
monitoring tool called
Amazon cloudwatch
Amazon cloudwatch basically
is a powerful monitoring tool
which offers your most
reliable scalable and flexible
way to monitor your resources
or applications which
are currently active.
One Cloud it's
usually offers you
with two levels of monitoring
which are basic monitoring
and detailed monitoring
if you want to resources to be
eligible for basic monitoring.
All you have to do is to sign up
for 80-plus feet here
in basic monitoring.
Your resources are
monitored less frequently,
like say every five minutes
and you're provided
with a limited choice
of metrics to choose
from whereas in detail
monitoring all your resources
are monitor more frequently
like say every five minutes.
And you're provided
with a wide range
of metrics to choose from but
if you want your resources to be
eligible for detail monitoring,
you'll have to pay
a certain amount
of money according
to a SS pricing details.
Now, let's have a look
at few monitoring services
offered by Amazon cloudwatch
Amazon cloudwatch firstly
it provides a catalog
of standard reports,
which you can use
to analyze Trends
and monitor system performance
and then it monitors stores
and provide access to system.
And application
log files moreover.
It enables you to set
up high-resolution alarms
and send notifications
if needed and Amazon cloudwatch
also send system events
from AWS resources to AWS
Lambda functions SNS topics Etc.
So if you have not
understood any terms,
which I've used
here, don't worry,
we'll get to know more
about this terms
as we progress
through the course
of this session earlier.
I mentioned that Amazon
cloudwatch allows administrators
to monitor multiple.
Sources and applications
from single console
these resources include virtual
instances hosted in Amazon ec2.
Database is located
on Amazon RDS data
stored in Amazon S3
elastic load balancers
and many other resources
like auto-scaling groups
Amazon Cloud 12 Etc.
So guys now let's try
to understand Amazon cloudwatch
a little deeper firstly
we'll have a look
at few Amazon
cloudwatch Concepts
and then I'll explain you
how Amazon cloudwatch
actually operate So
it's metric or metric represents
at time audit set of data points
that are published a cloud.
So what I mean by that
is suppose let's say you have
three variables XY and z
and you have created a table
which has values of X
with respect to Y
over a period of time
in this scenario the variable X,
which have been
monitoring till now
is a metric so you can think
of metric as a variable
which needs monitoring next.
We have Dimensions.
Let's consider same variables
XY & Z Basically,
you had created a table
which has values of X
with respect to Y now,
let's create another table
which has values of X
with respect to Z.
So basically we have two tables
which describes same variable X,
but from two
different perspectives.
These are nothing
but Dimensions.
So basically our Dimension
is a name value pair
that uniquely identifies
a metric and Amazon cloudwatch
allows you to assign up
to ten Dimensions
to a metric then you
have statistics previously.
We had created two tables
which are values of X
with respect to Y
and as well as that you can
combine data from these tables
like to create a chart
or maybe plot a graph
for analytical purposes.
This combination of
data is nothing
but statistics statistics
are metric data aggregations
over specific period of time,
then you have alarm.
Let's say you have
been monitoring this variable X
for some time now and you want
a notification to be sent to you
when the value
of x reaches certain.
Short all you have to do
is set an alarm to send
you a notification.
So basically alarm can be used
to automatically initiate
actions on your behalf.
Now that you have
clear understanding of concepts
of Amazon cloudwatch.
Let's see how Amazon cloudwatch
operates Amazon cloudwatch has
complete visibility into your
AWS resources and applications
which are currently
running on cloud.
So firstly it collects metrics
and locks from all
these AWS resources
and applications.
And then by using this metrics
it helps you visualize
your applications on
cloudwatch dashboard moreover.
If there is some sort
of operational change
in a SS environment
Amazon cloudwatch becomes aware
of these changes
and response to them
by taking some sort
of corrective action,
like maybe it sends
you a notification
or it might activate
a Lambda function Etc.
And finally it provides
your real-time analysis
by using cloudwatch metric map.
So if you're wondering
What cloudwatch metric Mathis
it is a service
which integrates multiple
cloudwatch metrics
and creates a new time series
and you can view
this time series
on cloudwatch dashboard as well.
So working this way
Amazon cloudwatch provides you
with system by disability
it even provides
you actionable Insight
so that you
can monitor your application
performance moreover.
It allows you to optimize
resource utilization
if needed and finally
it provides a unified.
I'd view of operational health
of your AWS environment.
So I hope that by now
if you know
what Amazon cloudwatch has so
now let's try to understand
how Amazon cloudwatch works
with help of a demo.
So guys, this is my AWS console.
Let's say AWS Management console
and the services
which you can see
on the screen are the services
offered by Amazon AWS.
But in this demo we are going
to use only few Services.
Let's say cloudwatch,
and then you have easy to
and a service called
Simple notification.
Service and when I click on ec2
it takes me to ec2 dashboard
where you can see
that I have four instances
which are currently active,
you know that here in this demo.
I'm supposed to get
a notification saying that
CPU utilization of
my instances less than or
if a person for me to receive
a notification first,
I'll have to create a topic
And subscribe to it
with my email ID.
So let's explore
a service called
Simple notification service
where you can create
a topic And subscribe.
To it.
Once you reach
SNS dashboard click on topics
optional navigation Pane
and click 'create new topic
give you a topic a name.
Let's say CW topic and
if the display name as well,
let's give the same name
and click on create
topic option here.
You can see
that I've successfully created
a topic now click on the topic,
which you have created
and select actions
and subscribe to topic option.
Well, I want notifications
to be sent to me
in form of email you
Of different options as well
and form of Lambda function
or Jason Etc.
But I'm going to choose it
as email and give my email ID
which is her and then click
on create subscription option.
So now whenever AWS console
wants to send me a message.
It will send to the email ID
which are used to
subscribe the topic now,
let's go back
to cloudwatch dashboard.
So guys this is
my cloudwatch dashboard and you
can see different options
or navigation pane firstly.
I have dashboard
where I can view all
my metrics at same place.
Then you have alarms
which shows the list of alarms
which you have configured
and then you have
events and locks
which will be exploring later.
Our topic of interest
is the last one
which has metrics select
the metrics option here
and then choose ec2
and then / instant metrics
when you do that or list
of metrics will be shown to you
like Network out.
Soup utilization Network packet
in network packets out
and various other metrics
for various resources,
which are currently
active on your Cloud.
So but we are interested only
with CPU utilization.
So I'm going to type that here.
Well, it shows
the list of instances
which are active on my cloud
and I'm going to choose Windows
to instance and then click
on graph metrics option here.
Okay, let's select Windows to
only and then on the right side,
you can see you have
a alarm button when you click on
that a dialog box will be open
where you can configure
your alarm firstly.
Let's give alarm a name.
Let's say low CPU utilization.
And a brief description as well.
Let's say lower
than 25 percent lower
than 25 percent CPU utilization.
Now I'm going to set
the threshold Which is
less than 25% in this case
and on the light side,
you can see of period option
if you resources are eligible
for basic monitoring
the speed option
by default as five minutes.
And if your resources
are eligible for
detailed monitoring,
it's usually one minute
and when you scroll down you
can see a send notification
to option here so
select the topic
which you have previously
created that will be C Topic
in my case and then
click on create Allah.
But there is some error.
Okay.
It says there's an alarm
already with this name.
So let's give it another name
of my instance.
Now, let's try again
and when you click
on this alarm button And click
on refresh option here.
It says that I've successfully
created a alarm here.
You can see that low
CPU utilization of my instance.
And when you click on that it
shows you all the details
like description threshold
and what action it
is supposed to take
when alarm is configured
and all the details.
So guys try it out.
It'll be easy
for you to understand
cloudwatch console much better.
Okay guys.
Now, you know
what Amazon cloudwatch has
what it does and wait operates,
but to understand
the capabilities.
You have Amazon cloudwatch
completely we should be aware
of two important segments
of Amazon cloudwatch,
which are cloudwatch events
and cloudwatch locks.
So let's discuss them one
by one firstly we have
Amazon cloudwatch events
consider the scenario.
Let's say you've created
an auto scaling group
and this Auto
scaling group currently
has terminated an instance
so you can see this as some sort
of operational change
in area Bliss environment
when this happens
Amazon cloudwatch becomes aware
of these changes.
Changes and response
to them by taking some sort
of corrective actions,
like in this case.
It might send you
a notification saying
that your auto scaling group
has terminated an instance
or it might activate
and Lambda function
which updates the recording
Amazon Route 53 zone.
So basically what Amazon
cloudwatch Evans does is
it delivers a real-time stream
of system events
that describe change
in your AWS resources.
Now, let's have a look
at few concepts related
to Cloud watch events.
First TV happy Venter
and even indicates change
in a SS environment and
AWS resources generate events,
whenever the state changes.
Let's say you have terminated
an active ec2 instance.
So that state
of this ec2 instance has changed
from active to terminated
and hence an event is generated.
Then you have rules rules are
nothing but constraints
every incoming event
is evaluated to see
if it has met the constraint.
If so, the event is routed
to Target Target is is
where the events are handled
Target can include
Amazon ec2 instances
or a Lambda function
or an Amazon SNS topic Etc.
Now let's try to understand
Amazon cloudwatch events better
with help of use case
in this use case.
We are going to create a system
that closely mimics
the behavior of Dynamic DNS.
And for those who don't know
what Dynamic DNS has Let
me Give an example.
Let's say you want to access
internet at home then
internet service provider
assigned to an IP address,
but Since internet service
provider users different kind
of online systems.
This IP address keeps changing
because of which it
might be difficult
for you to use this IP address
with other services
like webcam security camera
thermostatic cetera.
So this is where Dynamic
DNS comes into picture
what Dynamic DNS does is
it assigns a custom domain name
to your home IP address
and this domain name
is automatically updated
when IP address
changes so basically
dynamic ANS is a service
that automatically
updates a name server
in domain name system
and Amazon office you
with a similar kind of service
called Amazon Route 53.
So in this use case,
we are going to update
Amazon dropped 50 3 whenever
a Amazon ec2 instance
changes its state.
Now.
Let's see how the use case
actually works this use case
precisely works this way.
So whenever an ec2
instance changes,
it states Amazon cloudwatch
event becomes aware of these.
Operational changes and it
triggers a Lambda function
this Lambda function
uses different kind
of information regarding
the instance like that's
public and private IP address
and it updates a record
in appropriate Route
53 hosted zone.
So let's say you have
an ec2 instance and you
have terminated the instance.
So Amazon cloudwatch events
become aware of this
and it triggers
a Lambda function
and this Lambda function
deletes the record
from Amazon Route 53 similarly
if you have created
a new instance,
Once again Amazon cloudwatch
events become aware of this
and it triggers
a Lambda function
in this Lambda functions creates
a new record in Amazon Route 53.
I hope you have understood
what Amazon cloudwatch even sees
and what it does.
Now, let's discuss
how Amazon cloudwatch events
works with help of a demo.
So in this demo,
we will schedule to stop
and start ec2 instances
with help of Lambda function
and cloudwatch events.
So let's go ahead with demo.
So guys, you can see
that I have four instances
which are currently Deaf first,
I'm going to create
a Lambda function
which is going to stop
my windows to instance
and you guys need to know that
for Lambda function to do that.
We need to assign permission.
So Amazon provides you
with the service called I am
which is identity
and access management
where you can assign
permissions when you search
for I am in the tab,
it shows you the service select
that and on IM dashboard
on the navigation pane.
You can see a policies option
here select that and click
on create policy option.
First it's asking you
for a service here.
We should be easy
to in our case click
on easy to function and actions
which will be to start
and stop may see two instances.
So let's search
for start instance.
Well, a predefined function
is already there.
So you can choose
that then you have stopped
instance again select
that And then I wanted to be
eligible for all the resources.
So I'm going to choose
all resources here and click
on review policy option.
Let's give our policy a name
that is to start
and stop ec2 instances
and description as
well a brief description.
Let's say to start
and stop instances.
And now click
on create policies.
It's taking a while.
So I've successfully
created a policy here.
Next we have to assign
this policy to Lambda function.
So click on rolls here then
click on create role choose
Lambda function here
and click on next permission.
Search for the policy
which we have created earlier
that is to start and stop
the found the policy select
that and click
on next view option
that's asking for a name.
Let's give a name
start-stop instances
and click on create role.
I've successfully
created a role.
So what we have done here is
we have assigned permission
for Lambda function
to control ec2 instances.
Now, let's create
a Lambda function.
You can search
for Lambda in the search
that and there R click
on create function give you
a Lambda function a name.
Let's say to stop instance
and select the role,
which you have previously
created and click
on create function.
You can see
that I've successfully created
and Lambda function
and now I'm just going
to copy the code to stop
ec2 instances here.
I'm going to select this
and paste it over here
and make sure to save it
as you can see here
in this function a task
for instance region
and instance ID.
So let's configure the details.
Let's give it a stop instance
and here you will have to insert
instance region and ID.
and Stan's region an instance
ID Novel have to copy
the instance region
and ID of the instance,
which I ever need.
So let's go
to ec2 dashboard here.
Now let's say I want my windows
to instance to be stopped.
But this is the instance ID,
which I'm going
to paste it over there.
similarly instance
region now Well,
in this case, I'm choosing
Windows to instance.
You can choose whichever
instance you want to stop.
Once you're done
that you click
on create option here test
the configuration details.
When you scroll down you can see
the execution results here.
It says that my instance
has been successfully stopped.
Let's go and check
and easy to dashboard here
on the ec2 dashboard.
I'm going to refresh
it and you can see
that my windows to instance
has successfully stopped now,
we'll create
another Lambda function
which will restart this function
again the same search
for Lambda function
in the search tab
and click on create function
option it ask for a name.
So let's say start instance.
And choose the role
with your previously
created and click
on create function again.
You'll have to paste the code to
start the instances over here.
And click on Save option.
Let's try to configure this.
Let's name it as start instance.
and again a task
for to our tributes which are
instance region and ID.
Now what we have to do is copy
the instance region and ID here
like we did earlier.
Let's go to easy to
dashboard and copy
the instance ID and region.
Well, you guys
can see that here.
My windows to instant has been
successfully stock now.
I'll copy this
and paste it over there.
similarly instance region as
well and click on create option
not test the configuration and
when you scroll down you can see
that my instance
has successfully restarted
in the ec2 dashboard.
I'm going to refresh this.
Well, my windows
to instance is on its way
to get restarted till now.
I've used Lambda function
to start and stop my instances.
But now I'm going to automate
this process with help
of Amazon cloudwatch.
So let's go to
cloudwatch dashboard here.
Well, it's taking a while to
load then choose events option
and click on create true.
So here we are going to share
Jewel to stop my instances
every day at 6:30 p.m.
And to restart this instances
every day at 6:30 a.m.
So click on schedule.
If you want to know more
about Grand Expressions,
you can visit
Amazon documentation.
So let me show you it has
six Fields firstly it's minused.
Then you have hours then day
of month day of the week
and your your concern.
Only with minutes and house
because we want
our instances to be start
and stop every day every month.
So let's give the details.
So if you're going to create
a rule to stop the instance,
let's say 6:30 in the evening
30 minutes and 18,
which is nothing but 6 p.m.
And then rest all you
don't have to mention anything.
When you give a proper
cron expression sample timings
would be provided to you.
You can see her the rest
of the sample timings
and now let's add
the target function
which is Lambda function
in our case and select
on stop instance function
and click on configure details
give you a rule a name.
Let's say stop my ec2 instance
and description to stop
my ec2 instance.
At 6:30 p.m.
Every day.
And click on create
video you can see
that I've successfully created
a rule to stop my instance
every day at 6:30 p.m.
Now.
Let's create another rule
to restart this instance
every day at 6 a.m.
In the morning.
Again.
The scene shows the schedule
here and cron expression
which will be 6 a.m.
In the morning.
Again, the sample time
is shown here.
Then that's that Target function
again Lambda function
and select the function
that is to start instance
and click on configure details.
Let's name it
as start my ec2 instance
and the scripture has
to start my ec2 instance
every day at 6 a.m.
And click on create.
So now we have successfully
created two rules to start
and stop the easy two
instances at 6:30 p.m.
And 6:30 a.m. Respectively.
So what we have done is we
have saved our time here.
We've automated the
process of stopping
and starting ec2 instances.
So try it on yourself.
It will be easier
for you to understand.
So guys now let's discuss
our next topic which is
Amazon cloudwatch locks.
Have you guys heard
of log files?
Well log files are nothing
but detailed record
of events that occur
when you are using
your AWS environment,
you can view a log files
on your on-premise server as
well search for an app called
Event Viewer select the app
and click on Windows locks
and select systems
or list of log files
will be shown to you
when you choose a particular
log file all the details
regarding the clock files
will be shown like the number of
keywords the login time number.
Of hours, the file
has been logged
onto and various other details.
Similarly.
You have log files created
when you use AWS
environment as well.
So you can consider this log
files is a data repository.
Most of the metrics are
generated from these log data.
So whenever a metric
is generated a part
of data is extracted
from this log data.
So you're designing metrics
according to your like
by choosing a part of data
from this log data.
So basically this log files are
what we call
a primary data store.
Please and Amazon cloudwatch
locks is used to monitor store
and access log files
from AWS resources,
like ec2 instances cloud
trail Route 53 Etc.
Let's try to
understand cloudwatch locks
better with help
of some features firstly you
can use Amazon cloudwatch locks
to monitor your application
and system log files.
Let's say you have made
a lot of errors,
but trying to deploy
your application on cloud
in this scenario.
You can use cloudwatch locks
to keep track of your errors.
And send a notification to you
when the error rate
increases certain threshold
so that you can make
avoiding errors again,
then you have log retention
by defaults logs
are kept indefinitely
but cloudwatch provides
you with an option
where you can set the period
between 10 years to one day.
Then you have locked storage.
You can use cloudwatch logs
to store your log data
and highly durable storage
and in case of system errors,
you can access raw log data
from this storage space
and then you have DNS queries
you can use Watch lugs
to log information
about the DNS queries
that Route 53 DC's
now let's have a look
at few Concepts
regarding cloudwatch locks
firstly we have something
called log even so log even
is just to record a fact
DVD that has occurred
in AWS environment.
It's straightforward.
Then you have locked
stream a log stream
as a sequence of log events
that have same Source.
Then you have something called
Law Group Law Group defines
group of lock streams.
That has same.
And access control
settings by default.
You have to make sure
that each log stream
belongs to one
or the other Law Group guys
not let's try to understand
cloudwatch logs better
with help of this use case
in this use case.
We are going to use
Amazon cloudwatch
looks to troubleshoot
the system errors,
you can see that I have
three instances here
and a cloudwatch agent
which is monitoring all
these three instances.
So what cloudwatch agent does is
it collects custom level metrics
from all these easy to instances
and then This metrics
and locks collected by the agent
are processed and stored
in this Amazon cloudwatch
Lots Amazon cloudwatch locks,
then continuously
monitors these metrics
as you can see here by then.
You can set an alarm
which will send you notification
when some sort of error
occurs in the system.
So whenever you receive
a notification saying
that some sort of error is there
in the system you can access
the original log data,
which is stored in Cloud
watch locks to find the error.
So this is how you can use
Amazon cloudwatch locks to
troubleshoot the system errors.
So basically you are having
a look at original data
so you can solve your problems
faster and quicker.
So this is it guys today
in this session.
We are going to discuss about
the service AWS cloudformation.
So without wasting
any more time,
let's move on to today's agenda.
So we'll start today's
session by discussing
why cloud formation
is actually needed
in the first place.
Once we're done with that,
we'll move on to the what of
what is cloud formation.
Actually after that.
We'll be discussing what things
are needed to get started
in the cloud formation service.
Now among those things.
You have a Json document.
So we will be learning
how to create a Json document.
So before that we'll
be seeing the structure
of a Json document.
Once we learn
the structure will see
how a Json document
actually looks like so we'll see
how a sample Json document looks
and in the end we'll be
doing a demonstration.
Ocean so in the demonstration
will be doing two demos.
The first one will be
a really simple one
and the other one will be
a little Advanced.
Let's move on
to the first topic.
That is why AWS cloudformation?
So why do we
need cloud formation?
So for example,
you have an application now most
of you guys know that for
and we have done this
in the previous sessions as
well that we created
an application right.
Now.
The application is
actually dependent on a lot
of AWS resources.
Now if we were to deploy
and manage all these resources
separately it will take up a lot
of time of yours, right?
So to reduce that time or to
manage all these resources.
What if I told you
you have a service?
Yes.
Yes, you got that, right.
So you have a service
called AWS cloudformation.
So using AWS cloudformation,
you can manage
and create and provision
all these resources
at a single place.
Now, this is
what cloud formation does.
But now what is
cloud formation exactly.
So a cloud formation
is basically a service
which helps you model and set
up your AWS resources
so that you can spend more time
on your application rather than
setting up and provisioning
these resources, right?
So basically It's a tool using
which you can create
your applications quickly.
Also, you can create templates
in AWS cloudformation.
Now, how do you
create templates?
Basically, you would be using
the cloud formation designer
you'd be putting in
all the resources
that are needed.
You would be defining the
dependencies of these resources
and then you'll be saving this
design as a template right now.
What will you do
with this template?
This template can be used
to create as many copies
as you want right?
Say for example Example
you have a use case wherein
you want your application
in multiple regions
for backup purposes.
Right?
So if you want
that you won't be implementing
or you won't be creating each
and every resource one by one
in each of the regions.
What you can do is you
will create it at one place
in cloud formation have
the template in your hand
and deploy that template
in the other regions as well.
Right?
So what will this do?
So first of all,
your replication will
be very precise,
right so they won't be
Any changes in the copies
that you have made second of all
you will be doing that quickly
because you don't have to do
the process all over again.
You just have to click a button
and that template
will be provisioned
or will be launched
in that region.
So this is what
AWS cloudformation is all about.
It makes your life simpler
by handling all the creation and
the provisioning part, right?
So this is what is
AWS cloudformation.
Now, how do we get started
in cloud formation says
it's a very useful.
Is how can you
as a user use the service
so let's move on.
So for using
the cloud formation service.
First of all,
you need a Json script now.
Why do you need a Json script
because you would be creating
a template right
in the cloud formation designer.
You would be using
the drag-and-drop option
and filling in the AWS
resources right now
when you will be doing
that in the back end
it will actually
be creating a Json script.
Now what you can do as a user is
if you're good in Json,
you can create
your own Json script.
Otherwise you can use
Cloud formation designer
to create a template now
for creating a template.
Like I said,
you need a Json script.
Now.
What is the Json script then?
So a Json script is basically a
JavaScript object notation file,
which is an open standard form.
And that means
it is human readable
so you can read it as well
as well as the computer.
So if you don't need the
programming knowledge for this,
what you as a user
would be doing is you
would be designing your template
in the cloud formation designer
and that will
automatically create.
Eight a Json script
you can do it.
The other side is well.
Like I said,
you can create your own
Json script and feed it
in the cloud formation designer.
So this is
how cloud formation works.
This is how you would
be using AWS cloudformation.
But then how can you
learn the Json script?
So it's very easy.
So basically you have
to follow a structure
in the Json document.
What is this structure?
So that structure is
like this you would be creating
the following Fields.
So the first field will be the
This template format version.
So this will basically contain
version of your template.
Next up is the description.
So description is a text-only
file or is a text-only field
wherein you will be describing
your template in words, right?
So if I'm a user
and I want to know
what your Json does
without reading your Json script
from beginning to end.
I can read the description
in simple English and understand
what ages from triple to right
then you have the metadata.
So metadata will basically
When the properties
of your template then
you have the parameters.
So any values
that you have to pass through
the template will be included
in the parameters
next comes mappings.
So mappings would basically
include the dependencies
between your AWS resources.
Then comes conditions.
The conditions are
basically the conditions
that you would be giving
to your template
when the Kristof will be created
or while the stack is upgraded.
So if we are stack
is being created
or their stack is being updated.
These conditions will be looked.
One two, then comes output.
So whatever outputs
your template will provide
or your creation of Stack
will provide will come
in the output header.
Then you have
the resources field.
So resources will basically
include all the AWS resources
that you want to include in
your infrastructure right now.
If you look carefully you
actually will be only dealing
with the resources part,
right because you will just
be populating in the resources
and creating the dependencies.
Right.
So basically you'd be populating
the resources part
and that is what it was all
about the resources,
but right now,
this is Theory now,
how does a Json document
actually look like right
a Json document looks
something like this.
So like I said,
you would be working
on the resources field, right?
So you'd be including
the resources field
and in that say you
so this Json document
is all about
if you had noticed
it's about S3, right?
So you are basically
including an S3 bucket.
It and the type you'd
be specifying the type
of service that will be
including this bucket.
Right?
Like in this example
a Json document
doesn't know what service
you're talking about.
So you specify the name
of the bucket
and inside the brace is
you'll be specifying
which service over here.
You'll be specifying
the S3 service.
Don't worry.
I'll be showing you guys
this Json document in a moment.
But before that
you should understand
how a Json document
is structured and this is
what we're doing right now.
Now guys, this is
the cloud formation dashboard.
Now, you have to create
a stack over here, right?
And for the creation of a stack
you require a template so
first we'll design a template
and then we'll create a stack.
So this is my cloud
formation designer.
Let's go back
to our slide and see
what we actually have to do.
So, this is our first
demonstration here
in will be creating a S3 Bucket
from cloud formation.
So we'll be designing a template
around that for first and then
we'll be deploying this code.
Right?
So let's do that.
So let's go to our cloud
formation window now
so we have to create
an S3 bucket.
So we'll scroll down
to the S3 service.
So here is AC Service.
We click on this we service.
Click on bucket
and drag it over here.
Right.
So this is
the recipe bucket guys.
Now you can edit the name
of the template over here.
You can name it
as either a car CF
that means and Eureka
cloud formation, right?
So you specify that now,
this is your Json code now you
can compare the Json code guys.
Let me make it a little
bigger for you guys.
Yeah.
So this is the Json
code guys now,
I didn't code
this Json script, right?
I just dragged
and dropped this Bucket
over here in cloud formation
and Automatically generated
this script comparing it
with the code
that we have
in our presentation.
Let's see so we have resources.
Yes.
We have resources.
We have the name
of your bucket part.
So basically this is
the name of your bucket
and then it's a type.
We're in you'll be specifying
this you service.
So you have type and specifying
the SC service over here, right?
So if you want to change
the name of the bucket,
we can do that over here.
Let's specify it as
and Eureka CF.
Alright, so we are done.
This is it guys this is
all you have to do.
So now for running this
in cloud formation,
all you have to do is click
on this icon create stuck.
Now this will lead
me to this page
which is the create stack page.
Now, it has automatically
uploaded this template
to the S3 bucket
and it has specified
the URL here, right?
We click on next you specify
the stack names.
Let's specify it as a lyric RCF,
right so you don't have to
specify anything are let's click
on next click on create.
So you'll be seeing
the events on this page.
Let's refresh this.
So it says create
in progress, right?
So my template is now
being created into a stack
and that stack will have
the AWS resource in it,
which is the S3 bucket.
Right?
So I think the time is enough.
Let's refresh it and check
if our stack has been created.
So it's still
in the creation phase.
Let's wait.
All right, so now it shows me
that the Creator is complete.
All right guys,
so let's go to our S3 service
and check whether we have Bucket
that are AWS cloudformation
created for us.
So we go to the AC Service.
And here it is guys.
So this is the bucket
that we created right?
I see you can see the time.
It's March 28th.
2017.
Today is March 28th, 2017.
And the time is 7 5
and the time is 7 7 here.
Alright, so this bucket
has just been created
by cloud formation.
So guys, like I said,
it is very easy.
It is easy to understand
and to deploy as well.
You basically just have
to create a template and
that is it AWS cloudformation
will do the rest for you
and the cool part is
that you can replicate
the template as many times
as you want.
Right?
So it will save you the time.
Okay this demonstration is done.
So we have created an S3 bucket
using cloud formation.
Let's see what our second
demonstration is all about.
So now we'll be creating
an easy domain students
in which we will be
deploying the lamp stack
which means in that
easy to instance.
You'll installing Linux
you installing a patch
a you'll be installing MySQL and
we'll be installing PHP as well.
Right?
So, let's see.
How will we do that?
So for our second demonstration,
we will again go back
to the cloud formation console.
We will click on create stack
and now we have
to launch a lamp stack.
So a lamp stack is basically
a sample template in AWS,
right so we can select
the sample template
and we'll click on view
or edit template in designer.
So a lamp stack is basically
an easy to instance
with Linux Apache MySQL and PHP
installed onto it,
right you can see the designer
that you have only specified
and easy to instance anyway
to ask the security group to it.
So you need
the security group obviously
because you have
to connect to this.
You do instance right now.
A lamp stack is basically
a web server remember?
Now, let's see the template
for this lamp stack.
So we discuss the structure
of a Json document
if you guys remember so
the first part was
the AWS template format version.
Then you have description.
Then you have
parameters so parameters
if you guys remember
it is basically the values
that will be passing
to the template right now.
If you are creating a lamp stack
you'd be needing
the database name
you'd be needing
the database password.
You'd be needing a lot
of things, right?
If you're installing MySQL
you be needing the username
you'll be needing the password.
So all of that you can feed
in here in the parameters
so you can specify the key name.
So if you are connecting
to the slough instance
through SSH connection,
you'd be needing a keeper right?
She would be specifying
the keep are here.
Then you will be
specifying the DB name
and the other detail now
how will that look
when you'll be creating a stack?
So let's do that.
We will click on this icon
which will now create
a stack automatically so
will be prompted.
It on this cage click on next
then you will reach this page
where in you are feeling
the entry right?
So you would specify
the stack name.
So this is by default
so stack name,
so we'll be specifying
the stack name first.
So I'll let us tag
name be lamb demo,
and then we move on
to the parameters part.
So whatever you specified
in the Json parameters field
will be reflected over here.
So we specified
DB name over here.
So it was asking me
for the DB name.
So let's give it as a rake.
And let's give the DB password
as something candy.
Be root password DB user
as a Eureka instance type
as Steven dot micro wide
even got micro because
if you guys noticed
in the template,
we didn't specify
a virtual private Cloud
that is a VPC now
all the instances
which are launched these days
of with all the new instances
which are there in easy to have
to be by default launch the VPC.
But since we are creating
a Json file and we
didn't specify a VPC you have
to select T' an older version
of your ec2 instance.
So let it be T 1 so T
1 is an older version.
It runs without a V PC as well.
And then you have to specify
a key name the key name
would basically be used
to create SSH connection
to your instance.
Right?
So our key pair was array
calendar score a will select
that and will click
on next now SSH location is
basically your IP address
if you want to specify I
don't want to specify it.
So we'll click on next you don't
have to enter anything over.
Click on next confirm
and click on create.
Now is happening
in the background as it
is picking up that Json file
and is creating a stack first
launch an ec2 instance.
It will then install the next
onto that it will then install
Apache MySQL and then
the end a PHP installation.
So what we will do the
once it says
that the creation is completed
we will go and check
if everything has been installed
on our server by creating
an SSH connection, right?
So let's wait until the stack.
complete Alright guys,
so as you can see
in the events
that the creation
is now complete.
So let's check
that if our installation
has been correct will go
to the ec2 instance.
Now this is our instance
which has just been created.
We can check that.
It's been created
on March 28, right?
So today is 28.
Alright, so now let's connect
to this instance.
So for that we will have
to copy the IP address.
This is the police officer.
For those of you who don't know
how to connect to easy
to you'll be pasting
an IP address here.
Right?
And then you have
this private file, right?
So this is of the pemex tension,
but the party software
needs a PPK extension.
So you have to convert
this pem file to PPK
that can be done using
the puttygen software.
So this is the footage
and software so I
will be dragging this file here.
Okay, it doesn't work.
So well click on load go
to downloads click
on all files select my pem file
click on Open click on OK
and then click
on save Private key.
So let's name it as a Eureka.
Underscore a click
on save so a file
has been saved will close it.
Go back to our party software
here enter the IP address here.
You will click on SSH
click on authentication.
Click on browse go
to your PPK file click
on open and click on open here.
So now you'll be connected
to your SSH through your SS has
to your ec2 instance.
So any Linux installation
on Your AWS infrastructure.
The login will be
easy to - user.
I see you're in let's see
if you can connect
to a MySQL installation.
So MySQL - Edge
so it is on localhost.
- P port number
which is your 6
and then the user
that we gave was a Eureka
and the password was this.
Okay guys, so we are in so
that means we successfully
created the Eddie Rekha username
which is specified
in the Json script.
That works.
Well and then you specified.
Okay.
We also specify
that we need a database right?
So, let's see if it
is showing a databases
or our databases
have been created as well.
Okay, so it has a data-based
called Ed, Eureka?
Right.
So the Json script worked.
Well now the thing
here to notice.
Is that how granularity you
can configure your Json file?
Right?
First of all,
it launched an ec2 instance
then install Linux
then install MySQL it
configured it settings
and inside MySQL it gave
you a database, right?
So this is awesome guys.
So this gives you
the whole control
of AWS just through Json script.
Right and this is the power
of cloud formation.
Now if you want
this infrastructure
or whatever you have created
right now to be replicated again
to some other instance
that can be done
with a single click of button,
right and it is
actually pretty awesome
because if you were
to install this lamp stack
on a server or on AWS again,
if you launch ec2 instance
with the Linux OS installing
Apache MySQL and PHP
may take time.
It actually takes time.
We can you have
to open the console.
All you have to open
the terminal you have
to enter the commands
and depending on
your internet speed you
will install all those packages.
So this is neat.
It does everything for
you automatically, right?
So guys, this is what cloud
formation was all about.
So I'll close the session.
Let me go back to my style.
All right, so guys we are done
with the lamb stock demo.
Today's session is going to be
on auto scaling and load.
And so so today
I'm going to tell you
how you can order
scale your resources
so that they become
highly available and this is
what we're going to do today.
All right.
So with that guys,
let's start with today's session
with the agenda for today.
So guys, this is
what we are going
to do today first.
We're going to see
what are snapshots
and am I so these are
basically the entities using
this using which you will be
or scaling your resources.
So once you know,
what are snapshots
in Amis will move on
to why do we actually need
or scaling and what?
Is auto-scaling exactly
after that we're going to see
what is a load balancer
and towards the end.
We'll be doing a Hands-On
which is going to
be very interesting
because I don't think
there's a demo out there
which can show you the kind
of demo that I'm going
to show you today.
All right, and if you think
about a guy's if you're
if you're thinking about moving
to the cloud industry order
scaling our load balancing
out the very important topics
in this in this in this domain,
right so you should
know about them.
So if you have been so
if you About them please
pay attention today
because you're going
and going to go and gain a lot
of knowledge today.
All right moving on guys.
Let's start with the first topic
which is snapshots
and am is so let us see
what are those so I guess
most of you are aware
of what an ec2 instances
of for those of you
who are not an ec2 instance
is just like a row,
so it's in fresh
piece of computer
that have just bought is
just like that, right?
So on that computer,
you can choose any operating
system that you want.
Want so once you have
the operating system,
you can install any kind
of software on it.
All right, so you have
to install every time you
launch a new in an ec2 instance.
You have to install all
the required software's on it.
All right, but
there's a workaround
what if you want
a specific configuration
of ec2 instance a want
five easy to servers
which are exactly like this
like each other, right?
So one way of doing
that would be to launch
a new instance every time
install the required packages.
Daytime and going about it,
right the other way
of doing it would be
to actually create an image of
once you will be configuring
your ec2 instance.
And after that you'll
be creating an image
of your ec2 instance.
And that using that image
you can actually deploy
for more easy to do servers.
All right, so this image
is basically what is
and am I so am I
which is an Amazon
machine image is nothing
but an executable image
of your already existing.
You do instance, right?
But before an am I
can be created there is
a thing called snapshot
now what a snapshots
snapshots are nothing
but the copy of the data
the data the copy of the data
that you have
on your hard drive.
So basically if you
have your C drive,
right and you want
to copy your C drive
you copy a CD drive
on to some external drive
so that becomes a snapshot
but if you can boot
from that external drive,
so that has to your whole
operating system comes up.
Some other machine
then it becomes an Ami.
So this is basically
the difference between
the two a snapshot is
not a bootable copy and Ami is
a bootable copy that you have.
Alright, so I hope
you got the difference
between what is in am I
and what is the snapshot?
So I'll repeat it again
and you use an Ami to basically
replicate an easy two wins
is easy to instance again,
so that you don't have
to do the configurations
all over again, right?
So now you'd be Oh,
we were we were to talk
about what is auto scaling.
What is load balancing?
Why do we need EMS
but be patient you
would be clear with everything
with the session.
All right moving on guys,
let's now discuss.
Why do we need auto-scaling
now before the right.
Now the way I will be going
through the session is I'll
be explaining you each topic
and then I'll show you it
in the AWS console.
All right, so we just discussed
what are snapshots
and what are a mere am I
so let me quickly show you
How you can configure our
how you can create
an Ami of an already
existing ec2 instance
in the AWS console.
So, let me give me a second.
So give me a second.
I'll just go to my browser
and my AWS console.
So guys, this is my AWS console.
I hope it's visible to you.
So the first thing
that you'll be doing
is you'll be going on
to your ec2 console
or all right.
So in your easy to console you
will have all your servers
that are running
right now, right?
So for the for the Sake
of Simplicity I have deployed.
I've already deployed to servers
which are server 1 and server
to now I have configured
them both with a purchase
so that they can have your
they can host a website.
Uh, let me quickly show you
how the website
actually looks like.
So if I go
to this particular IP address
of server 1 This is in part.
So what one right so this is
how the website looks
like right similarly
for my server to if I go to go
into my server to this is
how my server to be look like.
Here it is.
All right.
So these are my two servers.
Now.
What I want is I
will create an exact copy
or the of these servers
so that they can be replicated.
All right.
So when I say replicated
everything from software's
to this website will
be copied onto an image
and that copy or that image
when I will deploy it.
It will be deployed
inside one more.
He should do server in which
I don't have to do anything.
This website will be there.
I just have to go
to the IP address
and I can see this website.
All right.
So now what I'll be doing
is I'll be creating an Ami
of both the server.
So let's create an EMF
or server one first.
I'll select the server one.
I'll go to actions.
I'll go to image I
click on create image
and all I have to do is
give an image name for it.
So let me give the name
as live server one, right?
This is my image name.
I click on create image
and that is it.
It takes in your request
for Eating an Ami
and it does that right
pretty simple now similarly.
I will be doing it
for server to as well.
I'll select server
to I go to image.
I'll create an image
and I'll name the image
say live server
to So once I've done
that you can see the images
in your am I tab?
So if you look at over here
in the images section
you can look at Ami is
if you go to your aim is you
can see there are two images
which are just being created
which are in the pending State
as of now and they are live.
So one and lives over
to Now using these images you
can create any kind of server
that you can create
the exact same server
with just a click of a button.
All right, you don't have
to configure anything much.
Alright, so this is
how you create a new map
pretty straightforward guys.
Let's move on and discuss.
Why do we need auto-scaling now?
So you learned how to create
an Ami, let's go ahead
and stand auto-scaling and see
how they are connected
to Ami is all right.
So say you have an application
you have a website
and every machine now
this website is hosted
on server guys,
right and so was a nothing
but machines now every machine
has Has its limitation right?
For example say there's
this machine is say around
8GB + C i5 processor.
So say it can host
on hundred people.
Right only a hundred people
can come to this website
and easily and navigate
inside the website.
But if more than a hundred
people comes in this computer
or the server becomes slow.
All right, so say there are
a hundred people as of now
and they are trying
to access your website
and they can easily access.
Sit now your website
becomes a hit overnight.
All right, and now a lot
of people are trying
to access your website
which make sure
server overburdened now
in this scenario you
can do only one thing
that is deploy more servers
and distribute the traffic
equally among those servers
so that the requests
can be handled.
All right.
Now this thing is a manual task
and manual is a big No-No
in the IT world guys.
So we invented a service call.
Old Auto scaling
and using order scaling
what happens is it sees it
it actually analyzes
the kind of load
which is coming in right
and it deploys the server's
according to that.
So say around 300 people
are coming in and it sees there
that you need three servers to
handle those kind of requests.
It will do
that automatically, right?
And that is where your am
I comes in guys
because the new servers
that you will be launching
those new servers
have to be taken
out of some template right
so The first server has to be
the exact copy of the sorry.
The second server has
to be the exact copy
of server 1 the third server as
well has to be the exact copy
of server one, right?
And that is
where the am I comes in.
So what is what basically
happens is in the order
scaling service you
basically attach your Ami
which you created
and using that Ami it deploys
most servers, right?
This is why am I is significant
or this is how am I
is related to Auto scaling
and And this is why
do we need auto-scaling?
Let's move ahead and just
give us a definition that
what auto-scaling exactly is.
So like I said,
whenever you your load
increases and you have
to scale automatically up
and down you use Auto scaling,
so it's not only
about scaling up
that is when you load
increases a three or four
so as you have deployed and
never when you load decreases
Still Force, I was up
there to sitting I'd write
so that is not the case
with auto-scaling you can So
skilled down as per your needs
you can configure everything
which you can imagine
about scaling up
and scaling down
in the auto scaling properties.
All right.
So this is why
we need auto-scaling.
Now one more thing
that you need
with auto scaling is
if you would have noticed I
said the number of servers it
deployed gets deployed
in the order scaling.
So there are they
there are four servers
which get with get deployed you
during order scaling right now.
The traffic has
to be distributed.
It equally right.
So this traffic
which has to be distributed
has has nothing to do
with auto scaling.
It has to be done by
a separate entity.
And that is what we are going
to discuss in the next section.
But before that,
let me show you
how you can configure
or how you can configure
the auto scaling properties
and attach the related am I
so that the related servers
are launched right?
So let me go to my AWS console.
So here am I and
as you can see the aim
is have already been created.
They are lives over one
and live server to now
what I'll be doing is I'll
be creating auto-scaling groups
or I'll be configuring
the auto scaling properties
so that these servers
can be Auto scaled as
and when required right?
So before that I
actually have to create
a launch configuration.
Now, what is
the launch configuration?
So if you look at the a my guys
you have only specified
what kind of data should be
there in your server.
What you have not specified
is what kind of machine you
should launch every time
there's a need right?
So that is exactly what you do
in launch configuration.
So you have the data but you
don't have the information
about the kind of machine
that you want to launch so
that that that kind
of stuff you will be specifying
in the launch configuration.
So what I'll be doing
is I'll click
on create launch configuration
and now it will give me a wizard
as same as that of any issue.
So right in the ECU server.
I had to choose
an operating system,
right so same
it'll give me the wizard
but I don't have to go here.
I'll have to go
to a separate tab,
which is called
my m is right,
so I'll select my mice
and now I'll select
the newly created a match
which is the Mi
which I just created
which is say we are creating
a launch configuration for us
over one right now.
So I'll select the lives of A1.
I'll click on select
and now it will ask me the kind
of the configuration
that I want for my So right
so I need attitude or micro
because we are doing
a demo today,
right so we don't need much
of of computing power.
So we just have to select
E2 dot micro and will name
a launch configuration a thing.
So let's name it as life.
So one.
Right and the I am role
is not required and
I click on next now.
It will ask me for adding
the storage so easy be is enough
for anyone to machine.
I'll go to
configure security groups.
Right?
And in this regard to groups.
I just have to add the HTTP rule
because I have to connect
to all the instances
that I'm launching.
Right?
So I'll select the HTTP
rule from here right
and I click On review
so that is it guys.
Nothing else has
to be configured you.
All right, and it is asking
me to check everything
that I've just configured
everything seems fine.
I click on create
launch configuration.
Now it last me for the keeper.
Right?
So every server
which will be launched
it will be associated
with the with a key pair which
will be specifying here right?
You can create a new one
if you don't have already I
already have a key pair.
So let me choose my my keeper
so that is a month underscore
to and I acknowledge
that I have this keep your
and I'll create
the launch configuration.
It just takes a second
or two to do
that and we are done.
Alright, so now we have created
a launch configuration.
We have specified what kind
of machine we want.
We specified what kind of data
should go into that machine now,
we'll be creating
the auto scaling group
in which will be specifying
in which cases we want
to Auto scale.
All right, so let's create
an auto scaling group now.
All right.
So it has automatically picked
up the launch configuration
that we have just created
that it's life.
So one right let's name this
group as live server one group.
Right.
And what is the initial size
that you want
in your launch configuration?
That is the minimum number
of servers that you want.
So let it be 1
and remember guys.
This is the most important part
when you are creating
a launch configuration in sure
that you're doing it
in your default VPC to be
on the safe side
because there are
a lot of settings
that you have to do
if you create a VPC on your own
and that becomes a hassle.
All right, so
if you accidentally
delete your default VPC,
which I did right so you have
to contact the AWS support team
and they'll help
you out with it.
They'll basically
create one for you.
You cannot create
one on your own.
All right.
So always ensure
that you are in a default VPC
whenever you're creating
an auto scaling group.
Alright, so now I
will be specifying the subnets.
So basically you have
to select a minimum number
of to subnets right?
I'll need not getting
into what I said Nets
because then it will be
like a three-hour session.
I will click
on configure scaling
properties now over here.
You can specify the properties
that I was talking about that.
When do you want
your server to scale?
Right so over here
you can specify
the average CPU utilization.
Now, what do you mean
by average PT CPU utilization?
So there are four servers
running as of now, right?
So it takes the average
of all the four servers.
All right,
and if the average goes
beyond whatever number
you're specified here
say I specified.
70 over here, right?
So in that case whenever
the average pcpd utilization
will go beyond 70 it will launch
one more server similarly.
If it goes I can configure
one more property here,
which says if it goes below 20%
like scale down from one server.
All right.
So if there are five servers
in there and see people ization
has gone less than 20 percent
it will it will it
will scale down from one.
Seven and come down
to four servers.
All right, and you can also set
how many seconds should it
paid say the traffic
is spiking down and up
like to frequently, right.
So for that what you can do
is you can set a time.
So if the 20% Mark
has been not cross still say
like five minutes,
then it will scale down a server
or if the seventy percent
Mark of the CPU utilization
has been crossed
over five minutes.
It will then scone.
Scale up, it will not scale up
with at only once
for only one second.
It becomes 71 person.
All right, so you can specify
all of that over here.
But since I cannot load test
my instance over here,
I'll just keep it
at its initial size
with just means
that it will even
if I delete my instance
that is I one instance has
to be there in any case
if I delete the instance it will
automatically launch it again.
Alright, so let's will select
the keep this group at an edge
at its initial size and we'll go
to configure notifications.
So I don't want to configure the
notifications neither the tags,
I click on review
and I'll click on create
auto scaling group.
Alright, so I've successfully
created an auto scaling group
for my life server one.
All right.
Similarly.
I will do the same steps
for my server to as well.
I'll click on create
auto scaling group
and I'll select
a launch configuration
which was there.
For my so to so not done
that so let's create
a launch configuration first
for us over to will go to a mice
and we'll select
the server to part here.
Alright, so I've selected
server to I do the same steps
that I did earlier.
Right.
So let me give it the name
as live server to group.
I click on add storage configure
Security Group over here.
I'll add the HTTP rule.
Click on review
and launch configuration
select the key pair.
Acknowledge it create
lawn configuration doing
the same steps Kuiper
not doing any new thing here.
I've traced
launch configuration.
Now.
I create the auto scaling Group,
which is life's
over to group.
Right and then the vpz
as I said should be default
subnet minimum gruesomeness.
You should select You'll click
on scaling properties.
I keep it at initial
size configure review and create
the auto scaling group.
All right, nothing much guys.
So same things that I did
for my server one.
I've done for
my server to as well.
All right, so
since I've created
or or an auto scaling group,
if you go to your ec2 dashboard,
you would notice
that two more servers
are now being deployed, right?
So you can actually
identify them over here.
See these two servers
are being initialized with Eva.
These have just been created
by your auto scaling group
because we specified
that a minimum number
of one server should be there
at all times right now.
If you try to go
to the IP address
of this server.
Right, you will see
that it will have
the exact same settings
for my easy Tucson's instance.
So this is my sober one.
Right.
So as you can see a new instance
called created but with
the exact same settings,
I hadn't had to do
anything it automatically
created an instance
with the same settings.
All right, and same is the case
with server to as well guys,
if I go to my server
to and try to access it.
I'll see the same things
over there as well.
So I'll Show you a bit Yeah,
so this is my server to alright,
so my auto scaling group
is functioning fine.
So let us come back
to our slide now.
So we are done
with auto-scaling now.
Like I said,
you need to have an entity
which will equally divide
the traffic between the servers
that have just deployed right
so they say in I've created
to Auto scaling group Skies as
of now write the
and why I have created
a second Auto scaling group.
I will tell you in a bit,
but for now understand that
there is an auto scaling group.
All right and inside
that auto scaling group say
there are Five servers and
if a person is coming
in or a customer
who has logged onto
your website is coming
in How would how would
his traffic be treated?
How would he know
which server to go to right?
So there comes
in the third entity
which is called
the load balancer.
So what load balancer does is
a load balancer your customer
will basically basically
be coming to your load balancer
and the load balancer
will decide based
on the usage of yourself.
Others that which server is
more free and then we'll give
the connection to that server.
All right.
So this is basically the role
of a load balancer.
So like I said a load
balancer is a device
that acts as a proxy
and distribution Network
or application across a number
of servers now,
I've been saying it repeatedly
that your your servers
are actually sorry.
Your traffic is actually
distributed equally
among the servers right
but in a few moments, I'll tell.
That there is one more
one more way of Distributing
your traffic, right?
So before that,
let me again stress
on the point
that this was your auto
scaling group guys.
This is just the example that I
took in the beginning, right?
So there are like these set
of users and they're trying
to access your website
and they are being routed
to these server.
So this routing is actually done
by a load balancer right now.
Like I said the traffic
which is distributed
it is distributed
in in two types, right?
The first time would be
to equally distribute them
among the number of servers
like say there are five server.
So it will distribute it
among the file servers.
But if there are say there
are two kind of servers now
and so your load balancer
can identify what kind of
request is being made by a user
for example in your website
on in your application
you have you have a part
where in you can
process the Mitch
right and you have a part
where you can where you have
the your blogging section.
All right.
So if you want
to process the image,
you want your traffic to go
to a different set of servers
which are order scaled
at their own in their own
Auto scaling group.
Right?
And if you have
the blogging section,
you have a different
order scaling Group,
which is auto scaled
at a different weather
different Auto scaling group,
but you want everything to go
from one single link.
So the way to do that is using
an application load balancer.
So let me just repeat
what I just said.
So the say the this set
of servers they host
your image processing part.
They do all
your image processing
and these set of servers
that they host your blog's that
you have on your application.
All right, a user comes in.
He just logs onto your website
and he goes to a URL
which says say Eddie record
or KO / image.
All right.
If you go / image
your load balancer, we'll see.
Okay, he's asking
for the image kind of content.
So he should go
to this set of servers
because this this service
of the image purpose and
if you go to a Dirac array card
or KO / blog your
load balancer identify.
Okay, this user he is asking
for the blog content.
So you should go
to this set of servers.
All right.
So all of that is done
using your load balance or
if you compare it
with a classic load balancer it
is it does not have that kind
of Of intelligence, right?
What it will do is
basically all the traffic
that it has got in coming to it.
It will equally distributed
among the number of servers
that are under it.
All right,
but with application load
balancer you have this option
where in you can divide
the traffic according
to the needs of the customers?
All right.
Now when you have divided
the traffic again the same thing
will happen here as happens
in classic load balancer
that at this point it
will equally Traffic
among the number
of image servers, right
and similarly the people
who want to access
the blog it will equally
distribute the traffic
among the number of people
who want to access
the blog server.
All right.
So this is what an application
load balancer is all about.
So classic load
balancer was something
which was invented earlier
and these days nobody uses the
classic load balance anymore.
People are using application
load balancer, right?
And that is
what our demonstration
is going to be.
All about today.
All right, so enough of talks.
Let's move on to the hands
on that is the demo part.
So let me quickly show you
what we are going
to accomplish today.
So basically a user
will come in.
He will have the address
of your load balancer.
And if he asks
for the image path
or say server one in our case,
he will go to the auto
scaling group of server 1
if he asks for server
to he will go to server
to but all of them
will have the same at Is
that is using your address
of your load balancer?
All right.
So this is what we are going
to accomplish today.
Now for those of you
who didn't understand
that why did we create
to order scaling groups is
because we want these servers
that is the image processing
service to be skated as well.
And as as at the same time,
we want the Blog shows
to scale as well.
Right?
So that is the reason
we want we created
to Auto scaling group.
So I dated a server one,
which you can imagine is
for your image processing
and I created an auto
scaling group for server
to which you can imagine is
for your blogging section.
Right having said
that guys now,
let's move on to my AWS console
and go to our load balancers.
All right.
So what I've been doing
now is I'll be creating
a new load balancer
and that load balancer
would be of the type
application load balancer.
You can see I have
two options here.
I either I can create
a classic load balancer
or I can create
an application load balancer.
So I'll go on with
application load balancer
and I will name it
as life load balancer
and the scheme
is internet-facing.
So since mine is a website
that I want you
guys to access right
so it could be internet-facing.
Otherwise you if you
are working in a company
and that company wants.
A load balancer
for their internal websites
that the companies have
you can actually opted
for an internal internal
load balancer as well.
But since as we have
a website and we want
that to be used via we
will use the internet
facing load balancer,
right and the listeners,
it's HTTP, that's fine
and the availability zones.
Like I said,
you have to select a minimum
of two availability zones
and you click
on configure security settings.
All right.
So now you'll be specifying
the security group, right?
So in Security Group,
you'll it's better to create
a new Security Group.
Remember guys don't include
the default Security Group
for your load balancer.
It's a good practice to always
create a new security group
so that you can customize
customize your rules
according to your needs.
All right, so I'll create
a new security group
and specify the HTTP Rule
and I click on next.
And now comes the part
where in will be
specifying the targets.
All right.
Now what our targets now
in application load
balancer guys targets
are basing basically
but or scaling groups, right?
So Target one would be
your or scaling group
one your target to would be
Auto scaling group to Target
three Target for you
can have as many targets
as you want.
But in this wizard,
you have to specify
a minimum number one, right?
So we'll create a new
Target group will call it
as say Just killing
a life or two one.
All right,
and the protocol is HTTP Port is
80 will click on next
and I'll review everything.
I think everything is fine
and I'll create
this load balancer,
right so we have not done
all the settings guys.
I'll show you how to do
all the settings for now.
We are just created
a plane load balancer.
All right, so I have
created a load balancer
which is pointing
toward Target group.
Group one and that Target group
is not pointing to my auto
scaling group as of now.
All right, we will do
that now in this part
so we have created.
I just created a Target group
called live Auto one.
I'll create one
more Target Group
which will be called live Auto
to for my second
Auto scaling group.
All right, so I will
create this and done.
So I now have to Target groups
that is live Auto one
and live Auto to now these two.
Get groups have to point
to my auto scaling
Group C respectively.
All right.
Now the way to do that you
cannot appoint them here.
You have to go to your auto
scaling groups, right?
And in your auto scaling groups,
you have to select
the auto scaling group
that have just launched.
So it is live server one group
and lies over two groups.
So you I will go to live so
one group and go to details
and over here you click on edit.
All right, and inside edit
you have this option
for Target groups.
You don't have to specify
anything in the load balances.
This option is only
for classic load balancer,
but we are creating
an application load
balancer, right?
So we'll be specifying
everything in the Target groups.
So for live server one group
will be specifying
the demo server one.
So demo server one
has already been sorry.
Sorry, it will be live Auto
One the target group
that I just created
and live Auto One is connected
to your load balancer.
So basically your load balancer
will point to your target group
and your target group
is now pointing
to your auto scaling group
one which are pointing
to your instances.
All right.
So this is how it
the visibility comes in
so I save it.
The target group one is
live server one group
and the target group 2.
I'll be specifying in
the second Auto scaling Group,
which is here that is live
or two to write.
I'll save it and let
me quickly verify
if I've done everything, right?
So this is a lifesaver one group
and this is live Auto One Fine.
This is lice over to group
and it is live or to to fine.
So my load balancer can now
see the auto scaling groups
that I've just configured.
So let me quickly go
to my load balancer.
Now comes the part guys
wearing I'll be specifying
when to go to auto scaling Group
1 and when to go
to auto scaling group to like I
said will be specifying it using
the using the kind of request
that the that the user
has made, right?
So the way to do
that is using is by first
selecting your load balancer
and going to listeners.
So once you go
to listeners guys,
you will reach this particular
page now in this you have
to click on view or edit rules.
Alright.
So once you click
on view or edit rules,
you will reach this page
which is kind of an if else
which is kind of FL structured.
So now what will you do is
so you can see
that there is
a default rule as of now
that anything any requests
which is made it will go
to live Auto one.
All right, which means
any requests at which is made
it will straight away pointed to
the auto scaling group one now,
we'll specify if
the request is our is
if the user is asking for sir.
To he should be pointed
to server to so let us do
that the way we'll do it is
like this will click
on ADD rules will click
on insert Rule
and now I'll specify so you
have two options here either.
It could be the routing
could be based on your host.
That is the address
of your of your website or it
could be based on the path.
Now.
What is the difference say
Eddie record or Co this is
the host name right now if I try
If I type in
resources dot Ed u--
record or go it is still
point to my domain.
But if I have specified
resources dot ID record or go
and if I write it over here
and I specify it has
to go to server
to it will go to server
to otherwise if you type
in resources or Daily Record
or code nothing will happen
because now if you
have not configured anything,
right, so that is the host path
with paths the difference.
Is that say you right
Eddie Ricardo Coast.
- block right.
So that's / blog
becomes the path.
But with host the thing
is the difference is
resources dot edu record orko.
So that becomes
one host name, right?
But with path you're
basically putting a slash
and you are going
into a particular folder.
All right, so you can specify
the path here, right?
It doesn't matter
if you have not specified
in a server for different
for different say you could
the way you could have done.
The image processing
and block the other way round
rather than having it
on two servers was
that you have you could have
configured it inside to servers
in your root directory, right?
It could be server one
for your image processing
and server to for your blog's
but I don't want
that because you're
as distributed as a system.
Is it becomes
more reliable, right?
And that is the reason we
have two different servers
for two different set of things.
So the way you can route
your traffic to body servers is
by typing in the path.
So say if I have
to go to server one.
I'll type in server 1 /
star so star basically
means anything after server
one could can be accepted
but it has to go
to the request will be forwarded
to live Auto one.
All right, so if I
have server one in my path
anywhere in my path,
it will go to live Auto one.
So I'll save this rule.
Similarly, I say that if it
has a server to in its path
and anything after that.
It has to go to live Auto
to write and save it.
And that is it guys now
my load balancer has now
has saved its settings.
Let's hope for the best
and try executing it.
So this is the Ling guys,
right if you just
type in this link,
it will by default
go to server one.
Right.
So if I go to this link,
you can see it is going
to server one as of now,
but if I specify /
server 1 it will go
to my server 1 and
if I specify / server, too.
It will go to my second server.
Now.
You might be wondering
that he meant you might have
a different directory
in your same server.
So let me clear your doubt
according to that.
So what I'll do is I will go
to my ec2 dashboard, right
and so you have to server one.
And I'll quickly show you.
If what happens
if I type in server to hear?
All right, so this is
the IP address, right?
So if I type
in this IP address,
I'm going to server one.
If I type in / server
to it will give me a photo for
because there is no folder
called server to write
same is the case here.
So if I go to is IPL,
you can see Server one.
If I don't specify anything
after my address it will still
go to the same server
that is here.
That is this.
IP address right
but if I specify /
over two over here It
will not be able to do so
because this is
not a load balancer.
It is directly your IP address,
but over here
if I specify server to.
It will redirect me
to the second server one second.
Right, it will redirect me
to the second server and
that is all that I need.
All right.
So with one address you are
actually pointing to two servers
which be solving
your to problems.
Now the real life you skate.
Like I told you it could be
four different kind of task say
you have a blogging
section on our website
and you have an image processing
section on our website.
If you want
to different servers to host
your two different Services,
you can do that easily using
a load balancer.
Alright guys.
So with this I
conclude my session
for today today in this session.
We'll be talking
about Cloud security
without making any further Ado.
Let's move on to today's
agenda are to understand
what all will be covered
in today's session.
So we'll start of the session
by discussing the why and what
of cloud security after that.
We'll be seeing
how we can choose
between a public or private
and hybrid cloud.
For that we'll see
whether Cloud security is
really a concern among companies
who are planning to make
a move on the cloud.
So once you have established
a cloud security
is really important.
We'll see how secure
should you make
your application after that?
We'll be looking
into the process
of troubleshooting a threat
in the cloud after that.
We'll be implementing
that process in AWS.
So guys, this is
our agenda for today.
Let's move on to the first topic
of today's session
that white cloud
security is important.
So let's take an example here
and talk of three very
popular companies linked in Sony
and iCloud so LinkedIn in
2012 experience the cyberattack.
We're in 6.5 million
usernames and passwords
for made public by the hackers
after that soon experience
the most aggressive
Cyber attack in history
where in their highly
confidential files
like the financials
their upcoming movie projects
were made public by
the hackers, right?
And this made a huge impact
on the business front of Sony.
ICloud which is a service
from Apple also
experienced a Cyber attack
where in personal
or private photos
of users were made public
by the hackers, right?
So guys now in all
these three companies
you can see there's
a breach in security
which needs to be addressed.
Right?
So Cloud security
has to be addressed.
It needs to be there
in the cloud computing world.
So since now we've established
that cloud security
is really important.
Let's move on to understand
what cloud security actually is.
So what is cloud security?
So it is a use
of latest Technologies
and techniques in programming
to secure application,
which is hosted
on the cloud or the data,
which is hosted on the cloud
and the infrastructure
which is associated
with the cloud computing.
Right and the other part
of this is that
whatever security techniques
or whatever techniques
or technology
that Using to secure
application should be updated
as frequently as
possible because every day
new threats are coming
up right everyday.
There are new work
around two problems.
Right and you should be able
to tackle these problems
or these workarounds and hence.
You should upgrade
your security as frequently as
possible Right Moving ahead.
Let's understand how
we can choose
between a public a private
and a hybrid Cloud.
So we have understood
that what cloud security charity
actually is now
let's talk in terms
of security and understand
how we can choose
between a public private
and a hybrid Cloud.
So if you were to choose between
these three infrastructures,
what should be our basis
of judging which Cloud
we should choose right?
So you would offer
a private Cloud
when you have highly
confidential files
that you want to store
on the cloud platform right now.
There are two stories or there
are two ways of thinking
a private infrastructure.
You can either
offer private servers
or private infrastructure
on your own from Isis
or you can look up
for servers dedicated servers
by a cloud provider.
Right?
So that all comes under
the private infrastructure.
Then we have
the public Cloud infrastructure
in public Cloud infrastructure.
You would basically use websites
that are public facing.
So say if you have
a products page
where you have application
which can be downloaded
by the public
so that can be hosted
on the public Cloud
because there is nothing
that has to be seen.
Secret over there, right?
So things like websites
things like data
that is not confidential
and you don't mind
public seeing it can be hosted
on your public Cloud.
The third infrastructure is the
most important infrastructure,
which is the
hybrid infrastructure.
And this is the set of
that most companies
go for right?
So what if there's a use case
wherein you have private files
of Highly confidential files
and a website as well, right?
So if you have this kind
of use case Might go
for a hybrid infrastructure,
which is kind of Best
of Both Worlds,
you get the security
or the Comfort
or the private infrastructure
and the cost effectiveness
of the public Cloud as well.
Right?
So you your hybrid
cloud is basically
if you want your highly
confidential be stored
on your own from Isis
and your website be hosted
on your public Cloud.
This infrastructure would be
a hybrid Cloud infrastructure.
So basically you
would choose a private Cloud
if you have a highly
confidential files,
if you choose a public Cloud
if you have files that are
not that important or files
that you don't mind people
seeing and you would choose
a hybrid Cloud infrastructure
if you want Best of Both Worlds,
right?
So this addresses
how we can choose
between a public private
and hybrid Cloud moving on.
Let's understand whether Cloud
security is really a concern.
So we will discussed
that white cloud security
is important we've discussed
what is cloud security, right?
Now let's talk about
whether this really makes sense.
Right?
So if we say
that cloud security is really
important in this is no one
who is actually
thinking about it.
There's no point, right?
So let's see
if companies were making a move
to the cloud actually think
about Cloud security.
So here's a gardener
research on companies
who are making a plan
to move to the cloud
or who has not moved
to the Cloud yet, right.
So what are their concerns?
Why not they're doing so so
the topmost First reason listed
by these companies was security
and privacy concerns, right?
So as you can
see these companies
who want to make a move
to the cloud are also worried
about the security
on the cloud infrastructure.
And this makes it clear
that cloud security is actually
very important right now.
We have understood
that cloud security
is very important.
We have understood
that companies are looking
for cloud security
are actually following.
The practices
for cloud security,
but now how secure should you
make your application?
Right?
What is the extent
to which you should make
an application secure?
So let us start with this line.
So it is said
that cloud security is a mixture
of Art and Science right
why let's see
that so it's a science
because obviously you have
to come up with new technologies
and new techniques
to protect your data
to protect your application,
right?
So it's a science.
Because you have to be prepared
with the technical part,
but it is art as well.
Why because you should
create your techniques
or you should create
new technologies in such a way
that your user experience
is not hindered.
Let me give you a guy's
an example suppose you make
an application right
and for making it
secure you think okay
after every 3 or 4 minutes,
I'll ask the user
for a password right
from the security point of view.
It seems okay,
but from the users point
of view it Actually hindering
his user experience.
Right?
So you should have
that artist in you
that you should understand
when to stop or till
where should we extend
your security techniques
and also you should be creative
as to what security techniques
can be implemented
so that the user experience
is not ended.
For example, there is a two-step
authentication you get there
when you're logging
into your Gmail account, right?
So if you know your password
that is not enough you should
have Have an OTP as well to log
into your Gmail account, right?
So this might be hindering
with user experience
to some extent but it is making
your application secure as well.
Right?
You should have a balance
between your science
and the art part
that you're applying
on cloud security moving on.
Let's now discuss the process
of troubleshooting a threat
in the cloud.
So let's take an example here.
So like you're
using Facebook right
and you get a random message
from Person saying there is
some kind of stories
like you usually get
that by using Facebook right
that such and such thing
happened and click here
to know more right you get
the similar kind of message here
and by mistake you actually
click on that link.
You didn't know that it's a Spam
and you click on that link.
Now what happens
is all the users
that are there are all
your friends on the Facebook
Chat gets that message,
right and they get furious as
to why this kind
of spam messages.
They're in their inbox, right
and you get scared.
Now you get angry as well
and you have to bring your
frustration out on Facebook.
So you contact Facebook
and it get to know
that they already
know the problem
and they're already working on
it and then near to this leash.
Now.
How did they come to know
that there is
this kind of problem
and needs to be solved.
Right?
So basically Cloud security
is done in three stages.
So the identification process
or the thread identification
process is done.
Three stages the first stage
is monitoring data.
So you have ai algorithms,
which know what a normal
system behavior is
and any deviation
from this normal system
Behavior creates an alarm
and this alarm is then
monitored by the cloud experts
or the cloud Security
Experts sitting over there.
And there's a thread they
see there's a thread they go
to the next step
which is gaining
visibility, right?
So you should understand
what caused that problem right?
And Or who caused
that problem precisely.
So your Cloud Security Experts
look for tools,
which give them the ability
to look into the data and find
or pinpoint that statement
or pinpoint that event
which caused this problem.
Right, so that is done using
gaining visibility stage.
And once we have
established, okay.
So this is the problem
then come stage 3
which is managing access.
So what this basically will do
is it will give you a list
of users in case
we are tracking the
who will give you a list
of users who have access
and we will pinpoint
the user who did that,
right and that user can be wiped
out of the fit system using
the managing exist age.
Eight.
So these are the stages
which are involved
in Cloud security Now
if you were to implement
these stages in AWS,
how would we do that?
Let's see that
so the first stage
was monitoring data, right?
So if you have an application
in AWS and you are experiencing
this same kind of thing,
what will you do
for monitoring data?
So you have a service in AWS
called AWS Cloud watch now,
what is AWS Cloud watch?
So basically it's
a Monitoring tool
so you can monitor your ec2
and your other AWS
resources on cloudwatch
how you can monitor them.
You can monitor the network
in network out of your resource
and you can also
monitor the traffic
which is coming on
to your instance, right?
You can also create alarms
on your Cloud board.
So if there's deviation
from normal system Behavior,
like I said,
so it will create
an alarm for you.
It'll escalate the event
and alert you about that thing
so that you can go on
around and see See what
that problem actually is, right.
So this is cloud
the monitoring tool, right?
So this was about
AWS Cloud watch.
Let me give you a quick demo
of how the AWS Cloud
watch dashboard actually
looks like Okay.
I said this is
your ews dashboard.
So now for accessing cloudwatch,
you can go under the management
tools here is cloudwatch
Will click on cloudwatch.
Now over here you
can monitor anything right?
We'll go to Matrix.
And you can see there are
three Matrix over here.
You can monitor your EBS.
You can monitor your ec2.
You can monitor your S3
right now suppose.
I want to monitor my ec2.
So as you can see,
so I have two instances running
in my easy to one is called
for batch instance.
And the other is called
WPS instance right now.
These are all the metrics
which are there
so I can check Matrix
for my WPS instance
for network in I can check
the disk read Ops.
So let me select
the network out metric
and they'll be a graph over
here so I can see this graph
and as you can see
between six o'clock and 6:30,
I experienced.
Search in my traffic, right?
So basically this is
how you monitor
your instance in cloudwatch.
And you have all
these default metrics to check
how your instance is doing
and you know AWS, right?
So this is what cloud watches.
You can also set
alarms here, right?
So if you go to alarms
click on create alarm.
You go too easy, too.
And you can select your metric
from over here now
select a discrete bite.
So we're now once I do
that will ask me
if there's a Time range
to which I want to monitor
that instance, right?
Okay, let's not set.
Any time Ray.
Let's click on next.
So when I click next you
will be prompted with this page
so you can set your alarm name.
You can set your alarm
description here and then
you can specify that for
what read rights number.
You should get
this alarm for right?
So you'll be setting that.
Over here after that
we will go to actions.
So once an alarm is triggered.
We should that alarm go who
should that alarm go to right?
So you can see
as I said over here.
Now whenever the state
is alarm, right?
What should we do?
So when the state is alarm
you can send you a notification
to your SNS topic now,
what is this nation SNS?
So basically it's
a notification service
will be discussing what SNS is
in the next session.
Don't worry if you don't
understand so basically for now
what you can understand Is
that SNS is a protocol
where a new set
if you get a notification
what to do with that
notification or whom to send
to that notification, right?
So if there's a topic
called notify mean SNS,
so in notify me,
I have configured
an email address.
That is my email adress
that whenever a notification
comes to the SNS service
or the notify me
topic to be precise.
It sends an email to me right
with that message.
So I will get a message
with this alarm.
Such and such thing
that has happened in cloudwatch.
Now you do whatever is required.
The other thing
that you can do over here is
in the same as soon as topic.
You can also configure
Lambda function to
be executed right now
what that Lambda function
will do so say suppose
I configure the metric
to be of CPU usage.
Right and I say
whenever 40-person metric
is crushed create an alarm
or like go to an alarm State
and it notifies the SNS know
Or if I mean topic about this
in the notify me topic,
I can configure
a Lambda function to clear
all the background processes
in that easy ruins, right?
So if I do
that the CPU usage will
automatically come down, right?
So this becomes a use case
that you want to launch
a Lambda function,
wherever your CPU uses goes
beyond 40 percent, right?
And hence.
This is the way you would do it.
So this was about cloudwatch.
There's nothing much to it.
You create alarms
and you monitor metrics, right?
Moving ahead and let's move on
to the second process
which is gaining visibility.
So for gaining visibility,
basically, you have to track
your whatever activity
is happening in
your AWS account.
So this is service in AWS called
Cloud trade, right?
So the cloud rail service is
basically a logging service
where in each
and every log to each
and every API call is made now.
How is it useful?
Let's talk about
the security perspective.
Right?
So your hacker got
access to your system,
so you should know
how he got eggs.
Your system.
So if you have a timeframe say
he got access to your system
or you started to face
the problem say
around four o'clock,
right so you can set the time
between two o'clock
and whatever the damage
right now and monitor
what all has been going
around and hence.
You can identify the place
where that hacker got access
to your system right now.
This is the part
where you will get to know who
that person actually is
or you can isolate
the problems or which calls
that so if you take Q from
our Facebook example over here.
You can actually pinpoint
who is responsible
for those spam messages
because you all have those logs
right you will see the origin
of those messages now,
once you've done
that the next step is managing
this guy out of the system
or wiping this guy
out of the system.
But before that let
me show you guys
how cloud trail actually looks
like so let's go back
to our ews dashboard
and go to Cloud tree service.
So I again under
the management tools.
You have the cloud
forest service you click
on the cloud resources and you
will reach this dashboard.
All right.
So here you have the logs.
So as you can see you
can set the time range here,
but I'm not doing that.
I'm just showing you the logs.
So even for logging
into my console it is showing me
that I'm logged
into my console at this time
on this date, right?
So every event is logged guys.
Every event that is happening
on your ews console
is being blocked.
So let's talk
about the S3 bucket.
So somebody deleted a bucket
and that has again
been locked, right?
So it happened at 7:30 8:00 p.m.
On 28th of March 2017, right?
So any activity
any kind of activity,
which happens in AWS
would be logged where?
Okay guys, so this is
about Cloud Trails.
Let's go back to our slide
and move ahead and play session.
So like I said,
so now you have identified
who is responsible
for your problem.
Right?
So now the next step
is managing access, right?
So now you should be able
to throw that person
or remove that person
from the system.
So most of the times
what happens is like
if we take
our Facebook use case,
so basically there was a user
who triggered that problem right
so too Things that you
have to do is first of all,
you have to remove
that spam from a system.
So you've got to know
where it originated.
So now you start
wiping it after that.
You have to D by that user
from doing it again, right?
So from The Source,
you'll get to know who that user
is now using managing access.
You will actually get
access to do all that right?
So if you talk about AWS
this service is called AWS.
I am so what AWS I am does
is It basically authenticates
that particular service.
Now, you are a root user.
Right so you can do anything.
But what if you have employees
and obviously all employees will
not have all the rights right.
Now.
What if you want to give
granular permissions to
your employees now for
like in our example,
what if one specific employee
is capable to track down
this problem right or track down
what has to be done?
So you can give that particular
person the rights
how using I am right?
So I M is used to provide
granular permissions.
It actually secures your access
to the ec2 instances
by giving you a private file
and also it is free
to use right.
So, let's see how I am is used.
So let me go back
to my AWS console.
Okay.
I said this is my AWS dashboard.
I will go to the security
identity and compliance domain
and then click on I am.
Right now over here.
I'll click on rolls.
Now.
I can see all the roles
which are there
in my I am right?
So since I would have
identified which role
is creating a problem,
so I'll go to that role.
So for example,
I have a problem in save AWS
elastic Beanstalk easy to roll,
right I click on this now
once I click I will
be getting this screen.
So now I can see the The trust
relationship success advising
the revoke sessions, right?
So I'll go to revoke
sessions and I click
on the book active sessions.
And hence.
I will be able to wipe out
that user from accessing
my AWS resources, right?
So this is how you use I am
guys are now one more thing
that you can do over
here is you'll go back
to your dashboard go to Rose.
Now I get told you guys
you can actually create
a role for a person
who would be able to access
restricted things on.
Your AWS account, right?
So let me quickly show you
how you can do that.
So you will click
on create new role
and you will give
you a roll some name.
So let's give it
hello over here.
Right click on Next Step go to
roll for energy provider access.
Right, and now you can select
how that user of yours will
be accessing your AWS account.
Right?
So allow users
from Amazon Cognito
Amazon Facebook Google ID.
All right, so let's
select this now.
Let us select Facebook and
let's give it some random
application ID, right?
So anyways not going
to create this role.
I'm just telling you
guys how to do it.
Right?
So basically you get
an application ID
by Facebook over there.
You'll be since you
are using Facebook thoughts.
Educate that guide
to your AWS account.
You'll get an application ID
by going on to
graph at facebook.com.
You can do all
of that over there.
Okay, so that is not the concern
you'll enter the application ID
and click on next step.
Right?
So you get the policy document.
So whatever you configured
in your text boxes has actually
been created in a Json file,
right so you don't have
to edit anything over here.
Click on next step.
Now you have to attach
a policy now,
what are the policies
of policies basically
what all permissions you
want to grant that user.
Right?
So if you want to Grant
him the execution role
for Lambda you can do that.
You can grant them
the S3 execution roll, right?
So whatever policy
that you create you can actually
create a policy near I am right.
I'm not going much
in details of this
because all of this is covered
in your I am session,
but I'm showing you guys
because I just told you
guys This can be done
to let me show you
how it can be done.
Right?
So you'll select
whatever policy want
and click on next step
and review it
and create that rule.
This is it guys right so you
can actually select a policy
whatever policy you want
that role to have and hence.
So policies basically
a permission that you
want that role to have.
So if you get the permissions it
to just review your instances,
he'll be only able
to review your instances.
Okay, one more thing.
I want to make Make clear is
that you don't have to give
your security credentials
to that kind anymore
because now you'll be specifying
that user can will be able
to connect to Facebook.
Okay.
So also you have a part
here wherein you can specify
what specific user
can access it right
so I can type in my name here.
And if I'm being logged in
through Facebook is
my username is him
and Shauna only then I
will be able to connect
to my AWS account right now.
This is ID right I can also
set the local parameter.
Right so idea
I think is fine wherein you
will be adding the ID of the guy
whom you want this AWS account
be accessed by right?
So you all have
Facebook IDs, right?
So you all have to just
punch in your Facebook IDs.
We're here click on next step
and then you'll be able
to access this AWS account.
If I create this role
right now with the policies
that I will be attaching
to your role.
Right?
So this is
how you use I am guys.
Let us go back to our session.
Okay.
So these are
the three services guys.
So you have I am
you have cloud trail
and you have cloudwatch using
which you can control
or you can actually see
what is going on
in your AWS account.
So let's go ahead
and start with today's session
with the first topic
which is why do we
need access management?
All right, so to
discuss this topic,
let's understand it using an
example say you have a company
in which you have a server
and the server has
everything in it.
It has all the modules
in it and it gives you
the it gives different users
the permission to use
the different servers
right now in your company.
First of all,
you should have an administrator
which will have all All
the rights to to access
the server, right?
So nobody in the today's
it World works
on the root account, right?
So there has to be
an administrator account.
So first we will create
an administrator account
with all the permissions
now tomorrow say
a UI developer comes
into your company right now
A UI developer will only work
on the graphical tools, right?
So he should only be allowed
the graphical tools
and not some other tools.
Maybe he shall not be given
the internet access.
Or something like that, right?
Maybe he's not giving
the PowerPoint access.
Maybe he's not given
some folders access some drives
access anything like that.
So all of that can be defined in
the server by the administrator
and specific rights
will be given to a UI developer
right similarly if to
if after that
a business analyst comes in
so he should only be able
to access the analytics module
which is there
in your soul, right?
He should not be able to get
into the UI development.
In part, or he's not be able
to see the other aspects
of what is there in your server?
Right?
So each and every
user each every rule
will have specific rights
assigned to them.
Right?
And this is done by policies
which are in turn
given by administrators.
Right?
So this is
what access management is
that giving each role
the specific rights
that they deserve and this is
what we are going to accomplish
today in AWS, right?
So this this is We
need access management.
Let's go ahead and understand.
How can we accomplish
this in AWS?
Right?
So as to accomplish this in AWS,
you need a service called I am
you have a service called I am
which uses this concept
of access management
and allows you to give it
to your users who are going
to use your account.
All right.
So what is I am
so I am is basically
a service from AWS using
which you can give
permissions to different users
who are using
the same AWS account
that you have created, right?
So in a company like
in any company be it,
you don't have to have
two or three AWS accounts.
You can have one AWS account
on which a number
of people can work.
Right?
For example, you can Define
that maybe a developer
would like to Work
on your AWS account
and he should only
have the ec2 instances
or you should only work
on the ec2 instances
you decide that
right?
So you can only Define you
can define a policy like that
that only the devel
the developers will only be able
to access the ec2 instances
on AWS account.
Similarly if say
database administrator comes in
so you should be able only able
to access DB instances
on your AWS account
and so on right so all
of that is possible using I am
but what I am is not only
about creating users
and creating policies.
It's more there is more
to I am right and hence
will be discussing the different
components of I am now
so let's go on and see what are
the different components.
So there are basically
four different components
in I in the I am service.
So the first service is
user then we are groups
then we have Rose
and then you
have policies right?
So the way we are going to go
about these are first
I'm going to explain you
each role on each service
in I am each component
and I am and then we're going
to see how Can execute them
or create them
and the AWS console,
right?
So let's start with the users.
So the very first time you
actually create a AWS account
that is basically
the root account
that you have created, right?
So there is no user inside it.
So why do we basically
need a user you need a user
because you are supposed to give
permissions to someone right?
So say I first of all want
to give administrator Rights
to a user right?
So you understand you have
to have an entity first
to which you can assign
permissions, right?
So these entities are called
users on E. Wa so any person
who wants to access
your AWS account has
to be added as a user in I am
and then you can attach
different policies
on to that user.
Right?
So this is what
user is all about.
Let me go to my AWS Management
console and show you
how you can create
a user in I am.
All right, so give
me a All right guys,
so this is my AWS
sign sign in page.
All right.
So this email ID when you log in
through your email ID
and your password
that is basically
your root account.
So what I'm going to do
right now is I'm gonna log
in using my root account
and first create
a admin account for myself.
Alright guys, so you should
never work in your root account.
You should always have
an administrator account
through work in the root account
should only Used
when there is an emergency
say you have been locked
out of our administrator account
only then you should be using
your route accounts.
The first thing
that you should do
when you enter the root
account is go to I am
which is just right here go
to I am and then you will have
this dashboard thing
right over here.
You can see there is
a thing called users.
You will click on users
and you will click on add user.
All right, so now it will ask
you for the The username
so you can provide a username
say I'll add my name first
so that be hemanth,
right and what what kind
of access do I want to give
to this particular user?
So there are basically
two kinds of access
that I can give first is
the AWS Management console axis,
and then we have
the programmatic access, right?
So what is these two so
if you want to so
there are basically
two ways you can access
the AWS resources right?
You can either access.
Using apis that is using
your code say you
have created an application
which is interacting
with your AWS resources.
Right?
So in that case
if you're interacting
with the apis using the API is
that is called
the programmatic access,
right secondly is
the AWS Management
console access that is
when you are using
the AWS website to actually
deploy resources or create
or create or remove policies
or whatever, right?
So that That is called
the AWS Management console axis.
So for my user I'd be giving
it both the accesses
that is programmatic axis
and the Management console axis.
Also, there is
when you enable the programmatic
access programmatic access,
basically you get the access key
and the secret key as well.
What are these I will be
explaining you in a bit.
All right, so we have selected
both of these options
and then move ahead
to choose the password.
So do you want an auto
generated password?
A custom password.
I'll choose a custom
part for password
since I'm creating account
for myself, right?
So I'll choose a custom password
and do I want to reset
the password on the first login?
No, I don't want that.
So I'll click
on next permissions, right?
So what kind of permissions
do I want my account to have I
will become drink configuring
that over here.
So as of now there
are no groups,
there is no existing user
that I can copy from.
So I'll attach
existing policies.
And since I want to attach
the administrator access
that is the first
policy over here.
I'll select that and click
on next right so you
can review all the settings
that you did over here
and click on create user.
This will create a new user
in your AWS account.
So as you can see,
I have got my access key ID and
a secret access key now guys,
the secret access key.
You only get to see
one time only one time
when Created your account.
So it is essential
that is tore your access key
and secret access key
once you get this page.
All right, let
me store it quickly.
So this is my access key ID
why we are copying it.
You'll get to know
during the session.
Don't worry and
my secret access key,
which is this let me copy this
and paste it in the notepad.
All right, so don't worry.
You might be thinking
that I've exposed
my secret key to you.
So I will be deleting
this account afterwards
so you don't have
to worry about that.
All right, so I've got
my access key ID
and my secret access key.
So that is done.
Now.
What I'll be doing is
I'll be logging out
from my from my root account
and logging in this user account
that I just created.
All right.
So one more thing that you
have to be very careful of
that you will not be logging in
through the same login page
that is just saw right
so you'll have to log
Through a different
login page now and the URL
for that is this right?
So you will be logging in
through this link as a
from now on so what
whenever you create a user
if you want them to log
into your account,
you have to give them
this link to log into right?
So let us copy this link
over here and log out
from a root account.
All right.
So I've logged out I'll close
this and I'll come here
and go to this particular link.
All right.
So once you reach
this particular link,
it will be asking
you the account name which will
be self filled by your link.
Right?
So you have to give
your username now,
which is hemant and then
the password so I'll type in
the password that I've given it.
and click on sign-
in So now I have basically
signed in two months
to mature to the user
that I've just created
on my route account.
Right?
So I no longer have
to use my root account.
I can basically lock
away my root account
for emergency purposes.
I'll be using my administrator
account from now
on I can do everything
from administrators
on that could be done
from a root account as well.
But there are cases
where in you get locked out
from your administrator account
in that cases you will be
Notable success rate
so moving on guys,
so I'll go to I am not
so as you can see we
have created a user
and we have logged
in to that user.
And if I go to I
am now you can see
that it will show
that one user has been created.
That is here.
All right, so let's get
back to our slide
and discuss the next component.
All right, so we've discussed
what our users let's move
on to the second component
which are groups.
All right.
So whenever you create
users they can also
be combined into groups.
Now, why do we need groups?
We need groups because say
let's take an example.
So say you have five users
and these five users have
to be given identical axis.
Right say these five users
belong to the development.
And the developing team has
to have some common access
that they all will have right.
Now one way of doing
this would be
that I would go to each
and every user
and attach a policy
that they need right
the smart way to do this
would be to to include
them inside one group
and to that group.
I will once only
once I will attach the policy
and it will apply
to all these five users, right?
So these are why groups are
very important now
how we can create groups.
Let me shed a light on.
On that so you will go
to you can see you can click
on groups over here.
And what you'll do is
basically is you'll click
on create new group, right?
So, let me give
the group name as live demo.
All right, and I
click on next step.
Now lastly the policy
that I want to attach
to this particular group.
All right, so say for example,
I just want this group
to be able to access
the S3 service from AWS.
So what I'll do is I
will select the policy
which says Amazon S3 full access
and I'll click on next step.
Now this policy basically
tells you that you
can only use the S3 service
in the Management console
and no other service.
All right, so I'll
click on create.
Whoop and now whatever
whichever user I will be putting
in putting inside.
This group will
have this property.
All right, so I don't have
to configure the policy
for any user now.
So what I'll do is
I'll create a new user now.
So say I create
a new user saying test.
All right,
and then I'm not giving him
the programmatic access.
I'm just giving him
the Management console axis.
All right, I'll click
on this and I'll give
it a custom password.
And then I don't want
him to reset his password
and click on next.
Right, and now it is asking me
whether I want to include
it inside a group.
So yes, I do.
I want to include it
inside the group
that I've just created
and I'll click on next
and review all
the settings are adjusted
and click on create user.
All right.
So the test account
has just been created now
as you can see guys
in the case of my account,
which I created.
I got an access key
and a secret access key, right?
So in this case,
I'm not getting any
because I didn't select
the programmatic access only
when you select the programmatic
access it will give you the key
so that your application
can actually interact
with the services
that you have launched.
All right,
so I have have created
a test user successfully.
Let's log into this test user.
so I will type in the URL
that has been given to me.
Right now when I
reach this page,
I'll enter the username as test
and the password as
what I have entered right
and I click on sign in.
Now with this you can see that.
I will now be able to see
the Management console
the Management console
will exactly look like
how it was used to see
how I used to see it
in my root account
or my administrator account.
But when you will try
to access say a service,
which you have not
been assigned to say,
for example, I only have
access to S right now
because I've deployed it
in the group where it has
only the access to S3.
If I try to go inside easy
to let's see what'll happen.
Right.
So it says you
are not authorized
to describe running instances.
As a matter of fact,
I'm not authorized to see
anything on my ec2 page.
Alright, so that is
because I cannot I don't have
access to the ec2 dashboard.
But let's see
if I can see the S3 dashboard.
So I'll quickly go to S 3 and
if I have the S3 axis,
I will be able to see all
the buckets which are there in -
3 And yes, I do.
So let me go inside a bucket
and delete something
so that all right.
Let me delete an object
from this particular bucket.
So yes, I can lead it.
All right, so let me check
if what if what happens
if I delete or II detach
this particular policy
from that group?
All right.
Let's see what happens.
So I will go to I am
and I will go to groups.
I'll go to this particular
group and I can see
that the policy
is listed over here.
What I do is I click
on detach policy and let's see
what happens now, right?
So I'll go
to Management console.
So on if now I
try to exercise 3.
It will show me
that access is denied.
Right so I no longer have access
to the S3 service
on my AWS console.
So this is how you can control
access to different users.
You can revoke access
you can include access
right you can do all
of that and I am right.
So let us come back to our slide
to discuss our next component or
as we've discussed
what our users
we have discussed.
What a groups now let's come
back come down to rules.
All right, so rules
are Similar to users
but roles are actually
assigned to Applications.
All right, so users are actually
assigned to people right?
So whenever you have
a developer in the company,
you will have sine M
the developer rules, right
but when you have
rules rules are basically
assigned to Applications,
how let me explain you say
you create an ec2 instance and
inside that needs,
you know instance you're hosting
your web application.
Now that web application
has been has been designed
in such a way
that it has to interact
with your S3 service.
Is for example
that will be doing to a will be
I will be showing you the
demonstration today for this.
Right.
So say that application has
to interact with the S3 service.
Now if I want to want
that application to interact
with the S3 service,
I have to give it permissions
and to give it permissions.
I will use rule
so I will create a rule
wherein I will specify
that this role can
access the S3 service
and I will attach
this particular role
to that particular
e0 instance in which
my application is hosted
and in Kiss my application
will be able to interact
with the S3 service, right?
It might sound complicated guys,
but it is very
easy to implement.
Let me show you how so
what I'll do now is I'll go back
to my Management console
which is here.
All right, I'll go
to the dashboard and say
I will go to rolls now.
All right, so I'll create a new
role now roles can be assigned
to any either Lewis service
which is listed here.
What I'll do is I'll assign
it to I'll create a rule type
of easy to write
so I will select Amazon ec2.
And what type of role
do I want to apply
to I want to say have
the access to S3.
Right?
So I'll select Amazon S3
full access over here
and I'll click on next step.
So, it'll ask me the role name.
So let me specify the role name
as Eddie Rekha
underscore one right
and I'll click on create role.
So with this role
has now been created
but mind you guys
are not attached this role
to any easy to instance.
Right?
So what I'll do now is I'll go
to my ec2 console so over there.
I already have built
an issue instance.
It is stopped.
So I'll start it and attach
this particular policy
to that ec2 instance.
Alright, so my ec2 instance name
is hemant underscore one.
So here it is.
I go to actions I start
this particular instance.
Right.
And what I can do is I
can attach the policy
using instance test settings.
It says attach or replace.
I am roll.
I'll go here.
I will go to the drop-down
and select the role
that I just created which is
a lyric underscored one.
I'll select that and
I'll click on apply.
Now with this
what will happen is
my rule is now my sorry.
My ec2 instance is now
configured to interact
with the S3 service
in this particular account.
Alright, so any application
that I deploy in this ec2
instance will be able
to interact with the S3.
Okay, so I don't have
to specify any access key
any secret access key.
If you're still confused
with that be patient.
We are getting onto where do we
actually use these keys?
And where do we not?
All right.
So this is what
your roles are all about.
Right so roles.
Like I said,
they are for resources
in AWS users are
for people roles
and uses a similar things
you attach polled
policies on to them
and they basically
identify Particular instance
or a particular person
as the owner of that
particular service, right?
So we've discussed
what roles are let's move
on and discuss policies.
So if you think about it guys,
we've actually been
dealing with policies,
right so policies
and nothing but permissions
that you give to your
with whatever role
or user or group
that you have created, right?
So, for example,
I want to give
the ec2 instance axis,
right so that ec2 instance
access is basically a Policy
that I will be attaching
to the user or to the rules.
All right.
Let's see how we
can create policies guys.
So I'll go to
my Management console.
I'll go to I am Right.
So the you can either create
policies or you can actually
use already existing ones.
So there are a couple there
are a couple of policies
that have already been created
in your AWS account,
but you can go ahead and create
your own policy as well.
Alright, so let me show you how.
So say for my test account,
what I'll do is I will go
inside test account.
All right, and I
will add permissions.
And I will attach
existing policies.
Directly and here I am guys.
So now you can you
can create policies as well.
So you see the tab
over here guys,
it says create policy.
So if you feel you're
the kind of policy
that you want to create
is not listed over here
in the default policies.
You can actually create one
and creating a policy
is very easy guys.
You just click on create policy
and you will see this page.
All right, so you'll
have three options.
You can either copy
and AWS managed policy.
That is a default policy.
Can create our own policy
by just typing
in the Json code and if you're
not comfortable with coding,
what you can do is you
can use the policy generator.
Now.
What is policy generator?
Let me explain you.
So with policy generator,
you just have to select
what effect do you want poor?
Do you want it to allow it
or do you want it to deny it?
Right?
So say I want to allow
the easy to service
to this particular test account?
All right, so I'll go
too easy, too.
Right, here.
It is.
I selected easy to what kind
of actions can he perform say
I want to give him all
the actions you can do
anything with these two
and the show's name is
basically a particular resource.
So where they are and you can
identify a particular resource.
So I don't want a particular
resource to be assigned to him.
I want PE can access
every resource in easy to write
so I just add star
for all of them right
and click on Next Step.
So with this you
as you can see it
has Automatically created
a policy document for you.
All you have to do now
is click on create policy.
And it will create
the policy for use
as you can see there are
18 customer managed policies
that are now 19 so I
can go here and select.
T' the policy
a policy over here.
Alright, so if I go
to my user now,
which is test I'm
going to permissions.
I will just click on add
in line police policy.
Click on select again Guru ec2.
select actions all actions right
and pull it to Star.
So I click on ADD statement
click on next step
and click on apply policy.
So a policy has been applied
on the test user
that it can actually access
the ec2 instances now,
so if I go to my test user now
which in which I was not allowed
to access the ec2 instances,
I can actually use
easy to instances now,
so if I go too easy,
too You can see the Lord give
me the access denied thing,
right so I can access all
the instances over here
as if I was using
the root account,
but only for
the ec2 service right?
If I go to S 3 you
can see I will still have
the access denied page.
Because I'm not been
assigned the access
to this particular service.
Alright, one more thing is
if what if you add an allow
and Adonai policy
together inside a group
what will happen then?
So in that case so since I
have allowed easy to access
what I'll do is I'll deny
is you access as well
in this particular user.
So I'll click create
one more policy
and I'll say deny
I'll select ec2.
Right as like the actions
as all actions.
I will give the resources all
at the statement and click
on Next Step apply the policy.
So now I have denied
ec2 instances as well
and created and allowed ecd
instance ec2 instances as well.
What do you think
will happen now?
So if now I try
to go too easy, too.
Let's see what will happen.
So it will say you're
not authorized to use Easy
to anymore because whenever
you creating policy guys,
you either get the along option
or the deny option.
If you have selected
both of them,
it will always prefer
the least permission
that you have given.
So in our case
that is the deny option,
right so it will always
deny the case.
Even if you have allowed
it in the same user,
right if you have mentioned
that that particular
service has to be denied
to that particular user.
Alright, so this was
about policies guys.
Let me come back to my slides.
So we have discussed
what our users
what a groups for a rose
and what apologies let's go
ahead and discuss the very
important part of authentication
which is called
the multi-factor authentication.
So what is multi-factor
authentication guys,
so multi-factor authentication
is basically something like OTP
that you get when you log
into your Gmail account, right?
So you enter a Gmail Email ID
you enter your password
and when you click on continue,
it will ask you
for your OTP, right?
So same as the case
here as well.
You can configure
your AWS account in such a way
that you will enter username.
You'll enter your password.
And when you click on login,
it will ask also
ask you for a code
that has to be given to it.
Now that code is basically
the multi-factor authentication
thing that we document
so there are basically
two layers of security Now
one layer is a password
and second layer.
MC code that will be entering
right now with AWS.
There is an application called
the Google Authenticator right
which you can use to create
a virtual multi-factor
Factor authentication device.
Now for those of you
who already are using
multi-factor authentication
in your company's you so there's
a thing called gemalto, right?
So people who work from home
and they have to connect
to the company's Network
the way you connect it
is using a gemalto token.
And so those of you
who are from the IT background
you can relate to it.
Right but if you
want to go through to
through a simpler way,
you can actually create
a virtual multi-factor
authentication device
and to create that
in your AWS is pretty simple.
You just have to download
an application called
the Google Authenticator
on your phone
and you have to connect that
application to your AWS account.
And that is it now.
It might sound tough,
but it's very simple.
Let me show you
how so you you will basically go
to your AWS Management console
and you will go
to the particular user
that you want
that multi-factor authentication
to be assigned to.
All right.
So for example,
I wanted to be assigned
to the test user right.
So what I'll do is
I'll go to users.
I'll go to test right and
in the security credentials tab,
I will have this page
which says assigned MFA device.
So it says no as of now,
so I'll assign it
a device I click on edit
and now it'll give me an option
between a virtual MFA device
and a hardware MFA device.
Now.
I have to choose among the two.
So since I said,
you can create a virtual
MFA device very simple easily.
So I'll select
the virtual MFA device.
And now it's basically
asking you to install
the application on your phone.
So we have already done that.
Let's click on next step
and now you'll be presented
with this screen.
So basically now
what you have to do is you
would be logging in to
your Google Authenticator app,
and you will be scanning
this barcode from your phone.
So let me show you
how let me connect
my phone to the computer
so that you can see the screen.
Give me a second.
Alright, so this is
the screen to my phone guy.
So what I have what I have
to do now is I have to go
to the Google Authenticator app.
I'll ask me to
create an account.
So I click on begin
and once I have
that basically now I'll have to
scan the barcode from my mobile.
So the way to do
that is I'll click
on scan a barcode
and then I'll scan
this barcode over here.
Right, it might take some time.
So be patient.
Yeah, so it's done now,
you're all set.
Right.
So you just click on done
and now you have
to enter two codes
that you are you
will be receiving on your
on your Google Authenticator.
So basically these codes change
from every 30 seconds, right?
So I have to endure
these codes over here.
So it's 2 0 4 and then 3 5.
Sorry 0 2 0 & 3 5 3 Zero
two zero three five three,
and I have to enter
the next code as well.
So let's wait for
the next code and it's
1 2 7 8 9 1 so I'll enter
that over here as well.
So it's 1 2 7 8 9 1
and that is it guys.
So now I'll click
on activate virtual MFA
and it says the MFA device
was successfully Associated.
So I'll click on finish
and that is it guys
you're done, right?
so now if I log out
from my test account
that is From here, right?
This is my test account.
So if I log out
from here right now.
And try to login
again using test.
So I come to my normal
login page, right?
So I'll enter my username
and my password.
Which is this and now I'll click
on sign in so now it will ask me
for the MFA code.
So let's see.
What is our MFA code as of now.
So it has changed to
seven three four five five two.
So let us enter that seven
three four five five two.
And click on submit.
So with this I will
now be able to log
into my AWS console
using the test account
which are configured using
the administrator account
in I am right so it's
very simple guys.
It's you can actually get
a world-class security
with the click of a button
using I am alright,
so we have seen how we can do
multi-factor authentication.
Let's move on
to the Hands-On part now,
so this is what Is you
guys have been waiting
for so just give me a second?
So that I can configure
everything on my end.
All right.
So what we'll be doing now is
I have created an application
which can interact
with the S3 service.
All right.
So using that as
the service now.
We will be.
Uploading files to RS3 console
and how will we are going
to do that first?
We are going to do that using
Local Host and that is
where our secret keys
and my accesskey comes in
and then we will be
we have assigned role
to are easy to instance.
Right?
So we'll be accessing
that website using easy
to without the access key
and the secret access key
and we can
and we'll see do
we get the access
to our SEC service or not?
Alright, so let us do that.
So now what I'll do is I will go
to my local host application.
So guys this is
basically my application.
What I have to do is I'll choose
a file upload a picture
from any sample pictures
and then it will upload it
to a particular bucket
that I've defined in S3 and
that the bucket looks
something like this.
It show that buckets name
is quarantine demo.
So let me show you the bucket.
So as of now,
I think there are some objects.
So let's delete those objects.
So here it is.
This is the
bucket quarantine demo.
So I have like three objects
over here as it's now.
So let's delete these objects.
Alright, so now
what I'll be doing
is this is the code
for my application guys.
All right.
So in this code
as you can see,
I'm not specified the key
and the secret key as of now,
so I'll get the key
and the secret key
from here, right?
So let me quickly.
So let me show you without
the secret can access key.
How is this localhost
website functioning?
So if I try to upload a file
as it is now See,
this is the file
that I want to upload I
click on upload image
and I will get an error
right because it
is not authenticating
itself to the service
that I want to go to.
So now I'll add the credentials
that that is a key
and the secret key.
Now the way to do that is
like this so I'll copy it.
And I'll paste it here.
I'll delete this
and this is well not required
and now I'll paste my key
and my secret key,
which is this right
so I'll copy the key.
Over here and then
my secret key as well.
over here and now I'll save it
if I try to access
my Local Host website now,
I should be able
to upload a file right so
if I try to upload the file now.
It says well done
S3 upload complete.
So these credentials
that have just entered
our basically credentials
for my him and account.
So if you want to see
where did I get
these credentials from again?
You can basically go
to users you can go
to your user and you can go
at security credentials
and over here.
It will last you
the access key ID lot list
you the secret access key
because it is only available
once you can only use it once.
Copy it once you will
not be able to see it again.
And if I make this particular
key inactive from over here,
and if I try to
Upload anything again.
I will again get an error
because without the keys
my account will not be.
I will not be authenticated
to the S3 Service
as you can see it says
invalid access key
because it is not valid anymore.
All right, so I can make
it active again,
but that is not required as now.
So what I do now is I
have already configured
this website on the ec2 console.
All right, so let me go
to my easy to Right, here.
It is.
So remember in this starting
of the session we created a role
for S3 full access, right?
So that role has been attached
to my ec2 instance.
So let me show you the website.
Here it is.
All right, so I can access
the website on my ec2.
Now if I choose a file as of now
and I try to upload the file.
I'll be able to do so
because my policy
has been attached now.
Let's see what happens
if I d-- attach the policy.
All right, so I'll go
to this and I'll select
no role click on apply.
Yes detach.
And now if I try
to upload a file again.
As you can see I see a blank
page with basically means
that an error has occurred.
All right, so I am
not able to upload any file
because my role has been
detached from my ec2 instance.
So if I wanted
to be working again,
I'll just simply go here go
to actions settings.
Attach the rule.
That is this click on apply
and it will again work.
Right, I'll choose a file see
this file upload the image
and your work again works
like a charm, right?
So that is it guys.
You don't have
to configure much.
You just have to have
the knowledge of I am
and with that you
can do complex procedures
with the click of a button
and you don't have
to swear about it, right?
You might want to you
might be wondering did I change
anything in the code
when I uploaded to easy to
so you don't have
to do anything guys.
You just have to delete
the Choose key and secret
and you will upload the code
as it is you don't have
to change anything it will
if it doesn't have
the key mentioned
in this particular function,
it will basically get those keys
from the metadata of easy to
and metadata is the place
where your role is actually
assigned or your role
is actually attached right?
So if it doesn't find
the key in the code,
it basically goes
to the metadata and picks
the key from over there.
All right.
So guys that is it
for the demo part
in this session.
We will be discussing
about Amazon redshift
the most popular
cloud-based data warehouse.
So let me run you
through today's agenda quickly.
We will Begin by taking a look
at traditional data warehouse
will be discussing
its underlying architecture
and the disadvantages of using
traditional data warehouse,
and then we'll move on
to our today's topic
which is I'm redshift
here will be discussing
its architecture its key
Concepts its unique features
and the advantages
of using Amazon redshift.
And finally,
we'll be doing a demo
on Amazon redshift in this demo.
We'll see how to import
data from Amazon S3
to Amazon redshift
and perform queries
on this data very easily.
So I hope that was
clear to you guys.
Let's get started.
I'm sure you know,
what a data warehouses you
can think of data warehouse
as a repository.
Story that data generated
from your organization's
operational systems
and many other external sources
is collected transform
and then store you can host
this data warehouse
on your organization's
Mainframe server or on cloud,
but these days companies
are increasingly moving towards
cloud-based data warehouses,
instead of traditional
on-premise systems and to know
why we need to understand
the underlying architecture
and the disadvantages of using
traditional data warehouses.
So let's begin
By looking at architecture,
but it is important to
understand where the data comes
from traditionally data sources
are divided into two groups.
First.
We have internal data
that is the data
which is being generated
and Consolidated from
different departments
within your organization.
And then we have external data
that is the data
which is not getting generated
in your organization.
In other words.
That is the data
which is coming
from external sources.
So this traditional
data warehouse follows,
It's a simple three-tier
architecture to begin
with we have bottom tier
in bottom tier.
We have a
warehouse database server
or you can say
a relational database system
in this jar using different kind
of back in tools and utilities.
We extract data
from different sources
and then cleanse the data
and transform it before loading
it into Data Warehouse
and then comes the middle tier
and middle tier we
have olap server.
Olap is an acronym
for online analytical processing
this Oily performs
multi-dimensional analysis
of business data
and transforms the data
into a format such
that we can perform complex
calculations for analysis
and data modeling
on this data very comfortably.
Finally.
We have top-tier.
The stopped here
is like a friend
and client layer this jar
holds different kind of query
and Reporting tools using
which the client applications
can perform data analysis query
reporting and data mining.
So to summarize
what we have Vlad till now
traditional data warehouse as
a simple three tier architecture
in the bottom curve.
We have back in tools using
which we collect
and cleanse the data and then
in mid 80 or we have tools
which is olap server using
which we transform the data
into the wavy Ward
and then finally dropped your
in which using different query
and Reporting tools.
We can perform data analysis
and data mining moving on
to the disadvantages
of traditional data
warehouse concept there.
Is this leading
us Business Service Company.
And this company is running
a commercial Enterprise data
warehouse this data warehouse
as data coming
from different sources
across different regions.
The first problem
that this company faced was
when it was setting up
a traditional data warehouse
as we discussed earlier,
the architecture of
traditional data warehouse
is not very simple.
It consists of data
models extract transform
and load processes,
which we call ETL and you
have bi tools sitting on top.
So this US based Denis
had to spend lot of money
and resources to set
up a traditional data
warehouse data warehouse,
which was initially
5 terabytes is growing
over 20% year-over-year
and it was expected
that the might be
higher growth and future.
So to meet this continuously
increasing storage
and compute needs
the company had
to continuously keep upgrading
the hardware again this task
of upgrading the hardware
continuously involves lot
of money Manpower
and so many resources so,
To scaling and traditional
data warehouse is
not an easy concept and
since the company
could not meet all the storage
and compute needs easily.
It was facing a lot
of performance issues as well.
And finally the company
had to deal
with increasing cost initially
that to spend a lot
on setting up data warehouse
like that to spend
on Hardware Manpower
electricity security real estate
and deployment cost
and many other and
as their data warehouse grew
they had to spend again to meet
Courage and compute needs
so to sum it up setting
up a data warehouse
and deploying it
and managing it later
involves lot of money
and resources moreover
auto-scaling in traditional data
warehouse is not an easy concept
because of all
these reasons many companies
are increasingly moving
towards cloud-based
warehouses instead of
traditional on-premise systems.
So guys in this session,
we'll be dealing with one
of the most famous
cloud-based data warehouse
provided by Amazon,
which is arms,
And redshift and simple
what's Amazon redshift is
a fast scalable data warehouse
that makes it simple
and cost-effective
for you to analyze all your data
across your data warehouse
and data leak guys.
I have a definition
which is put up on the screen
and I have few words,
which I have
highlighted over there.
So as we progress
through the course
of the session will know
what those words exactly mean.
So let's ignore them for now,
but there are certain
key Concepts which you
should be aware of
when you're dealing
with Amazon redshift.
So we'll discuss them now.
Now Amazon redshift data.
Where is a collection
of compute resources,
which we call notes
and these notes
when organized into a group
they become clusters each
of these clusters run
an Amazon redshift engine
and it contains one
or more databases.
So this cluster
has a leader note
and one or more compute nodes as
for the leader node,
it receives queries
from Klein applications.
And then it passes these queries
and develops a suitable
query execution plan and then
it coordinates the power.
Execution of these plants
with one or more compute nodes
watch the compute nodes finish
executing this plan.
Again, the leader node
Aggregates the results from all
this intermediate compute nodes
and then sends it back
to client application.
Then we have compute nodes you
can think of this compute nodes
as a compute resources
that execute the query plan
which was developed
by leader node,
and when they are
executing this plan,
the transmitted data
among themselves to
solve many queries.
These compute nodes are further.
Added into slices
which we call note slices each
of this note slices receive part
of memory and disk space.
So the leader node distributes
data and part of user query
that receives from Clan
application to this note slides
and all this note splices walk
in parallel to perform operation
and increase the performance
of your redshift data warehouse.
So to say we have leader node,
we have compute nodes
and nodes slices.
But how do they interact
with line application?
That is the question here.
So I This line applications
basically bi tools or it can be
any other analytical tools
which communicate with Amazon
redshift using drivers like jdbc
and odbc jdbc ref is to Java
database connectivity driver.
It is an API for
programming language Java.
Then we have odbc
it refers to other
database connectivity driver
and it uses SQL to interact
with leader node.
So basically using
this drivers client application
sends a query to lead
a new read a note
on receiving the client
applications queries.
It passes these queries
and develops a
suitable execution plan.
Once the plan is set
up compute nodes
and compute slices start
working on this plant
the transmitted data
among themselves to
solve this queries.
So once the execution
is done leader node
again Aggregates the results
from all this intermediate totes
and sends it back
to client application.
So this is the simple
explanation of Amazon
redshift Concepts moving on
when you launch a cluster
you need to specify the know.
But basically we have two types
of nodes then storage notes.
These are storage optimized
and I used to handle
huge data workloads.
And basically they
use hard disk drive
or HDD type of storage
and then we have dense
compute distance compute nodes
are compute optimized
and they are used to handle high
performance intensive workloads
in the mainly
use solid-state drive
or SSD kind of storage,
but there are three things
that you should keep in mind
when choosing one among them
firstly you should be aware.
If the amount of data
that you want to import
into your Amazon redshift
and then the complexity
of the queries
that you run on your database
and the need
of Downstream systems
that depends on the results
of these queries.
So keeping this three
things in mind,
you can choose
either Den storage nodes
or dense compute nodes.
So guys that
was the architecture
and its key Concepts now,
we'll take a look
at few reasons as to why
Amazon redshift is very popular
as we discussed earlier
setting up a You
smell data warehouse involves
lot of money and resources,
but it's very easy
to setup the deploy
and manage a suitable data
warehouse using Amazon redshift
on Amazon redshift console.
You will find create
a cluster option.
When you click on that option
Amazon redshift ask you
for certain details,
like the type of node.
You want to choose the number
of nodes the VPC in which you
want to create your data
warehouse user ID password
and many other details.
Once you feel that you
have given the right set
of details you have an option
which says launch the cluster
and one click your data
warehouse is just created.
So with one click you can easily
create a data warehouse
in Amazon redshift.
Once your data warehouse is set
up Amazon redshift automates
most of the common
administrative tasks
like managing monitoring
and scaling your database.
So you don't have
to worry about managing
or scaling your database needs.
So that's how easy
it is to develop
or set up a data.
Using Amazon redshift.
We also learned
that auto scaling is difficult
in traditional data warehouse,
but you can scale quickly
to meet your needs
and Amazon redshift.
Well, we already know that
a cluster node as a leader note
and one or more compute nodes.
So if you want to order
scale an Amazon redshift,
all you have to do
is resize your cluster size
as we know this compute nodes
are like compute resources.
So if you want to scale up,
you can increase the number
of compute notes similarly
if you want to scale.
Held up you just have
to decrease the amount
of compute nodes alternatively.
We have something
called single note
and multiple new and single
node cluster one node takes the
responsibilities of both leader
and compute functionalities
and the multi node cluster
contains one lead in node
and user specified number
of compute nodes.
So suppose you want
to resize your cluster
and you are using
a single mode cluster,
then you can change
from single node cluster
to multi-node kirsta.
Similarly.
You can change
from multiple node cluster
to single node cluster.
Of a need so that's
how easy it is to scale up
and down and Amazon
redshift moving on.
We learned earlier
that while using
traditional data warehouses.
It's possible
that the performance of your
data warehouse might decrease
but with Amazon redshift,
you can get ten times better
performance than any other
traditional data warehouse.
It uses a combination
of different strategies,
like columnist storage
and massively parallel
processing strategies
to deliver high throughput
and response times.
So let's discuss the strategies
one by one will first we
have columnar data storage
to understand what
that is first.
We should know row storage most
of the traditional data
warehouse and database is used
this row storage in row storage.
All the data about the record
is stored in one row.
Okay.
So let's say I have
this database here.
I have three columns
and two rows
the First Column contains
the unique number associated
with student the second column
contains the name of a student
and the third column
contains the edge
as we already know.
Data is stored in form
of blocks in databases
or data warehouses.
So as you can see
in row storage the block
one contains all information.
There is about a particular
student has SSN his name
and then age.
So basically it stores
all the information
that there is in a single Loop.
So in the first block you have
information about first student
and in the second block you have
information about second student
and it goes on now
the columnist storage again.
I'm using the
same database again.
I have three columns
and two rows.
Rose but Colin storage stores
data by columns with data
for each column store together.
So again, we have blocks
but the first block
here has all the data
that is there in First Column.
So you have all assistant
stored in first block
and all named store
in second block and all
the ages Stone in third block.
So it goes on there are a lot
of advantages of using
this column storage firstly
since and column storage
a single block contains
same type of data.
You can achieve
better data compression.
As you can see columnist storage
can hold values
3 times the records
as robe a storage because of
this the number of input/output
operations decreases
and thirdly by storing
all the records
for one field together
columnar database can query
and perform analysis
on similar type of data far
quicker than row storage.
So this is how the concept
of columnar storage
which is used by
Amazon redshift provides
us a better performance.
And then we have
massively parallel processing.
I'm sure you might have
or of parallel processing
and computer science.
It's just
that number of different
processors walk together
or compute together
or in Palin similarly
massive parallel processing
in Amazon redshift is nothing
but cluster we have already
discussed this earlier.
We have a cluster
and this cluster
has a leader node
and one or more compute nodes
and this compute nodes is
further divided into something
called note slices.
So when this leader node
receives a query it
develops execution plan and
this compute nodes and computes.
Isis walk together or in
parallel to execute
this plan and later.
Thus leader node
sends the results back
to client application.
So basically this compute slices
and compute nodes work
in parallel to achieve
better performance moreover
Amazon redshift is also able
to smartly recognize the data
or notes before running a query
which dramatically
boost the performance.
So that's how we can get
our ten times better performance
using Amazon redshift
and then the cost
and traditional data.
A bear houses people
had to spend a lot
of money to set up
and then later to maintain
the data warehouse.
But Amazon redshift
is the most cost-effective
cloud-based data warehouse.
If you remember
in traditional data warehouse,
they had to spend
on Hardware real estate man,
power electricity
and deployment cost
and many others and
as their data warehouse
grew they had to spend again
on meeting the storage
and compute needs
but an Amazon redshift.
We don't have to pay
any upfront cost.
So Amazon, Redshift is
most cost effective
and it cost one tenth
of traditional data warehouse.
You can start small fishes point
two five dollars per hour
without any commitments and you
can gradually scale up later.
If you need in addition
to all those advantages
Amazon redshift allows
you to query data
from data leak data leak
is a storage repository
that holds a vast amount
of raw data in its native format
until it is needed.
So in data Lake you have data
in different formats
you can Can load data
from Amazon S3 into
your Amazon redshift cluster
for analysis very easily
that is from data leak
you can store easily
20 or Amazon redshift
but it needs more effort
and cost the first
because loading data into
Amazon redshift cluster involves
extract transform and load
which we simply called
ETL process and this process
is very time-consuming
and compute intensive
and it's costly
because uploading lots
of data cold data
from Amazon S3 for analysis.
Is growing your clusters,
which is again costly
and requires a lot of resources.
So as a solution,
we have something called
Amazon redshift Spectrum,
which acts as the interface
between your Amazon S3
or data Lake
and Amazon redshift.
So you can directly query
data stored in Amazon S3
or data lake with this red shift
Spectrum without need
for Unnecessary data movement.
I hope that was clear and
finally with Amazon redshift.
Your data is safe
and secure it offers.
Backup and recovery.
So as soon as data is created
or stored in Amazon redshift
a copy of that data is made
and through secure connections
a snapshot of it a sin
to Amazon S3 for later.
So suppose you lose your data
or if you have deleted the data
from Amazon redshift by mistake,
you can restore the data easily
from Amazon S3 service
Amazon redshift also
provides you with an option
to encrypt your data.
So when you enable
this encrypts option all
the data in your cluster
in your leader node,
and Compute nodes
and nodes slices is encrypted.
And this way your data
is very safe and secure.
So Guys, these are
all the advantages
of using Amazon redshift.
So now you have a basic idea
of its architecture.
Its various key Concepts,
like clusters nodes
leader node note slices now,
it's time to work
on a demo in this demo.
We'll see how to transport data
from Amazon S3 to Amazon
redshift data warehouse
and perform simple queries.
So I hope that was
clear to you guys.
Let's get started
the first First thing
there are certain software's
which you need to pre-install
so that you can start working on
Amazon redshift first suppose.
You want to perform queries
on the data on Amazon redshift.
Then you need a SQL work bench
where you can perform
your queries and
as we learned earlier
the client application
need a connection
to communicate with redshift.
So we need to install
a jdbc driver
and for that jdbc driver
to run we need to have
a Java runtime environment.
So we have three things to
install your now I'll show you
how to install it.
And I have this Java runtime
environment download link
by Soft tunic.com.
So it says free download
and you click on that.
It will be downloaded.
You can store it anywhere
and once you're done
with that search for
Amazon redshift documentation.
So here it is.
Okay, not that not that just one
and when you scroll down
it says Amazon redshift
get started click on that
and in the step one,
we have prerequisite UPS.
Okay, scroll down
and Chase in the Step
2 you have an option
where you can download a go
to SQL work bench website
and download it.
So click on that and here it
says build current version
and you have download generic
packages for all systems.
You can download it.
Once you click on
that it'll start downloading
and there is one more thing
which is jdbc driver.
Go back to documentation part
scroll down in the step 4,
you can see configure
a jdbc connection click on
that it will take you
to a page where you have.
I've jdbc drivers
of different version.
You can download
the first one click on this
and it will be downloaded.
So once all these three things
are downloaded stored them
in a file of your choice.
Well, I have stored
them on my desktop.
I have this AWS folder
and in that which ifft
so here's my workbench.
Zip file.
It was a zip file.
So extracted all the files
and then I have my jdbc driver
your well Java runtime
environment as in download,
so that's okay.
So I hope that was easy to just
install all these things
and you are set to go
And your backdoor
Amazon Management console?
I have previously used
the Amazon redshift.
So I have this Amazon redshift
in recently visited Services.
Anyway, you can search for
Amazon redshift here your it is
whether it's taking
time to load.
Okay.
This is my Amazon redshift
console page and you have
different kind of options
on your navigation pane
on the left side
and there are two ways to create
a launcher cluster first.
You have quick
launch cluster option
and launch cluster option.
This is the very easy way
to launch a cluster
but suppose you want
the freedom to specify
all the details as in the vp's.
He's the security groups
different type of notes
username password and all that.
You can go for launch
clustered option.
Let's go ahead an Explorer.
So first it asks for a name.
Let's say my cluster
and database day T1.
And the poor this is default
Port 5 4 3 9 is a default Port
which would be handled
by Amazon redshift you
then the master user name.
Let's say AWS user and password.
That's it and confirm
your password and click
on continue option.
So cluster details are done
and dusted then you
have note configurations.
Well for the free tire,
you only have DC too large
but suppose you have
a premium membership.
Then you can choose any
of this for this DC to large.
This is the CPU capacity
memory and storage
and the input output performance
has moderate you can go ahead
and choose the cluster type.
We discussed this.
We have multi node
and single load and single node.
We have both the leader
and the compute nodes.
Note responsibilities handled
by single note the multi node.
We have a single leader node
and use a specified number
of compute notes
click on continue
and then here it asks for
the VPC details parameter group
in suppose you want encryption
or not and all the details.
So basically in this
launch cluster option,
you have the freedom
to specify all the details,
but for this demo,
I'm going to use
quick launch cluster option.
So again as for the free tire,
I'm using DC too large
and again for the free tier.
I'm using DC to large type
it says Our to compute
nodes and let's retain
the same cluster name as
for the master
user AWS user now.
Let me give the password.
And the default Port is 5 4 3 9
and last option we have
to choose among the viable.
I am users or IM roads,
but the question is why we need
our I am role here in this demo.
I said that we're trying
to import data from Amazon S3,
but you need certain set
of permissions to access data,
which is stored
in Amazon S3 for that.
We need to create a I am roll.
So let's go back
to I am service.
Let me close all the steps.
Okay, here you
have roles option.
You can click on that
and click create true.
And since we're dealing
with Amazon redshift
select red shift,
let's shift customizable
and click on next permissions.
So we want Amazon redshift
to access data from Amazon S3.
So search for S3 of
and you have a permission
which says Amazon S3 read-only
access well for this demo,
this is an if but there
is one more permission,
which is Amazon S3 full access
so you can perform read
and write operations as
well as for this demo.
I'm going to
choose this permission,
which is Amazon S3 read-only
access provides read-only
access to all the buckets
and Amazon S3 and click on next
to view give you a role in name.
Let's say my redshift role
to and click on create rule.
So now our Amazon redshift
database as permission to access
data from Amazon S3.
Let's go back
to redshift console.
Okay, let me refresh this
and now it's showing the role
which has been created
by showing your so
as you can see unlike
other launch option
in this I didn't have to specify
By much details just the node
type the number of notes
and then the master user name
cluster identifier and password
and the default database port
and you can click
on launch cluster option.
So with one click you
have easily deployed a database
on Amazon redshift.
If you remember
when we try to use
this launch cluster option
we had option to select
a default database or use
or create our own database,
but when you use this quick
launch cluster option
a default database called
death will be created for us.
So guys this cluster
has been created.
So before we connected
to your SQL work bench.
Let's try to explore here.
You need to make sure
that the database health status
and in maintenance state is
everything is in green color as
for the cluster
a cluster status.
It should be available.
And for the database Health,
it should be healthy
only then you can make
a perfect connection
with your SQL work bench.
So you have this icon
here click on that.
Well, you get all
the information there is
about your cluster
or you can just go ahead
and click on this cluster.
So this is the end point
this tells me all about
how to make a connection
with this cluster.
I have this when I click on that
it says publicly accessible.
Yes in the username as AWS user
and the security groups.
Apparently, it just
shows the TCP rules
which are set so
that's about the end point then
the cluster name you have
cluster type node type
and it shows the nodes and
the zone and the date and time
when it was created
and you have cluster version
as well on the right side.
You have cluster status,
which is why Syllable
database health healthy.
So is it currently
in maintenance mode?
No, and then you have
parameter group apply status
which is in sync
with your database and there
are few other features as well.
But here you can see
this VPC group click
on that go for inbound
and make sure it is set for TCP.
Okay edit make
this custom TCP Rule and here
are five four three nine.
Custom that's it
and click on Save option.
So that's the default port
with which you can access
the redshift and it's go back.
Clusters.
Okay, where were
we we will change
the default group of a PC.
So this is the URL
with which you can connect
to the SQL work bench.
So let's copy this
and paste it in our x
file I pasted over there.
Well, if you
using odbc connection
and you can use this URL
when you scroll down you
have capacity details
of your entire cluster,
it's DC too large.
So seven easy to compute units
total memory storage
and platform, okay.
Let's go back to the I am role
but I should have
an IM roll option here.
Let me see check it out.
Okay, there's an option.
It's acim rules.
You can copy this entire thing
and paste it again the editor
so that while connecting
it will be easy
for us to find it.
Okay, then so now we
have cluster has created
your database or data
warehouse is set up now.
You can just connect it
with SQL work bench
and start working on it.
So let's go back to the folder
where I stored
my Workbench here it is.
When you scroll
down there's a file
which says SQL work bench
executable jar file.
Open so here it is.
It's asking for
a default profile name.
Let's say new profile one.
Okay, then driver
that was Amazon redshift driver
only jdbc driver.
And this was the yarol.
We copied it earlier
in the editor.
So I'm going to paste
it over here.
Now.
This is the URL control C
and pests AWS user
in the password.
Okay, that should work make Sure
that you select
this order commit save it
and then click on OK it says
connecting new database now,
it's successfully connected
so I can easily perform queries.
Now first.
Let's create some tables.
Well, I'm using the sample
database from Amazon S3.
So you have this AWS
redshift documentation.
Go back to that and here
it says get started
and in the step 6 you have
this default SQL queries
and tables provided.
You can go ahead and use
that I have it stored
in my data.
So I'm going to copy first.
I'm going to create
all the tables.
Control C and paste
it over there.
Let's check what tables are
there first we have user table.
Well, this is like
an auction data schema.
So you have
user table many users.
When you have category users
the category different
categories to which users belong
to then you have
a date date on which
a particular event occurred.
Then you have even
table all the details
regarding an event
listing as in the items,
which are being sold are listed
here all the details
about the items.
Then you have sales
as in which user is Sighing
how much which item
in on that details?
So basically we have
six to seven tables.
I'm going to select all
that and say run option.
So here it says
table users created table
when you created category
date event listing and sales.
So all the tables
are easily created now as
for the next part,
we need to copy the data
or the data for the database
from Amazon S3
to Amazon redshift.
Let's go back to the editor
and I have this copy command.
I'll explain you the format.
Control C.
And let's paste it at herb.
Okay, let's explore
this copy command.
It says copy to the table users,
which you just created from
this path that is from the file,
which is toward an S3 bucket.
But this is the credential AWS.
I am role which we copied
to the editor the earlier.
Apparently, we just
giving a permission
to access the data from S3.
So we need to copy
this I am rollio
and then we have delimiter
as then let me go back
to a return show you an example.
Amber Okay, let's say
I've added all the child's name.
Archana space some h b. Hobbies
so you can see the straight line
This is the delimiter
as in the thing
which are using two separate.
All the fields are the columns.
So going back.
So that's delimiter
which separates the data
and this is region in which
you are S3 bucket is located.
So that's it.
We have to replace diam roll.
This is the AR and if
the role I'm going to copy it
and wherever this is you
need to just paste it ctrl-v.
Can the dawn last one
so select everything
and click on the execute button.
It might take a while
because the data set
which was stored
in Amazon S3 might contain
large number of rows.
So it might take a while as
far as you can see it states
executing statement here.
It says one out
of seven finished
so we have six more to go.
So this is good work bench
has successfully executed
all the script
which we have written here.
Let's go and start performing
some simple queries.
Let's say I want to extract
the metadata of user table.
I have this query OK select star
from page table definition.
So since we are extracting
metadata from table name,
let's say users and click
on execute option.
So you have so many columns.
You ought to taste
First Column user ID
of type integer
and coding Delta.
Then you have user name first
name last name city state email.
So basically that's the metadata
or the structure of user table.
So we have sales ID list
ID seller ID by your ID
and many other details.
Let's execute another command.
Let's say I want to find
total sales on a given date.
Okay some the count
your have some function.
Which will count the number
of sales from sales and date
where the sales data is date ID
and the date on which I want
to calculate a specified here
and then click.
Okay the summit at your number.
Let's just walking on it
that is not working.
I've selected the user table
and I've asked them
to display all the all
that dairies in the user table.
So this has the data say
select star from users.
So I want to extract
the names of people
who are from let's
say some states.
Let's consider some State.
Let's take an edge
so s Tage Like
and hatch it should work now
it is executing statement.
So these are the people
who are from State and Edge.
So basically once you
if the perfect connection
from your SQL work bench
to your Amazon redshift,
you can perform
whatever queries you like.
So let's go back
to our Amazon redshift console.
Well, So this is the cluster.
I'm going to click
on this here you have queries
when you click on that
all the queries,
which you performed
till now will be shown.
So this is the query
so it says first name
from users was from State NH.
This was the query
which we performed earlier.
So you have all the data
or all the information
regard the queries
which are executed.
Well, that's all
about Amazon redshift.
So guys, this is
how easy it is to create
a data warehouse using
Amazon redshift go ahead
and explore different many other
features of Amazon redshift.
Well, I've just showed
a part of them here.
So go ahead and create
a database perform
various queries and have fun.
So when you talk
about software development,
you have to mention develops.
Now.
Let's try to understand
why to do that.
Let me give you
this basic definition first.
So it is nothing but a set
of practices intended to reduce
the time between committing
the change to a system
and the change being placed
into normal production
while ensuring high quality.
Yes, very text bookish
and again for people
who do not know
what devops has this
might seem a little way.
So let me just simplify this
definition for you people again.
See an image here
what you see is
you see a developer.
You see an operator
and there is a deployment wall
which none of these two
is ready to take responsibility
of they're pushing the
responsibility on someone else.
So yes, this is
what the scenario is
when you talk about
software development again,
let me give you a little more
idea about this particular term.
So let's try to understand
how developers work
and how operators work and
when you talk about developers,
their responsibility is
to create code to update
this code whenever required wait
for the next releases and
if there are any changes
commit those changes submit
those changes and again move it
to the production environment
where the operators take care
of it then wait for the feedback
from The Operators
if there is any
and then again go
through the changes
if there are any likewise wait
for newer software is
newer products to work on.
So, yes, this is what
their responsibility is create
code create applications, right?
So what happens here is
when you do create a software,
so there are constant releases
that you need to focus on.
We all know that every now
and then you'd be getting
a Windows update
or Our mobile phone update
saying that okay,
you have a new operating system
new release new version updated.
So this is
how the technology is working.
Everything gets updated every
now and then so the reason this
is happening is people want to
stay competitive in the market.
The software company is at least
and they want to ensure
that the product has
the latest features.
So this puts burden
on the developers
because they have to constantly
update the software now
once they update
a particular software.
It has to go and work
in the production environment,
but at times it does not work
in the production environment
because the developer
environment And the production
environment might be
a little different.
So something that works
in the developer environment
might not work
in the production environment.
So again, some changes
are thrown back by The Operators
and developers again get stuck.
So they have to wait
till they get the response
from The Operators and
if it takes a longer
while their work is stuck.
Now if you take a look at it
from The Operators perspective
the job is to ensure
that whatever is working
in the developer environment.
It has to work in the production
environment as well.
They deal with the customers
get their feedback and
if there are any changes
which need to be implemented.
At times the
implemented themselves
if there are any core
or important changes
that are required those have to
be forwarded to the developers.
So yes, what happens
at time says what works
as I've already mentioned works
in the developer environment
does not work
in the production environment
and operators might feel
that this was the responsibility
of the developer
which they did not do and
probably they are facing problem
because of it again
the customer inputs.
If those are forwarded back
to the developers team.
The operator team has to depend
on the developers to make
those changes, right?
So as you can you see
these two teams are
interdependent on each other
and at times they feel
that somebody else's work.
The developers work is pushed
upon the administrators
or the developers feel
that the administrators teams
work is pushed up on their side.
So there is this constant tesl
with the company owners have
to take care of they
have to think as an okay
if this goes on
how can I generate
or produce new releases
new software's every now
and then this could be
a problem, right?
So this is what devops does
as the name suggests.
It is deafplus Ops that means
it combines the operation.
Team and the devops team
when I say combined
they bring in this approach
where integration
and deployment and delivery.
It happens continuously
and the fact
that these things
happen continuously.
We do not see the tussle
between these two teams.
So yes as you move further
develops helps you unite
these two teams and they
can work happily together.
So this is what happens
in devops you code your plan
you release this deployment.
There's operations.
There's monitoring this testing
everything happens in a Pipeline
and these are some
of the popular devops tools
that let you take care
of all these things.
But now again this is the warps
in general you have get
you have puppet you have Chef
you have ansible saltstack
that help you automate
this process of integration
and deployment of your software,
but the fact
that everything is moving
to Cloud these days we
are thinking about how can we
do all these things from cloud.
Do I need to move
in these many tools
if you want definitely you
can move all these tools
but a platform.
Ew s which is a popular
cloud service provider
what they have done
is that ensured
that all the requirements
of develops can be taken care
on the platform itself and you
have various services
that are made available to you
that help you in this process
now say for example,
you have easy
to write instances.
Now you can launch servers
at your will you can launch
instances at your will so
if your concern
is scaling up and down,
aw takes care of it you
have various Services,
which help you
monitor your process.
So monitoring is something
that is taken care of.
There's auto-scaling
their various other services
which this cloudfront
which actually lets you create
content delivery networks.
I mean, you can
have temporary caches
where you can store your data
and stuff like that.
So there are
various AWS services
that actually help
you carry out the divorce
or the CI CD process
with a lot more ease and
that is why it develops an AWS.
They form a very good
combination or a combo, hence.
We are talking
about this term today.
That is AWS develops.
Not that we have some idea
about what AWS is what devops
is let's try to understand
how continuous integration
delivery and deployment
work with AWS and
how they incorporate
the devops approach to do that.
Lets try to understand
continuous integration
and delivery first.
So let's take a look
at this diagram
to understand this process.
So these are the four steps
that are there you
have split the entire chunk
of code into segments.
So guys think of it as more
of your mapreduce kind
of an action.
I mean, I mean what happens is
in your continuous
integration and delivery.
We are trying to bridge the gap
between the developer team
and the operations team, right?
So we try
and automate this process
of integration and delivery.
So the fact
that continuously you have
various software updates,
which I just mentioned right?
So what if I have like
50 or maybe a hundred developers
who are working parallely now,
there are certain resources
that need to be
used by everyone.
Right?
So what problem it
creates is suppose
if I'm working
on a particular code.
I work on that piece of code.
And if somebody else is working
on that piece of code
and we have this Central system
where the data
needs to be stored.
So I'm working
on this piece of code.
I make a particular change
and I store it there now
someone else is working
on this piece of code
and that someone
makes a change and he
or she stores it there, right?
So tomorrow if I come back
probably I need a fresh copy
of this piece of code.
What if I just start working
on the piece of code
that I'm working and then
I submit that code there
so there would be an ambiguity
right whose coat to be accepted
who's codes copy should be made
so we need this Central system
to be so smart that each time.
I submit a quote it updates.
It runs tests on it and see is
whether it's the most
relevant piece and
if someone else submits
that deputies of code then tests
are run on that piece of code.
This system should
be able to ensure
that each of us next time
when we go and pick
the piece of code.
We get the latest piece of code
and we get the most updated
one are the best piece of code.
So this process of meeting
the code putting in that piece
of code and automating
this whole process so that
as it moves further,
it also gets delivered
and deployed to the production
in the similar manner
with the tests
that need to be conducted is
called as continuous integration
and delivery now integration
as I've mentioned here
the continuous updates
in the source code or the code
that I'm building the code
is built compiled and
when I talk about delivery and
deployment the pieces of code
once they're ready to move
to the production environment,
those are continuously
he deployed to the End customer
now deployment seems
a very easy process, right?
I mean picking up the code
and giving to the End customer.
No, it's not that easy
deployment actually involves
taking care of all the servers
and stuff like that
and spawning up.
These servers is
a difficult task.
So automating this process
becomes very important.
And if you do it manually
you're going to suffer a lot.
So yes, this is
where continuous integration
and delivery comes
into picture code.
It is continuously generated.
It is compiled it is built
and compiled again then tested.
And then delivered and made sure
that it gets deployed
to the End customer
the way it was supposed
to be so you can see
that there are certain steps are
it says split the entire chunk
into codes or into segments
keep small segments,
of course into manageable form
basically integrate these
segments multiple times a day,
which I mentioned
that there should be
a central system
and then adopt a continuous
integration methodology
to coordinate with your team.
So this is what happens.
I mean you have
a source code repository
where the developers
work they continuously.
Submit their pieces
of code now repository think
of it as a central place
where the changes
are constantly committed.
Then you have a build server
where everything gets compiled
reviewed tested integrated
and then packaged as well.
Finally certain tests
final tests are run to go
through the final integrity's
and then it goes
to the production environment
where this process
the building the staging
and the committing process it
gets kind of automated
to reduce your efforts.
So guys when you talk
about a double Dress
in particular you have something
called as AWS code pipeline,
which lets you
simplify this process.
It lets you create a channel
or a pipeline in which
all these processes
can be automated.
So let's take a look at
those processes as well first.
Let's get through
the definition part.
Let's see what it has to say.
I wouldn't be blankly
reading this thing
and then promptly
we'd be having the explanation
part that follows.
So as the definition says
it is a code pipeline
which Is nothing
but a continuous
delivery service we talked about
continuous delivery already
and you can use the service
to model visualize
and automate certain steps
required to release
your software something
that we've already
discussed in continuous
integration and delivery.
So this is basically
a continuous delivery service
which lets you automate
all these processes.
So as I mentioned
automating these processes
becomes very important.
So once you do use the service,
these are some
of the features it provides
you it lets you monitor
your processes in real-time
with Comes very important
because we are talking
about deploying software's
at a greater pace.
So if this can happen
in real time,
I mean if there
is any change and
if it is committed right
away probably just saving a lot
of time right you ensure
consistent release process.
Yes as I've told you deploying
servers is a difficult task
and time-consuming task.
If this can be automated a lot
of effort is saved
speed of delivery
while improving quality.
Yes, we've talked
about this as well
and will pipeline history
details monitoring becomes.
Very important guys.
So what court pipeline does is
actually lets you take a look
at all the processes
that are happening.
I mean if your
application is built,
it goes to the source,
then it moves
to the deployment.
All these processes
can be tracked in the pipeline.
You get constant
updates as a new cat.
This happened at this stage.
If anything failed
you can detect
as know K. This is the stage
where it is feeling maybe
stage number 3 stage number
four and accordingly
you can edit the stuff
that has happened at that stage
only so weaving the pipeline.
Details actually helps a lot
and this is where code
by plane comes into picture.
So this is what the architecture
of Code by plane looks like.
It's fairly simple guys.
So some of this might seem a
little repetitive to you people
because the concepts
are similar the concepts
which we discussed
those can be implemented
by using Code pipeline.
So ESF talked
about these things,
but let's try to understand
how the architecture works
and we will be using
some other terms
and discuss some terms
in the future slides as well,
which we've already
talked about but each
of these Isis they do
this task a little differently
or help you automate these
processes hence the discussion.
So, let's see
how much level can we keep
it unique and let's go ahead
with this discussion as well.
So, let's see
how the code pipeline Works.
Basically there are developers
as I've already mentioned these
developers would be working
on various pieces of codes.
So you have continuous
changes and fixes
that need to be uploaded.
So you have various Services.
One of them is code commit
which lets you have
a initial Source
management system kind of a
Which lets you basically take
care of repositories
and stuff like that.
So it lets you directly connect
with get I would
be talking about get
what get is but for people
who know what get is
if you have to manage
your git repositories,
you have a service called
as code commit.
So this is what happens
if there are any changes those
go to the source developers
can commit those changes there
and then it goes
into the build stage.
This is where all
the development happens.
Your source code is compiled
and it is tested then it goes
to the twist aging phase.
Where it is deployed
and tested now when I say tested
these are some final tests
that have to be implemented
before the code gets deployed.
Then it has to be approved.
Manually.
It has to be checked manually
whether everything is in place.
And finally the code is deployed
to the public servers
where customers can use it again
if they have any changes
as I've mentioned those
can be readily taken from them
and it goes back again
to the developers
and the cycle continues
so that there is
continuous deployment of code.
This is another look at it.
It is very Simple
but this is more
from AWS perspective.
So if there are any changes
that developers commit those go
to the source now,
your data is stored
in a container called as S3
that is simple storage service
in the form of objects.
So if there is anything
that has to happen
the data is either fetched
from the storage container,
which is S3 and the changes
are built and then again a copy
of it is maintained
in the form of zip
as you can see here.
There are continuous changes
that are happening
and those get stored.
In the S3 bucket now
S3 should preferably be
on the region or in the place
where you are pipeline.
Is that helps you carry
out the process of continuous
integration and delivery
with he's in case
if you are concerned
with multiple reasons,
you need to have
a bucket at each reason
to simplify these processes.
So again here to the code
gets to the source.
It is probably submitted
to the build stage
where the changes happen
a copy is maintained at S3.
And then it goes to the staging
again a copy is maintained
and then it gets deployed.
So this is
how the Quarter pipe line works
and to actually go ahead
and Implement all the actions
of quarter pipe line.
You have a service
or the services
that is your code deploy built
and code commit in AWS.
So these Services actually
help you carry out some
or most of these processes
that are there.
Let's take a look
at those services
and understand what do they do?
So first and foremost you have
your code deploy code built
and code commit.
So this is not the order
in which you deal
with these things.
Now these things actually
help you in Automating
your continuous delivery
and deployment process they have
their individual commitments.
Let's talk about them
one by one first.
Let's talk about code commit
which is last in the slide.
So basically I talked
about moving a piece
of code to a central place
where you can continuously
commit your code and get
the Fresh store the best copy.
That is their right
so code commit
what it does is
it helps you manage?
Your repository is
in a much better way.
I mean think of it as
a central repository.
So it also lets you connect
with get Which itself is
a central storage or a place
where you can commit
your code you can push
and pull that piece
of code from their work
on it make own copy
of it submitted back
to the main server
or your main or
Central operating place
where your code gets
distributed to everyone.
So that is get
and what core come it
does is it lets you integrate
with get in a much better way
so you do not have
to worry about working
on two different things.
It helps you
not Ematic authorization
pulling in the repositories
that are there
in your gate account
and a number of other things.
He's so yeah,
that is what code commit as then
you have something
called as code built
as the name suggests.
It helps you automate the
process of building your code
where your code
gets compiled tested
certain tests are performed.
And again, making sure
that artifacts of the copies
of your code are maintained
in your S3 and stuff like that.
So that is what code billed as
and then you have code deploy
as I've already mentioned
deployment is not an easy task.
I mean if we are stuck
in a situation
where we are supposed
to manage the repositories
we're supposed to On quite
a few things in that case
if we are forced to kinda take
a look at the servers as well
for new instances pain
new piece of servers
that could be a tedious task.
So code deploy
helps you automate
these processes as well.
So this was some basic
introduction to these things.
Let's just move further
and take a look at the demo
so that we can talk about some
of these terms and the terms
that we've discussed previously
in a little more detail.
Now in one of
my previous sessions.
I did give you a demo
on continuous integration
and delivery I believe
If they were certain terms
that people felt were taken care
of in a speedy way hope
that I've explained
most of the terms
with more finesse this time
and in more detail
as we go through the demo to
I will try and be as low as
possible so that you understand
what is happening here.
So let's just jump
into the demo part guys.
So guys, what I've done
is I've gone ahead
and I've switched
into my AWS console for people
who are new to AWS again.
You can have a free
tier account with AWS.
It's very easy.
You have to go and sign
input A credit card
or debit card details
a free verification would happen
and probably you would be given
access to these Services most
of these services
are made available to you
for free for one complete year
and there is certain limitation
on these services.
So you have to follow
those limitations
if you cross those limitations,
maybe you'd be charged
but that happens rarely.
I mean if you want
to get started
definitely this one year
free subscription is more
than enough to get Hands-On
on most of the services.
So I would suggest
that you create
this free tier account.
If you've taken a look
at my previous videos,
you know that how to create
a free to your account.
If not, it's fairly simple.
Just go to your browser
and type AWS free tier
and probably you would be guided
as in what details
have to be entered.
It's not a complex process.
It is fairly simple
and it happens very easily.
So we just have to go
ahead and do that.
Once you do that again,
you'd be having access
to this console guys.
So once you have an access
to this console,
you have all the services
that you can use.
So in today's session we would
be working on a similar demo
that we worked in our one
of the previous sessions here.
We would be
creating an application.
In a pass application platform
as a service application
and we would be deploying
that application using
our core pipeline.
So there would be talking
about other terms as well.
Like code commit code
different code built.
So do not worry we would
be discussing those as well.
So this is what the demo is
for today's session.
So guys, let's start by creating
a pass application to do that.
We would be using
elastic Beanstalk,
which lets you have a ready
to use template and using
which you can create
a simple application
at this being a demo guys.
We would be creating a very
simple and a basic application.
So just Come here
and type elastic Beanstalk.
So when I come
to this page guys,
if you've created
an application,
it would show you
those applications,
but the fact that
if you're using it
for the first time,
this is the console
that you'd be getting
that is why I have created
this demo account.
So that probably we get to see
how you can start
from the scratch.
So if you click on get started
as creating an application
here is very easy,
like extremely easy you have
to enter in certain details
only it takes a while to create
an application under Stan double
I would tell you why it takes
the time but once it happens,
it happens very quickly.
So all you have to do is
give your application name.
Let's call it
say deployment tap.
I'm very bad
at naming conventions.
Let's assume that this is good.
You can choose a platform guys.
You can choose
whatever platform you want.
Say PHP is
what I'm choosing right now as I
told you it's a pass service
past that is platform
as a service means
that you have already
to use platform guys.
That is why you can just choose
your platform and your elastic.
In stock would ensure
that it takes care of all
the background activities.
You do not have to set
up your infrastructure.
It takes care of it.
So once I select the platform I
can use the sample application
or use the code
if I have in this case,
I would be using a sample code
that AWS has to offer
and I say create.
There you go guys.
This thing is
creating my application.
So whatever is happening here,
it shows that these are
the processes now,
it is creating a bucket
to store all the data
and stuff like that.
So it would take care
of all these things guys.
It might take a couple
of minutes of meanwhile.
Let's just go ahead
and do something else.
Let me just open it up
loose console again.
Somewhere else.
I hope it does not ask
me to sign in again.
I've already signed in.
So meanwhile that
application gets created.
Let me just go ahead
and create a pipeline guys.
So code pipeline again
as fairly simple guys.
What happens here is very easy.
I just go ahead and put in
certain details here as well
in my pipeline would be created.
So do you want to use
the new environment
or wanna stick to the old one?
You can click on Old right
and you can go back and create
it the way it was done
or you can use
the previous environment.
I'm going to stick.
And I was very
comfortable with that.
So let's just stick with it.
If you want you can use
the new interface.
There's not a lot
of difference certain little
are minor differences.
So you can just come
here and add in the name
of the pipeline that you want
to creates a demo pipeline.
I see next Source provider guys.
I would be using GitHub here
because I want to basically
pick up a repository from GitHub
that helps me in deployment.
So I need to connect
together for that.
It would ask me to authorize
if you have an account.
You can always do that so
that it can basically
ringing all the repositories
that you have.
So just say authorized if not,
you'll have to sign in once
so my account has been added
here guys repository.
I need to pick a repository.
This is the repository
that I would be picking.
Do not worry.
I would be sharing
this piece of cord or is
what you can do is you can just
go to GitHub and type AWS -
Cole pipeline -
S3 - code deploy -
Linux now it is a repository
given to you by AWS
if you take a look at it,
and if you type it just the way
it is named here from AWS.
You should get
that repository in GitHub.
You just have to go
ahead and Fork it
into your GitHub account
and probably you
would be able to import
that repository directly.
You can see that repository
has been fought.
Here into my GitHub account.
You just type the name hear
this name search it
and probably there would be
an option your fork.
I fucked it.
So it does not activate
this option for me in your case.
It would be activated.
You have to just click on it
and the repository
would be forked
into your account.
So I am getting or importing
a fork from my GitHub.
I was authorized my account
and then I can just go ahead
and do the stuff
Branch Master Branch.
Yes, and just do the next step
build provider no build here.
I don't have Teenager to build
so I don't need to go ahead
and provide a bill provider.
You can use code
build right guys,
if you want to move
or basically deploy
your code to ec2 instances.
You can use code build.
If you want in this case.
I have an application
in which I have an ec2 instance
and stuff like that.
So I don't need to go ahead
and do any building stuff.
Hence no build for me.
So I say next deployment
provider in this case.
My deployment provider
would be my EBS
so we have that option.
Yes.
Yes select EBS
elastic Beanstalk.
Naughty BST b stands
for elastic block storage.
That is a different thing guys.
Elastic Beanstalk.
Make sure you do
that application name deployment
a pause the name, right?
Yep, and the environment.
This is the environment.
It creates the environment
on its own.
I believe that it
has created the environment.
It says it is starting.
I hope the environment
has been created.
So guys, let's just see
whether our application
is up and running
so that probably I
can pass in the details.
Yes, the application
has Been created guys.
So let's just go back
and select this say next
now create an IM role is already
saying so let's say sample.
Okay guys, so
what happens normally is and I
am user gets created each time.
You create a role.
So in this case it is asking me
to create one taxes create a new
item role database code pipeline
nice shell of successful.
So role has been In
created next step now.
It gives me the details guys.
Basically it would tell
me what are the stuff
that I've done.
So everything is here.
I don't think I need
to cross check it.
You might just cross
check the stuff
that has happened and
say create a pipeline.
So guys, the pipeline
has been created here
as you can see.
These are the stages
that have happened.
If you want you can just go
ahead and say release
a change now these things
are happening guys,
and let's hope the deployment
also happens successfully.
We've just created an eye.
User let's see
whether it falls in place.
Everything is in place.
As far as the source part
is concerned it has succeeded
and now the deployment
is in progress.
So it might take a while.
Meanwhile just go back and take
a look at this application.
So if I open this application
guys It would give me
an overview of what has happened
with this application guys,
as you can see,
these were the steps
that were implemented.
Now the application
is available for deployment.
It successfully launched
the deployment environment.
It started with everything
that it was supposed
to do like create
or launch an ec2 instance
and stuff like that.
So everything is mentioned here
what happened at what time so
this is a passive is guys
and it works in the background.
I mean if you actually go ahead
and launch an instance
on your own configure,
I am users can
As you go to groups,
it takes a longer while
but what the service does
is it automate that process.
It understands that you need
an ec2 instance.
It launches that instance.
It assigns security groups.
We PCS and stuff like that.
All you have to do is run
your application on top
of it as simple as that.
So it has taken care
of everything and run
a PHP application for me.
So yes, this is
what has happened here.
If I just go back here.
Meanwhile, let's see
whether our code
has successfully run you can see
what has happened here.
I am released the change as
well and you can move
the pipeline history.
If you want you can click
on this icon and all the details
would be given to you
what happened in what stage.
So these are the things
that have happened
till time now guys,
let's just go back
and take a look at something
that we could so I'm going
to come here and say
service easy to because my app
launched an ec2 instance.
So there should be
an instance created
by elastic Beanstalk
C1 instances running.
It has a keep your attached
to it as well.
So He's any details guys.
I have a public IP
associated with it.
If I copy it.
There you go copy this IP
and I say run this IP you have
successfully created a pipeline
that retrieved this
source application
from an Amazon S3 bucket
and deployed it
to three instances.
It did not deploy
to three instances
using Code deploy.
It deployed it
to only one instance.
You see this message
that it deployed it
to three instances is
because the code or
the repository that I used it.
Supposed to deploy
two different instances
if there are multiple
instances and hence.
This message would have made
more sense than but the fact
that we've deployed it
to only one ec2 instance.
It should actually
display that message.
So the message
that you're supposed to give
you can actually come back here
and make change to the piece
of code that you worked on.
If you go to the readme MD file,
I think this is
where the piece of code is.
There you go not here.
Where is that file
that needs to be edited?
Let me just take a look at.
Some other files as well.
Yeah.
This is the file.
Sorry.
So if you go to the index dot
file here is the message guys,
so you can probably make
a change to this message instead
of seeing three you can say
one here edit this piece of code
and then you submit
the code again.
So when you do launch or type
in this IP address probably
that change would be reflected.
So guys, what we've done
is we've actually gone
ahead and created
a pipeline successfully
and in that process we've
actually gone ahead and move.
Move or deployed
our application from here.
So guys in case
if I do go ahead
and commit changes to the code
that I just talked about those
would get reflected
right away in my history
when I talk about this pipeline.
So it does give you a continuous
integration and deployment.
So, I hope
that this session made
sense to you people
and we've talked artist
upon most of the stuff
that I wanted to talk about.
And as far as the
session goes guys,
I would be resting it here.
So let's start
with the first question.
Now I first question says
I have some private servers
on my premises.
Also.
I have distributed
some of my workload
on the public Cloud.
What is this
architecture called?
So basically our workload
has been divided
between the public cloud
and the private Cloud now,
they're asking me what is
this architecture called?
It's a pretty
basic question guys,
but if you look at the options
are quite confusing,
the first option is
a virtual private Network
then We have private Cloud,
which is obviously not there.
Then we have a virtual private
Cloud could be the option
and then we have hybrid Cloud.
All right guys.
So what do you think?
What do you think is
the right answer for this?
Come on guys,
let's be more interactive
in this session
because if it's
a two-way thing then
it's going to be interesting
for you and for me as well.
So let's make it
as interactive as
possible and let's get the most
out of this session today.
Alright, so a she says it's
either virtual private cloud
or hybrid cloud.
So as usual,
it's actually only one
out of all the for
so give one answer.
Okay, I can see some
of you are saying the right
answer some are confused.
It's okay.
I shall clear your doubts.
Alright guys, so the answer
is hybrid Cloud now,
why hybrid Cloud because okay.
So let's actually discuss
the first three options
which are actually
not the right answer.
So it is not a virtual
private Network because
a virtual private Network.
Is something that you use
to connect your private cloud
and your public, right?
So to connect between
your private cloud
and the public Cloud
you actually have
to make a connection
and that connection is done
using a virtual private Network.
Alright, then we
have private clouds
or private cloud is something
where in you have
your own servers
on your own premise, right,
but in our case we have
public Cloud involved.
So it is obviously not private
Cloud virtual private cloud is
not the As well
because a virtual private
cloud is basically
a logical isolation kind
of thing wherein you
isolate your instances
from the rest of the instances
on your AWS infrastructure.
And this logical
isolation cloud is called
a virtual private cloud
and then you have hybrid Cloud
which I think fits aptly
by its name as well.
We're in it's a mixture
of your public cloud
and your private
Cloud infrastructure,
right?
So, let's see the answer.
So the answer is hybrid cloud
and the nation is like this
because we are using
both the public cloud
and you're on from Isis servers,
which is a private
Cloud be called
and hybrid architecture,
right and it says here
that if you want to be better
if your private and
public Cloud were all
on the same network, right?
So basically when you
connect your public cloud
and private Cloud together
using virtual private Network,
you basically are accessing
one network and you feel
that all your resources.
Is it says dead
on the public cloud
and the private Cloud
are actually there
in one network, right?
So it seems It's
a virtual private
and virtually you feel
that you are
on the same network,
but it's they are actually
two different resources
or two different locations
from where you are
accessing your resources.
Alright guys,
so guys any questions regarding
to the first question
that we have discussed anything
that you're not clear
whether it was
a very basic question,
but then we are getting
a very lot of Concepts.
Here, we have a virtual private
Network concept then we have
the virtual private
Cloud concept, right
so it can be confusing
and this is how they asked
to you in interviews as well.
Right?
So you have to be
very clear in your answer.
You have to be very clear
in your thoughts
that what shall be
the right answer.
All right, so I can see
that people are giving
me a go there all clear.
Okay guys, so let's move on
to the next question then
so our next question starts with
our Section 1 which is easy.
Questions, so it's from here.
We'll be talking all about AWS.
So let's start
with the question first.
So we have a video
transcoding application
and the videos are processed
according to work you
with the processing of a video
is interrupt in one instance.
It is resumed
in another instance.
Okay, good enough.
Then currently there is
a huge backlog of videos
which needs to be processed.
But this you need
to add more instances,
but you need these as mrs.
Only until their backlog
is Oost right.
So once your backlog
is reduced you don't need
those many servers.
So which pricing option
should be the efficient
should be the most
cost efficient for this?
Okay guys, so first of all,
when you have
question like this,
a lot of things are added
into it to make it confusing.
So first of all,
the things is the first
line reads that it's
a video transcoding application.
So it is not relevant
to your question, right?
It is not relevant to
what is being asked
so you Discard that out
and then it says
the videos are processed
according to work you again,
it's their confuse.
You don't the first thing
that you should look out
into a question,
which are trying to men
are trying to figure out
an answer is the important part.
What is important
in the question you
should be able to unfair
that so according to me.
The thing that is important is
that there is a huge
backlog of a video.
So there is a lot
of pending work
and this pending work has
to be reduced right and one.
Is it is reduced we
will not be needing
those many servers.
So basically we are increasing
our number of servers
to actually reduce the number
of backlogs that we have.
And once we have reduced
that we have an application
wherein we don't need those too
many servers anymore
so we should get rid of them.
Right?
So now it is asking
me which pricing option
should be efficient
for this seller.
Now, you have three kind
of pricing options you have
on demand pricing then
you have spot pricing.
And then you have
reserved place, right?
So you spot pricing
is basically used
when you want servers
at the minimum cost.
So basically what happens is
why spot pricing has
an introduced is because of this
that new AWS has centers, right?
It has service zones
where it has a lot
of servers now not all the time
that the servers
are actually being used.
Some of the times
are idle, right.
So in times like this
when the servers
are ideal, what eight?
SS does is it gives
you a discount that
since no server is being used.
I shall give you a discount.
If you want to use
my servers now in this case
you use pot pricing.
So if you are going
for spot pricing you
see these reduced rates from AWS
whenever their servers are idle
and you should bid rate, right?
So say example servers
are being offered
at some particular price.
And you say Okay.
I want these many servers,
but I can only afford $10.
So as long as the server
And be allotted to me for $10.
I shall use them.
Right so you set
your price a $10
and then you use the service
but the moment
the demand increases
in that particular server
location the prices go up again.
All right,
and if the price crosses $10
your server shall be shut down,
right you will not be able
to access that server anymore.
Right?
So this is what spot pricing is
you basically bid
for the minimum price
and whenever the price.
Co-
op your server is taken
from you right then
second type of pricing
is called reserved pricing.
When you reserve your servers
for a particular amount
of time say a one-year term
or a three-year term, right?
So it the application
for this could be
when say I have a company right?
And my company has a website.
So my website is hosted on AWS.
Now, my website
is going to be there
till my company is there right?
So it makes sense
for me to actually
reserved the instances
for like maximum Dome.
Possible because I have
no plan to sell my company
and hence take down
my website right now.
The reason people offer
reserved instances is
because as compared
to the on demand pricing
the reserve pricing is
actually pretty cheap, right?
So if you reserve your instances
for a longer term,
you get discounts
from AWS, right
and then we have
on demand pricing
where and we can get
as many servers
as you want at the time
what we want as
per your requirement
at whatever time you Choir
and the pricing for them
are standard right?
I'll not say they are high
but they are standard
but they are more
than reserved pricing
and your spot pricing.
Now.
Our question says
that we have to reduce
the backlog and once
a backlog has been reduced.
We'd have to get rid
of the service.
So obviously will not be
using reserved instances
because we cannot save
and our backlog
will be ending right?
We cannot be using spot prices
because we want that backlog to
be reduced as soon as possible.
So what we'll do
is we'll be using
on-demand instances or on
demand pricing and using
that we will reduce the workload
or will reduce the backlog
of the videos.
And once it's been reduced we
will reduce the server size
for our instance.
Right?
So the answer for this
should be on-demand instances
and if you read the explanation,
you should be using an on-demand
instance for the same
because the workload
has to be processed now
meaning it is urgent.
Secondly you don't need them.
Once you have
a backlog is cleared.
Therefore is evidence is
out of the Picture and
since the work is urgent.
You cannot stop the work
on engines just
because the spot price by right.
So therefore spot price
in can also not be used
and hence will be using
on demand has.
All right guys,
so any doubt in this question
anything that you're not clear
with by are we using
on demand pricing?


---AWS Certified Cloud Practitioner Certification Course----


hey this is Andrew Brown over here on
free Camp bringing you another free
Cloud certification study course and
this time we are looking at the ads
Cloud practitioner also known as the clf
C02 and the way we're going to achieve
ads certification is through lectur
content Hands-On labs and as always I
provide you a full free practice exam
the best way to support uh more free
study courses like this one is to
purchase the optional paid additional
materials it's going to help you on your
exam and it's going to allow me to
produce more of these uh great Cloud uh
certification study courses if you don't
know me I'm Andrew Brown and this is the
fourth time I've taught this uh
certification so it's really refined at
this point and I've taught a bit of
everything in the cloud so we've looked
at ads Azure gcp terraform kuber denes
you name it I've taught it uh but that's
about it and I will see you in class in
the next video ciao
[Music]
hey this is Angie Brown and we are at
the start of our journey asking the most
important question first which is what
is the adus cloud practitioner well it's
ad's entrylevel certification that's
going to teach you things like the cloud
fundamentals so we're talking cloud
Concepts architectures deployment models
it's a close look at adus core Services
which would be our compute our storage
our Network or databases and it's a
quick look at the vast amount of adus
services and functionality around adus
so we're looking at identity security
billing pricing support and a lot more
stuff and we'll get into that in the
course and we'll even look at the exam
guide outline but uh yeah there is a lot
of stuff um the course code for this
certification is now the clf C02 the old
one was the
c01 uh the way to know if there is a new
course is if this becomes the c03 if you
see that then this course um may be out
of date um but uh yeah right now it's
the C02
um often people refer to this
certification as the CCP to stand for
the certified clock practitioner how you
want to refer to it is up to you but uh
there are a few ways of describing this
certification I want to point out that
adus is the leading cloud service
provider in the world and the cloud
practitioner is the most common starting
point for people breaking into the cloud
so even if you're going to uh utilize
another cloud service provider I'm just
going to say that you're going to get a
really good uh Foundation with this
certification even if it's not the uh
same provider uh so who is the
certification for well consider the
cloud practitioner if you are new to
cloud and you're learning the
fundamentals you are at the executive
management or sales level and you need
to acquire strategic information about
Cloud for adoption or
migration or you are a senior Cloud
engineer or Solutions architect who
needs to reset or refresh their adus
knowledge after working uh with cloud
services or adus for multiple years
um it's always a surprise that when I
come back and I refresh this course uh
the things that have changed and it's
very easy to miss those things so yeah
this this certification is for everybody
so what is the value of the
certification well this uh certification
provides the most expansive view
possible of cloud architecture and ads
it'll uh we I would describe this as
having a bird's eye view or the 50,000
ft view so with that in mind uh the idea
here is to promote big picture thinking
we're zooming out and assessing the
cloud or itus landscape for things like
changes Trends opportunities um and it's
important to understand about being
strategic about the approach and process
for your journey and that's why I like
the certification so much and I strongly
uh recommend it for everybody's Journey
so what is the value of the
certification well it's not a difficult
exam uh it's it's not going to validate
that you can build Cloud workloads so if
you are trying to obtain a technical
implementation role like develop Cloud
developer Cloud engineer devops engineer
uh it's not going to be enough to attain
those technical Cloud roles um but it
could help short list your resume for
interviews um the exam covers content
not found in other certifications so it
is recommended as an essential study
guide uh for your adus journey do not
skip this one uh some people like to go
straight to the solution architect and
then they realize that they didn't set a
good foundation or they just have gaps
uh in their knowledge which could really
help them out in their careers so really
do not skip this one um I like to make
these road maps to give you an idea uh
in terms of where you can go after this
certification so here is uh all the
certifications currently that has
notice that I have the data engineer
it's a really small one it just became
uh came out as a beta exam it's not as
hard as the professionals it's just
where I place it on this diagram um but
the idea is that we have a lot of
different ways that we can navigate or
uh work through these certifications
these can generally map to particular
roles in the cloud so uh very often
people go right to the uh Solutions
architect I'm just getting my pen out
here but very often this is the approach
that'll go straight to here uh right
after the solution architect because
they're very similar um in terms of uh
scope and Challenge and difficulty where
the solution architect is a broad
certification just like the cloud
practitioner but it's more focused on
the technical knowledge uh whereas this
one of course is much more broad the
cloud prer
and then after that people will
generally go for the developer or the
CIS office administrator in my personal
opinion I really do think that people
should study all three Associates and do
all three Associates at the same time uh
because really I don't find that uh it
makes sense to leave out the ssops admin
or developer knowledge um it's just the
way that itus Engineers their
certifications but when you go to other
ones like let's say Google they only
have one associate and they have all the
um they call Cloud engineer and it has
everything in it and so uh again I just
feel like you should take all three but
you decide what works for you um and you
know you can see that there are various
routes but I want to just make it very
clear that certifications do not
validate programming they do not uh make
you do technical diagramming they don't
necessarily make you do code management
and there's many other technical skills
that are required for obtaining
technical roles like these roles um and
that is not the purpose of certification
certification is supposed to give you
knowledge specifically on us and so just
understand that you need to make sure
you get those skills uh somewhere else I
do try to uh slot in a lot of these uh
technical skills uh where I can and so
if you're uh if we're doing something in
the course and you're wondering why are
we doing this when it's not on the
certification it's because I'm trying to
give you those adjacent skills uh so
that you are successful um in the future
okay so how long does it take to uh
study to pass this exam well depends
right it depends but if you're a
beginner we're probably looking at 30
hours so this is someone who's never
used datab bus or cloud provider before
uh you've never written code or had a
technical role if you're experienced uh
your study time is going to be very low
like as low as 6 hours even lower uh if
possible um especially if you've already
taken the certification I sat it um uh
blind right I didn't look up anything
and I passed it uh no problem um but uh
so it says here you know if we've
practiced we have experience working
with ads if we have an equivalent
experience in another cloud service
provider some people are coming over
from Azure or gcp so they can kind of
map their knowledge over to adabs or if
they have a strong background in
technology uh you might really be
already familiar with these kind of
offerings from
another uh like from another discipline
and so your study time can be really low
but I would say that um you know the
average study time is probably 24
hours so yes it's closer to the beginner
level but that's the average study time
that we found and so it's basically a
split between 50 lectures and Labs so
labs are Hands-On skills and 50% with
practice exams uh a lot of people forget
that practice exams are part of the
study process so make sure that you do
do that uh we do recommend a study a
study time of one to two hours a day uh
for 14 days uh what does it take to pass
the exam we're still going on with this
here but you know you have to watch
those lecture videos and memorize key
information this is a knowledge based
exam it's not a uh it does not test your
skills so knowledge is key here uh you
should do Hands-On Labs uh we call those
follow alongs within your own account uh
this is just going to help uh sment the
knowledge in your head it really makes a
a huge difference so really do those
Hands-On labs and get practice exams to
simulate the real exam you absolutely
need to do this because if you don't
you're going to find that you did all
the study materials and then uh the exam
is its own uh Beast so make sure that
you go get some practice exams there's a
lot of places that you can get uh get
them from uh we offer a full free
practice exam I think we're the only
provider that does this but um we give
you like a full free practice exam and
we also have some paid ones so the best
way to support this this content that we
produce is to purchase our additional
paid materials uh if you don't have the
money that's okay we still have at least
one full free practice exam to help you
out you can find that over at exampro
doco
cfy
C02 it looks like it know but it's a
zero uh let's talk about the content
outline so there are four domains and
you have to understand that each domain
has its own waiting this is going to
determine how many questions in that
domain will show up on your exam the
first one is cloud Concepts so that's
for 24% so we're looking at between 15
to 16 questions domain two is about
security and compliance so that's 30%
it's a a quite high up there so we have
about 1920 questions for cloud
Technologies and services it's 34% so
understanding the offerings of ads is
the most important thing in the exam
it's the highest percentage here so we
we're going to definitely get 22
questions and then uh we have domain
four so billing pricing and support
where it's at 12% so we have eight
questions not a lot for billing pricing
support definitely important because
it's very easy to get overbuild in the
stuff but just you know point out that
you need to know a wide range of adus
services you need to know about core
Services more in depth so where do you
take this exam well you can take it at
the um at an inperson test center or
online from the convenience of your own
home I personally like to take it in a
test center if there's one near me I
used to live in Toronto now I don't so
there's no test centers near me and so I
have to do it online it's just so much
less stressful walking into a building
and everything is uh controlled whereas
at home you might have a lot of things
going on and that can cause a lot of
stress but you know do what makes sense
for you so adus delivers the exams via
Pearson View and so uh there's pearon
view they have the online system which
you do uh you install on your computer
and then they also have a network of
test centers their partner with uh
previously adabs also offered it via PSI
um they don't do this anymore I'm not
sure why they changed this before it was
only PSI then they added Pearson and now
they've dropped PSI so your only option
is Pearson view I just want to point out
what a project exam means it means that
it's it's someone is supervising uh your
um your exam while you're taking it so
you're not cheating so it's very common
that when you check in they're going to
ask to look around your room you might
even have to talk to them uh and it's
just again to make sure that what you do
is um your exam was legit legit so when
they issue your badge you know it's for
real anyway let's talk about grading
here so the passing grade here is 700
out of a a th000 points and so you need
to get around 70% to pass ads like many
other CL providers use scaled scoring so
um that doesn't mean if you get exactly
70% that you'll pass but uh I mean more
less it works out to to be that okay so
the response types uh we have here well
first of all we have 65 questions and
there are 50 questions that are scored
and then there's 15 that are unscored
and if that sounds bizarre I mean I I
agree with you I think it's odd that
they give you 15 unscored questions but
the reason ads will do this is that they
want to introduce new questions um to
help test against the difficulty of the
exam um because you know maybe some
people know more than uh what they're
expecting so they can adjust the
difficulty of the exam I think that they
use it as an anti-che mechanism as well
but from the test taker it can get a bit
stressful because you can get 15 really
crazy wild questions that were not in
your um uh course studies and it's just
a of us testing things out and so I just
want to point out don't get stressed out
when you take this exam and you get a
really funny question it's probably one
of those unscored questions but on top
of that you know there are 15 scored
questions you can get wrong so you can
get a total of 30 questions wrong on
this exam and pass I just want to make
that uh really clear there there is no
penalty for wrong questions so
absolutely always submit an answer and
take your best guess the format of the
questions are multiple choice and
multiple answer so you know it's not too
stressful in terms of uh the formatting
of questions um there are again 15
unscored questions of the exam they will
not count towards your final score why
are there unscored questions uh they're
there to evaluate the introduction of
new questions they're there to determine
if the exam is too easy and the passing
score of the question difficulty needs
to be increased to discover users who
are attempting to cheat the exam or
steal dump exam questions if you
encounter questions you've never studied
for uh that seemed really hard keep your
cool and remember that they may be
unscored questions just really want to
emphasize that there in terms of the
duration you get 1.5 hours so you have
about 1.5 minutes per question your exam
time is 90 minutes your seat time is 120
Minutes what are we saying when we say
seat time this is the time it takes uh
or that you should allocate for the full
exam uh that includes uh things like
reviewing the instructions uh showing on
uh showing the online Proctor your
workspace reading accepting the NDA
completing the exam provide the feedback
at the exam so a lot of people go okay
my exam start star in or I have 90 or 90
minutes exam but really you want to show
up 30 minutes prior uh because that
checking process can be really really
stressful so you know just consider that
uh the full scope of time you need to
dedicate for these exams this uh
certification is valid for 36 months so
that's 3 years before recertification
some other providers uh like Azure if
you do the fundamentals it's forever um
other ones have require you to refresh
every year other ones um you don't have
to take the full exam you have a
reassessment that is free inabus likes
to do it this way the nice thing though
is that when you do pass a certification
um somewhere adus allows you to get the
next exam half off uh so at least there
are cost saving mechanisms if you do
pass an exam for the next follow-up
certification but yeah uh that is pretty
much a breakdown of uh the exam guide we
will go and take a look at the actual
exam guide so we can uh understand the
full scope of what's in there uh but
yeah we'll see you in the next one okay
[Music]
ciao hey everybody it's Andrew Brown and
we are here on the training and
certification page on the adus website
and what I want to do here is I want to
pull up the exam guide so that we can um
make sure that we know exactly what it
is that we're getting ourselves into uh
we did cover this in summary in the
previous video but uh I think it's
always useful for you to know exactly
where these things are
adabs is always changing their marketing
pages and I've already noticed a few
changes here so um just understand
that's the nature of cloud notice here
that it's talking about the uh beta exam
certification so even earlier we talked
about the data engineer or we at least
showed it on our journey map and it's
not even it's not even 100% out beta so
you can see we're kind of prepping for
the future here I also want to point out
that they have this like certifications
path uh thing and I I don't really like
it because I don't think it's very
accurate so the first thing they show is
Solutions architect and they don't even
say you need to get the other two
associate certifications which you
absolutely should do if before you go
for your Solutions architect
professional the data analytics is no
longer a uh certification that adus is
producing so this is an out-of-date
document so I just want you to
understand that these are marketing
Pages they're here to maximize the
amount of certifications you need to
obtain my goal is not to make you take
every certification my goal is to make
sure that you are prepared uh to do the
job and I just want to you know help you
avoid going down the certification route
and getting too many certifications that
aren't going to benefit you so just take
these with a grain of salt when you're
reading them okay so anyway what I want
to do is drop this down and go to Cloud
partitioner um and here on the cloud
cloud practitioner page if we scroll on
down we got prepare for the exam and
here we'll click the exam guide and
it'll open up a PDF and it'll give us
all the information we need to know this
is what AB has been doing for a long
time is making these um exam guide p F
which I really like uh but anyway the
first thing we should do is confirm the
course code so this one says CF CO2 so
we know we are on the right track and
then down below here it says this exam
validates the candidates ability to
complete the following task I want to
highlight some key words
explain understand describe and identify
so understand that this certification is
not checking whether you know how to do
Cloud it's more if you understand Cloud
um and the majority of aable
certifications in fact all of them are
multiple choice and multiple answers so
they can't really check if you were able
to do something in Cloud so just
understand the limits of certifications
at least eight of the certifications
based on their testing mechanisms so
when it says Target candidate it's
saying uh where you should be in order
to pass this exam and so they're
suggesting that if you had six months of
exposure to adabs uh with Cloud design
implementation operate operation then uh
you should be able to pass it it's just
weird uh worded strangely because it
makes it sound like you should have this
experience um before you even start
studying which is not true they just
mean like if you want to pass it you
don't need six months to pass this exam
that's crazy you just need what we
recommended which was um the amount of
hours we said the average hours is 24
hours so um I'm not sure why they put
six months I guess it's just they're for
those who are really having a hard time
with Cloud they give you a lot of uh um
scope for room there but you can see
they're pointing out from non it
backgrounds recommended knowledge Cloud
concept security core Services economics
that's that's just a repeating of the
domains um notice it says job tasks that
are out of scope is
coding um Cloud architecture design load
performance and testing I'm highlighting
these three because I just want to point
out that in associate level professional
specialty they actually do ask questions
around troubleshooting implementation
and I suppose they do architectural
design but they never ever ever No
certification in 8s is going to test
your coding skills architectural diagram
skills and they're not really good about
load and performance testing they have
like use case scenarios but um just
understand again the limits of the
certifications coming down below to the
response types we got our multiple
choice our multiple
response um so that's pretty clear there
there is uh 50 scored questions there's
15 unscored questions so that is very
clear
the uh uh the the the point system is
based out of a thousand to th points the
lowest you can get is 100 points I don't
know how that works why like why can't
you get zero points I don't know the
passing score is
700 so that's what we need to score
there then down below here it's just
talking about the course outline and it
actually has a comparison of the old clf
co1 so we can take a look there and see
what actually has changed so down below
here we have our Cloud Concepts as
security compliance our Cloud technology
services our billing pricing support and
then it comes in and starts describing
all this stuff now I need to make it
very clear how inabus makes their exams
they give you a huge list of things you
need to learn but if you learn um each
one of these things you can end up
overstudying or you'll find that the
like the exam guide outline is not one
to one I'll give you an example we'll
look at something else I'm going to go
to Hashi Corp here for a second hashy
Corp terraform
certification as a as an example of how
different adus certifications are so for
hashicorp they will this is their exam
guide they'll give you each of these
items and you can be 100% sure that
every single thing every one one of
these things will show up on the exam
one to one so it's very easy to know
exactly what you need to study for um
and uh if you know all these things
you'll you will pass in ad us they list
all these things but they won't all show
up they they're pulling from a very
large pool so to kind of narrow down
what you need to study you need to have
a good sense of um overall everything
and and you're just going to get some
things wrong but um anyway coming back
here the first Cloud Concepts they're
talking about the benefits of
cloud so we have a section on benefits
of cloud and so they talk about the
value proposition so there's like six or
nine of them I forget have a multiple
slides on that and so we're talking
about economics scale benefits of global
infrastructure advantages of high
availability elasticity and uh agility I
think we call these Cloud architecture
terminologies because they're not really
benefits I mean they are benefits of
cloud but I I like to group them a
little bit differently then we have
identified design principles for Abus
Cloud so we have the well architect
framework this uh was for the most part
never in the clf co1 for 90% of its
history and then they decided maybe like
last year or something to add it in um
and and uh before even wasn't even the
solution architect associate but now
it's even at this level and that's
totally fine you only need to know it at
a very high level so um it's not too
difficult to learn but it it's a white
paper it's a PDF that um you know just
describes how adus thinks that you
should design uh your architecture then
we have understand the benefits of
strategies and migration to the cloud so
we have Cloud adoption strategies Cloud
adoption framework so um this was this
was not in the last exam but luckily I
included it because I thought it was
something that was very important and so
I already have it in the certification
course even from the last one they
actually do ask quite a few questions
around the cloud adoption framework but
when you look at and again this one's
like a white paper just like this one
above here and we'll talk about what
white papers are if you you never heard
that term uh it'll make sense in the
course but the cloud adoption framework
um there's not a lot to it but the exam
they'll ask you a lot of questions
around it so you just have to have good
common sense um about uh choosing those
answers if that makes sense um
identifying appropriate migration
strategies sure I guess so I never got
any snowball questions um they they say
snowball here go down below here
understand concepts of cloud
economics so cost Savings of moving to
Cloud aspects of cloud
economics uh fixed cost compared with
variable cost
they're talking about um Opex Opex and
capex understanding the associate of on
premise
environments uh understand the
difference between licensing strategies
and adabs never ever really ever
mentioned uh bring your own licenses
ever in their certification courses and
I never got this on the exam and other
people I sat for the new exam never uh
encountered this still good to know but
I'm just saying that I don't know why
it's listed in here because it's
definitely not on the exam but it is a
good thing to not the basic level
understand the concept of right sizing
um and maybe I'll go back and make a
slide on that CU I don't think I
actually make a deliberate slide on that
but I think what they mean there is
understanding uh like how horizontal
scaling and stuff uh stuff like that
works but um you get no questions on the
exam for right sizing at least not from
its technical definition like that
identify benefits of
automation I think there might have been
one question of saying like hey which
one lets you automate stuff and you just
chose Cloud information but they really
don't talk a whole they don't ask a lot
of questions on the exam about IAC
infrastructure as a code identifying uh
manag ad services this is something they
do a lot in exams like describe a
service you pick it we have security and
compliance so we have the Ed shared
responsibility model you absolutely need
to know that that for sure always always
appears on the exam um customers
responsibility they'll do this a lot
they'll say like they'll give you a
scenario of um of like a typical
workload or resource and then you have
to uh determine if it's the customers's
responsibility or adab Us's
responsibility describing responsibility
the customer adus share so again this is
just all the share responsibility model
still here describing how itus respons
responsibilities and customer
responsibilities can shift depending on
the the service used so yeah this is
basically the share responsibility model
understand the cloud security governance
compliance so uh a compliance
governments
Concepts benefits of cloud security
they don't really talk about that uh
They Don't Really directly ask that in
the exam but yes we do cover that where
to capture and locate logs that are
associated with Cloud security they
absolutely do not ask that on the exam
I'm not sure why that's here um identify
where to find A's compliance information
that will absolutely be on the exam
understanding compliance needs among
Geographic locations and
industries sure I mean they're talking
about we have a slide in this in the um
Global infrastructure but it's um
like data sovereignty and like gov cloud
and things like that describing how
customers secure resources for ads so
just generally knowing the security
services y that absolutely is on the
exam identifying different encryption
options um I never got this on my exam I
never heard of anyone else getting this
but um if they are going to talk about
this they're probably going to talk
about it around
S3 recognizing services that Aid in
governance and compliance absolutely
absolutely for sure that the you will
get questions around uh things like fips
or Hippa or like common common
compliance certifications not specific
datab best but just in general um here
they're just talking about specific
Security Services this is kind of a
repeat of what they're talking about up
here um but there's the same there's
identity service governance service it's
all the same thing here recognizing
compliance requirements that uh vary
among ad Services sure identify itus
management capabilities so they're
talking about IM am um the itus root
account we got it uh separate slide on
that uh principal of lease privilege
absolutely absolutely will they will ask
that there a single sign on also known
as Adis am identity Center I don't know
anyone who's gotten this as a question
on their
exam but uh it's we got a slide for
it understanding access Keys yep we
cover that PO uh password policies
credential storage Secrets manager
systems manager um just a bunch of stuff
identify components and resources of
security describing a security features
so acl's uh ad US wff security groups
they really don't ask these on the exam
so I'm just trying to make a point that
they're asking for all this stuff and
they don't even it doesn't even show up
in the exam so um and you know we can
just keep going and going through this
and I could keep telling you what is and
isn't but if you go down below it gets
even crazier because they go any of this
stuff could show up in the exam it's
just like a big list it's
crazy so you know I know that seems
stressful but you know just follow
follow follow me uh in this course and I
you will absolutely pass if you go
through my content you'll have no issue
there and we'll avoid all the stuff that
doesn't show up and don't stress out
about this exam guide now let's go take
a look here and see where the rebalance
has changed so notice here that this
went from 26% to 24% they never used to
do this so I really appreciate this is
now in the exam guide but we got 25 to
30% 33 to 34% 16 to 12% why they would
reduce this one I don't know but it is
is a shuffle whatever um they of course
increased uh the technology section more
and did some basic rewarding support
should have always been in there so it
was always under that section but uh
it's nice that they labeled it as such
um so notice here it says no content was
deleted from the exam and um this was
the largest struggle for me for the
certification because I already made all
the content for the last one my old one
is not expired and I was struggling
because I already had this as well this
is the only thing that they added that
was new to the certification and then
they just rework these numbers here and
so um you know I just I added I did add
more I added more Labs I added more um
other stuff there but I'm just going to
say like I don't know why they did an
update from co1 to CO2 because barely
anything changed now I shouldn't say
that the exam questions did change I
noticed that the exam questions um the
quality of them kind of uh have dropped
I wondering if they're using generative
AI to generate out questions or or
something but um there's something the
quality of questions are are definitely
um different and I would say that
they're more uh they're not worded as
clearly as they used to be for whatever
reason um but anyway you'll still be
okay it's totally fine uh
recategorization of clf CO2 and so they
just did a shuffle of um of these points
and I again I really don't think that
the the new one is better how useful is
this exam guide I should probably give
them survey feedback but anyway just
give you an idea how much stuff there is
in here do not stress out just stick
with the course you'll absolutely pass
uh and uh you know hopefully that gives
you uh some better confidence there but
we'll see you in the next one uh chiao
[Music]
chiao hey this is Andrew Brown from exam
Pro and what we're looking at here is a
free practice exam that I provide with
you uh for this course and all you have
to do is sign up on exam Pro you don't
even need to credit card and you can
redeem uh the free available content
here and this is really up to date and
very well simulates what you will see on
the actual exam and it's a full set full
65 questions so you're getting a real
simulation here but what I'm going to do
is just start it off here we're not
going to do the whole thing I'm just
going to click through and show you a
couple of them so you have an idea um
the level of difficulty these questions
are so the first question we got
presented with here is which a support
plans provide access to the seven core
for trusted advisor checks and so that
is a question that you might need to
answer I don't want to spoil this for
you so I'm not going to tell you the
answer I will go to the next one so a
large accounting firm wants to utilize
OS to store customer accounting
information in archive storage and must
store this information for 7 years due
to Regulatory Compliance which dat
service meets this requirement so the
first one you'll notice this one is
multiple choice or sorry multiple
answers so you have to select multiples
before you can submit your answer and
the next one here is is just a single
choice so those are the two types of
questions you will see on the exam
they're not going to ask you anything
about coding you're not going to see any
kind of code um in terms of length
that's pretty much what we'll see in
terms of the uh questions I think in
many cases I wrote a little bit more
more like um in the style the solutions
architect associate to make it slightly
more difficult just so that you're a
little bit overprepared so if you do
well on these practice exams you're
going to do uh well on the real exam
okay okay so I just wanted to kind of
get you that exposure there
[Music]
okay hey everyone it's Andrew Brown and
I have opened our exam simulator this is
on the exam Pro platform and this is the
freet uh that I promised uh folks in the
course so no cost to go get this one you
just have to sign up and and access it
but the reason I have it open is because
I really want to talk about a very
specific type of question that we've
included in here that will not appear on
your exam
so uh for those who are familiar with
Azure certifications um at the associate
level or higher there's this question
type called a case study and what a case
study is I'll I'll just pull it up here
but I believe uh in this randomization
of this practice exam set I think it's
this one here but what a case study is
it gives you a scenario that you have to
read through or a a a case study about a
company so you read about the company
you look at the objective its
requirements and constraints this stuff
can all be different there could be
diagrams all sorts of stuff in here but
the idea is that you are contextualizing
a business use case and they're going to
be asked a series of questions uh
multiple choice multiple select and it
all ties back to that case study so the
reason we included this is that um we
believe that this is going to give you
better comprehension and a higher chance
of passing so it's not going to appear
in your exam but we include it uh as an
extra challenge to you so that you have
um a higher chance of passing now if you
don't like this we do have other
practice exams they of course are paid
that uh that are just the normal style
which is all multiple choice multiple
select for um this this course the cloud
practitioner um but you know we do have
them in half of the practice exam sets
because uh again I think that it's going
to be good for you so hopefully you see
that as a bonus but I just wanted to
give you a heads up um about this uh
because you'll encountering me like what
the heck is this um the other thing I
want to point out is that when you enter
a case study it's like having a mini
exam within your exam so once you've
answered all these questions uh you
can't go back and and um you can while
you're within the case study but if you
get to the end of this and submit the
case study you can't go back and update
it so just be aware of that um and you
know again hopefully you like this we
love feedback to hear what people like
but it's just they always appeared in
Azure exams and uh we want to see them
in ad us ones as well because I think
they're just really good for uh testing
your knowledge but anyway we'll see you
in the next one okay ciao
[Music]
hey everyone it's Andrew Brown your
favorite Cloud instructor and what I
want to do in this video is to show you
um a unique feature that is in our
platform um just in case you come across
it while you're while you're uh doing
the materials I can't remember if it's
in the free or paid tier I believe it's
in the paid tier so I'm not trying to
upsell anyone but I just want to make
sure people are aware of that while they
are um taking this course but sometimes
what you'll see in the follow along so
like for example we have S3 down here
here which is for uh Cloud simple
storage uh and I don't have them always
included in the videos but um at some
point I might do that but the idea is
that um we have these validators and
validators what they can do is they can
verify uh whether you actually have uh
the resources uh deployed in your cloud
account um so it's like an additional
check to make sure that you did it right
so for example we have this one for S3
so it says set up an S3 bucket it is
account validation so this tool perform
perms an automated check on your
personal cloud infrastructure to confirm
its alignment with the build project
requirements make sure you input precise
values for your infrastructure
components so let's go through that and
show you this I'm showing this as an
example but you know you'll see them in
other in other follow alongs and lookout
for for that stuff I believe in the
to-do it'll even show it uh here so if
you watch the video and you watch it to
the end or you press that button there
but you'll get your your uh your star uh
for that but the way it works let's go
through it so the first thing is I want
to uh click on this new run button and
then what we'll do is we'll have an
agreement so this agreement is
confirming that you understand that you
are using your own cloud account uh and
we are going to uh need to get readon
access to it and just understand that
you are using uh you're providing us
access to account that is your own
account and it's not your company's
account because obviously we don't want
to get in trouble for accessing data
that we're not supposed to have and you
don't want to get in trouble for that so
that's just a a friendly reminder so I'm
going to click the I agree and the
accept the next thing it's going to ask
for is your adus account ID the region
that you're deploying in and then it
there might be additional uh parameters
that it wants to know so that we can
test against it so what I'm going to do
is just log into my adus account it'll
just take me a moment and we'll fill
this out for real okay now of course I'm
filling out this example here but I just
want to point out that um uh you know
you're just going to have to follow this
procedure and it'll be slightly
different for each one uh for that okay
be back in just a second all right so
I'm logged into
and uh one of my ad accounts I have a
lot of them I think this one is my
developers one so uh for this particular
follow long you would have created an S3
bucket right and so um what I'm going to
do here is go to S3 and I already know
what to do so it's not too hard for me
but I'm going to go ahead and create
myself a new bucket I'm going to make
note of the region that I'm deploying in
so S3 is a bit unique because it shows
Global but you are still deploying to a
specific region so we'll go ahead and
create that bucket I'm just going to say
my validator bu it as a test notice
where it's deploying Us East one I could
change that to anything else like ca
Central um I am in Canada so doesn't
hurt to deploy where I am and we'll go
here and go all the way down and I'm
going to go and create this bucket okay
so um that bucket name was I forget it
was like something like validator and so
what I need to do is copy that name
we'll go back over here and so it's
asking for the bucket name so there's
the bucket name we need the it account
ID that always appears in the top right
corner and they have a nice um clipboard
button there to get that in there and
the region so we deploy that in CA
Central 1 so it says there CA Central
one you're always using this uh
programmer's name not the full name but
this this fun handle you can see them
all here on the right hand side if
you're not sure about that but what
we'll do is go back over here we'll
paste in that user region and so what
this is going to do is create a um a
cloud formation template that's going to
give access to us to uh your account so
we'll go ahead and hit save and continue
and so now we uh We've inputed our
parameters those have been saved and now
it's saying we need to access your Cloud
resources so we want you to generate
this cloud formation template we're
going to press the button we'll wait a
moment and we can either download this
template or use the ads CLI to run it um
the CLI command is a lot easier to use
and I'm going to recommend that you
always do that and uh so what we're
going to do is generate out this CLI
command and we're going to get this
oneline command and I'm going to go back
over to AWS sorry I know I'm going
really fast but it's just how it is and
at the top left corner we have this
little button here that's for cloud
shell we're going to open it up I know
coding scary but it's really important
to get as much coding experience or
scripting as you can so strongly
recommend you follow along here but uh
it's going to open up and once it's it's
open we can paste that in now sometimes
this wants to have some kind of EBS
storage so you might have to say yes and
wait a little bit um that's just the
norm for cloud storage but I'm going to
go back here I'm going to copy this
command okay and we're going to go back
over here I'm going to right click and
paste and uh this always happens when
there's a multitext line we got a pop up
here and we're just going to review it
looks good so notice it has a template
URL so that's the template it's pulling
in um there's temporary credentials to
uh to allow that uh it's going to create
a stack name called exam Pro validation
and it's going to say capability I named
I am now this might fail because I've
done it before but we'll go ahead and
paste it in I'm going to hit enter
and it looks like it's creating the
stack so we'll go over to cloud
formation and uh we'll go
here and I'll just get this out of the
way I don't want that open right now and
so I'm just going to give this a
refresh and did that create that right
now that was the name of the stack right
exam Pro validation that is correct and
if I go over
here uh what's the date today I don't
even know
cuz that might be an older date I mean
it's November so I I don't think that
one worked because I already had it uh
working there before um so what I'm
going to do
here I'm going to go ahead and delete
this one okay so I just want to point
out like if you're doing multiple
validators in the system you always have
to roll it back tear it down okay like
the old one so I'll delete that one
again because I just don't have a strong
confidence that it was actually deployed
so I'll be back in just a moment as it
tears down
all right it actually did uh finish
tearing down so that is um there but I'm
going to go back here I'm going to
attempt to run this command again so go
ahead and copy this and I will paste it
in again we'll say paste and I'll hit
enter and uh says already existed in the
sack well what are you talking about
it's definitely uh definitely not there
that's what I thought I would get as an
error the first time
around so this is CA Central 1 oh you
know what it is I'm in North Virginia so
you got to be very careful with your um
your regions so I go over
here so I I did I did delete one that
was from another one that's why I was
confused because I thought it already
existed I have to delete it out th this
is normal and Cloud right so just
understand that when I do follow alongs
I don't edit out the tricky Parts
because I know it makes it a little bit
confusing but it really does help to uh
demonstrate uh how confusing Cloud can
be and how to work through those
problems but over here see Central so
this is deployed 11:15 that's the date
that I've deployed this on so that makes
sense uh here uh so we just got to be
very uh aware of that so this is in C
Central one uh but we'll go back over
here and so this is done so we know that
it's done because it's here it's in the
region that we expected to be in so now
the uh the permissions are done we can
run the polar so what the polar is going
to do is it's now going to pull data
from your account uh uh and that way
we're going to uh be able to then
validate whether things are correct so
we'll go ahead and run the pull and
notice it says S3 API list buckets it
flashed it really quickly but the way
this tool works is it's actually using
the adus CLI underneath so I'm just
going to go ahead and just show you what
this
is uh and just show you a quick
reference here so the C is a pratic way
to um uh access uh information uh for
eight of us we probably show that
somewhere in this course and so the
command was running I believe was I
should know I coded this was S3
API and then it was like list
buckets uh list buckets so that's the
command it ran so really what the
validator did it it did ads S3 API list
buckets okay and if you notice this it
returns back Json so we get back the
payload that's what we are storing in
our own itus account which by the way we
delete after a period of time I don't
remember how much time but we don't hold
on to your data for cuz we don't really
want it um but yeah so here it's
returning back that data and so
somewhere in here that there the the
buckets in here right so we've pulled
that data and it's there and so now we
can run the validator we'll click run
validator and it's super fast because we
already have the data downloaded and
it's doing one check here so it says
should have bucket matching name so you
can see it's it's doing it's loading
from a Json file that's called S3 API
list buckets we always name our the Json
files after the commands and it's
looking through buckets so if we go over
here all the top here for a moment you
can see buckets so it's looking with in
this array and it's trying to match a
name called my validator bucket which
which which you provided to
us so somewhere in here I have a lot of
buckets in this account somewhere in
here uh there it is it's there and so
that's how that works um but yeah just
look out for those validators um and uh
try to run them and and validate that uh
you are able to uh do this stuff okay
but we'll see you in the next one okay
ciao oh wait wait wait wait wait wa wait
I didn't show you how to clean up I'm
just running off screen here so once
you're
done uh what you can do is you can go
over to cloud formation here and you
should do this is go ahead and delete
the stack okay um because that's going
to tear down the permissions so that we
no longer have access to your account um
so that's kind of an important thing to
do um but uh we'll go ahead and the
other thing about these permissions is
that we're only asking for exactly what
we need access to so in this in this uh
permissions it only generate up to get
access to uh the S3 bucket specifically
what we're accessing for so even if you
left it up it's usually okay it's safe
but um you know if there's no reason for
us to have access anymore you should all
obviously delete it um but yeah that one
is now gone and so now we are absolutely
done I'm going to go ahead and just
close this out here but yeah hopefully
uh that makes it pretty clear how
validators work in our system and you
see the benefit uh to getting that uh
check in your real account
[Music]
ciao hey this is Andrew Brown from exam
Pro and we are at the start of our
journey asking the most important
question first which is what is cloud
computing so cloud computing is the
practice of using a network of remote
servers hosted on the internet to store
manage and process data rather than a
local server or personal computer and so
when we're talking about on premise you
own the servers you hire the IT people
you pay or rent the real estate you take
all the risks but with a cloud provider
uh someone else owns the servers someone
else hires the IT people someone else
pays or rents the real estate and you
are responsible for configuring cloud
services and code and someone takes care
of the rest of it for you
[Music]
okay so to understand cloud computing we
need to look at the evolution of cloud
hosting going all the way back to 1995
where if you wanted to host your website
or web app you'd have to get a dedicated
server so that would be one physical
machine dedicated to a single business
running a single project a site or an
app and as you can imagine these are
expensive because you have to uh buy out
right the hardware have a place to store
it the network connection having a
person to maintain it um but it did give
you a guarantee of high security um and
they still do as of today so this model
hasn't gone away but it's been
specialized for a particular use case
then came along the virtual private
server so the idea is we still had one
physical machine but now we were able to
subdivide our machine into submachines
via
virtualization and so essentially you're
running a machine within a machine and
so you had better utilization of that
machine um running multiple web apps as
opposed to having a physical machine per
project so you got better utilization
and isolation of resources and so uh
these two options still required you to
purchase a machine machine a dedicated
machine and so that was still kind of
expensive but then came along shared
hosting and so if you remember uh the
mid 2000s like with GoDaddy or HostGator
or any of those sites where you had
really cheap hosting the idea is that
you had this one physical machine shared
by hundreds of businesses and the way
this worked it relied on uh tenants
underutilizing their resources so you
know you wouldn't have a submachine in
there but you'd have a folder with
permissions that you could use um and so
you would really share the cost and this
was very very cheap um but you were
limited to whatever that machine could
do and you were very restricted in terms
of the functionality you had and there
was just poor isolation meaning that you
know if one person decided to utilize
the server more they could hang up all
the all the websites on that single
server then came along Cloud hosting and
the idea is that you have um multiple
physical machines that act as one system
so this is distributed computing and so
the system is abstracted into multiple
cloud services
and the idea is that you basically get
the advantages of a lot of the things
above so it's flexible you can just add
more servers um it's scalable it's very
secure because you get that uh virtual
isolation you get it extremely at a low
cost because you're sharing that cost
with the users where in the shared
hosting it might be hundreds of
businesses we're looking at thousands of
businesses and it was also highly
configurable because it was a full
virtual machine now uh Cloud actually uh
still includes all of these types of of
Hosting they haven't gone away uh but
it's just the idea that you now have
more of a selection for your use case uh
but hopefully that gives you an idea uh
what cloud hosting looks like and it
really has to come down to distributed
computing
[Music]
okay hey this is Andrew Brown from exam
Pro and before we talk about AWS we need
to know what is Amazon so Amazon is an
American multinational computer
technology corporation headquartered in
Seattle Washington and so this is the
Seattle skyline with the Bas needle and
Amazon was founded in 1994 by Jeff Bezos
and the company started as an online
store for books and expanded to other
products so as you can see this is Jeff
Bezos a long time ago and he has this
interesting spray painted sign and his
desk is held up by cinder blocks and it
looks like his uh desk is like an old uh
table or something and he's working
really late and he used to be a
millionaire at this time and he would be
driving into work in his Honda Accord
because you know he just his motivation
was always to put all the money back in
the company so it really shows that he
worked really hard and it did pay off
because Amazon has expanded uh Beyond
just an online Ecommerce store into a
lot of different things such as cloud
computing which is Amazon web services
Digital streaming such as Amazon Prime
video Prime music they bought twitch.tv
they owned the Whole Foods Market
grocery store they have all this
artificials intelligence they own low
orbit satellites uh and a lot more stuff
it's hard to list at all and so Jeff
Bezos today is not the um the CEO it's
actually Andy jasse is the current CEO
of Amazon he was previously the CEO of
AWS so Jeff Bezos can focus on space
travel so there you
[Music]
go hey this is Andrew Brown from exam
Pro and we are taking a look at Amazon
web services and this is the name that
Amazon calls their cloud provider
service and it's commonly referred to
just as AWS so here is the old logo
where we see the full name and here is
the new logo but I like showing the old
logo because it has these cubes which
best represent what AWS is and it is a
collection of cloud services that can be
used together under a single unified API
uh to build uh a lot of different kinds
of workloads so adus was launched in
2006 and is the leading cloud service
provider in the world I put an aster
there because technically uh adus exist
before 2006 and a cloud service provider
uh which is what adus is is often
initialized as CSP so if you hear me
saying CSP I'm just saying cloud service
provider okay so just time to look at
the timeline of when Services rolled out
the first one came out in uh 2004 it was
simple Q service sqs and this service
still exists as of today but at the time
it was the only service that was
publicly available so it wasn't exactly
a cloud service provider at this time
and it was neither ads it was just sqs
but then a couple years later we had
simple storage service also known as S3
which was launched uh in March of 2006
and then a couple months later we had
elastic compute Cloud also known as ec2
um and ec2 is still uh like the most
used service within AWS and is like the
backbone for pretty much everything
there then in 2010 it was reported that
all of amazon.com's retail sites had
migrated to AWS so even Amazon was using
ads uh Full Steam and to support
industrywide training and and skill
standardization ads began offering a
certification program for computer
Engineers on April 2013 uh and this is
the type of certifications that we are
doing as we speak um so I just want you
to know that ads was the one leading uh
Cloud certifications and we just want to
take a look here at the executive level
as of today the CEO is Adam he's the
former CTO of Tableau and he spent a
decade with adus as a VP of Marketing
sales and support so he was there he had
left for a bit and now he is back then
we have uh wner and he's the CTO of AWS
he's been uh the CTO for pretty much the
entire time aw existed with the
exception of some time of the first year
he's famous for uh quoting everything
fails all the time and then there's Jeff
bar who's the chief evangelist so um if
you're ever wondering who is writing all
the blog posts and talking about anys
it's it's always Jeff bar okay
[Music]
all right so what I want to do here is
expand on what is a cloud service
provider also known as a CSP just
because there's a lot of things out in
the market there that might look like a
CSP uh but they actually are not so
let's go through this list and see what
makes a CSP so this is a company which
provides multiple cloud services ranging
from tens to hundreds of services those
cloud services can be chained together
to create cloud architectures those
cloud services are accessible via a
single unified API so in ad's cases that
is the adus API um and from that you can
access the CLI the SDK the Management
console those cloud services utilize
metered building based on usage so this
could be per second per hour uh vpcu
memory storage things like that those
cloud services have Rich monitoring
built in so you know every API action is
tracked and you have access to that so
this case it's a cloud trail and the
idea here is those cloud services have
infrastructure as a service offering so
IAS that means they have networking
compute uh storage databases things like
that those cloud services offers
automation via infrastructure as code so
you can write code to set everything up
and so here's just kind of a example of
an architecture where we have a very
simple uh web application running on ec2
behind a load bouncer with the domain
with r 53 but the idea is just to show
you that you know you're changing these
things together if a company offers
multiple cloud services under a single
UI but do not meet most of or all of
these requirements it would just be
referred to as a cloud platform so when
you hear about twilio or Hashi Corp or
datab bricks those are Cloud platforms
and adab US Azure gcp are cloud service
providers
[Music]
okay all right let's take a look here at
the landscape of CL service providers
this is generally broken down into tier
one tier 2 tier three but I've modified
it to give each tier its own name as I
don't like to think of them as rankings
and more so that uh these cloud service
providers are specialized uh for a
particular thing um and I've also added
a fourth tier because you know the
internet has always talked about three
tiers but there really is a fourth tier
and I wanted to make sure we had uh the
full scope here included so in the top
tier you're going to recognize uh some
common names there Amazon web service
Microsoft Azure Google Cloud platform
and Alibaba cloud in North America and
Europe uh adab us Azure and gcp are
known as The Big Three um but Alibaba
cloud is huge as well if you're in the
Asia region specifically China so it's
really just going to be dependent on
where you live where uh which are
considered the most um commonly known or
popular uh but we'll talk about that
here in a moment but the reason um I
call tier one top tier is that these are
you know very well-known providers
they're ear early to Market they have
strong synergies between their services
um they're just really good cloud
service providers you cannot go wrong
with uh these providers then we have our
tier two or I would call our mid- tier
uh these are backed by really well-known
tech companies but I would just say that
um their ability to become top tier uh
did not work out the way they planned so
IBM at one point was looking to be a top
tier provider um but they just did not
keep up with um AWS and and they just
slipped into this mid tier and kind of
specialized at least for a while into ml
AI services and now they're just more
like very expensive um Enterprise uh
managed
infrastructure for their existing
clientele Oracle um very very
inexpensive that's their play they try
to uh be the cheapest but their uh
service um overall is not uh fun to use
interestingly enough believe Microsoft
Azure was just signing a contract to use
Oracle Cloud so it's not unusual for
some of these cloud service providers or
these organizations to use other
providers because they want to use their
Global infrastructure but uh yeah Oracle
cloud is uh not doing that great there
are other ones in the Asia region like
Hawaii cloud and 10cent Cloud I honestly
don't know a whole lot about them but
they do show up on the magic quadrant so
it's possible in the Asia region that
these are are the big three and uh AWS
Azure and gcp do not play a larger role
but from our perspective I put them into
that mid tier because they just don't
have Global uh awareness or Global um
market dominance like the other three uh
up there looking at the light tier uh
these were traditionally virtual private
servers so they just specialized in that
and they turn to offer more core
infrastructure service offerings so we
have a vulture we saw it was pronounced
voler but it's actually vulture digital
ocean and aimi connected Cloud which was
formerly known as Leno or lenoe um so
they merg their companies together and I
mean they want to be like a cloud
service providers but they're very very
light in terms of their offering so um
you know they'll have things like
serverless and being able to run uh
kubernetes cluster and some cloud
storage and stuff but they won't have
things like um the the same level of of
event driven um metered billing or or
other kinds of uh functionality that you
you come to expect in the top tiers but
you know if you're working with a
smaller organization they are a lot
simpler to uh to utilize so they are a
great introduction to Cloud for
companies that find the top tier uh too
complex and then looking at the fourth
tier I call this the private tier this
is basically software that you can
deploy onto your own uh machines and
your data centers to get the same same
kind of um functionality that you would
if you were using let's say adabs or any
of these other providers and um you know
previously I would put open stack into
the mid tier because in a sense that it
was kind of like a cloud service
provider that was using uh quite a bit
but I didn't feel like it had had good
fit there so that's why we made this a
fourth tier and we have a few different
softwares we have open stack apachi
Cloud stack those are both open source
and there's VMware vpar I have an aster
there because it's not really the same
thing but it is used a lot everywhere to
manage a lot of virtual machines and so
I I kind of feel like it should fit in
there but that gives you kind of an idea
of the landscape of cloud and we'll see
you in the next
[Music]
one so how do we determine who is the
leader in Cloud well one way of
indicating that is the Gardner magic
quadrant for cloud so the magic quadrant
is a series of market research reports
published by the IT consulting firm Gard
that rely on proprietary qualitative
qualitative data analysis methods to
demonstrate market trends such as
Direction maturity and participants so
it says a series of reports uh but the
only thing I've ever seen are these
Graphics where they show um a uh the the
quadrant it's a it's a diagram that
summarizes all the information so I
think you have to you might have to pay
to access uh the reports um because it's
definitely not just uh publicly
accessible knowledge and I don't think
they would show all of uh how this stuff
is calculated but uh let's just take a
look at this graphic here so notice we
have challengers in the top left corner
leaders in the top right corner in the
bottom left corner we have Niche players
and then in the bottom right corner we
have Visionaries so the idea here is
that The Closer you are to this top
Corner uh the better you are doing and
the one that is closest to it is Amazon
web services followed with Microsoft
pretty close uh in second Google to the
left Alibaba Cloud next Oracle and we
have IBM 10cent and Hawaii and there are
other players but they are so small that
they are not showing up there and we
showed that in the landscape of csps or
um maybe this is only for first they
consider what we call First tier or top
tier cloud service providers it's really
useful to look at last year's uh mq and
to see how things have moved so it looks
like uh it uh Microsoft has shifted a
little bit forward here and gone a
little bit closer to
Google has cifically moved up and um
Alibaba Cloud it seems to be moving more
uh to the right um and again I'm just
showing what their movements were from
this year to that year so they are over
here now Oracle is way over here now and
for whatever reason Huawei cloud is on
the board so it's interesting to see how
they move another thing that's um
interesting here is that this one is
2022 of June and this one is July of
2021 and
right now as the time I'm recording this
video it's 2023 near the end of the year
um and I could not find a 2023 one so
even if it says June or July they will
release these out in October November
Etc way later in the year and so for
whatever reason they have yet to make um
the latest one available so I'm still
curious to see what that is here so I'm
just giving you the information that we
have but you can look at this stuff um
basically on the the Garder website if
you want to see
um any of these magic quadrants for any
of the industries there and what I find
is that if a compan is doing really well
they'll always post it on their website
so it's very easy to find the uh Magic
quadrant for cloud on the a website
because they're the leader so they
definitely want to show that there uh
but yeah there you
[Music]
go so a cloud service provider can have
hundreds of cloud services that are
grouped into various types of services
but the four most common types of cloud
services for infrastructures of service
uh and I call these the four core would
be compute so imagine having a virtual
computer that can run applications
programs and code networking so imagine
having virtual Network defining internet
connections or network isolation between
services or outbound to the internet
storage so imagine having a virtual hard
drive that can store files databases so
imagine a virtual database for storing
reporting data or a database for general
purpose web applications and uh AWS in
particular has 200 plus cloud services
and I want to clarify what cloud
computing means because notice that we
have cloud computing Cloud networking
cloud storage Cloud databases but the
industry often just says cloud computing
to refer to all categories even though
uh it has computer in the name so just
understand when someone says cloud
computing uh they don't just generally
mean the subcategory they're talking
about all of cloud okay
[Music]
so adus has a lot of different cloud
services and I just want to kind of go
quickly over the types of categories
that we can encounter here and just
mention the four core so any CSP that
has IAS will always have these four core
service offerings we have computes so
Nat this would be ec2 VMS storage this
could be something like EBS virtual hard
drives database so that could be RDS SQL
databases networking and content
delivery but really it's networking uh
and this would be VPC so private Cloud
Network okay so uh let's just look at
all the categories that are outside the
four core so there could be analytics
application integration arvr ads cost
management blockchain business
application containers customer
engagement developer tools and user
Computing game Tech iot Machine Learning
Management governance Media Services
migration uh and transfer mobile Quantum
technology s robotics satellites
security identity and compliance if
there was more I would not be surprised
but you can see there's a lot of stuff
that's going on
here so let's take a look at all the ITA
services that are available to us so if
you're on the marketing website which is
adab.
amazon.com what you'll see in the top
left corner is products and so these are
all the categories and for whatever we
want if it's like ec2 we can go into
here and we can read all about it so
usually we'll have our overview all
right and that's not very useful and
then we'll go over to features and so
this is can be kind of useful to get
some basic information and pricing which
is something you'll do a lot in AWS is
you're always going to be going to a
service looking up its price and so
you'll make your way over uh here every
single one is different uh a very
important page would be like getting
started so this will give you basic
information but what I do is I like to
go all the way down to the bottom here
and find my way over to the
documentation so I'll go here to
documentation to get that deeper
knowledge about that service and as you
can see things get pretty deep with AWS
in terms of the information they have so
hopefully that gives you an idea of the
scope also when you're logged into AWS
and this will be when we create our
account uh you can explore all the
services this way as well so these are
all the ad Services uh but you just
notice that there's two ways to uh
explore them where this is actually you
just actually utilizing the services and
then the marketing website is you
reading about them and learning all
about them
[Music]
okay hey this is Andrew Brown from exam
Pro and we are looking at the evolution
of computing your cloud service provider
has all of these offerings and the idea
is that you need to choose the one that
meets your use case a lot of times this
all has to come around the utilization
of space that's what we're trying to
illustrate here in this section here and
the trade-offs of why you might want to
use some of these offerings okay for
dedicated we're talking about a uh a
physically a physical server wholly
utilized by single customer that's
considered single tenant and uh for
Google Cloud we're talking about um
single node clusters and bare metal
machines where you have control of the
virtualization so you can install any
kind of hypervisor or virtualization you
wanted the system the trade-off here
though is that you have to guess upfront
what your capacity is going to be and
you're never going to 100% utilize that
machine because it's going to have to be
a bit under in case the utilization goes
up that's you're choosing the CP use and
the memories you're going to end up
overpaying because you're uh you'll have
under underutilized server uh it's not
going to be easy to vertically scale
it's not like you can just say resize it
because the machine you have is what you
have right you can't add more I mean I
suppose they can insert more memory for
you but that's a manual migration uh so
it's very difficult um and replacing the
server is also very difficult okay so
you're limited by the host operating
system it's not virtualized so whatever
is on there is on there um and that's
that's what your apps are going to have
access to if you decide to run more than
one app which is not a good practice for
these kind of machines uh you're going
to end up with uh resource sharing where
one machine might utilize more than the
others technically with a dedicated
machine you have a guarantee of security
privacy and full utility of the
underlying resources I put an aster
there because yes it's more secure but
uh but it's up to you to make sure that
it's more secure so you have that's up
to your skills of security right whereas
if you had a virtual machine or anything
above that there's more responsibility
on the cloud service provider to just
provide a secure machine and they can do
a better job than you so why would you
use a dedicated machine well maybe
you're doing high performance Computing
where you need these machines like very
close together and you have to choose
what kind of virtualization you need to
have okay so then we're looking at
virtual machines the idea here is you
can run a machine within a machine the
way that works is we have a hypervisor
this is a software layer that lets you
run the virtual machines uh the idea
here is now it's multi-tenant you can
share the cost with multiple customers
you're paying for a fraction of the
server uh you'll still end up overpaying
for the underutilized virtual machine
because a virtual machine is just like
you have to still say how many V vcpus
how much memory and your app is you you
don't want an app that uses 100% right
you want to use exactly the amount you
need but you can see here you know
there's still going to be some
underutilization uh you are limited by
the guest operating system now but now
it's virtualized so at least it's very
easy to uh possibly migrate away if you
choose to run uh more than one app on a
virtual machine it it can still run into
resource sharing conflicts uh it's
easier to export or import images for
migration it's easier to vertically or
horizontally scale okay and virtual
machines are the most common and popular
offering for compute because people are
just very comfortable with those then
you have containers and the idea is you
have a virtual machine running these
things called containers the way they do
that is similar to a hypervisor but
instead you have um like here is a
Docker demon so it's just a um a
container uh software layer okay to run
those containers there different kinds
Docker is the most popular uh and the
great thing is you can maximize the uh
the the capacity because you can easily
add new containers resize those
containers use up the rest of the space
it's a lot more flexible okay uh your
containers will share the same
underlying OS but they are more
efficient than multiple VMS uh multiple
apps can run side by side without being
limited uh by the the same OS
requirements and not cause conflicts
during resource sharing so containers
are really good but you know the
tradeoff is there a lot more work to
maintain then you have functions
functions go an even step further and
the idea is that you uh the the
containers where we where we talked
about that's a lot of work to maintain
now the cloud service provider is taking
care of those containers generally
sometimes not it depends if it's
serverless or not but the idea is that
you don't even think about this is
called seress compute but you don't even
think about uh the OS or anything you
just know that what your runtime is you
run Ruby or python or node and you just
upload your code and you just say uh I
want this to be able to run uh uh for
this long uh and use this amount of
memory okay you're only responsible for
your code and data nothing else it's
very cost effective you only pay for the
time the code is running uh and VMS only
run when there is code to be executed
but because of that there is this
concept of cold starts and this is uh
where the machine has to spin up and so
sometimes requests can be a bit slow so
there's a bit of tradeoff there but
functions or serverless compute is
generally one of the best offerings as
of today but most people are still
getting kind of comfortable with that
Paradigm
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at the
types of cloud computing and the best
way to represent this is a stacked
pyramid and we'll start our way at the
top with SAS also known as software as a
service so this is a product that is run
and managed by the cloud service
provider you don't have to worry about
how the service is maintained it just
works and remains available so examples
of this and actually uh the first uh
company to coin this was actually
Salesforce um then there's things like
Gmail office through 65 so think
Microsoft Word Excel things like that
and they run the cloud okay and SAS is
generally designed for customers in mind
then came along platforms of service um
also known as pass and these focus on
the development or sorry the deployment
and management of your apps so you don't
worry about provisioning configuring or
understanding the hardware or operating
system and so here we' have things like
elastic beant stock Heroku which is very
popular among developers that just want
to launch their code or Google app
engine and that is the old logo but
that's the logo I like to use because I
think it looks cool and so these are
intended for developers the idea is that
you just deploy your code um and the
platform does the rest
then there is infrastructure as a
service um there's no way to say that
like it's easy to say SAS or pass but
there's no easy way to say IAS so this
is the basic building blocks for cloud
it it provides access to networking
features computers and data storage
space and the idea here is you don't
worry about the IT staff data centers
and hardware and so that would be like
Microsoft Azure AWS Oracle Cloud things
like that and these are for
administrators okay so there you go
[Music]
hey this is Andrew Brown from exam Pro
and we are taking a look at cloud
computing deployment models starting
with public cloud and the idea here is
that everything when I say everything
I'm talking about the workloads the
projects the code is built on the cloud
service provider so here is a diagram
where we have a ec2 instance a virtual
machine running her application and then
we have our database in RDS and we have
the internet coming into our adus
account and so everything is contained
all of our infrastructure is within AWS
all right uh and so this is known as
being Cloud native or Cloud first and I
put an aster beside Cloud native because
that was a term uh that was used prior
to cloud service providers to refer to
Containers or open- Source um uh models
being deployed and being mobile other
places so just understand that it has
two meanings but in the context of this
Cloud native just being like native to
the cloud like using Cloud to begin with
okay then we have private Cloud so
everything built on a company's data
center uh and being built on a data
center is known as being on premise
because that is where the data center
resides near where you work and so here
you could be using Cloud but you'd be
using openstack which would be a private
Cloud so here we have our on- premise
Data Center and uh the internet's coming
into our data center and we're running
on open stack where we can launch
virtual machines and a database okay
then there's the concept of a hybrid
Cloud so using both on premise and a
cloud service provider together and so
the idea here is we have our on premise
Data Center and then we have an
established connection maybe it's a VPN
connection maybe it is a direct
connection um but the idea is that we're
bridging that connection and uh
utilizing both our private and our
public uh stuff to uh create a cloud
workload then there is a fourth one
called cross Cloud um some sometimes
it's known as multicloud and sometimes
it's erroneously referred to as hybrid
Cloud but it generally is not hybrid
Cloud okay the idea here is when you're
using multiple Cloud providers and so
one example here could be using services
like Azure Arc so Azure Arc allows you
to extend your um control plane uh so
that you can deploy containers for
kubernetes in um Azure within Amazon eks
within gcp kubernetes engine but you
know being cross Cloud doesn't
necessarily mean that you're running a
uh using a service that use Works across
the cloud and manages it it could just
mean using multiple providers at the
same time another service that is
similar to Azure Arch but is for a
Google Cloud uh platform is also know as
anthos um adab us has traditionally not
been um cross Cloud uh friendly and so
we haven't seen any kind of developments
there where we see uh these other
services that are or cloud service
providers behind AWS trying to promote
to uh grab more of the market share
[Music]
okay so let's talk about the different
deployment models and what kind of
companies or organizations are still
utilizing uh for these particular
categories so for cloud again this is
where we fly utilizing cloud computing
hybrid is a combination of public cloud
and on-prem or private cloud and then on
Prem is deploying resources on premise
using virtualization Resource Management
tools sometimes called private cloud or
could be utilizing something like open
stack so for companies that are starting
out today or are small enough to make
the leap from virtual private server to
a cloud service provider this is where
we're looking at Cloud so we're looking
at startups SAS offerings new projects
and companies um so maybe this would be
like base camp Dropbox Squarespace then
for hybrid these are organizations that
started with their own data center but
can't fully move to Cloud due to the
effort or migration or security
compliance so we're talking about Banks
fintech Investment Management large
professional servic providers Legacy on
Prem so maybe CIBC which is a bank deoe
uh the CCP or CPP investment board and
then for on premise these are
organizations that cannot run on cloud
due to strict Regulatory Compliance or
the sheer size of the organization or
they just have like an outdated uh idea
of what cloud is so they just have a lot
of uh difficulties in terms of politics
adopting Cloud um so this would be
public sector like government super sens
of data like hospitals large Enterprise
with heavy regul insurance companies um
so again hospitals maybe AIG the
government of Canada and so I shouldn't
say that they aren't using Cloud but um
you know because uh adabs and all the
cloud service providers have um uh
public sector offering so um you know
I'm just trying to Stage as an example
of things that could be still using on
premise so you know I know the
government Canada definitely uses uh
cloud in a lot of ways same with AIG and
hospitals but you know generally these
are the the last holdouts of on Prem
because there really isn't a a good
reason to be fully on premise anymore uh
but again there are some things that are
still doing that
[Music]
okay hey this is Andrew Brown from exam
Pro and we are at the start of our
journey creating ourselves an adus
account so what you need to do is go to
adab us. amazon.com if you don't have a
lot of confidence how to get there just
type in adus into Google and then click
here on the link where it says adabs
amazon.com it'll take you to the same
place now notice we have a big orange
button in the top right corner so this
says sign into the OS console um it's
the if it's the first time you've ever
been to this website so if I go ads.
amazon.com Incognito it will have the
create anus Account button um I don't
know why they don't keep this consistent
across the board but I wish they did but
if you are on the screen you can click
here or there um but if you do see
something that doesn't say uh you know
create an account or or Etc you can just
sign
in okay and then down below you can hit
create a new a account so that's the way
you're going to get in there and so
you're going to put an email a password
and create an adist account name um I've
created this so many times and it's so
hard to set up new emails I'm not going
to do this again it's not complicated
but one thing I need to tell you is that
you do need to have a credit card you
cannot create an account without a
credit card um and for those who are in
places where maybe you don't have a
traditional credit card maybe you can
get a prepaid one so up here here in
Canada we have a company called coo and
so coo is um a Visa debit card and so
it's basically a virtual prepaid credit
card and so these do work on the
platform as well so if you have a
traditional credit card or possibly
could find one of these uh you still
have to load up with money but it does
give you a bit more flexibility to
create that account so what I want you
to do is go through that process
yourself it's not complicated and I'll
see you on the other end okay
so once you finished creating your
account you should be within the adus
Management console and this is the page
you're always going to see when you log
in it's always going to show the most
recent Services here um and you'll
notice in the top right corner that I
have my account called exam Pro if
you're wondering how do you change that
name what you do is go to my accounts
here and once there you'll have your
account settings up here if you go to
edit uh you can change that name here
okay so you know sometimes when you
create your account you don't like the
account name that you gave it and so
that's your opportunity to fix it um but
once we're in our account what I want
you to do is immediately log out because
I want you to get familiar with the way
you log into AWS because it is a bit um
different than other providers and so I
don't want you to uh get hung up later
on with your account so I've logged out
I'm going to go ahead and log back in so
you can click the orange button or what
I like to do is drop down my account and
go to adus Management console it's a lot
more clear and you notice we're going to
have two options root user and I am user
so this is what I'm talking about for
the confusion so when you log into your
root user account you all are always
using an email and when you're logging
as an I am user you're actually going to
be entering the account ID or account
Alias but what we'll do is go to the
root user and this is the email you use
to sign up with the account so for me uh
I I called this one Andrew plus sandbox
exampro doco I'm going to go to next
sometimes you get this character box
it's very annoying but it happens time
to time and so what I'm going to do is
just go ahead and type that in
okay and hopefully it likes it and then
I'm just going to enter in my
password all right and I'll be back into
my account and so notice it takes me
back to abis Management console so the
root account is not something we want to
be generally using uh except for um very
particular use cases and we do cover
that in the course uh but what I want
you to do is go set yourself up with a
proper account and so what we'll do is
go to the top here and type in and this
stands for identity and access
management and we'll click on I
here and on the left hand side we're
going to see a bunch of options here um
and so notice right away we get to the I
am dashboard where it's going to start
to make some recommendations for us the
first one is always to add MFA
multiactor
authentication another thing you can do
is set an account Alias so you can see
that I've set one here prior so if I
just go ahead and remove it the way we'd
have to log in is via the account Alias
uh which is the same as the account ID
and so I don't really like that so I'm
going to just rename it to Deep Space 9
and uh these are unique so you have to
pick something that is unique to you so
it could be your company name or things
like that it's going to make it a lot
easier to log in uh when we create our
additional user here so we'll come back
to MFA at some point here what I want
you to do is go over to users and go
ahead and make yourself a new user and
so I'm going to call this one Andrew
Brown and I'm going to enable
programmatic access I'm going to enable
adus Management console so this one's
going to allow me to use the apis to
programmatically work with ads and this
one here is going to allow me to just
log into the console which is uh pretty
fair here so now that I have this we can
autogenerate it or give it a custom
password I'm just going to autogenerate
for the time being and here it says You
must create a new password at the next
sign in which sounds fair to me and we
can go ahead and create ourselves a new
group so it's pretty common to create a
group called admin and notice here this
is where we're going to have a bunch of
different policies so the first one here
which is admin and access provides full
access to AO services and resources and
this pretty much gives you almost nearly
almost the same capabilities as the um
AWS root user account uh and so that's
going to be okay because we are an admin
in our account so I'll checkbox that on
but I just want to show you here if you
Dro down filter policies and you went to
adus manage job functions these are a
bunch of uh pre-made uh adus uh policies
that you could apply uh to different
users so what's really popular after the
administrator access is to usually give
the power user access and so this one
allows um a user to do basically
anything they want with the exception of
management of users and groups so you
know it could be that that's something
that you'd want to do for some of your
users I just don't want to have any
trouble so I'm going to give us um admin
access here and we're going to go ahead
and create this
group and so here is the group that we
are creating we're going to go next we
can apply our tags if we want I'm not
going to bother we're going hit next
review and then hit create
user all right and so now what it's
doing is it's showing us the access ID
and the access uh key secret that we can
use to programmatically access AWS and
then there's a password here so I'm
going to go ahead and show it and what
I'm going to do is just copy this into a
clipboard anywhere
and so I'm just copying that off screen
here because I'm going to need it to log
in and I'm just going to remember my
username as well all right and so what
we'll do is go ahead and hit
close so what I'll do is go back to my
dashboard here and remember I set my
account Alias as Deep Space 9 but we
could also use the account ID to log in
I'm just going to grab my account ID off
screen here and what I want to do now is
go ahead and log out and now log into
this I user and this is the one that you
should always be using uh within your ad
account you shouldn't be using your root
user account so what I'll do is go over
to I am user here and notice now that it
says account ID so 12 digits or the
account alas so here I can enter in uh
these numbers here or I can enter in my
Alias which is Deep Space 9 and again
you'll have to come up with your own
creative uh one there for yourself and
we'll go ahead and hit next and so
notice what it's going to do is now ask
me what my IM am username name is so I
defined mine as Andrew Brown and then uh
we had an autogenerated a password there
so that we had saw and so I'm going to
place that in there we'll go ahead and
hit sign in and so now right away it's
going to ask me to reset the password so
I'm going to put the old password in
there and so now I need a new password I
strongly recommend that you generate out
uh your passwords to be very strong I
like to go to password generator and
I'll drop this down and I'll do
something really long like 48 characters
and um if you don't like
weird characters you can take those out
there sometimes it loads here so you got
to try it
twice um and I'm going to go down to
whoops
48 there we go and so that's pretty darn
long so I'm going to copy that off
screen here so I do not
forget and you probably would want to
put this in a password manager something
like Dashlane or some sort of thing like
that and we'll go ahead and we will
paste that in and we'll see whoops I
don't want Google to save it uh and
we'll see if it takes it and so there we
go so what I'll do is now log out and
I'll make sure my new password works
because you really don't want to have
problems later so we'll type in Deep
Space 9 Andrew Brown again this is going
to be based on what your uh what you
have set and we'll go ahead and log in
and there I am and so now notice there
doesn't say um exam Pro whatever it says
Andrew Brown at Deep Space 9 so it's
using the county ellias and showing the
name and that's how I'm going to know
whether I'm the root account user or
whether I'm logged in as an I am user
all right so there we
[Music]
go okay so now that we have the proper
user account to log in I just want to
point out uh about regions so in the top
right corner you'll notice it says North
Virginia here it possibly will say
something completely else for you but
what you'll do is you'll click and drop
that down and you'll see a big list of
regions and so so sometimes when I log
in ads it likes to default me to U East
uh Us East Ohio but I honestly like to
launch all my stuff in Us East North
Virginia even though I'm in Canada I
probably should be using the Canada
central region down here um but the
default region is going to be based on
your locality okay so just understand
that it might be different I strongly
recommend for um all of our follow
alongs you run in Us East one because us
east1 is the original um the original
region and it also has the most access
to Ada services and some adaa Services
um such as like billing and and cost and
things like that are only going to show
up in Us East uh North Virginia so just
to make our lives a lot easier we're
going to set it there but I want you to
understand that some services are Global
Services meaning that it doesn't matter
what region you're in it's going to
default to Global and one example could
be cloudfront so if I jump over to
cloudfront here for a
moment and uh we do seem to have uh some
CLR distributions here from a prior uh
follow along but notice up here that it
now says Global so CLR does not require
a region selection let's make our way
over to
S3 all right and this one's also Global
so again this one does not require a
region selection but if you go over to
something like
ec2 okay this has a region dependency so
just be really careful about that
because a lot of times you'll be doing a
follow along and you'll be like why
aren't these resources here or whatever
and it's because this got switched on
you and it can happen at any time so
just be uh cautious or aware of that
[Music]
okay so one of the major advantages of
using ads or any cloud service provider
is that it utilizes metered billing so
that is different from a fixed cost
where you'd say Okay I want a server for
x amount of dollars every month but the
way ATS works is that it's going to
build you on the hour on the second
based on a bunch of factors and so
you're going to be able to get services
at a lower cost however if you choose an
expensive service and you forget about
it or there's misconfiguration where you
thought you were launching something
that was cost effective but turned out
to be very expensive you could end up
with a very large Bill very very quickly
and so uh that is a major concern for a
lot of people utilizing Cloud but
there's a lot of great toolings built
into ads to allow you to catch yourself
if you happen to make that mistake and
before we go ahead and learn how to do
that I want to show you uh some place
where you could end up having excessive
spend without knowing it so one example
and this is actually happened to me when
I first started using AWS uh before I
even knew about all the billing tools is
I wanted to launch a reddis uh instance
and so you you just have to watch you
don't have to do this but um elasticache
is a service that allows you to launch
either a mem cach or
uh database and I just wanted to store a
single value and so I went here and I
scrolled down it looked all good and I
hit create but I wasn't paying attention
because apparently itus likes to default
the no type here to the cash
r6g do llarge all right and you know you
might think that adus has your best
interest in play and most services are
pretty good they they make sure that
they're either free or very low spend
but some of these and elastic cash is an
older service where they just have these
weird defaults so um you know if we were
to go look up this the
RG6 uh
large all right and look at its
spend all right and we would go over
here whoops I think I went to the China
One but if we were to go over here and
look for that instance I'm just trying
to find it here for cost this
one down
below um this doesn't say pricing does
it say our pricing
here here it is so this one cost
um this one costs about 2 cents per hour
it doesn't sound like a lot but if we go
here and we do the math we say 730 730
is the amount of hours in a month that
is
$150 okay so if you don't know about
that and forget about that that's going
to be $150 and I'm going to tell you
that it used to be a lot higher I'm
pretty sure they used to have it default
to something like like this or that
because I remember I did this and I had
a bill that came in that was like $3,000
USD and I'm in Canada so like $3,000 USD
is like a million dollars up here and so
I remember um it was a big concern and I
freaked out but that was okay because
all I had to do was go to support and
what I had done is I went to the support
center and I had opened a support case
and I just said hey I had this really
big Bill so you go here right and you
look for billing and uh you look for
something like charging query or
misspend and you say you know um you
know like help my bill's too
high and you just say like you explain
the problem saying hey you know I was
using elastic cash and it was set to a
large default and I wasn't aware about
it can you please give me back the money
and the great thing is that ads is going
to give you a free pass if it's your
first time where you've had a
misspending they generally will say Okay
um you know don't do it again and if it
happens again you will get build but go
ahead and learn how to set up billing
alerts or things like that okay so just
so you know don't freak out if you do
have a really high Bill you're going to
get a single free pass but now that we
know that let's go learn uh how to set
up a budget
[Music]
okay all right so now that we've had a
bit of a story about um over spend for
misconfiguration let's learn how to
protect ourselves against it and we're
going to go ahead and set up a budget so
go to the top here and type in budget
and what that will do is bring us over
to the billing dashboard another way to
get here is to go click at the top here
and go to my billing dashboard and then
you'll see the leftand menu here and so
the great thing about budgets is that
the first two are free it says there is
no additional charge for any those
budgets you pay for configured us usage
but I'm pretty sure that that's not true
because it used to be ABS budget reports
okay so that cost
something it used to be that Abus
budgets um after success enabled will
occur 10 cents daily so in addition to
budget monitor you can add actions to
your budgets the first two action enable
budgets are free okay so just be aware
that just because it says there's no
additional charge read into it because
sometimes the the Fine Line will tell
you it does cost something but I know
that the first two are free what we'll
do is go ahead and create a budget just
going to close these other tabs here
since we have no need for them and we're
going to be presented with a bunch of
budget types uh we're considered about
cost today so we're going to go with a
cost
budget and notice we can change the
period from monthly to daily to
quarterly to annually if you change it
to daily um you won't get forecasting so
I don't want that today but a monthly is
pretty good you can have a reoccurring
which is strongly recommended and then
you can put a fixed cost notice that I
already have some spend on this account
so it was like 25 bucks last month I'm
going to set it my uh budget here to
$100 and you can add filters here to um
uh filter that cost out so if you want
to say only for this region or things
like that you could do that uh notice
that this is my spend over here um so
this is my budget and that's the actual
cost notice my cost has been going up
the last few months because I've been
doing things with this account and so
what I'll do is say simple budget here
we'll hit
next and so now it's asking us if we
want to configure alerts we probably do
so you'd hit ADD alert and then you'd
set a threshold like 80% or you could
say an absolute value and then you put
your emails like Andrew exampro
doco and I want to point out that this
is using um itus
SNS or it should be anyway so Amazon SNS
has no upfront cost based on your stuff
here so even though you're filling out
an email you know and it maybe it
doesn't show it but I'm pretty sure that
this would create an SNS
topic but what we'll do is hit next here
we have an alert so we're just uh
reviewing actually this is for attaching
any actions so maybe we want some kind
of follow-up thing to happen here so we
say add
action and uh require specific I and
permissions on your
behalf okay sure so I guess you could
follow up actions that's no different
than um a building alarm but we're not
really worried about that right now I'm
not going to bother with an action and
we'll go ahead and create a
budget and so here it's going to say
that our budget is $100 it's going to
show us the amount use forecast amount
current budget sometimes this takes time
to uh show up so I'm going to hit
refresh and see if it shows up
yet there we go so notice we have
forecasted amount $23 current budget Etc
forecasted budget uh forecasted versus
budget so it's pretty straightforward on
how that works U I'm just curious if it
actually created an SNS event so I'm
going to go over here because a lot of
services utilize SNS so if I go over
here default Cloud watch alarm um so I
think this is something I had created
before so I'm going to go ahead and just
delete it says default Cloud watch
alarms I'm going to just click into here
and see what I
have
confirmed so I think it might have used
this when we created it but um the
reason I'm bringing up SNS is that
there's a lot of services that allow you
to uh email yourself for alerts and it
always integrates with this service and
so I just want kind of want to point
that out so that you remember what SNS
is for um but yeah so setting up a
budget is not too hard so there you
[Music]
go all right so now that we've set a
budget what I want to talk to you about
is the free tier and the free tier is
something that is available to you uh
for the first 12 months of a new adus
account and allows you to utilize adus
services without incurring any cost to
you and so it's in your advantage to
utilize this free tier um as you are
experimenting and learning cloud so if
you want to learn about all the
offerings what you do is go to Google
type in adus free tier and you'll get
this page that explains all the sorts of
things here so you can get uh 750 hours
on ec2 RDS things like that there are
stipulations in terms of what it would
be so here this is a T2 or T3 micel mic
uh micro running Linux Red Hat um or
other type of os's okay so there are uh
details you have to read the fine print
some services are only available for the
first two months things like that so
it's going to highly vary based on
service but it's worth giving us a read
in areas that you are interested in now
the thing is is how do you know that you
are still in the free tier or you go
outside of it and that's what I want to
talk to you about right now so I am
actually in another ad account so no in
the top right corner it says brown. laap
or hyphen laptop exampro doco sometimes
I will switch into different a accounts
during these fall alongs so I can best
show you um you know the settings so if
you make your way over to
billing and actually I should show you
up here if we go to my dealing B
dashboard just trying to be consistent
here and you go to the left- hand side
to billing preferences what you can do
is enable receive free tier usage alerts
and then put your email in there and
save that and so turn on this feature to
receive email alerts when your adabs
service usage is approaching or exceeded
databus free tier usage limits if you
wish to receive these alerts etc etc etc
right and while you're there I want you
to also checkbox receive billion alerts
so I can show you how to set a billion
uh a billi alert and AD us says you know
budgets are a new thing but bliing
alerts are still something that we use
as of today so if you checkbox that on
we'll be able to see your cost if we go
back here uh it should show you um it's
because I'm out of the free tier on this
account but it would show you in the
alerts you know your usage there so
example here is if we scroll down this
is the documentation tracking your AIS
free tier usage you would see like a box
like this and would say hey your free
tier usage limit is here and you're over
it okay so that generally would show up
on this panel here but again I'm outside
of the free tier so I'm not seeing it
here um today okay so you know hopefully
that is clear um but yeah there you go
[Music]
all right so we created ourselves a
budget we're monitoring our free tier
but there's another way that we can
monitor our spend and that is through
building alerts or alarms and it is the
old way before uh we had it was budget
this is the only way you could do it but
I still recommend it because there is a
bit more flexibility here with this
service and so I wanted to teach you
early on so that you know it's available
to you or if you want to play around
with it in the future so what you'll do
is go to the top here and type typ in
cloudwatch and cloudwatch is one of
those Services where it's actually a
collection of services so there's
cloudwatch alarms cloudwatch logs
cloudwatch metrics those are all
Individual Services and Abus loves to
update their interface so sometimes
you'll be present with this option to uh
change the latest interface I'm going to
try out the new interface here um and
that is one challenge with datab is you
always have to expect that they're going
to change the UI on you and you're going
to have to work through it so just
understand that I try to keep my videos
up to dat as best I can but part of the
challenge is getting used to that so
this is what they have today I don't
know if they're going to stick with this
but this is what it looks like but what
I want you to do is make your way over
to alarms on the left hand
side and notice that we actually have a
section just for billing which is
interesting I don't remember them having
that before so it's new so uh here it
says Abus Cloud watch help can help you
monitor the charges of Abus Bill
remember that we had to turn that on get
10 free alarms with 1,000 free email
notifications each month as part of the
free here so understand that if you
create billing alarms they do cost money
um as well if you go over that limit but
you sure get a lot 10 free alarms is
quite a bit what we'll do is go ahead
here and create ourselves alarm we're
going to go and choose a metric and so
here are the options we can choose from
and so we I think would like um billing
and see we can do buy service or total
estimated charge we're going to do a
total estimated charge we can only
select USD I've never seen any other
currency over there and so here we kind
of get this little graph where we can
see stuff um but this is a lot more
powerful than budgets because you can do
anomaly detection uh so like here it
will actually check base between a range
as opposed to just going through a
particular value but what I'll do is
just set a value here like uh $50 right
so notice that it sets the line up here
and this is my current spend here right
and so back to anomaly detection this is
a lot smarter so uh the idea is that if
something is outside inside this band of
a certain amounts um then it would alert
okay but I'm going to go back here I'm
just going to set this to
$50 and that looks okay to me you can
change the period 6 hours is fine um and
there's additional configuration that's
fine as well we're going to go ahead and
hit next uh and so the idea is that um
you know if it passes that red line it
will go to an in alarm State and then
what it will do is uh we want to uh have
it to trigger an s topic so I would
generally just create a new one here
we'll just say my billing
alarm Okay and then here we'll just set
the email Andre exam
pro. and we will go ahead and create
that topic and so that is now set I
don't know if it would uh confirm it we
might have to go to our email to confirm
it so notice it says pending
confirmation so what it has done is it
sent me out an email and it wants me to
click that link to confirm um that I
want to subscribe to it so I might just
do that offc screen to show you here
okay so I'm just going to pull up my
email here just give me a
moment okay and so if I come back here
this is the email that came in so I'm
just going to confirm that subscription
says I'm confirmed good and if I refresh
this page we can now see that that that
is confirmed all right so we'll scroll
down here so we can uh trigger an
autoscaling action so maybe you know if
you have too many servers you say hey
the cost is too much shut down those
servers there's ec2 actions things like
that so these are kind of similar to um
budgets right there's system manager
actions I imagine all these things are
available in budgets as well but budgets
just makes it a little bit easier to
look at so I just say my simple building
alarm
here we'll hit
next all right we'll hit create alarm
and there you go so billing alarms don't
have like forecasting and things like
that um but you know they are they do
have their own kind of special utility
and so I utilize both okay so there we
go we'll just go back to our Management
console move on to the next
[Music]
one so one of the strongest
recommendations that Abus gives you is
to say to set MFA on your adus root user
account so that's something we're going
to do right now so make sure you're
logged into the root user account so I'm
going to go log out as my IM user I'm
going to go back and log in and I'm
going to log in as my uh root user here
so to do that no sometimes it will be
expanded as the I am user click and sign
into root user here we'll have root user
I'm going to go ahead and enter my email
that I used and if you do switch
accounts frequently they will ask you
these silly captures which drive me
crazy but uh you know it happens you
probably won't encounter it as much as I
do and so I'm going to go ahead and grab
my password here and paste it on in and
so now that I'm in what I want want to
do is make my way over to I
am and I'm going to go and look for
users actually sorry just right here add
an MFA root user we're going to go ahead
and hit add
MFA all right and so that's going to
bring us to this screen and so here we
can activate our MFA and so we have a
few options here so we have virtual MFA
device u2f security key other Hardware
like a uh Gem Gem gy Alto token so you
know I generally use this because I have
a security key and I want to show you
what I'm talking about so this is how I
log into my machine or my ad account
this is a security key an UB key that
sits on my desk I tape it so it doesn't
fall fall off the cord but the idea is
that when I log in I have to press this
little button here to double confirm
before I get into my account uh but if
you don't have a security key you can
just use a virtual MFA and all that
means is you're going to um use
something on your phone to log in so
we'll click continue here and so it says
install a compatible app on your mobile
phone or device and so if you click and
open this what it will do is tell you
about some things that you can use um so
if we scroll down to
Virtual here they suggest uh if you have
Android iPhone so AI dual mobile last
path Microsoft authenticator Google
Authenticator so Google Authenticator
Microsoft authenticator and a I have all
those three installed um honestly aie
has the the nicest simplest um UI but
I'm using Microsoft authen authenticator
quite a bit so anyway whichever you want
to do it's fine but what we'll have to
do is go back here and then it says use
your virtual MFA app on your device
camera to scan your QR code so once you
have one of those apps installed like
aie or whatever one you want what you're
going to do is open up the application
and I can't tell you exactly where it is
but you'll have to hit add account in
your in your app and then from there it
will ask you to scan your QR code and so
once you're ready you hit show The QR
code you hit scan the QR code on your
phone I'm holding my phone up to my my
um uh my computer screen here and it's
going to find it and I'm just going to
take a moment here to rename the account
so I can tell what it is so I'm just
naming it a
WS sandbox cuz that's what I call this
account and I'm going to go ahead and
save that
and so now what I can do is enter uh two
consecutive MFA codes now this always
confused me what they wanted here but
the idea is that you're going to see one
code right whatever is on the screen
right now so I'm going to type in it it
says
734
051 and I'm going to wait until the new
code shows up so there's like a timer in
all these apps and they go across the
screen or they count down and so you
have to wait for that to happen and so
I'm just going to wait here a little bit
and once I get the new number here this
one is
07153 0 I'm going to hit assign MFA and
there we go and I can't tell you how
many times I like messed that up because
I didn't understand the consecutive
numbers but you're just waiting for uh
the number that's on the screen to enter
it in and then enter the next one in to
turn on MFA and so now your account is
protected and every time you log in
you're going to have to enter in MFA so
let's log out and see what that looks
like so we'll go ahead and sign
in and uh again we'll put in our root
user account here we'll type in 74m
32t
submit and I need to go grab my password
so that's in my password manager just
give me a moment
here and now it wants the MFA code so
this is in my phone and so I'm going to
go enter it in so this one says 475
841 all right we'll hit
submit okay and there we go so that's
going to happen every single time we
want to log in uh I'm going to tell you
that if you get one of these they're so
much easier to use because you just
press the button okay so that's why I
have this because I cannot stand
entering the code in time and time again
um but you know those are your options
there
[Music]
okay hey this is Andrew Brown from exam
Pro and we looking at the concept of
innovation waves so when we're talking
about Innovation waves we're talking
about krava or k waves which are
hypothesized cyclik phenomena in the
global World economy and the phenomenon
is closely connected with technology
life cycles so here is an example where
each wave is irreversibly changes the
society on a global scale and if you
look across the top we can kind of see
what they're talking about so we have
steam engine cotton uh Railway and steel
electric engineering chemistry
pet chemicals automobiles information
technology and so the idea is that cloud
technology is the latest wave and I'm
not sure if you'd fit web 3 in there as
well ml AI but maybe they're all part of
the same wave or they're separate waves
but generally they're broken up based on
this
prde here where it says perspective
recession depression and movement uh
Improvement sorry and so this is the
common pattern of wave where we see a
change of supply and demand and so if
we're seeing this we know that we are in
a wave and where we are in a wave
[Music]
okay hey this is Andrew Brown from exam
Pro and we are looking at the concept of
a burning platform so burning platform
is a term used when a company abandons
old technology for new technology with
the uncertainty of success and can be
motivated by fear that the
organization's future surv uh survival
hinges on digital transformation and
just to kind of give you a visualization
here is a literal burning platform
platform so imagine you have to jump to
it jump from it to make a change so um
you know burning platform could be you
know stop using on Prem and start using
cloud or maybe it going from Cloud to
web 3 um and that's generally the idea
when we talk about a burning
[Music]
platform so I just want to quickly show
you that digital transformation
checklist that I mentioned and the way
you can get to it is by typing in
digital transformation aw us and so it
should bring you to the public sector
page and here it is so we click there
and all it is is a PDF uh so it's not
new it's from 2017 but that doesn't mean
that it's not uh valid anymore uh it's
just that that's when it was made so we
scroll on down and we can see
transforming vision and so we have a
checklist there so if we click into this
uh we can see things like communicate a
vision of what success looks like Define
a clear governance strategy including
the framework of achieving goals uh
build a cross functional team identify
tech technical uh part ERS they talk
about Shifting the culture and then down
below I assume that this one is related
to that one it's unusual because you
know they just have a checklist here but
then they have a sub checklist which
must be clear to that so reorganize
staff into smaller teams things like
that so it's not super complicated
you'll see each category go go Cloud
native they'll have a
checklist um you know and if you are at
at the executive level or the sales
level or trying to convince your VPS or
stuff like that give this a read it
might give you something uh useful in
the end uh to help better communicate
that transformation for you
[Music]
okay hey this is Andrew Brown from exam
Pro and we are looking at the evolution
of computing power so what is computing
power it's the throughput measured at
which a computer complete computational
tasks and so uh what we're pretty much
used to right as of these days is
general computing so good example here
would be a Zeon CPU processor uh that's
more of a high-end processor not
something you'd find your home computer
but when we're talking about data
centers specifically uh um you know
inabus data centers Zeon CPU processors
or what you're going to come across uh
then came along a new type of compute
which is GPU Computing um when we're
talking about Google uh Cloud they have
tensor Computing and so this is where I
get the 50 times faster based on that
metric and so I didn't have an exact
metric here for AWS as um solution for
this mid tier of computing power so I
just borrowed that 50 times there but
the idea is that GP Computing or tensor
Computing uh is is 50 times faster than
traditional CPU and generally that's
going to be used for uh very specialized
tasks when you're doing machine learning
or AI so it's not something you're going
to uh be doing for your regular uh web
workloads but just understand that all
of these uh fits so we're not getting
rid of general computing we're just
adding uh new levels to compute then
there's the latest which is uh Quantum
Computing and so here we have an example
of the rig R retti 16q Aspen 4 and so it
literally looks like it's out of um
science fiction and this thing is like a
100 million times faster it is super
Cutting Edge and we don't even know
exactly how it works and there's not
even anything that's very applicable
that we can use this for but the idea is
that we're not done with the evolution
of of computing power things are going
to get a lot faster once we solve this
last one here and so a service offering
here would be for general computing
you're looking elastic compute Cloud ec2
so we have a variety of different uh
instance types and they're all going to
have different types of Hardware with
different types of general computing um
for GPU Computing this is a specialized
chip that adus has produced called the
adus um and I don't know how to say it
but we'll just abbreviate it to infer so
adus infer chip um and this was designed
as a direct competitor to uh gcps uh
tensor Computing uh unit the t uh TPU um
so this is intended for AI ml workloads
but it works with not just um tensor
flow but it works with any machine
learning framework so that is one
advantage it has over uh tpus um and
then the last one here is adus brackets
so you can actually use quantum
Computing as a service on adus you uh as
of even today um the way adus is able to
do this is they work with Caltech so
that's the California
technology um University or Institute
I'm not sure the name of it there um so
it's not exactly adus producing this but
adus is doing this as a partnership to
give Quantum Computing accessible to you
okay so I'm here in the ab console
because I just want to prove to you that
you can use quantum Computing on AWS
it's that accessible so all you'd have
to do is go to the top here type in
bracket uh and then you make it over to
Amazon bracket and so here uh you can
like set up Quantum tasks the first time
you set it up you got to uh go through
this process here um I think I have to
go through this onboarding to be able to
show you the next steps so I'm going to
go ahead and enable bracket in this abis
account okay and I'm not going to launch
anything I'm just going to try just kind
of show you a little bit of what is
accessible to you because it's not super
exciting but the fact that you can do it
is kind of interesting so here I am on
the inside here and we have all these
different types of quantum Computing so
d-wave I know I I NQ uh retti things
like that and then down below these are
the quantum processing units the qpu and
then down below you have the simulators
so you can kind of simulate uh these
things here um so I think that's kind of
interesting uh but in terms of the cost
like if you scroll on down here um so
abis bracket is part of Theus free tier
it gives you one free hour of quantum
circuit simulation time per month during
the first 12 months so it's free to do
uh a circuit simulation but if you
actually want to run it on the actual
Hardware you can see the cost there's
the per task price the per shot price
things like that uh what could you do
with this I don't know there's things
called like quad bits or something like
that and I can't imagine that you're
going to be doing anything useful but I
think it's just more so like you are
sending out quad bits or whatever they
are and you're observing them um but
what you could do with them I have no
idea but it's just exciting that you can
do that I didn't have any spend just by
activating that I'm just kind of just
showing you there okay
[Music]
hey this is Andrew Brown from exam Pro
and we are looking at the benefits of
cloud and this is a summary of reasons
why an organization would uh consider
adopting or migrating to utilizing
public cloud and so we'll quickly go
through the list here uh because in the
followup slides we actually go into them
a bit more detailed so we have agility
page a go economy of scale Global reach
security reliability High availability
scalability um and elasticity so the
thing is is that eight of us had this
before it was called the six advantages
of cloud but they have reworked it to
include additional items um and so where
you see these uh sub bullets here those
are the original six as you see 1 2 3 4
five six and so I kind of just put them
where they kind of uh fall under the new
categories there and you'll notice that
has included High availability
elasticity reliability and security as
uh new on here okay and so the thing is
is that
um I have always always even in my
original uh uh I think in my original
cloud practitioner had Cloud
architecture as a separate section and
included all these things in here so
it's a great thing to see that abis has
included it um but in terms of how I
organized this course we're not going to
cover them in this section because I
have the cloud architecture section so
just understand that we will come to
those eventually and I would just say
that adab us is still missing something
on this list which is Fault tolerance so
you know my list like this except I
would add fault tolerance to it so you
have everything there um and Disaster
Recovery okay so the benefits of cloud
is a reworking expansion of the six
advantages of the cloud and we will look
at the original six advantages um and
then look at another one that is more of
a generalized one that I I've used
across my courses so that we fully
understand the benefits
[Music]
okay all right let's take a look here at
the six advantages to Cloud defined by a
and so these are still uh part of 's
marketing Pages um but you know it's
interesting because you can't find the
benefits of the cloud in a single page
on any at least the time of making this
so there's a bit of Disconnect between
the um exam guide and the actual
marketing material but that's okay I
fill it all in for you so you know I'm
just again noting that the six advantage
of cloud was the original description
for cloud benefits and we'll go through
them okay so the first is trade Capital
expense for variable variable expense so
you can pay on demand meaning that
there's is no upfront cost and you pay
for only what you consume or you pay by
the hour minutes or seconds so instead
of paying for upfront costs of data
centers and servers the next is benefit
from uh massive Eon uh economies of
scale so you are sharing the cost with
other customers to get unbeatable
savings hundreds of thousands of
customers utilizing a fraction of your
server stop guessing capacity so scale
up or down to meet the current needs
launch and Destroy Services whenever so
instead of paying for idle or
underutilized servers we have increase
Speed and Agility so launch resources
within a few clicks and minutes instead
of waiting days or weeks of your it to
implement the solution on premise we
have stopped spending money on running
and maintaining data centers so focus on
your customers developing and
configuring applications so instead of
operations such as racking stacking and
powering servers the last is Go Global
in minutes so deploy your app in
multiple regions around the world with a
few clicks provide load latency and a
better experience for your customers at
minimal cost the six advantage of the
cloud still apply and um I like to
include them here because they just have
a different kind of lens or or or uh
angle when you're looking at this stuff
and so we've looked at the six
advantages of cloud and now let's take a
look at the next slide my reworking of
the six advantage of the cloud to be
more generalized
[Music]
okay all right I just wanted to show you
where that six advantages of cloud
computing comes from it's part of the
itus documentation so I typed it in here
and you can see that it is still around
uh and so it's unusual because this used
to be part of the marketing website it
had those nice little Graphics um but
for whatever reason it's over here now
in the overview of Amazon web services
and by the way if you're starting
starting out with adabs this is a very
light read but it is a good read uh to
get started with we obviously cover all
this stuff in the course um but you know
maybe you'll get something different
here but the idea is that Abus has
definitely expanded on this but for
whatever reason this documentation
hasn't changed so just understand that I
polyfilled that for you in this course
[Music]
okay all right so this is the seven
advantages to Cloud I said six but I
meant to say seven and so um you know
since I've created fundamental courses
for all the Clusters providers I started
to notice kind of a trend and so what I
did is I normalized it into my own seven
advantages and this actually Maps up
really well to the new benefits of the
cloud so it looks like OS was thinking
the same as I was
um with the exception of those Cloud
architect stuff which I keep in a
separate section but let's go through it
and see what is here so the first is
cost effective you pay for what you
consume no upfront cost on demand
pricing so pay as you go P YG with
thousands of customers sharing the on uh
sharing the cost of resources adabs used
to refer to this always as on demand
pricing and Azure always said pay as you
go and so it looks like adus now uses
both on demand and pay as you go to
describe them which is great um but
there you you go then we have Global so
launch workloads anywhere in the world
just choose a region it's secure so
cloud provider takes care of physical
security cloud services can be secured
by default or you have the ability to
configure access down to a granular
level uh it's reliable so data backup
Disaster Recovery data replication fault
tolerance it's scalable increase or
decrease resources and services based on
demand uh elastic so automate scaling
during spikes and drop in demand current
so the underlying hardware and and
managed uhof software is patched
upgraded and replaced by the cloud
provider without interruption to you so
I think this is one that isn't on the
benefits of the cloud which is a really
good one um but uh yeah that's the
[Music]
seven hey this is Angie Brown and we're
taking a look at Aus Global
infrastructure so what is it well the
adus global infrastructure is a globally
distributed hardware and data centers
that are physically networked together
to act as one large resource for the end
customers so what does that mean well if
you look at the globe on the right hand
side and that Globe is really cool
because adab us used to have a website
where you could uh see a 3D uh globe and
see where all their resources are for
whatever reason they took it down but I
still have the screenshot of it but the
idea is that um the global
infrastructure represents all that
hardware and the connectivity between
that Hardware around the world so what
kind of resources are we talking about
we're talking about regions we're
talking about availability zones direct
connections uh pops also known as as
point of presence local zones wavelength
zones uh and we should point out that
Abus has millions of active users uh or
customers and tens of thousands of
Partners globally so they really are uh
kind of everywhere um and if you're
wondering well what are all these
resources that's what we're going to get
into next we're going to break down what
all these particular resources are
because you definitely need to know what
they are but hopefully that gives you at
a high level that adus has this thing
called Global infrastructure okay
[Music]
hey this is Andrew Brown and we are on
the marketing website for adabs under
Global infrastructure and this is a
great way if you want to explore more
and make sense of that Global
infrastructure so if we scroll on down
here we have a nice map and it's kind of
indicating as to where those regions are
notice that there is uh ones in red
which are coming soon the Canada West
they've been talking about that for I
think a couple years now so still
waiting for those but you know just like
every cloud service provider they're
always expanding looks like we can get a
full list here here um and it should
indicate where when they launched and if
they're launching more things so you
know that is a nice little list uh that
we can get access to but if we go all
the way to the top across the top we can
go to Regions and azs uh and this is
where we should get better information
this is definitely different from before
and I don't think the top of canidate is
supposed to look like that but uh I
guess it's the best that they can do so
uh what I want to point out on these
pages is uh the terms of uh the number
of resources so I'm just going to bump
up the font because it's a little bit
small even for me if we go on down below
here you can see that it's describing um
let's say a particular region so here in
Canada we can see uh we have three
availability zones and when it launched
sometimes they have these Asters on here
so it says located in the Montreal uh
metropolian area so that's a good
indicator because central Canada could
mean Toronto could mean Winnipeg so
that's why they put the asterisk on
there um but just notice that what
you'll usually see for availability
zones you'll never see anything beyond
six I'm not sure why but that seems to
be the max usually when a region
launches it should have three
availability availability zones I think
in the past there might have been some
that did not have um at least three and
the reason why it's important to have
three in a zone is that is how we get
high availability uh the way you do that
is you should have um let's say we're
talking about compute that compute
should be um running redundantly into
two other uh data centers in your region
to ensure um that you have up time in
case the other two go out so just make
note of that if you're coming from Azure
Azure uh will launch things without
having all of their uh zones uh gcp is
really good where they'll always at
least have three so uh each provider
Works a little bit differently there um
but yeah you can see here for North
America we just scroll through here you
can find your particular area and look
at the map uh and wonder why it's so
distorted but yeah hopefully that gives
you kind of an idea there and if you
want to explore any of these other uh
particular offerings you absolutely can
of course we do cover in the course so
it's not really necessary to do that but
I thought uh it'd be nice to show you
this page okay
[Music]
ciao hey this is Andrew Brown from exam
Pro and we are taking a look at a
regions and regions are geographically
distinct locations consisting of one or
more availability Zone and so here is a
world map showing you all the regions
that abos has in the world and the blue
ones represent regions that are already
available to you and the orange ones
represent ones that adus is planning to
open so adus is always expanding their
infrastructure uh in the world so always
expect there to be uh more upcoming ones
every region is physically isolated from
independent of every other region in
terms of location Power and Water Supply
and the most important region that you
should give attention to Is Us East one
uh in particular so this is Northern
Virginia it was ad's first region where
we saw the launch of SQ us and S3 uh and
there are a lot of uh special use cases
where things only work in Us East ones
and we'll find that out here in a moment
what I do want to show you is what it
looks like for an architectural diagram
when you are seeing a region so notice
that we have this um uh little flag here
it says Us East one US West one and
inside of it we have an E2 instance so
that is going to represent a region in
our architectural diagrams uh but let's
look at some of the facts here and
understand why Us East or Us East one is
so important so each region generally
has three availability zones and that is
by intention and we will talk about that
when we get to the availability Zone
section some new users are limited to
two or uh to two uh but generally
there's always three okay new Services
almost always become available first in
Us East and specifically Us East one not
all services are available in all
regions all your billing information
appears in Us East one so that's a US1
particular thing uh the cost of AA
Services vary per region and so if you
were on the marketing website or forious
Global infrastructure you can see uh
here in North America they will say like
when it launched how many availability
zones and there might be some conditions
so you'll notice there's like asteris uh
beside these things here or um in this
one particular there's an asteris saying
hey there are three zones but generally
you're limited to two Okay when you
choose a region there are four factors
you need to consider uh what are the
Regulatory Compliance does this region
meet what is the cost of this Inus
service in this region what in services
are available in this region and what is
the distance distance or latency to my
end users and those are those four
factors that you should remember
[Music]
okay all right so we just talked about
adus regions now let's talk about uh how
that affects our services versus
regional and Global Services so Regional
services are scoped based on what is set
in the adus Management console on the
selected region so you have this drop
down and that's what you'll do you'll
say Okay I want to have resour sources
in Canada or in Europe uh so this will
determine where a na service will be
launched and what will be seen within
the ad Services console you generally
don't explicitly set the region for a
service at the time of creation I
explicitly mentioned this because when
you use something like gcp or Azure when
you create the resource that's when you
select the region but ads is it has this
kind of global thing which is unique to
their platform um then there's the
concept of Global Services so some a
Services operate across multiple Reg
and the region will be fixed to the word
Global and for these that's services
like S3 cloudfront R 53 am so the idea
is if you were to go over to cloudfront
and go into the cloudfront console
you'll notice that it will just say
Global and you can't switch out of that
uh for these Global Services um at the
time of creation it's a bit different so
we were saying up here for regional ones
that you don't select the region but
when you are clearing Global Services if
you're using something like I am there
is no concept of region because they're
just globally available so you don't
have to determine a subset of regions if
you're using S3 bucket that has to be in
one region so you actually do have to
select a region at time of creation um
and then there's something like Cloud
firm distributions where you are
choosing a group of regions so you
either say all of the world or only
North America which is more like
geographic distribution so you don't say
the region in particular but you know
hopefully that gives you a distinction
between Regional services and Global
[Music]
Services hey this is Andrew Brown from
exam Pro and we are taking a look at
availability zones so availability zones
commonly abbreviated as a and I'll
frequently be using the term a is
physical locations made up of one or
more data centers so a data center is a
secured building that contains hundreds
or thousands of computers uh and this is
one of my favorite Graphics I like to
show of course uh you know ads would
never have a dog um in their data center
but I just thought that would be fun a
region will generally contain three
availability zones and I say generally
because there are some cases where we
will see uh less than three so there
might be two um data centers within a
region will be isolate from each other
um so there will be in different
buildings but they will be close enough
to provide low latency and that is
within the uh 10 milliseconds or less so
it's very very low uh it's common
practice to run workloads in at least
three azs to ensure Services remain
available in case one or two data
centers fail and this is known as high
availability and this generally is
driven based on Regulatory Compliance so
a lot of companies uh you know they have
to at least be running in 3 A's and
that's why itus tries to always have at
least three azs within a region uh A's
are represented by a region code
followed by a letter so here you know
you'd have us East one which would be
the region and then the a would
represent the particular availability
Zone in that region um so a subnet which
is related to availability zones is
associated with a uh two availability
zones so you never choose an a when
launching resources you always choose a
subnet which is then Associated to an a
a lot of services um you know don't even
require you to choose a subnet because
they're fully managed by AWS but in the
case of like virtual machines you're
always choosing a subnet okay so here is
a graphical uh representation or a
diagram that's representing two
availability zones so here we have the
region Us East 1 and US West 2 and then
we have our 2 a so here is 1 a and 1 B
and so these are effectively the subnets
okay and so within those subnets then
you can see or availability zones you
will see that we have two virtual
machines okay so the US east1 region has
six azs and I thought that's just kind
of like a fun fact because it is the
most out of every single one um I don't
think anyone comes close to us East one
but of course it is the most popular it
is the uh first uh um region or so it's
not a surprise that that has that many
[Music]
a okay so we just covered regions and
availability zones but I really want to
make it clear uh what they look like so
I kind of have a visual representation
so let's say we have our adus region and
in this particular one we have Canada
Central which in particular is Montreal
so CA Central 1 uh and the idea here is
that a region has multiple availability
zones so here you can see that we have
uh 1 a one 1B and 1D for some reason
adus decided to uh not launch 1 C maybe
it's haunted who knows you know um and
then within your um availability zones
they are made up of one or more data
centers so just understand that an a is
not a single data center but could be a
collection of buildings and that these
azs um are interconnected with high
bandwidth low latency networking they're
fully redundant dedicated to metrof
fiber providing high throughput low
latency networking between so just very
fast Connections in between
and all traffic between azs is encrypted
and these azs are within 100 km so about
60 mil uh of each other
[Music]
okay so what I want to do here is just
show you uh how regions and availability
zones work with some different adus
services so you have a general idea when
you are selecting uh a region or a and
when you're not so within AOS when you
want to select a region you're going to
go up here and it and this is going to
apply to Regional Services a very famous
example of a regional service would be
ec2 so we go over to ec2 which is
elastic uh cloud computing or compute
whatever I just forget the name of it
and what we can do is go over to
instances I'm going to launch an
instance I'm not going to complete the
process I just want to show you what
would happen when you go select some
things here so I'm going to go with
Amazon L 2 um we're going to just go to
uh next here and so here here is where
we're going to select um our
availability zone so up here we have
North Virginia that's our region and
when I say we're selecting availability
Zone we're actually selecting the subnet
so so here we are choosing a subnet and
a subnet is associated to a availability
Zone and every single um uh region has a
default VPC and that VPC has uh subnets
set up and the subnets are defaulted to
each of the availability zones available
so USC 1 has six of them so this server
is going to launch in Us East 1B so this
is a regional service okay uh then we
have Global Services like S3 so we go
over to
S3 and it says it's Global right and so
we're going to go ahead and create our
bucket and so here we choose the region
so we go down we're going to say the
region we want to be in but we don't
choose the availability Zone because
there's nothing to um uh choose because
adabs is going to run these in multiple
A's and it doesn't matter to you what
it's doing there okay um so there's that
and then there's something like
cloudfront so Cloud front's a little bit
uh different here so we go over to
cloudfront and we create ourselves a
distribution um and so yeah if if you
don't have that option there because
sometimes databus has like a splash
screen just click on the left hand side
then go to
distributions okay and so here well they
changed it again on me they're always
changing this UI but if we scroll on
down it should allow us to
change um change where this is going to
launch it's like Global stuff like that
literally they just recently changed
this and that's why I'm
confused uh we'll scroll on down
here it used to
be maybe it's under
Legacy
additional
customized oh it's here sorry okay so
noce here the price class that says use
the edge locations for best performance
North America and Europe North America
Europe Asia middle uh Middle East and
Africa so we're not choosing a
particular region we're picking a
geographical area and so those are
pretty much the major um uh uh examples
of that uh then there's of course things
like in I am where you don't even say
where it is so you go into I am you know
and if I create something like a group
uh over here a user group whoops
here I say create group you know I'm not
saying oh this is for this particular
region or something like that okay so
yeah hopefully that makes
[Music]
sense hey this is Andrew Brown from exam
Pro and let's take a look here at fault
tolerance specifically for Global
infrastructure and so before we jump
into that let's just Define some fault
terminology here uh so let's describe
what a fault domain is so a fault domain
is a section of a network that is
vulnerable to damage if a critical
device or system fails and the purpose
of a fault domain is that if a failure
occurs it will not Cascade outside that
domain limiting the possible damage and
so uh there's this very popular meme
called This is fine where uh there's
obviously a serious problem but uh the
person's not freaking out and I gave it
some context to say well the reason
they're not freaking out because they
know that this is a fault domain and
nothing outside of this room is going to
be affected okay so you can have fault
domains nested inside of other fault
domains uh but generally they're grouped
in something called fault level so a
fault level is a collection of fault
domains um and the scoping of a fault
domain could be something like a
specific specific servers in a rack an
entire Rack in a data center an entire
room in a data center the entire data
set are building and it's really up to
the cloud service provider to define
those boundaries of a domain adus
abstracts it all way so you don't have
to think about it but just to compare it
against something else when you're using
Azure you actually Define your fault
domain so you might say like okay
uh make sure that this workload is never
running on the same VM on the same rack
for these things uh and you know you
might like to have that level of control
but I really like the fact that itus
just abstracts it away I'm not sure how
they segment their uh their their fault
domains but they they definitely are
some broader ones which we'll describe
right now so when we're looking at an
abis region this would be considered a
fault level and then within that fault
level you would have your uh
availability zones and these would be
considered fault domains and of course
those data centers can have have uh
fault domains within them okay like
maybe you know they have everything in a
particular room and that room is secure
so like if there's a fire in that room
it's not going to affect the other room
things like that um so each Amazon
region is designed to be completely
isolated from the other Amazon region
the uh they achieve this with the
greatest possible fault tolerance and
stability uh each availab availability
zone is also isolated but the
availability Zone in a region are
connected through low latency links each
availability zone is designed as an
independent failure Zone
and so here we have some kind of
different language that adus is using um
I've never experienced this terminology
in other any other cloud service
provider so I kind of feel like it's
something that it us made up but
basically a failure Zone they're just
basically saying a fault domain but
let's kind of expand on their fault uh
failure Zone terminology so availability
zones are physically separated within a
typical Metropolitan region and are
located in lower risk uh flood planes
discret uninterruptible power supply so
UPS and an on-site backup uh generation
facilities uh Data Centers located in
different azs are uh designed to be
supplied by independent substations to
reduce the risk of an event on the power
grid impacting more than one
availability Zone availability zones are
all redundantly connected to multiple
tier one Transit providers and we'll
talk about what those are uh in an
upcoming slide and just one thing I want
to note here is that when you adopt
multi-az you get high availability so if
an application is partitioned across A's
companies are better isolated and
protected from issues such as power
outages lightning strikes tornadoes
earthquakes and more so that's the idea
behind you know why we want to run in
multi-az okay because of these fault
[Music]
domains hey this is Andrew Brown from
exam Pro and we're talking about the
adus global Network so the global
Network represents interconnections
between a global infrastructure and it's
commonly referred to as the backbone of
AWS so is EC to so just understand that
that could be used in more than one way
but think of it as a private express way
where things can move fast between data
centers and uh one thing that is
utilized a lot to get data in and out of
AWS very quickly is Edge locations they
can act as on and off ramps uh to the ad
Global Network of course you can uh get
to the network through pops which we'll
talk about um you know in the upcoming
slides here but let's just talk about
Edge locations and what services use
them so uh when we're talking about
things that are getting on to the adus
network we're looking at things like
Abus Global accelerator adus S3 transfer
acceleration and so uh these use agile
locations as an on-ramp to quickly reach
adus resources and other regions by
traversing the fast adus Global Network
notice that the names in it say
accelerator acceleration so the idea is
that they are moving really fast okay on
the other side when we talk about like
an offramp or looking at Amazon
cloudfront which is a Content
distribution Network this uses Edge
locations to uh as an offramp to provide
at the edge storage and compute near the
end user uh and one other thing that is
kind of always utilized in the global
Network are VPC end points now these
aren't using Edge locations but the idea
here is that this ensures your resources
stay within the Aus Network and do not
Traverse over the public internet so you
know if you have uh you know a resource
running in Us East one and one in uh EU
it would and they never have to go to
the Internet it would make sense to
always enforce it to stay within the
network because it's going to be a lot
faster so there you
[Music]
go hey this is Andrew Brown from exam
Pro and we are taking a look at point of
presence also known as Pop and this is
an intermediate location between anabis
region and the end user and this
location could be a data center or a
collection of Hardware so for AWS a
point of presence is a data center owned
by AWS or trusted partner that is
utilized byus Services related for
Content delivery or expediated upload so
a pop res could be something like an
edge location or Regional Edge cache so
as an example over here we see an S3
bucket and it has to go through Regional
Edge cache and then get to an edge
location let's go Define what those are
so an edge location are data centers
that hold cache copies on the most
popular files so web pages images and
videos so that the delivery of the
distance to the end users are reduced
then you have Regional Edge locations
and these are data centers that hold
much larger caches of less popular files
to reduce a full round trip and also to
reduce the cost of transfer
[Music]
fees so to kind of help put pops more in
presence just in the general sense here
is a uh diagram I got from Wikipedia
that kind of just shows a bunch of
different networks and notice where the
pop is it's on the edge or the
intersection of uh two networks so here
you know we have um you know tier three
and then there's tier two and there's
this pop that is in between them okay so
tier one networks is a network that can
reach every other network on the
internet without purchasing IP transit
or paying for peering and so the inabus
availability zones or A's are all
redundantly connected to multiple tier
one Transit providers
[Music]
okay all right so let's take a look at
somea services that are utilizing pops
or Edge locations for Content delivery
or expediated upload so Amazon
cloudfront is a Content delivery network
service and the idea here you point your
website to cloudfront so that it will
route requests to the nearest Edge
location cache it's going to allow you
to choose an origin so that could be a
web server or storage that will be the
source of the cach and cach is the
content of what origin would return to
various Edge locations around the world
then you have Amazon S3 transfer
acceleration this allows you to generate
a special URL that can be used by the
end users to upload files to a nearby
Edge location once a file is uploaded to
an edge location it can move much faster
within the adus network to reach S3
then at the end here you have adus
Global accelerator you can find the
optimal path from the end user to your
web servers so Global accelerators are
deployed within Edge location so you
send user traffic to an edge location
instead of directly to your web
application this service is really
really great for if let's say you are
running a web server in Us East one and
you just don't have the time uh to set
up infrastructure in other regions you
turn this on and you basically get a
boost okay
[Music]
hey this is Andrew Brown from exam Pro
and let's take a look at itus direct
connect so this is a private or
dedicated connection between your data
center office collocation and AWS and so
the idea here is imagine if you had a
fiber optic cable running from your uh
data center all the way to your ads so
that it feels like uh when you're using
your stuff on your data center like your
local virtual machines that uh there's
like next to no latency okay so Direct
Connect has two very fast network
connection options we have have the
lower bandwidth which is at 50 to 500
megabytes per second and then you have
the higher bandwidth which is 1 GB to 10
gbes per second so using Direct Connect
helps reduce Network costs increase
bandwidth throughput so great for hi
trffic networks provides a more
consistent Network experience than a
typical internet-based connection so
reliable and secure U I do want to point
out the term collocation if you never
heard of that before a collocation or a
carrier hotel is a data center where
equipment space and bandwidth are
available for rental uh to retail
customers and I do want to also point
out that even though it says private up
here and this is the language that AWS
used I usually just say dedicated but
the connection is private but that
doesn't necessarily mean it's secure
okay so uh we'll talk about that when we
reach ads vpns and how we can use that
with direct connect to make sure our
connections are secure
[Music]
okay all right so let's take a look at
what a direct connect location is so a
direct connect location our trusted
partner data centers that you can
establish a dedicated highspeed low
latency connection from your on- premise
to AWS so an example of a partner data
center would be one like here in Toronto
the Allied data center so you can tell
that's right down in uh the Toronto
Center and so you would use this uh uh
as part of direct connect service to
order and establish a connection
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at local
zones which are Data Centers located
very close to densely populated areas to
provide singled digigit millisecond low
latency performance so thinks like 7
milliseconds for that area so here is a
map of uh local zones that exist and
ones that are coming out I believe the
orange ones are probably ones that are
on their way and so to use a local Zone
you do need to opt in so you got to go
talk to AWS probably open a support
ticket to get access to it the first one
to ever be launched was uh the LA one uh
and so um you know when you want to see
it looks just like a an availability
Zone it's going to show up under
whatever region that is because these
are always tied to existing regions so
the la1 is tied to us West uh region and
the AZ would look like us West 2 hyphen
LAX hyphen 1A okay so only specific a
Services have been made available so
there's particular ec2 types EBS Amazon
FSX application load balancer Amazon VPC
they probably have extended it to more
services do you need to know that for
the exam no but you know the point is is
that there's a limited subset of things
that are available the purpose of local
zone is to support highly demanding
application sensitive Delancy so media
and entertainment electronic design and
automation adte machine learning so it
kind of makes sense like you look at La
they're in the media entertainment and
so they're dealing with lots of media
content so it has to be really low for
them
okay hey this is Andrew Brown from exam
prep and we are taking a look at Abus
wavelength zones and these allow for
Edge Computing on the 5G networks and
applications will have ultra low latency
being as close as possible to the users
so Abus has partnered with various
telecom companies to utilize their 5G
networks so we're looking at Verizon
vhone kddi SK Telecom and so the idea
here is that you will create a subnet
tied to a wavelength Zone and then and
just think of it as like an availability
Zone but it's a wavelength Zone and then
you can launch your VMS to the edge of
the targeted 5G Network so that's the
network you're using uh AWS to deploy an
ec2 instance and then when users uh
connect to you know those radio tower
those um cell towers they're going to be
routed to um you know nearby hardware
that is running those virtual machines
okay and that's all it is it's just it's
just uh ec2 instances um but you know
the advantage here is that it's like
super super low latency okay
[Music]
hey this is Andrew Brown from exam Pro
and we are taking a look at data
residency so this is the physical or
geographical location of where an
organization or Cloud resources reside
and then you have the concept of comp uh
compliance boundaries so a Regulatory
Compliance so legal requirement by
government or organization that
describes where data and Cloud resources
are allowed to reside and then you have
the idea of data sovereignty so data
sovereignty is the jurisdictional
control or legal Authority that can be
asserted over data because its physical
location is within a uh jurisdictional
boundary and so the reason we care about
this stuff is that if we want to work
with the Canadian government or the US
government and they're like hey you got
to make sure that you know if you want
to work with us all the data has to stay
in Canada and you need to give them that
guarantee so data residency is not a
guarantee it just says where your data
is right and compliance boundaries are
those um controls that are in place to
say okay this is going to make sure that
data stays where we want to be and data
of sovereignty is just like the idea of
the scope of the the legal the legal
stuff that ties in with compliance
boundaries so how do we do that on AWS
well there's a few different ways but um
let's just take a look at some ways that
we can meet those compliance boundaries
one uh which is very expensive but also
very cool is adamus outposts so this is
a physical rack of servers that you can
put in your data center and you'll know
exactly where the data resides because
you know it's physical if it's in your
data center and you're in Canada that's
where it's going to be okay uh and I
believe that you know there is only a
subset of uh adus services that are
available here but you know that is one
option to you another is using like um
services for governance so like one
could be adus config this is a policy as
a code service so you can create rules
to continuously check adus resource
configuration so if they deviate from
your expectations you're alerted orus
config can in some cases Auto remediate
so if you were expecting you know um you
know you had an adus account and you're
saying this account is only to be used
for candid reason resources and somebody
launches let's say something in another
region then you could get an alert or
tell it was config to go delete that
resource okay now if you want to prevent
people from doing it uh Al together
that's where IM policies come into play
so these can be written explicitly to
deny access to specific adus regions and
you know this is great if you're
applying to users or roles but if you
wanted to have it organizational wide
across all of your um AIS accounts you
can use something called a service
control policy that is just an i policy
that is used within 's organizations
that makes it organizational wide
[Music]
okay hey this is Andrew Brown from exam
Pro and we are looking at ads4
government so to answer that we first
have to understand what is public sector
so public sector includes public goods
and government services such as military
law enforcement infrastructure public
transit public education Healthcare and
the government itself so AOS can be
utilized by the public sector or
organizations developing Cloud workloads
for the public sector and AES to achieve
this by meeting Regulatory Compliance
programs along with specific governance
and security controls so this could be
meeting the requirements with HIPPA fed
ramp um cjis uh and fips okay so IIs has
a special regions or special regions for
us regulation called gov Cloud which
we'll talk about next
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at govcloud
and to understand what govcloud is we
need to know what fedramp is so fedramp
stands for federal risk and
authorization Management program it's a
US government-wide program that provides
a standardized approach to security
assessment authorization continuous
monitoring for cloud products and
services so that we know what fed ramp
is what is gocloud well uh and again
it's not particular to AWS because Azure
has go Cloud as well but a cloud service
provider like inabus or Azure J will
offer an isolated region to run fed ramp
workloads and so in it's called govcloud
and these are specialized regions that
allow customers to host sensitive
controlled unclassified information and
other types of regulated workloads so
govcloud regions are only operated by
you uh by US citizens on us soil they
are only accessible to us entries and
root account holders who pass a
screening process customers can
architect secure Cloud solutions that
comply with fed ramp uh do the doj's uh
criminal justice Information Systems
security policy the US International
traffic and arms regulation uh uh export
Administration regulations the
Department of Defense cloud computing
security requirements and guides so if
you want to work with the US government
you want to uh engineer and use gov
Cloud
[Music]
okay hey this is Andrew Brown from exam
Pro and we're taking a look at uh
running adab us China so adus China is
the adus cloud offering in mainland
China adus China is completely isolate
intentionally from adus Global to meet
Regulatory Compliance for mainland China
so that means that if you make a
workload on the adus global uh you can't
uh interact with it within the adus
China One okay it's basically treated
like a a completely separate service
like adus has its own Chinese version uh
and so adus China is on its own domain
at
amazon.cn and for everybody else that's
what's considered Global so when I'm
using adus from Canada or use it from
the US or from India or from Europe or
wherever that is the adus global okay so
in order to operate in adus China
regions you need to have a Chinese
business license so ICP license not all
services are available in China so uh
you will not have the use of Route 53 uh
and you might say well why not just run
in Singapore you it was Global and you
could do that but the advantage of
running in mainland China means that you
would not have to Traverse the great
firewall okay so all your traffic is
already within China so you don't have
to uh deal with that Abus has two
regions in mainland China so uh there's
this one here which is the northwest
region operated by NS WCF and then you
have the one in Beijing North one
operated by sinnet so you know iTab us
just could not meet the the compliance
requirement so they had to partner with
local providers uh or data centers and
so that is how that works
[Music]
okay all right so I want to show you how
you get over to the um Chinese adus
Management console so this one is adab.
amazon.com that is the global one for
everyone outside of mainland China but
if you want to run resources uh on data
centers within mainland China this is at
amazon.cn and so it looks very similar
if you go to create a free account
you're going to fill in this stuff but
uh notice that you need to have your
business registration certificate uh and
additional information in order to run
these data centers down below that ad is
partnered with also notice that the logo
doesn't say AWS in it and there's a good
reason for that if I type in AWS
trademark
China uh adus is actually banned from
using the adus logo in China uh for
whatever reason it's a weird reason if
you ever want to read about it but
that's why you don't see AWS here all
right um so yeah there you go
[Music]
hey this is Andre Brown from exam Pro
and we are looking at sustainability for
adus Global infrastructure and before we
talk about that let's talk about the
climate pledge so Amazon co-founded the
climate pledge to achieve Net Zero
carbon emissions by 2040 across all of
Amazon's businesses which includes AWS
if youall want to find out more
information go to to
sustainability. amazon.com there's a lot
of great information there and you'll
learn exactly how uh is achieving this
in particular like their data centers
it's very interesting okay so adus Cloud
sustainabil goals are composed of three
parts the first is renewable energy so
adus is working towards having their
adus Global infrastructure powered by
100% renewable energy by
2025 and Abus purchases and retires
environmental attributes to cover the
non-renewable energy for Abus Global
infrastructure so they would purchase
things like renewable energy credits
also known as Rec's guarantees of
Origins so go the second Point here is
cloud efficiency so adus infrastructure
is 3.6 times more energy efficient than
the medium of us Enterprises data
centers surveyed so that's going to
really rely on that survey surveys are
not always that great so you know take
that with a grain of salt okay then we
have water uh stewardship so uh direct
evaporative technology to cool our data
centers use of non portable uh water for
cooling purposes so the recycling water
on-site water treatment allows us to
remove us them to remove scale forming
minerals and reuse Waters uh for more
Cycles water efficiency metrics to
determine and monitor optimal water use
for each adus region and you'll find
that water plays a large part on uh
making these um uh these data centers
very efficient
[Music]
okay so I just wanted to show you where
you get to that sustainability
information so I just went to Adis
Global infrastructure you click
sustainability
and that's going to bring us over to
whoops I have my Twitter open there to
the sustainability in the cloud so if
you want to read a bunch of stuff here
about things that are going on that it
us is up to see uh how they are
progressing with renewable energy um
there's Cloud efficiency up here so you
know how are they being efficient it's
worth the read to really understand that
there's a lot of water involved like
reducing water in data centers I thought
that was really interesting um I mean
they have as podcast but I don't think
there's really much to it a bi-weekly
podcast of bite-sized stories about how
Tech makes the world better that's not
necessarily A sustainability podcast
it's just the invis podcast in general
there's a download Center um Amazon's
2020 sustainability reports so I guess
you can download the reports to see what
is going on there so we can download the
progress here and see what they've been
up
to okay so there's a bunch of numbers
things like that okay very short reports
but hey at least you can download them
okay so just in case you're uh very
interested in sustainability all
[Music]
right hey this is Andrew Brown from exam
Pro and we are taking a look at Abus
ground station so this is a fully
managed service that lets you control
satellite Communications process data
and scale your operations without having
to worry about building or managing your
own ground station infrastructure and so
when we're talking about ground station
a really good way to cement what the
service is is just think of a big anten
ey dish that's pointing into the sky
trying to communicate with satellites
because that's essentially what the
service is doing so the use cases here
could be for weather forecasting surface
Imaging communications video broadcasts
and to use ground station the idea is
that you would schedule a contact so
that's where you're selecting a
satellite a start and end time and the
ground location and then you use an a
ground station ec2 Ami and Amazon
machine image to launch ec2 instances
that will Uplink and downlink uh data
during the the contact or receive
downlink data in an Amazon S3 bucket a
use case could be something like you are
a company you've reached an agreement
with a satellite image provider to use
their satellites to take photos for a
specific region or time or whatever and
so the idea is that you are using adus
ground station to communicate uh to that
company satellite and download that s uh
that image data to your S3 bucket okay
[Music]
hey this is andreww brown and we are
looking at Aus outposts and this is a
fully managed service that offers the
same Aus infrastructure Services apis
tools to virtually any data center
co-cation space or on premise facility
for a truly consistent hybrid experience
and just to kind of summarize it it's a
rack of servers running adaba stuff on
your physical location okay so before we
jump into the service or technology
itself uh let's talk about what is a
rack server or just a rack so it's a
frame designed to and organized it
equipment so here's an example of a 42u
rack uh and there's the concept of rack
heights so the U stands for rack units
or U spaces uh with it equal to 1.75 in
and the industry standard rack is a 48 U
um so that is a 7t rack so a full size
rack cage is commonly the 42 High okay
and uh in it you might have equipment
that is of different sizes so they could
be one u 2 U 3 U or 4 U high so here's
an example of you know of an interior of
a rack and notice that like one u 2 U 4
U they're all a little bit shaped
differently uh but they give you kind of
an idea of um you know what those are so
it Outpost comes in three form factors
the 42 U the one U and the 2 U so the uh
the first one here the 42u this is
basically a full rack of servers
provided bys so you're not just getting
the frame it actually comes with you
know servers uh and so adus delivers it
to your Preferred Physical site fully
assembled and ready to be rolled into
the final position it is installed by
adus and the rack needs to be simply
plugged in to the power and network and
there's a lot of details about um the
specs on this on the adus website so you
know I'm not going to go through them
all here um then there are servers that
you can just Place into your existing
racks so we have the one U so this is
suitable for 19in y 24 in deep cabinets
it's using UST uh Gravitron 2 um CPUs
and you can have up to 64 virtual CPUs
we have 128 gigabytes uh 4 terabytes of
local
NVM storage um and then you have the U
or sorry the 2 U so suitable for 19in
wide 36 in deep Intel processors up to
128 virtual CPUs 256 gbt of memory 8
terab of local nvme storage so there you
go
[Music]
let's take a look at Cloud architecture
terminologies before we do let's talk
about some of the roles that are around
uh doing Cloud architecture so the first
is Solutions architect this is a role in
a technical organization that Architects
a technical solution using multiple
systems via researching documentation
and experimentation and then you have
the cloud architect this is a Solutions
architect that is focused solely on
architecting Technical Solutions using
cloud services under understand that in
the uh actual Marketplace a lot of times
Solutions architect is used to describe
both a cloud architect and a Solutions
architect and you know these are going
to highly vary based on your locality
and how companies want to use these
terms but this is just me broadly
defining them here so just don't take
them as a perfect word in terms of what
they're representing so a cloud
architect needs to understand the
following terms and factors uh and
Factor them into their designed
architecture based on the business
requirements so we have the idea of
availability your ability to ensure
service remains available scalability
your ability to grow rapidly or
unimpeded elasticity your ability to
shrink and grow to meet the demand fault
tolerance your ability to prevent a
failure disaster recover your ability to
recover from a failure and there are a
couple other things that uh that should
be considered they're not terminologies
but they're definitely important to a
Solutions architect or Cloud architect
and uh these are things you always need
to consider uh as well and this is just
me talking to my Solutions architect
friends where they'll always ask me
these two questions after presentation
they'll say how secure is the solution
and how much is this going to cost all
right and so for the terminologies up
here we're going to Define these right
away and we're going to figure these out
throughout the course we have two giant
sections just on cost and security alone
uh so there we
[Music]
go the first term we're looking at is
high availability and this is your
ability for your service to remain
available by ensuring there is no single
point of failure and or you ensure a
certain level of performance so the way
we're going to do that on ews is you'd
want to run your workload across
multiple availability zones to ensure
that if one or two availability zones
became unavailable your servers or
applications remain available because
those other um those other servers are
going to be there and the way we would
accomplish that is via elastic load
balcer so a load balancer allows you to
evenly distribute traffic to multiple
servers in one or more data center if a
data center or server becomes
unavailable or unhealthy the load
bouncer will route the traffic to only
the available data centers within the
server and understand that just because
you have additional servers doesn't mean
that you are uh you're available you
have to you might need to meet a
particular threshold of availability so
you might need to have at least two
servers always running to meet the
demand so it's based on the the demand
of traffic
[Music]
okay let's take a look here at high
scaleability so this is your ability to
increase your capacity based on on the
increasing demand of traffic memory and
computing power and we have the terms
vertical scaling so scaling up um this
is where you upgrade to a bigger server
and then there's horizontal scaling
scaling out this is where you add more
servers of the same size and the great
thing about scaling out or adding
additional servers is that you're also
going to get um High availability so if
you do need two servers it's always
better to you know add an additional
server as opposed to having a larger
server but it's going to be very
dependent on a lot of factors okay
[Music]
so scalability and elasticity seem very
similar but there is a crucial
difference and this is your ability to
automatically increase or decrease Your
Capacity based on the current demand of
traffic memory and computing power again
it's the it's the fact that it happens
automatically and you can go both ways
increase or decrease so for horizontal
scaling we have the concept of scaling
out so add more servers of the same size
and then scaling in removing
underutilized servers of the same size
and vertical scaling is generally hard
for traditional architectures so you'll
usually only see horizontal scaling
described with elasticity um and the way
we would accomplish uh being highly
elastic is using autoscaling groups asgs
and this is an naus feature that will
automatically add or remove servers
based on scaling rules you define based
on those metrics
[Music]
okay let's talk about being highly fault
tolerant so this is your ability for
your service to ensure there is no
single point of failure preventing the
chance of failure and the way we could
do that is with fail overs so this is
when you have a plan to shift traffic to
a redundant system in case the primary
system fails a very common example is
having a copy or secondary uh uh uh of
your database where all ongoing changes
are synced the secondary system is not
in use until a fail over occurs and it
becomes the primary database so when
we're talking about databases on abs
this is the concept of RDS multi-az so
this is when you run a duplicate standby
database in another availability Zone in
the case your primary database
[Music]
fails and last here is high durability
so this is your ability to recover from
a disaster and to prevent the loss of
data so solutions that recover a
disaster uh from a disaster is known as
disaster recovery so do you have a
backup how fast can you restore the
backup does your backup still work how
do you ensure current live data is not
corrupt and so maybe a solution ads
would be using Cloud endurer which is a
disaster recovery uh service which
continuously replicates your machines in
a lowcost staging area in your target AB
account and preferred region enabling
fast and reliable recovery in the case
of an IT data center fails
[Music]
okay so to understand Disaster Recovery
we need to know more about uh things
around it like business continuity plans
BCPS and RTO and rpos so a BCP is a
document that outlines how a business
will continue operating during an
unplanned disruption in services so it's
basically the plan that you're going to
execute uh if that happens and so here
we have a disaster and you can see that
there's a chance of data loss and
downtime and these two um uh factors as
RPO and RTO are going to define the
length of these durations so recovery
Point objective is the maximum
acceptable amount of data loss after an
unplanned data loss incident
Express this amount of time so how much
data are you willing to lose and then
recovery time objective so the maximum
amount of downtime your business can
tolerate without inuring a significant
financial loss so how much time you're
willing to go down okay so those are the
two there and now let's go take a look
at the disaster recovery options that we
can use to define in our our
[Music]
BCP so now let's take a look at our
disaster recovery options uh and based
on what you choose they're going to be a
trade of cost versus time to recover
based on the rpos your RTO of course and
so sometimes this is rep represented
vertically like a a thermostat or you
can do it horizontally here um both are
valid ways of displaying this
information but I just have it
horizontally here today and so we have
low or high or you could say um even
though I don't have it written here this
could be cold or this could be hot okay
so um on the left hand side we got
backup and restore pilot light warm
standby multi-active site notice we're
using the like the words like pilot
light warm things that are relating to
temperature so again cold and hot all
right so let's just walk through what
each of these things conceptually do uh
in terms of architecture so when you're
doing a backup and restore you're back
you basically back up your data and at
the time of Disaster Recovery you're
just going to restore it to New
infrastructure uh for a pilot light the
data is replicated to another region
with the minimal Services running to
keep on replicating that data and so you
might have some core Services running a
warm standby is a scale down copy of
your infrastructure so you basically
have everything that you would
absolutely need to run an application
but the idea is it's not at scale and so
at any time when there's an incident
you're going to scale up to the capacity
that you need and then you have
multi-site active active where you you
have a scaled up copy of your
infrastructure in another region so
basically everything you have
identically in another region and so in
terms of the rpos and the RTO for back
and restore you're looking at hours uh
with the pilot light you're looking at
10 minutes with a warm standby you're
looking at minutes and multi sight uh
active active you're looking at uh real
time so you know hopefully that gives
you an idea of you know the difference
in terms of scale but let's just look at
more detail so for a backup and restore
this is for low priority use cases
restore data after event deploy
resources after an event and it's very
cost effective uh for pilot light this
is where you have less stringent RTO and
rpos so you're going to be just running
your core Services uh you're going to
start and scale resources after the
event and this is a little bit more
expensive this is uh very good for warm
standby is good for business critical
services so you scale resources after
the event uh and it's uh almost very
it's very it's costly but it's not as
expensive as a multi-site active active
so you get zero downtime near zero loss
uh you have it's great for Mission
critical services and it's just as
expensive as your original uh structure
so you're basically doubling the class
there
[Music]
okay so we already defined RTO but let's
redefine it again based on what adus
describes in their white paper and just
look at how it Maps against um the
disaster recovery option so re recovery
time objective is the maximum acceptable
delay between the interruption of
service and restoration of service this
objective determines the uh what is
considered an acceptable time window
when service is unavailable and is
defined by the organization and so this
is the diagram found in the white paper
and so on the left hand side we have
cost and complexity here and then
lengths of serious Interruption and what
you can see here is that the cost and
complexity for a multi- sight active
active is very high but the length of
service Interruption is zero and then as
we go down we have warm standby so it's
significantly like at least half uh the
complexity of that one then we have our
pilot light down here and back up and
restore but notice backup restore takes
the longest amount of time and notice
here we have a recovery time objective
so in your BCP you kind of Define where
that is based on the cost of business
impact so you might have to calculate
that saying okay what is our cost over
time based on the length of service
Interruption where do we want our RTO to
be what is the acceptable recovery cost
and this is where you're going to decide
what you want to do so here we have
pilot light and backup and restore and
so this company has to decide whether
they want to do a pilot light or they're
going to do a backham restore but it
sounds like this is where they're going
to be which is at the pilot uh light for
what is acceptable in their business use
case
[Music]
okay let's do the same for RPO so
recovery Point objective is the maximum
acceptable amount of time since the last
data recovery point the objective
determines what is considered an
acceptable loss of data between the last
recovery point and the interruption of
service and it's defined by the
organization again we pulled this from
the a white paper for disaster recovery
and and uh we have cost and complexity
but this time it's replaced with data
loss before service Interruption so uh
for multisite again it's going to be
very expensive and high up here as you
notice it's not like a perfect um uh
curve it's just it's a bit different in
terms of what it looks like so here we
have warm stand standby pilot light um
and so you'll see that the data loss is
um not a big deal but for backup and
restore it really juts out there so you
can see that you can get pretty good
good results just with the pilot light
and the cost and complexity is very low
again we have to look at our cost and
business impact so we got to follow that
line and we need to see where our
acceptable uh recovery cost is and so uh
you're going to notice that uh we have a
bit of an intersection here okay and so
we need to determine you know like are
we going to be doing a warm standby
looks like we have the cost to do it um
uh but you know it just really depends
you know do we want to be down here or
down there okay so hopefully that helps
and visualize that information for
[Music]
you hey this is Andrew Brown from
exampro and what I want to show you here
is a real world architectural diagram I
created this a while ago this is a
previous version of the um exam Pro or
technically teacher seat platform uh
that powers The Learning Experience uh
for by class certifications and so I'm
hoping that by giving you some exposure
you'll absorb some information here uh
and that will carry through to really
help you cement what these services do
and how they work together
um now you might be asking how did I
make this well I'm in Adobe XD it's by
Photoshop or sorry Adobe it's free to
download but there's a lot of options
out there and but the first thing you'll
need is those AWS architectural icons so
these are free on AWS you can download
them in PowerPoint download them as
asset as svgs and pgs which is what I
have done and start using them in your
um uh whatever software you like there's
also third party providers out there so
like there's Lucid charts I love Lucid
charts but I don't use it to make
architectural diagrams uh for AWS um but
you know you can drag drop and stuff and
they already have the library there and
there's a bunch of them that you can
choose from so uh you know that's
interesting but let's take a look at one
that we can download maybe everyone's
familiar with PowerPoint so here is the
adus architectural icons and the reason
I'm showing you this is not because it
just contains icons but it also suggests
how you should build them so if I go
through here they'll give you a
definition of those system elements uh
how they would look like here so we have
our group icons our layer group our
service icons res icons where they
should go uh and then they have some
interesting guidelines of like dos and
don'ts so here's like a simple example
of a get to an S3 bucket um here's an
example of using VPC subnets and things
like that on the inside um and then you
can see kind of like all the groups that
we
have and it'll show all like the uh the
um arrows it's a big faux PA to make U
diagonal arrows that's just something
eight of us toine but you'll see a lot
of people do them anyway
and then you'll see all the icons so do
you have to make them like ad a suggest
no but you know if if you like the way
they look that is fine everyone just
does whatever they want honestly so
anyway now that we've seen you know how
we can go get the resources to make our
own I have Adobe XD open up here and so
I just kind of want to walk you through
what's going on here so again I said
this is a a
traditional um architecture meaning that
it's powered by virtual machines and so
what we need to look for uh is ec2
because that's where it's going to start
that's our virtual machine and you'll
notice we have one here so there's a T2
um uh that's running over here and then
over here we have a T2 okay so uh we
have a blue and a green environment so
this is our running environment so I'm
just going to zoom on in here okay so
the web app would be running on this and
um and then on the outside here we have
an autoscaling group and so autoscaling
groups allow us to um manage a group of
ec2 instances and they will
automatically scale if the demand
increases or or or decline so if this
machine can't handle it it will just
automatically provision a new one and so
I've contained it in this environment
here because I'm representing a blue
green deploy meaning that when I deploy
this will get this will be the
environment that replaces things and so
you can see I have a lot of lines being
drawn around here so um over here we
have uh um parameter store so parameter
store is a place where we can store our
environment variables um or application
configuration variables and so I have
this line going here and it's just
saying we're going to take these
environment variables and put them into
the application okay uh and then there's
also uh the database credential so here
we are using postgress over here so and
then we need the database credential so
we're grabbing those database
credentials those are stored in Secrets
manager and we're giving to the
application so the app knows how to
connect to the database and this one
knows how to uh configure it okay then
we have um a bunch of uh buckets here
for different organiz ganizations and so
you know S3 is for storage so this is a
way we're going to um store a variety of
things so like user data assets
artifacts Cloud information templates so
some of this is for the app some of them
is for the infrastructure so that's one
thing there okay then over here we have
u a cicd pipeline so we have code
Pipeline and so code pipeline is
triggered by GitHub so we put our code
in GitHub and when that happens it's
going to do a code build so that's going
to build out a server
um and then from there it's going to run
another code build server and then from
there it's going to then um uh uh use
code deploy and so code deploy is going
to trigger a deploy what it will do is
create a new environment so it's going
to create a copy of this um sorry it's
going to create a cop this is actually
the environment that's running so we'll
copy that and that will be our new
environment right okay and so when the
deploy is done it will swap and then
that environment will become this new
one
um and so you know again this is
actually really the the running server
it's just kind of easy to get hung up on
this one but the idea here is that um
you know that's how deployment works but
let's say uh you know we want to get uh
traffic to this actual instance this is
going to come through the internet and
the internet's going to probably go to
rid 3 so ref3 is used for domain names
so this' be like exampro doco
teacher.com we pass that over to our
elastic load balancer which in this case
is an application load bouncer that's
why it's called ALB and that's going to
distribute the traffic there if we
wanted to run the server in another um
in another availability zone so that we
make it highly available um you know ALB
the elastic load bouncer application
load balcer is going to uh have some
traffic go here and some traffic go
there so this is just uh the blue
environment or whichever the current
environment is over here now when we
want to deploy new versions we're going
to use launch templates and launch
templates um uh are necessary when using
Autos scaling groups so um you know you
do have to Define launch template it
just says like what is the shape of this
instance type like what's this family
what should it be and then we need an
Amazon machine image so our Amazon
machine image is custom built because we
are installing all the stuff that we
want on it and so in order to automate
that process we are using um SSM
automation documents so SSM stands for
systems manager and automation allows
you to automate that step so what it's
going to do is launch an instance
install Ruby install postgress download
the codebase then it's going to create
that Ami and then um it will do a bunch
of other stuff here as well and this is
going to run weekly or actually at the
time uh it was running nightly so we're
doing nightly builds so that we would
always get the latest um updates to our
server um because it's a virtual machine
there could always be uh new updates for
that Linux version or Amazon machine Li
version we were using and then there's a
bunch of other stuff here so you know um
hopefully that kind of gives you an idea
like the complexity of it and you know
this is how I like to make my
architectural diagrams very in detailed
so that we can um look at them but yeah
if that was too much that's fine but you
know that's just the complexity of it if
you build your own you'll start to
really grasp this stuff pretty well
[Music]
okay so what I want to do is just show
you how high availability is built into
some ad Services where in other cases
you have to explicitly choose that you
want something to be highly available uh
so what I'm going to do is make my way
over to S3 and so with S3 this is where
you can create S3 buckets and this
allows you uh to store things and so the
great thing about S3 is that it's
basically serverless storage so the idea
is that you're just going to choose your
region and by default it's going to
replicate your data across multiple um
uh data centers or azs and so this one's
already highly available by default with
the standard tier uh so that is
something that's really nice but other
services uh you know like ec2 the idea
is that you are going to launch yourself
an ec2 instance so we launch that one
and the problem with this is that if you
launch a single
ec2 that is not highly available because
it's a single server running in a single
um AZ so here you know we would choose
our subnet our subnet is our
availability Zone but you'd have to
launch at least two additional servers
and then you'd have to Route um uh you'd
have to uh have something that would
balance uh the traffic to the to the
three which is a load balcer and so in
this case you have to construct your
high availability then you have services
like elastic beanock this is a platform
as a service um and we'll go to
environments here I'm not sure I wasn't
showing up there um and so the idea is
that with elastic beant stock I'm just
going to click on the main service here
you're going to go ahead and uh create
your application or create your
environment you probably want to create
an environment first here okay and so I
would choose a web server and then the
idea is I just name so my application
here my uh environment and then down
below you go configure more options
whoops it wants me to choose everything
that's totally
fine and we say configure more options
we're not going to create it because um
we don't want to create one but the idea
is that uh you you could choose whether
you want this to be high highly
available or not so see it says single
instance so free tier uh and then if you
choose this what it's going to do it set
up a bunch of stuff for you so it's
going to set up an application load
balancer for you it's going to set up
Auto scaling groups for you to make it
highly available it's going to run at
least uh between 1 to four instances so
this does everything that uh ec2 you'd
have to do manually setting up so that's
really
nice okay so you know some options have
that if we make it our way over to RDS
and again we're not cating anything
we're just looking at the options it
gives us when we uh start things these
up
here we'll make our way over to RDS when
it gives us a moment here
and if we go ahead and create ourselves
a new
database and we look at something like a
postgress database notice that we have a
production option and a Dev test option
and so I mean usually it shows us the
price down here so even test Dev is $118
which is not true you can make it
cheaper than that but the idea is that
when you choose between these two
options um it's going to set up uh
multi-az it's going to that means that
it's going to run a an additional um uh
database in another availability Zone
replicate that data over so that it
stays highly available um you know it's
going to have autoscaling uh uh part of
it and so some Services you just choose
it abstractly so you just have to
understand what highly availability is
going to mean underneath so hopefully
that kind of gives you a picture of high
availability on
[Music]
AWS hey this is Andrew Brown from exam
Pro and we are looking at adus
application programming interface also
known as adus API so before we talk
about uh the API let's describe what
application programming interface is so
an API is software that allows two
applications or services to talk to each
other and the most common type of API is
via HTTP requests and so the ads API is
actually an HTTP API and you can
interact with it by sending HPS requests
using an application interacting with
apis like Postman and so here's kind of
an example of what a request would be
that would be sent out and so the way it
works is that each ad a service
generally has a service endpoint so see
where it says monitoring that's going to
be Cloud watch so sometimes they're
named after the services sometimes the
name is a bit obscure and of course you
can't just call an uh call an API
request without authenticating or
authorizing and so you have to sign your
request and so that's a process of
making a separate request uh with your
adus credentials to get back a a
temporary token uh in order to authorize
that and I don't have room to show it
but the thing is is that what you'd be
also going along with those requests
would be to provide an action so when
you look at um the adus API it will show
you a bunch of actions that you can call
they're basically the same ones you'll
see in the IM policies so it could be
like describe ec2 instances Or List
buckets um and they can also be
accompanied with parameters okay so you
know we're probably not going to show
you how to uh make an API request
directly because that's not something
that you would generally do um but what
you would do is you'd probably use the
abis Management console which is powered
by the API use the Adis SDK which is
powered by the API or using the ad CLI
so we'll cover all those three
[Music]
okay all right so what I want to do is
just point you to where you'd find the
resources to use the API program
automatically uh we're not going to
actually use the API because there's a
lot more to it uh than what I'm going to
show you here but at least you'll be
familiar with how the API works so I'm
on the ads. amazon.com website if you
type in docs the type top there it's
going to bring you to the main
documentation and what we're looking for
if we scroll on down there should be a
general reference area where we have
service endpoints if we click into here
it's going to uh talk about um how a
service endpoint is structured and if we
go down to abis API we can see some
additional information of course to use
um the API you're going to have to sign
API requests first which is not a super
simple process but you have to use an
authorization header um and send along
uh some credentials and things like that
so if you want to know what service
endpoints are available to you if you
search service endpoints list for AWS
this is the big list and so if I was to
go down here and look for ec2 uh might
be a common example here it's going to
tell us what the points are and as you
can see they are Regional based but the
idea here is that I could take something
like this okay I could grab that and
using something like
Postman I could go and create a new
request and it's probably a post I'm not
sure what it's supposed to be it's
probably a post and then you'd set your
authorization header there might even be
one in here for adabs see where it says
adabs signature so you can go here and
put your access key and secret within
here um so that's something nice about
Postman so it's going to do the signing
requests for you so that makes your life
a lot easier and then from there what
you do is you go to your body and you'd
want to enter in Json so to do Json
would probably be raw you drop down the
format Json and then you'd send your
payload whatever it is so I again I
haven't done this in a while because
it's not a very common uh thing that I
have to do like describe ec2 instances
but there probably is like an action and
some additional information that you'd
send along um so you know hopefully that
gives you kind of an idea how the API
works but you know you should never Pro
uh in practice ever have to really work
with the API uh This Way directly
[Music]
okay hey this is Andrew Brown from exam
Pro and we are looking at the itus
Management console so the itus
Management console is a web-based
unified console to build manage and
monitor everything from simple web apps
to complex Cloud deployments so when you
create your adus account and you log in
that is what you're using the adus
Management console and I would not be
surprised uh if you're watching this
video and they've already changed um the
default page here since AOS loves to
change the UI on us all the time uh but
uh the way you would access this is via
console. ab. amazon.com when you click
sign in or go to the console that's the
link that it's going to uh and so the
idea here is that you can point and
click to manually launch and configure
adus Resources with limited programming
knowledge this is known as click Ops
since you can perform all your system
operations via clicks okay
[Music]
let's talk about the adus Management
console in brief here so you know of
course when you're on the homepage you
go to adus Management console and you
will end up logging in and from there we
will uh make our way over to the adus
Management console when I say adus
Management console I'm referring to uh
this homepage but I'm also referring to
anything that I'm doing in this web UI
whether it's a subservice or not so you
know a lot of times people just call
this the dashboard uh or the homepage um
but you know it is technically cre the
adus Management console but everything
is the adus Management console you can
drop down Services here if there's some
that you like you can favorite them on
the left hand side I don't find that
particularly useful you can see the most
recent ones here they'll also Show
recently up here as well we have the
search at the top notice that there's a
hotkey for alt S I don't think I ever
use it but if I was to type in a service
like ec2 it's going to get me the
services and then down below it's the
subfeatures of it so if I just click
into that there into this you this is
the main this is a service console so I
would call this the ec2 console or the
ec2 service console so if you ever hear
me saying go to the ec2 console that's
what I'm saying and you'll notice here
like there is stuff on the left hand
side so I come back here ec2 image
Builder ec2 Global views these are
considered services but if you drop down
it says top features or you go down here
it says dashboard limits Amis you go
over here uh the ec2 dashboard limits
Amis are here and limits are somewhere
here right there so okay so those kind
of map over pretty well plls and
documentation knowledge based articles
Marketplace I don't think I've ever
touched those in my life uh this here is
the cloud shell so if you click it it
will launch a cloud shell we'll cover
that when we get to that section here we
have this little bell it tells us about
open issues I think this is for the
personal health dashboard yeah it says
PhD in the bottom left corner or left
corner so if I open that up it'll bring
up the PHD the personal health dashboard
all right uh our region selector our
so nothing super exciting here but just
kind of giving you a bit of a tour so
that you know there are some things you
can do um can you change the look of
this I don't think right now as of yet
um there is any way I'm sure AOS is
thinking about it because it's been a
high uh request that's in demand but uh
this is what it looks like as of today
[Music]
okay all right so I just want to
describe what a service console is so an
an a service each have their own
customized console and you can access
these consoles by searching the service
name so you would go ahead and type in
ec2 and then what we refer to this
screen as as the ec2 console the reason
I'm telling you this is that when you're
going through a lot of labs or follow
alongs you'll hear the instructor say go
to the ec2 console go to the stagemaker
console go to the RDS console what
they're telling you is to go type the
the name of the service and go to um
that particular Services uh console okay
uh some a service consoles will act as
an umbrella console containing many adus
services so uh you know VPC console ec2
console systems manager console
stagemaker console uh cloudwatch console
these all contain multiple services so
you know for um for ec2 you might say
okay well I need a security group
there's no security group console it's
under the ec2 console okay uh so just be
aware of
[Music]
that so now I want to show you some of
the service consoles to kind of
distinguish how they might vary per per
service okay so if we were to look up
ec2 um and we just did look at this but
the interesting thing is that some uh
consoles the ec2 console uh is the home
for other databus services and you just
have to learn this over time to know
that so for instance elastic Block store
is its own service but it's tightly uh
linked to ec2 instances so that's why
they always have it here same thing with
Amis uh security groups same thing with
that so these are interesting because
these are basically part of virtual
networking and so you think they'd be
under the VPC console but they are
actually under here with ec2 and so load
balancing Auto scaling groups tightly
coupled to um uh to ec2 if we make our
way over to
VPC um you know here it's going to
contain all the new stuff does it have a
new experience no I guess this is the
newest one it looks a bit old and a
little bit new here but you know we have
a lot of different things here like fire
firewalls vpns Transit gateways traffic
mirroring we make our way over to Cloud
watch okay and cloudwatch has uh very uh
focused Services they're all actually
named and this is more like a s feels
more like a single service where you
have these very focused um Services
where you have alarms logs metrics
events insights right but you're going
to notice that like the UI highly varies
so we had looked at Cloud watch and then
we had looked at U VPC and looks like
this and then we looked at ec2 and it
looked like that and so there is
inconsistencies because each um Service
uh Team like that work on per service or
whatever they have full control over
their UI and so some of them are in um
uh different states of updating so some
people might have updated the left-and
column but this part is old or you might
click around like under something else
like ec2 dashboard um or maybe a better
example might be Amis I remember we're
in here something looked old here yeah
see these are the old buttons and that's
just how it is so everything is very uh
modular and so they get updated over
time so that is the challenge that
you're dealing with you're always having
like three different versions that are
cobbled together in each uh um UI one
thing that I found really interesting is
that um VPC has its own console
Management console but if you were to
look up this in the uh the SDK so if I
was to look up um ABS
SDK uh
ec2 okay I'm just looking up r Ruby here
as an example because that's what I know
how to do um if you look under here
let's say you want to pragmatically work
with vpcs you think that it would have
its own top level VPC because it has in
the console its own uh its own
Management console but actually VPC is
tightly coupled ec2 and so when you want
to pragmatically use VPC you're going to
be um using actually ec2 uh as as how it
was built so the the the what I'm trying
to get is the apis don't one to one
match with this kind of stuff and so
it's just kind of interesting that
there's those kind of uh differences uh
but again it's not that big of a deal
I'm just trying to say like you know
keep your mind open when you're looking
at this stuff
[Music]
okay so every adist account has a unique
account ID and the account ID can be
easily found by dropping down the
current user in the global navigation so
what I'm going to do is pull up my pen
tool here and just show you it's right
there uh the ab account ID is composed
of 12 digits and so it could look like
this or this or this theab account ID is
used when logging in uh with a nonroot
user account uh but generally a lot of
people like to set their own Alias
because it's tiring to remember your
account uh ID the uh you use it when
you're creating cross account R so you'd
have the target account d The Source
account ID to gain access to resources
in another a account when you're uh
dealing with the support cases ads will
commonly ask you what your account ID is
so they can identify
the account that they want to look at
and it is generally good to keep your
account ID private as it is one of the
many components used to dentify an
account for an attack by malicious actor
uh so you don't have to be overly
sensitive with it but you know try to
hide it when you can when it's easy
[Music]
okay all right so let's talk about the
account ID which appears up here in the
top right corner uh where you can get
the account ID it also appears in IM am
so if we go over to I am and you look on
the right hand side it should show you
the example here it keeps on trying to
take us to the old dashboard that's fine
um but you'll notice that it's over here
and I don't have MFA turned on because
I'm in my IM user account but it should
be turned on on everything that's given
but uh you know I just want to show you
where it is and also where you might be
using it so one example where you would
use you would need to know your account
ID would be something like creating a
cross account policy so I went here and
went to policy and went create policy
um and we went to maybe it's a roll I
think we actually sorry we want a cross
account roll it's not the policy sorry
we go here and we say I want to access
something in another A's account what we
have to do is specify the account ID
specify the accounts that can use this
role so you give I think the the ID of
the other
account okay and so that is one place
where you use it another place would be
when you're creating policies so if I go
back to policies here I can create a
policy
here and I can just choose something
like S3
okay and I'll just choose a list and
under the request
conditions I might specify I think the
account IDE it should be in
here um I know I can limit based on
account
ID principal account
so you could do principal account so if
I just looked up this here ABS principal
account and you just got to get used to
Googling things because that's always
what's happening here and so we should
be able to specify an account ID yeah
like that so that would be the principle
there so if I just took that and doesn't
matter what it is we just put the value
in
here um string equals this add I should
be able to go over here and now see the
full statement no sometimes that happens
because we don't have it fully filled
out but um yeah so that pretty much
that's pretty much how we use it like it
would normally show up as that so if I
just go ahead and go next the policy
contains an error you are required to
choose a
resource what do you mean the resource
is this right oh down here okay sorry uh
so we'll just say all resources then we
flip over now it's valid and so here we
can see our condition saying only from
this account ID that it is allowed um
other places we're going to see account
IDs are in um ARS right so if we had an
ec2 instance we don't have one launched
right now but if I was to go ahead and
oh maybe we have some prior ones yeah so
if I was to checkbox this
here and you might not have any prior
ones so there might not be nothing for
you to see but if you look for the
AR where's our AR
sometimes it doesn't show the Arn in the
services sometimes it does I wish that
abot always showed the Arn to make our
lives a bit easier but it could be
because of other reasons why but even
though we don't have the R I think it
shows us shows us the owner ID and so
that's the account uh the account ID
number you can tell because it's 12
digits so hopefully that gives you kind
of a tour of the account ID and what its
purpose is in the account okay
[Music]
all right let's take a look at it tools
for Powershell so what is Powershell
Powershell is a task Automation and
configuration management framework is a
command like shell and a scripting
language so here it is over here uh if
you are a Windows user you're used to
seeing this because it has a big blue
window so unlike most shells which
accept and return text Powershell is
built on top of the net common language
runtime CLR and accepts and Returns the
net objects so uh has a thing
called the adus tools for Powershell and
this lets you interact with the adus API
via Powershell commandlets commandlets
is a special type of command in
powerwell in the form of the capitalized
verb and noun so in this case it'd be
new hyphen S3 bucket so you know we
looked at the a CLI and that is
generally for bash um uh you know shells
and so po shell is just another type of
shell that's very popular and I just
wanted to highlight it for those people
that are uh you know used to using
Microsoft workloads or azure workloads
uh that this actually exists
[Music]
okay all right let's take a look at the
pow shell tools um I actually haven't
used this one yet so I'm kind of curious
I am out of Windows machine so if I was
to um open CM or
Powershell and you probably can't see
this but if I just bring this over here
if I type in Powershell on my
computer you'll notice that I have it um
so that's how you would launch it looks
like a blue screen here okay um if
you're on a Mac you're not going to have
that but that's totally fine we don't
need to have a Windows machine to use
Powershell because we can go ahead and
use cloud shell so make sure you're in a
region that supports Cloud shell so I
switch back to North Virginia uh this is
not important for the exam but it's just
kind of fun for me to go through this
with you and if you just like want to
watch uh here and so I want to change
this over to Powershell so I imagine
that it must be over
here um so how do we change to
Powershell so we'll type in ad us po or
it us Cloud
shell power shell like how do we do
it okay and so I'm just going to scroll
down
here so the following shells are
pre-installed uh The Bash the power
shell the Z shell you can identify them
by that yeah of course to switch to New
Shell enter the Shell's program name in
the command line prompt oh wow that's
easy so um if we want
pwsh do we just type pwsh let's find out
give it a moment to think oh there we go
okay so now we're using Powershell and
so I would think that ads would give
this pre-installed for us so if we go
over here to the instructions and we
scroll on down there's probably like oh
wait like I don't use Powershell a lot
it's very easy to install modules um
I've done it before but I never remember
how to do it but let's just see what we
can find here so I want the
documentation for Powershell here and
I'm going to go to the um the maybe the
reference here because I just want to
see some examples for the commandlets
and so we'll look for S3 again never
done this before but I'm always great at
jumping uh into these things and all I
want to do is just list out the buckets
so I'm going to just search for the word
list um and just see if I can find
something very simple
here and calls to get the list buckets
API operation so I think that is what
we're going to be doing here so I'm
going to click into that okay
and then from there what I'm going to do
is just see if I can copy this command
so we will go ahead and copy this and
paste it in here and I like how we got
this little shell here so we can tweak
it so we need the bucket name but I
don't want to return a list of all the
buckets owned by the author so we don't
have a bucket name that we want
explicitly set here so it's required
false so we can remove that okay we'll
look at the next one select required
false use the select command to control
the command L output the default is
bucket specifying select will result in
turning all the whole
buckets for that specifying the
name uh but it says it's not required so
let's just take that out as well I don't
think we need any of these actually
let's just go and put that in there and
I think that there must be something we
need to put in front of that right well
let's just see what
happens uh the term is not recognized as
the name of the command L function
script is operable so I think we're
missing something in front of
here we'll go to the user guide here
quickly and we'll get to the getting
started I just want a super simple
example
here new bucket get bucket well let's
try this one here because they have it
here and so it should just work
right I'm going change this to us East
one
the term new bucket is not recognized as
the name of the commandlet function so
I'm guessing that the commandlets not
installed I would have thought that they
would have installed it by default so I
guess what we'll do is look at how to
install it so installing
on Linux I
suppose
so you can install the modulized version
of the Powershell on computers to
install adus tools on Linux PW H to
start Powershell core session so I guess
that's how you must start it on Linux
and then install the module this way so
yeah I said it's easy to install these
things we'll hit
enter cross your fingers hope this works
hope this is
fast I'm just going to take a look here
peek forward here if you are not uh if
you're notified the repository is UN
trusted you're asked if you want to
trust anyway just hit Y so we're waiting
for that here um you're installing this
module from usted repository it's funny
that it's untrusted by but it's by AWS
maybe that's some kind of drama between
Microsoft not letting AWS have an
official module there but it looks like
it should be installed now so if I type
in get S3 buckets
here um unless I typed it wrong that
still doesn't seem to be working if I go
up here and try to create a new bucket
still does not recom recognize the
command command lit here so there must
be more going on
here um
if you notified you can now install the
module for each
service okay well what did we
do you're installing the the the modules
from untrusted if you trust it change
the uh change it installation policy
value by running set policy command are
you sure you want to install this module
from the PS Gallery so I said yes and I
gave it a capital Y and it didn't do
anything
else so
oh hold on here so this is the installer
and then here is the actual tool that we
want to install so it install to oh so
we just installed this thing and now we
use this thing to install S3
okay great not hard
okay and so we'll just say yes to
all and so that's going to install I
guess everything oh we said ec2 and S3
well we didn't need both but that's fine
and so what I'm going to do is go get
bucket and so now recognizes it lists
out the items here we can go and create
ourselves a new bucket so we'll do that
okay we'll make our way back over
databas Management console we'll go to
S3 just because I don't need all these
buckets lying around here and I'm going
to go ahead and delete some of these
buckets here so we'll say
delete my bucket great and we'll go to
this one here and say delete my bucket
excellent all right so we have an idea
how to use Powershell and so Powershell
is just really popular because of it's
the way you do inputs is very
standardized and the outputs that come
so it's very popular um and a very
powerful scripting tool that's or CLI
tool as well so uh you know hopefully
that's that was interesting for you but
what we'll do is just close these off
here and go back to our our homepage
always just clicking that logo there and
there we
[Music]
go so Amazon resource names uniquely
identif AWS resources and ARS are
required to specify resource and
Ambiguously across all of all of AWS so
the AR has the following format
variations so there's a few different
things here but just notice here that
sometimes it has a resource ID or it has
a path so with a resource type or could
be separated by a colon so the partition
um could either be ads China or gov
Cloud because this is basically the adus
uh portal or URL that are completely
separated from each other uh as we
talked about those earlier in the course
uh then there's the service identifier
so ec2 S3 IM am pretty much every
service has their own uh service that uh
name here that would be identified then
the region would be pretty obvious Us
East one CA Central 1 you'd have a count
ID which would be 12 digits uh the
resource ID uh could be a name or a pass
so like for um IM users we have user Bob
this is an ec2 instance and most of the
irns are accessible via the AAS
Management console and you can usually
click the r to copy to your clipboard so
here is it is for an S3 bucket and
notice that it's a little bit different
because it is a global Service AWS
there's no reason to specify the region
or the account ID or uh anything else
there like the resource type so straight
away we already know it's a bucket so we
can just say my bucket so that one's
really short but in other cases it's
really long so here it is for a load
balcer and it has all the information
there and notice that like this has a
pass this is low bouncer app my server
will be and then it has the ID okay for
paths and ARS they can also include a
wild card Aster and we'll see these like
with IM policies or or paths these are
really useful when you are doing um uh
policies where you have to specify AR
you want to say a group of things and
things like that so there you
[Music]
go all right so now let's take a look at
Amazon resource name or also known as
Arn um and so ARS are used to reference
objects they're very commonly used when
you're using the CLI or the SDK to
reference to something um the easiest
example is S3 right so we go over to S3
here and we create ourselves a new
bucket um so I'll go ahead and create
ourselves a new one here we'll say my
new bucket I'm just going to put a bunch
of numbers in here it doesn't matter
we'll hit create bucket and what we will
see if we click into this is the orange
should be under
properties and there it is okay so so
there are many cases where you might
want to use the RN and a lot of times
you'll just copy it and uh a very common
example would be again with I am
policies so we go over to I am policies
right and I want to get to policies here
to save myself some trouble and we
create a
policy you know I might want to restrict
someone to use only that bucket so I say
S3 okay and then I'm going to say um I
want to be able to read and write from a
particular bucket we go drop down the
resources here and so here we have a lot
of
options um maybe I'll just get rid of
the read
option and I'm going to actually expand
right because it's just creating too
much work for me here and I just want to
have um put put object that's that's the
what we use to put something into a
bucket so we expand the resource here
and notice it says add the irn so we go
here and we could type the bucket name
so do that or we just paste it on in
here at the top so it's probably easier
just to grab it
sometimes but if you don't know an AR a
lot of times you can just expand this
and then fill it in and that's how you
get an AR so put that there let's list
oh you could also do it that way which
is easier too and so now if I go to Json
is it valid there we go so here it's
saying um this policy allows somebody to
put an object into this particular
bucket and so that would be an example
where we would use um an R okay or if
you're doing uh if you're using uh a the
support you might have to use an R to um
to get help from support saying hey look
at this particular resource exactly here
and then the the cloud support engineer
can help you
[Music]
okay hey this is Andrew Brown from exam
Pro and we are looking at the adus
command line interface but before we do
that we got to Define some terms so what
is a CLI so a command line interface
processes commands to computer program
in the form of lines of text operating
system Implement a command line
interface in a sh L okay so then we have
a terminal so a terminal is a text only
interface so it has input output
environment then you have a console this
is the physical computer to physically
input information into a terminal then
you have the shell a shell is the
command line program that users interact
uh with to input commands popular shell
programs are bash uh zsh Powershell and
uh you might remember this one MS DOS
prompt so this has been around for
obviously a very long time so maybe this
kind of primes your mind for what is a
shell and just so you know people
commonly erroneously use terminal shell
or console generally describe
interacting with a shell so if we say
shell or console or terminal we're just
talking about the same thing but there
is technically a difference between
these three things but most people do
not care and I wouldn't worry about it
too much okay so now let's take a look
at the itus command line interface which
allows you to programmatically interact
with the itus API via entering single or
multi-line commands into a shell and
then here I say or terminal but really
it's just the shell Okay so so uh here
is an example of one so we're trying to
describe uh ec2 instances and then we're
getting the output because we asked to
have it back in this table like view so
the a CLI is a python executable program
so python is required to install the a
CLI the a CLI can be installed on
Windows Mac Linux Unix the name of the
ca program is AWS you'll notice that up
here in the top left corner there's a
lot more to this but this is all we need
for now okay
[Music]
hey this is Andrew Brown from exam Pro
and we are taking a look at the Aus CLI
and the easiest way to get started with
this is actually via the cloud shell so
you'll notice this little icon here in
the top right corner that is cloud shell
and it's going to allow us to um uh
pratically do things without having to
set up our own environments so if I just
click that there okay uh and I say do
not show again close and by the way if
you don't see Cloud shell here it could
be your region so like if I go to Canada
Central it doesn't have it there and so
if I was to search cloudshell
here okay it's going to say it's only
supported in those regions so that's a
bit annoying but once Cloud shell loads
it already has our uh credentials loaded
within our account and so this is going
to save us a lot of time in terms of uh
you know trying to get set up with the
exception that you have to wait for this
environment to create so it takes a
little bit of time but it's not that bad
um and while that is waiting what I'll
do is show you actually how you would
install the CLI yourself so if we typed
in itus CLI
install all right and uh we went here
the way you would install I believe it's
a python library but if we went to
version two and we just said Linux uh
you go down here they have instructions
so you just curl it unzip it and do
that um so you know if it's this and
then once it's installed you'll have the
a CLI commands this is still going so
you know maybe I can show you what it
would be like to install the CLI by hand
so if we want to do that one easy way to
do this is if we just go to GitHub
doesn't matter what repository I'm just
looking for anything here and if I open
up git pod so if we go on the top here
and type in git pod
uh.com maybe
that I just want to see
whoops maybe it's get
pods
that oh get pod you're not giving me oh
you know what it's doio that's why okay
so if we go back here sorry and we type
in
doio what this will do is launch me a
temporary environment and so this is
outside of AWS so I'd actually have to
install the CLI so this would be a great
opportunity to show you how to install
the CLI I'm just doing it this way
because git pod is free to use and um
you know it's going to set up an
environment and how let us simulate
installing the CLI so here is the CLI
here I'm going to see if I can bump up
the font um let's make the font as large
as we can go light or dark dark sounds
good to me
and so if we type in
AWS give it a moment we can see that we
have uh the command here so if I say ABS
S3 LS whoops it should be able to list
things out in a bucket so this is what's
currently in the bucket if you're
wondering how do I know what these
commands are I can just type in a CLI
commands Okay and if we go here um and
we go to the CLI ref reference then we
have um anything we want here right so
we go down here and I just want to see
what's running in S3 and I go here and I
scroll on down it's going to show me
commands like copy move remove sync uh
MBR RB uh list
right and if you're looking for a
particular command you go down and say
okay I'll look at LS here and it will
explain to me all the little options
that we can do with it and then it will
always give me examples right so I can
see examples like that so if I wanted to
move something into an S3 bucket so
let's say I want to create a new S3
bucket um we'll type in AWS S3 and just
hit enter and it should tell
us um the sub commands maybe if I do
like help like
this and if we scroll on down so I guess
it just pulls up documentation let's
hoping it would give us like a tiny
summary okay so what we can do here
because I want to create a
bucket type in like
buckets if you don't know something you
just go ads S3 CLI create bucket we'll
go
here um and then what I do is I always
just go to examples here so here have
ads S3 API create bucket and I know it's
unusual there's an S3 and there's an S3
API I don't know why that is but it's
always been that way and I I just don't
question it anymore and so here I can go
ahead and create a new bucket so I'll
just go ahead and paste that command in
I do want to change it up a bit here
because this name could be that has to
be unique so just to make sure I get
what I want I'm putting random numbers
in here we're going to choose the region
as us East one if I wanted to do other
things here I could scroll up and look
at some Flags here
so uh it looks all fine to me so I think
I'll go back here and just
hit uh
paste okay and so it created that bucket
for me if I go over to
S3 and we'll wait here a moment we can
see that bucket now exists if I wanted
to place something in that bucket what I
can do is just like touch a file so I'll
just say um touch touch is a Linux
command to make just an empty file so
we'll say um
hello.txt and then it'll be a S3
um it would be SP to copy it and I'm
going to give it the local path
hello.txt and then I need to give it the
bucket address so it' be S3 colon SL SL
the bucket name so we named it this I'm
not going to try to type that in by hand
because it's too hard and then I want to
say where I want to put this file so I'm
going to say hello.txt and if I'm right
that should work as expected and so it
says I uploaded that file I make my way
back over to S3 I refresh there is the
file if I want to copy this file back
locally um all I have to do I'm just
going to remove I'm going to delete the
original hello txt file LS to show you
that there's nothing there and what I
need to do oops is just revert this so
instead of saying the address
here we can go and type in
hello.txt and if I do LS there's the
file if you don't know what the address
is of the bucket um a lot of times you
can go here and find it so it should be
because they're always changing this UI
on me but we'll go to properties here
and there that's the
Arn uh usually a good way to find it is
if you go into an actual object so if
you go here it'll give you the full URL
so I could have grabbed that and I could
have just pasted that in there um but
you know you learn after time it's not
hard to remember this S3 Co SL the
unique name I do want to show you how to
install it by hand so here I'm in git
pods um I'm not sure how I can change
this to a dark theme because I really
don't like this on my eyes we'll go down
below here to color
theme and we'll say get PA dark there we
go and so this is a temporary workspace
so when I close it it'll be gone so
that'll be totally fine and so I'm going
to typee in AWS to see that it's not
installed we're going to go over here
this runs Linux by default so I already
know that I'm going to use Linux we want
to use version two here um
so for the latest version use this
command for a specific version no we
just want the generic one so I'm going
to go ahead and copy this whoops yes
allow we'll paste that in we'll hit
enter okay then we'll take the next
command paste that in hit enter
we'll go take the next command
here we'll hit
enter you can now run uh AWS so we type
AWS and there's the command so uh the
only thing is that if we do AWS S3 LS
it's not going to work because we don't
have any credentials set so we'll give
it a moment to think so it says unable
to locate credentials you can configure
credentials by running adus configure so
we type in ad
configure and by the way if this font is
too small I believe I can bump it up
like
this not a great way to do it but um it
works and so it says ads access key ID
so what we can do is go over to
IM and what I'm looking for is my
particular user over
here and if you remember when we first
created our account it generated out
access key so I go to security
credentials and so we have a key here
but I need the secret so this key is
useless to me so I'm going to go ahead
and deactivate
it just cuz I don't even want this key
and I'm going to create myself a new key
so I'm going to have an access ID in
secret whenever you generate these out
never ever ever ever ever show anyone
what these are these are your yours and
yours alone okay so this is cloud shell
we're fine we're just going to close
that for now and I'm going to go back
over to get pods here and hit enter so
that's the ID I'm going to go grab the
secret
hit enter paste and I want it to go to
us East one to save myself some trouble
uh you can change the output from Json
to tables I'm going to leave it as the
default here and so now if I type ABS S3
LS I get a list and so if I want to grab
that file there I'm going to grab that
S3 U and we type in a S3 API or sorry
it's just LS sorry or sorry CP and we're
going to paste that link in and we're
going to say hello.txt
and I must have done the command wrong
it's because we're missing S3 here I
just hit up on the keyboard to get that
command back and so I type in LS for
list and I mean I have some other code
here so you know again any repo you want
on GitHub doesn't really matter uh but
you'll see there is that file probably
shouldn't have used this one because it
makes a bit of a
mess um but yeah it's pretty
straightforward just to one thing to
show you is where those credentials are
stored so by default they're going to be
stored in um it's going to be in the
hidden directory in your root or your
home directory called ad. credentials so
if I just do like LS here you can see
there's a config file and a credentials
file cat lets me print out the contents
of that file so I go here and it's
saying the default region is us east1
this is a tomel file even though it
doesn't have a toml on the end of it I
just know by looking at it that's what
it is config lets you set uh defaults
that are going to apply to all of your
credentials and then uh within the
credential file here is the actual
credentials so if you wanted to just set
them you could go in here and just set
them in here you can also set multiple
credentials so if I go here and I'm
going to open up and buy because I'm not
sure how to open it up here in the main
one but if you wanted multiple accounts
you do like exam Pro and then you just
repeat these with different keys right
and then when you wanted to use an a CLI
command actually I'm going to go back
here for a
second okay and if you want to um
and by the way I'm using VI you never
use Vim it's it's a bit tricky to use uh
you might want to use Nano instead if
you're if you're kind of new to this uh
because this will use like regular key
key cuts and then down below it shows
you what it is so this is like control X
or alt X alt X NOP contr X there we go
um but anyway so if I go into this file
and I delete the original one right and
now I try to
do um this command here even though we
already have that file it should either
hang or complain I Could Just Kill that
by doing control C if I do ads S3
LS notice that it's hanging so unable to
locate credentials because there's no
default one but if I go and I put
profile and I say exam
Pro right it it'll now use that profile
so that's the way we do it um but
hopefully that gives you kind of a a
crash course into the CLI um so yeah
there you go okay so I'm just going to
go ahead and close these off you can
delete this bucket if you don't want it
it's probably a good idea to delete this
here and I'm just going to say
permanently delete
okay very very good okay close that off
and yeah that's the introduction to the
CLI so yeah there you
[Music]
go hey this is Andrew Brown from exam
Pro and we are taking a look at
development kits uh so a software
development kit or SDK is a collection
of software development tools and one
installable package so you can use the
AWS SDK to programmatically create
modify delete or interact with adabs
resources so the adabs SDK is offered in
a variety of programming languages so we
have Java python nodejs Ruby go.net PHP
JavaScript C++ and so here would be an
example of uh some rub code where we are
creating ourselves um an S3 bucket so
we're just uploading a file there
[Music]
okay okay so now what I'm going to do is
show you how to use the adabs SDK and so
uh to do that uh we're going to need
some kind of IDE um a a basically code
editor and so we had looked at G pods
which is a thirdparty service and that's
fine but let's take a look at Cloud9
because that is built into AWS so if I
just type in Cloud9 here and go over to
IDE I'm going to launch myself a new
environment so I'll hit create I'm going
just say my SDK
environment EnV if you if you have hard
time typing environment like me and we
have some options so create an new2
instance for direct access create it via
assistance manager run a remote with SSH
I'm going to leave it as the default
then we have the option to choose what
size I want to leave it on T2 micro
because that is the free tier then we're
going to scroll on down we have Amazon
Linux 2 Linux Ami I'm going to stick
with uh Amazon link 2 and we can have it
turn off after 30 minutes a great option
for us here and we'll go ahead and hit
next and we'll hit create
environment and so we're going to have
to wait a little bit for this to launch
it'll take a few minutes as that is
going let's go to Google type in itus
SDK um to get to the main page and so
the idea here is that there are a bunch
of different languages you can use C++
go Java javascript. net node.js PHP
Python and Ruby uh and so I'm a really
big fan of Ruby I've been using Ruby
since 2005 and so that's what we're
going to do it in it's also really easy
to use and it's a really great language
so um you know down below it's just
showing you that there's all these
different things but if we go down to
the SDK here and we click on
Ruby we we have examples where we have
the developer guide the API reference
and so this tells you how to get started
even here it's saying like hey go get
started with Cloud9 which is great as
well I suppose um and so here it might
show you how to install it um and when
we open up the API references this is
what it looks like so a lot of times
when I want to do something I know it's
like I want to do something with S3 so I
scroll on down here and I look for
S3
right and then I just kind of like uh
scroll around and look you know what I
mean sometimes I have to expand it go
into the client every API is a slightly
different so you do have to kind of
figure out how to navigate that I'm
actually under S3 right now so I'm
looking for for the client and I just
know this for memory that this is where
it is so first you create yourself a
client and then you can do API
operations so if I wanted to like list
buckets I just search the word list and
I just scroll on down and there it is I
click into that and I have an example of
how to list a bucket so I'm going to go
back to Cloud9 and it is ready and it
started in dark mode um if yours is not
in dark mode which really honestly why
wouldn't you want dark mode um if we go
up to I think it's like file where is it
uh preferences here here got to click
the Cloud9 option and I'm just seeing if
it like remembers my settings I really
like two two soft tabs here but uh there
should be something for themes down
below and
so
um that doesn't seem like that's it used
to be like a oh here it is if you go
here and just choose like whatever you
want I'm on jet dark here and so if it's
on classic light or something you don't
like you can fix that there um but I'm
just going to go here and just fiddle
with my
settings because I really like to use
Vim uh keys I don't recommend this if
you are uh to change this if you are not
a programmer but I'm just going to
change it so that I can type here
efficiently so I'm just looking for the
option
here and they moved it on me where did
they move
it it probably be like key
bindings ah Bim mode there we go again
don't do that this is just for me so I
can uh move around in a different way so
what I want to do and by the way looks
like this default screen we could have
just changed it here I just clicked
through all that for nothing it was here
the entire time but um what we need is
we need to make sure that we have our
credentials so if we type in ads um S3
LS that's like my sanity check that I
always like to do to make sure I have
credentials notice that we didn't have
to set up any credentials it was already
on this machine which was really nice
and so I'm going to create a new file
here and it's okay if you don't know
anything about Ruby we're just going to
have fun here and just follow along so
I'm going to do example RB I'm going to
make sure Ruby's installed by doing Ruby
hyphen V so it is installed which is
great uh you need a gem file so say new
gem file
here and if we go back to the
installation guide uh we need the gem
STK
here actually I'm going to look at how
to generate a gem file gem file because
there's some stuff that goes to the top
of those
files like this
here I think we just need this line here
so I'm just going to grab that whoops
paste that in allow
good and uh I you can do gem ads SDK
that will install everything but uh we
only want to work
with S3 and so this is going to vary
based on each language but I know that
if we type in S3 we'll just get S3 and
that's all we really need and so once we
have that what we'll need to do is use a
bundle install so we're going to make
sure we're in the correct directory I'm
going to type in LS down below notice
the gem file is there uh and by the way
the fonts are too small I should
probably bump those up let's see how we
can do
that uh editor size font user
settings good luck trying to find
today um project
no you think it'd have to be under user
settings
right ah here it is okay so um this is
for
probably the editor so we'll go to 18
here Co code editor
here I'm I'm trying to find the one for
the terminal probably over
here there we
go much easier okay so notice we have
example. RB and Gem file so we're in the
correct directory make sure I save that
I'm going to type in bundle
install that's going to install the gems
give it a moment there it's going to
fetch notice that installed um the adabs
sdks S3 and everything that it was
dependent on and so now if we go over to
our example. RB file really when you're
coding for the cloud you can pretty much
copy paste everything so over here we
found this code here for S3 list buckets
um and so I'm going to go ahead and
paste that on in okay and I know it
looks really complicated but we can
quickly simplify this so I know that
this is just the output so I don't need
that okay and in Ruby you don't need
parentheses or curries if uh if you
don't have anything things there and so
all I need to do is Define a client so
if I click uh I go to the top here of
this file I think we're in the client
right
now all the way to the top all the way
to the top
here that's what we need
okay and so I'm going to paste that in
now uh we can set the region here so I'm
going to say Us East
one right and then you'd have your
credentials um because the credentials
are on the machine in the um credentials
file they're going to autoload here I
believe so I don't think I need to set
them so I'm just going to take that out
here for a
second okay and I can do this if I want
this is just slightly different syntax
it might be easier to read if I do it
this way for
you okay and I don't need double client
there so we have the client I like to
name this like S3 so I know what it is
and I put puts for the response I'm
going to do
inspect and so puts is like print okay
and so now if I type in bundle exact
let's just make sure that it's in the
context of our bundler file Ruby
example. RB um we have a syntax error on
this line here unexpected thing
here oh it's because of this it's
because I commented it out so I'm just
going to do curly parenthesis comment
out
here
okay actually to make it a bit easier
I'm just going to bring this down like
this okay and we'll paste that there
okay and we'll try this
again un initialized constants ad to us
oh yeah we have to require it so we have
to require Abus SDK S3 I think we'll hit
up and uh we got a struck back so it is
working we are getting an object back if
we want to play around with this a bit
more I'm just going to install another
gem called pry pry allows us to um
inspect code so we're going to do bundle
install and I'm going to go back to Ruby
here I'm going to put a binding pry in
here and then if I hit up and I do
bundle exact Ruby example.
RB um I installed it right install
yes undefine method
pry oh because I have to require it
again bad habit
here okay we'll hit up and so now I have
an Interactive shall and I can kind of
analyze that object so we have a
response so if I type in RSP here I have
the structure object I can type in
buckets here okay and it's showing me a
bucket I can give it get its
name
um oh I think it's an array so I think
I'd say like I'd say like zero here or I
could say first this is just the how the
Ruby language works we say name I get
the name creation date okay so you get
the idea whatever you want to do you
know you search for it you just say I
want to delete a bucket I want to create
a bucket right and you look for it so I
say create bucket here I click on this
and I can see the options and they are
always really good about giving me an
example and then down below they always
tell you all the parameters that you
have there so that's how the SDK Works
uh but yeah the credentials were uh soft
loaded here but you could easily provide
them yourself I should just show you
that before anything else just because
there's some variations there
um and I'm just trying to look for it
because it is separate
code so you could do this this is one
way of doing it so you could do it
separate from the code so if you only
wanted to configure it
once right because you could you could
have a lot of clients you wouldn't want
to keep on like for each client you
wouldn't want to put region in every
time so I could take this and put this
right here
okay and this is the file here where we
have the credentials so this would be
our um our access key and our ID and so
you never want to put your code directly
just in here so if I open up if we go
cat you would never want to do this but
I'm just going to show as an example
here uh
credentials oops I got to get out of
this
exit ads
credentials oh do they not even show it
on this machine which would be smart we
wouldn't really want to see our
credentials here uh hit up say
LS oh no it's there okay
cat
whoops
credentials there it is okay so you know
if we look here we can see that there
are credentials set it's a little bit
different we have this like session
token I guess it's to make sure that
this expires over time but if I was to
take these okay and I was just to paste
them in
here that's one way you would do it um
you never ever want to do this ever ever
ever ever you never want to do this
because you'll end up committing that to
your code um so this is really dirty to
do so I don't ever recommend to do it um
if you wanted to have this apply to
everything you could put it up here and
so now when we call the client we don't
have to do it um of course if the
they're loaded on the machine you don't
have to do it the other thing is like if
you if you want you could load them in
Via environment variables that's usually
what you want to do so you say ads uh
access
key right and then we say environment
databus access
secret and so you'd set those by doing I
think it's like an
export um environment
variables set in Linux you think I know
after like 15 years of doing this but I
never remember so you type in export so
you go down into oops here you type in
export and you just say something like
I'm going to just show an example to see
if it works so I'm going to say hello
world okay and if I do uh hello like
that Echo see it prints it out so that's
how you would set it you'd set those
there but there's actually very specific
ones that adabs uses for um the API and
it's these ones here so you always want
to use
those okay so you put that in
there and that in there but of course
you know like if they're already said in
your machine you don't have to even
specify those cuz it would Auto load
those environment variables I don't
think they're set right now if we type
in Echo just take a look here are they
going to get autoloaded
here no so but anyway so we could go
here just as an
example and well actually they just show
them right here so you see your access
key but we go and we type in um
export and I going paste the key in
there and I'm going to go to the front
of it we're type adab us access key ID
equals enter and so now if I did echo on
this ads access key ID okay it shows up
but I just want to show you how it can
kind of vary and those conditions around
it so yeah that is the adus SDK um and
yeah a lot of times you're just copying
pasting code and just kind of tweaking
it you're not really writing uh real
programming okay so hopefully that is
less intimidating so I'm just going to
close these off and I want to close down
this Cloud9
environment
um I might have to reopen this up in
another tab
and go to the Management console here
and then go over to Cloud9 and just
close this tab and then while go ahead
as and delete this environment oops I'll
just type delete here even if you didn't
it would turn off after 30 minutes and
you have that free tier so it's not that
big of a deal it's up to you whether you
want to use Cloud9 or G pods Cloud9 is
really good because it allows you to um
uh it allows you to uh use it runs on a
virtual machine right so you have a a
container run time there and so it's
very easy to run containers on it um
whereas in like I've had some issues
with G pods but um yeah those are the
two
[Music]
okay let's take a look at itus Cloud
shell which is a browser based Shell
built into the adus Management console
and so Cloud shell is scoped per region
it has the same credentials as the loged
in user and it's a free service so this
is what it looks like and the great
thing about this is that you know if you
have a hard time setting up uh your own
shell or terminal on your computer um or
maybe you just don't have access or
privilege to do so it's just great that
databus makes this uh available to you
and so what you can do is click the
shell icon up at the top and that will
expand this here some things to note
about Cloud shell is that it has some
pre-installed tools so it has the CLI
python nodejs git make pip pseudo tar
t-mo Vim WG vim and more it includes 1
gab of storage free per adus region it
will save your files in a home directory
available for future sessions for the
same adus region uh and it can support
more than a single shell environment so
it has bash Powershell and
zsh um and so Adis Cloud shell is
available in select regions so when I
was in my Canada region I was like
where's the little shell icon but I
realized it's limited for some areas
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at
infrastructure as code also known as IAC
and this allows you to write a
configuration script to automate
creating updating or destroying your
Cloud infrastructure the way you can
think of IC it's a blueprint of your
infrastructure and it allows you to
easily share version or inventory your
Cloud
infrastructure so adus has two different
offerings for IAC the first is cloud
formation uh a a commonly abbreviated to
CFN and this is a declarative I tool and
then you have Aus cloud development kit
commonly known as cdk which is an
imperative IAC tool so let's just talk
about the difference between declarative
and imperative and then we'll look at
these tools a little bit closer uh each
okay so declarative means what you see
is what you get it's explicit it's more
of aose but uh there is zero chance of
misconfiguration unless the file is so
big that you're missing something uh
commonly declarative files are written
in things like Json yaml XML so for
cloud formation it's just Json and yaml
uh and so that's that side there so for
imperative you say what you want and the
rest is filled in so it's implicit uh
it's less for both you could end up with
some misconfiguration that's totally
possible uh but it does more than
declarative and you get to use your
favorite programming language maybe
python JavaScript actually cdk does not
support Ruby right now but I just have
that in there just as a general
description of what imperative is
[Music]
okay all right so just a quick look at
cloud formation so cloud formation
allows you to write infrastructures code
as either Json or yaml the reason why it
was adus started with Json and then
everybody got sick of writing Json and
so they introduced jaml which is a lot
more concise which you see on the right
hand side so cloud formation is simple
but it can lead to large files or is
limited in some regards to creating
Dynamic repeal infrastructure compared
to cdk Cloud information can be easier
for devops engineers who do not have a
background in web programming languages
a lot of times they just know scripting
and this basically is scripting since
cdk generates outloud information it's
still important to be able to read and
understand Cloud information in order to
debug IAC Stacks knowing cloud formation
is kind of a cloud essential when you go
into the other tiers of AWS um like
Solutions architect associate
professional or any of the associates
you need to know Cloud information
inside and out
[Music]
okay okay so what I want to do now is
introduce you to infrastructure as code
and so we're going to take a look at
cloud formation and so we were just
using Cloud d for the STK so we're going
to go back and create ourselves a new
Cloud9 environment because we do have to
write uh some code so I'll go ahead and
hit create here and I'm going to just
say uh CFN that's sort for cloud
formation example and we'll hit next
step and we'll create ourselves a new
environment T2 micro uh Amazon x 2 is
totally fine we'll hit next it'll delete
after 30 minutes we'll be fine we're
within the free tier we're going to give
this a moment to load up um and remember
you can set your theme your your
keyboard mode whatever you want as that
loads and as that's going we're going to
look up cloud formation and so cloud
formation is very intimidating at first
but once you get through the motions of
it it's not too bad um so we'll go to
the user guide here as we always do if
you go to the getting
started it's going to just tell us some
things it's going to read about yaml
files um I don't think I really need to
read much about this here so I think
we'll just go start looking up some
codes so something that might be
interesting to launch is an ec2 instance
cloudformation so that's what I'll do is
I'll type in what I want so in E2
instance and I'll just start pasting in
code so we scroll on down below
here going to go to examples because I
want a small example here this is
something that I might want to do and
we're going to give that a moment here
it's almost
done you can do it ad bu come on as that
is going I'm going to open a new tab I'm
going to make my way over to cloud
formation okay
and um you can see I have some older
Stacks here notice Cloud9 when we create
an environment actually creates a cloud
formation stack which is kind of
interesting um but if we go here we can
create a stack and we can create a file
and upload it here
so okay this is good I'm going to go
ahead and make a new
file we're going to call it
template. yaml um just so you know yaml
can be y ml or Y ml there's a big debate
as to which one you use um I think that
adabs likes it when you use the full
version so I just stick with
yaml I'm going to double click into that
and so in the cc2 example I'm just going
to copy this okay and I'm going to paste
this in
here and I'm going to type in
resources oops
capital okay so that's a resource I want
to create
um when you create Cloud information you
always have a template version so I just
need a basic example here at the
top I guess that's a simple one is like
a Hello World
Bucket maybe we should do a bucket
because it'll be a lot
easier we don't have to make our Liv
super hard here okay um but what I'm
looking for is the version because
that's the first thing that you
specify I'm just trying to find it
within an example
here oh for freak's sakes cloud
formation
version if I don't have the format
version it's going to complain there it
is okay so we'll copy that we'll go back
over
here we'll paste that in there it might
be fun to do like an output here so I'm
going to do like an output
outputs and uh maybe instead of doing
this we'll type in ads S3
C
formation because what I'm looking for
is what we can set as output so we'll
say return values
here
um maybe we just
want Returns the domain
name so we'll just say
um
value ref that that's going to get the
reference for it and we have to say
hello
bucket uh type
string say outputs cloud formation
example and even though I've written
tons of cloud information it's just like
if you're not doing it on day in day out
you start to forget what it is so here
for outputs we need a logical ID
description value and Export
so um that is what I want so I'm going
to go ahead and copy that back here
this is just so that when we run it
we're going to be able to observe an
output from the cloud information file
okay so the logical ID is whatever we
want so hello bucket
domain it's funny because this is how
you do do um kind of that would be the
format for terraform I was getting them
mixed up so the
domain of the bucket the value here is
going to be
ref
hello bucket
domain
name that's the
output export value to
export uh can I get an example
here B
name oh you know what export is for uh
cross Stacks we don't need to do that
okay so that's fine so what we'll do is
set that and we'll take out our old one
and so this should create us an S3
bucket so with Cloud information you can
uh provide a template here by providing
a URL or you can upload a file directly
so um I'm just trying to decide here how
I want to do this you can also use a
sample file or create a template in the
designer I'm going to go over to the
designer because then we can just like
paste in what we want so if I go over to
yamel here and we go back over here I
copy
this I'm just going to paste this in
here
and we're going to hit the refresh
button nobody ever uses the designer but
this is just kind of a easy example for
me to uh place this in
here it's not really working maybe I got
to go to template dude here
refresh there we go so there's our
bucket it's nice to have a little
visualization and I believe this is
going to work as expected so now that we
have have our designer template I think
if we hit close what's this button say
validate template probably a good idea
validating the template template
contains errors unresolved resource
dependency in the output block of the
template
hello domain
it seems like it should be
fine let's go
whoops let's go back over
here that's what I did said reference
that
value oh uh maybe it's get a trib
okay it's get ATT
sorry get a trib Cloud information can't
remember if there's an r on the end of
it oh it's just at this is if you're
trying to get a return intrinsic value
so a reference is like what the default
one is but when every time we do like a
logical name and attribute that's how we
get that there so uh what I'm going to
do here is just hit refresh and I'm
going to validate that one more time now
it's valid if I hover over this is going
to upload it create the
stack we could save this save
it oh we can save it in an S3 bucket so
we'll say
hello bucket and so now we have this URL
so I'm going to copy it honestly I never
use this editor so it's kind of
interesting I'm going to
leave and we probably could hit create
stack but I just find it a bit easier if
we just kind of do it through uh this
here so go back create the stack we're
going to paste in the URL we're going to
say
next and we're going to say uh my new
stack and I didn't see what the name of
the bucket was oh there's no name so
it's going to randomize that's perfect
so we'll go next we have a bunch of
options here we'll hit hit
next we'll give it a moment here I guess
we have to review it create the stack
and this is the part where we watch so
it says create in progress and we wait
and we hit refresh
and we can see what's happening trying
to create a
bucket and if we go to resources this is
this is a lot easier to track because
you can see all the resources that are
being
created if you notice that when you use
the C uh when you're using databas
management call create s bucket it's
instantaneous but like with Cloud
information there's a bit of delay
because there's some communication going
on board but here it is and notice if we
go to our outputs this is the the value
of the bucket domain name if we were to
make it with self-hosting which is not
what we're doing with it we could also
have an export name which would be used
for cross referencing Stacks which is
not something we uh care to do um but
yeah that's how you create a stack that
way um but you know we can also do it
via the SDK here so what I can do um is
look up what is the Adis uh CLI cloud
formation because they have their own
commands here if I go here
there's a new one and there's an old one
so if we go create
stack yeah there's things like this like
create stack
update um so if we wanted to do it this
way okay and I copied this here I'm just
going to put this in my read me here for
a
second uh so here what you do is you say
my new stack
and you could provide the template URL
or you could specify the local path here
so we have like a template body so I'm
going to go ahead and grab
that okay this would be like
yaml and um I need to specify this file
here so template.
yaml and I'm just going to go PWD here
to get the full
path
okay and I'm going just paste that in
there whoops
okay I'm going to do LS okay so that
gives us the full path to the file you
can also specify the template URL um and
so this should work as well if I take
this and paste that on as a
command unable to locate parameter file
oh there's three three triple slashes
there we'll just fix that
there
paste unable to load param file no such
file directory and there's a t
missing okay be like don't be like me
and make sure you don't have spelly
mistakes okay I can type clear down here
so I can see what I'm doing we'll hit
enter
whoops unable to load the parameter file
no such file or
directory home well I you didn't want
the for slash so another thing we can
try to do I think it will take it
relative so if I do this it should work
I don't ever remember having to specify
the entire path and err occurred while
calling the crate stack my stack name
already exists if I go back over here
give this a refresh oh that's what we
named our stack the the one that we did
so I'm going to say stack two
okay template format unsupported
structure when calling the create stack
operation are you kidding me I do this
all the
time template body y file cloud
formation unsupported structure take a
look
here oh you know what I think uh this
one's out of date that's why so what we
can do is go to our old stack here and
we can actually see the template I can
go ahead and copy this whoops and we can
go ahead and paste that in there and
then now what I can do
so you know that's that's the reason why
it wasn't working okay so we'll hit
enter um unsupported
structure it should be
supported let's see if Cloud information
can help us
out um apparently there was very
unhelpful error message for batting so
try the validate template option I
wonder if we could just do
this maybe if that would help
here I'm just hitting up to try to run
it
again nope I guess we can try to
validate it here it's like I'm not
having much luck here
today so we'll just say this here maybe
it's not even loading that file where it
is so there's no
errors just going to make this one line
okay created so for whatever reason I
must have had a a bug there and so
putting sometimes putting on one line
helps that out because I must have had
an obvious mistake there and now we can
see the stack is cating it's doing the
exact same thing it's creating uh a
different bucket though if we go over to
our S3
here again you know you don't need to be
able to do this yourself to pass the
exam it's just so I'm just trying to
show you like what it is so you kind of
absorb any kind of knowledge about
what's going on here notes down below it
uses the stack name followed by uh the
re The Logical name of the resource
there okay um and what we'll do is wait
for that to create once that's created
we can go ahead and delete these Stacks
we could also use the ad us Cloud
information to say like delete stack but
I don't want to uh bore you with that
today and so we'll hit refresh here wait
for those Stacks to
vanish okay those are gone uh what I'm
going to do is kill this Cloud9
environment
uh if there's a way to do it from here I
have never known how to do it go back to
your dashboard well that's nice to know
we'll go ahead and just delete
this okay we'll close that Tab and so
now we are all in good shape and so that
was our introduction to Cloud
information
[Music]
okay let's take a look here at cdk so
cdk allows you to use your favorite
programming language to write
infrastructure as code and technically
that's not true because they don't have
Ruby and that's my favorite but anyway
uh some of the languages include nodejs
typescript Python
java.net and so here's an example of
typescript typescript was the first
language that was um introduced for cdk
It's usually the most upto date so not
always does cdk reflect exactly what's
in cloud formation but I think they're
getting better at that okay so cdk is
powered by cloud formation it generates
out cloud formation templates so there
is an intermediate step uh it does
sometimes feel a bit slow so I don't
really like that but you know it's up to
you uh cdk has a large library of
reusable Cloud components called cdk
constructs at constructs dodev this is
kind of the concept of terraform modules
and is really really useful uh and
they're really well ridden um and they
can just reduce a lot of your effort
there CD cdk comes with its own CLI um
and I didn't mention this before but
cloud formation also has its own uh CLI
okay cdk pipelines uh are are allow you
to quickly set up cicd pipelines for CD
K projects that is a big pain point for
cloud formation where you have to write
a lot of code to do this whereas um this
cdk has that off the bat makes it really
easy for you cdk also has a testing
framework for unit and integration
testing I think this might be only
limited to typescript because I didn't
see any for the rest of the languages
but um you know I wasn't 100% sure there
uh this one thing about cdk is that it
can be easily uh confused with SDK
because they both allow you to
pragmatically work with AWS uh using
your favorite language but the key
difference is that cdk ensures uh itap
poent uh of your infrastructure so what
that means that's such a hard word to
say but what that means is that um you
know if you use this cdk to say give me
an a virtual machine you'll always have
a single virtual machine uh because it's
trying to manage the state of the file
whereas uh when you use SDK if you run
it every time you'll end up with more
and more servers uh and it's not really
managing States so hope hopefully that
is clear between the difference
[Music]
there okay so we looked at cloud
formation but now let's take a look at
cdk cloud formation or cloud formation
Cloud development kit it's just like
cloud formation but you use a a
programming language in order to uh
Implement your infrastructure as a code
I don't use it very often I don't
particularly like it but um you know if
you are a developer and you don't like
writing Cloud information files and you
want to have something that's more
programmatic you might be used to that
um this I think should be deleting cuz
we were deleting the last one here and
notice how it's grayed out I can't
select it so don't worry about that
create a new one we'll say cdk example
we'll hit next T2 micro ec2 instance
Amazon X2 you know the drill it's all
fine here we're go ahead and create
ourselves a new environment we're going
to let that spin up there and as that's
going we're going to look up uh adus
cdk so Adis cdk um and we probably want
to go to GitHub for this
okay because it is open source and so I
want to go to getting
started and I have used this before but
I never can remember how to use it
probably the easiest way to uh use this
is by using
typescript
so here's an example initialize the
project make directory cdk oh first we
got to install it right so give that a
moment so this is node you know how we
did like bundle install this is like the
same thing but for uh typescript install
or update the it was cdk CLI from npm we
recommend using this version etc etc so
again we're just waiting for that to
launch but uh as we wait for that it's
very simple we're just going to install
it create a directory um go into that
directory initialize the example here
it's setting up an
sqsq which is um it's quite a complex
example um but you can see it's code
right and then we run cdk deploy and
we'll deploy it and then hopefully we'll
have that
resource so again we're just waiting for
cloud
n there we go so Cloud9 is more or less
ready a terminal seems like it's still
thinking and we have a JavaScript one
which I do not care about there we go
there's our environment we're going to
make sure we have mpm so we can type in
mpm great it says version
8.1.0 and so this is asking for 10
okay I don't know if this gives us like
MVM installed MVM it does so what we can
do is do MVM list that stands for node
version manager Ruby has one as well and
so it's telling us what version we're on
I want to update um looks like we have a
pretty uh pretty new version but what I
want is the latest version of oh but
that's node version that's not
necessarily mpm so we'll do node version
Oh 17 okay we're well well in the uh
range of the new stuff so what I'm going
to do is scroll down we're going to grab
this link here or this code here hit
enter and that's going to install the
adus cdk so it
says uh file already exists oh so maybe
it's already installed on the
machine um cdk we'll type in
cdk because of course adus wants to make
it very easy for us this software has
not been tested with what was that
warning uh with node 1701 you may
encounter runtime issues great AWS
you're like the one that installed this
stuff here so we get a bunch of the
commands which is great and so what
we'll do is follow their simple
instructions we'll say hello
cdk we will CD into
this and um now what we can do is run
cdk andit and this language
here and so that's going to do a bunch
of stuff creates tons of files it's
going to vary based on uh what you're
using like which language because cdk
comes available in a variety of
languages so if we type in ads
cdk um documentation
here notice up here python java.net so I
think it has more than just those three
languages but um you know I wish it
supported more like yeah see here is C
Java but I I really wish there was a
ruby so we'll give this a moment here to
get installed and I will see you back
here when it is done
okay okay uh it turns out I only had to
wait like a second there but it says
there's a newer version of the cdk you
probably should install it but I just
want to get going here so as long as I
don't run into any issues I do not care
um but anyway so looking at this and I
again I rarely ever look at this but I'm
a developer so it's not too hard for me
to figure out but under the lib this is
our stack that we're creating and here
is it is loading in sqs it's loading in
SNS and then the core Library it's
creating an sqs q and it's setting the
visibility of that timeout it's also
creating an SNS topic so those are two
resources that we expect to be created
if we scroll on down to the getting
started it just says cdk deploy so what
we'll do is go ahead and hit enter and
let that do whatever it wants to do
and it is thinking there we go so here
we have IM statement changes so it's
saying this deployment will potentially
make potential sensitive changes
according to your current security
approval options there is there may be
security related changes not in this
list do you want to deploy sure we'll
hit
why deploying creating cloud formation
change set so cdk is using cloud
formation underneath it's not
complicated um and as that is going
going what we'll do is we'll make our
way over to our IUS amazon.com
console and if we go over to cloud
formation we'll see if we see anything
yet so it's creating a stack here we can
click into it we can go over to our
events see that things are being created
this is always a confusing so I always
go to resources to see what is
individually being created and they're
all done so we go over here and they
exist so here it says that we have a Q
called this right sometimes they have
links you can link through it so notice
here I can click on the topic and get to
that resource in SNS which is nice for
sqs I'm just going to type in sqs
enter uh and there it is okay so we
don't really understand what those are
we could delete the stack this way
there's probably a cdk way to delete the
stack so uh cdk
destroy I assume that's what it is
destroy okay so we'll type in cdk
Destroy give it a moment we're going to
say
yes okay it's deleting in progress we
can even go back here and double
check still
thinking and again you know if we
deleted these for real it would take
like a second but uh you know sometimes
they're just
slow sometimes it's because a resource
can get hung as well um but uh I don't
think anything is a problem so here we
can see what the problem is not not
necessarily a problem but it's just the
sqs is taking a long longer time to
delete where the SNS subscriptions a lot
faster so I'll just see you back here in
a moment okay okay so after a short
little wait there it finally finished uh
I just kept oning refresh until I saw it
deleted and so it's out of there and so
we'll get rid of our Cloud9 environment
since we are done with it so type in
Cloud9 up at the
top and we'll go ahead and delete and we
will go ahead and delete this here thank
you and we will go back to our adabs
amazon. console here just so we can get
our bearing straight here and there we
[Music]
go all right let's take a look here at
the adus toolkit for vs code so adus
toolkit is an open source plugin for vs
code to create debug deploy it was
resources since vs code is such a
popular uh editor uh these days I use
Vim but it's very popular um I figured I
should make sure you're aware of this um
plugin so it can do four things you get
the Abus Explorer this allows you to
explore a wide range of adus resources
linked to your adus account uh and
sometimes you can view them sometimes
you can delete them it's going to vary
per service and what's available there
then you have the adabs cdk Explorer
this allows you to explore your Stacks
defined by cdk uh then you have Amazon
elastic uh container service ECS this
provides intellisense for ECS task
definition files intense means that when
you type uh and you uh you'll get like
Auto completion but you'll also get a
description as to what it is that you're
typing out then there is serverless
applications and this is pretty much the
main reason to have Theus toolkit allows
you to create a debug deploy service
applications via Sam and CFN so uh there
you can see the command pallet and you
can kind of access stuff there
[Music]
okay let's take a look here at access
keys so an access key is a key and
secret required to have programmatic
access to adus resources when
interacting with the adabs API outside
of the adus Management console so uh
access key is commonly referred to as
adus credentials so someone says adus
credentials so generally you're talking
about the access key not necessarily
your um username and password to log in
so a user must be granted access to use
access key so when you're creating a
user you can just checkbox access key um
you can always do the after the fact but
it's good to do that as you're creating
the user and then you can generate an
access key and secret so you should
never share your access keys with anyone
they are yours if you give them to
someone else is like giving them the
keys to your house it's dangerous uh
never commit access keys to a code base
uh because that is a good place uh for
it to get leaked at some point you can
have uh two active keys at any given
time you can deactivate access Keys
obviously delete them as well access
Keys have whatever access a user has to
adus resources so uh you know if you can
do it inabus Management console so can
the key so access keys are to be stored
in the ads. adabs credentials uh file so
um and if you're not familiar with Linux
this Tilda here this actually represents
your home folder so whether you're on
Windows or Linux that's going to be your
home folder and then you have this
period AWS that means that it's a hidden
folder but you can obviously access it
and so in the it's just a toml like file
I think it's toml um but I never uh 100%
verified that it's toml it looks just
like toml uh and so what you'll have
here is your uh default profile and so
this is what you would use um or this is
what uh any of your tools you use like
the CLI or anything else would
automatically use if um if you did not
specify a profile you can of course
store multiple access keys and then give
it a profile name um so if you are doing
this for the first time you might just
want to type in ads config and it'll
prompt and you'll just enter them in
there as well I think that's that's the
default one when you're using the
SDK uh you would rather probably use
environment variables because this is
the safest way to access them when you
are writing code all right um so there
you
[Music]
go all right let's talk about access
Keys access keys are are very important
to your account um and so what we'll do
is go to IM if you are the root user you
can go in and you can uh generate access
keys for people um but generally you're
doing it yourself for your own account
so I go to users I'm going to click into
mine here and we'll go over to Security
credentials and here you're going to
notice access keys and one thing that is
interesting is that you can only ever
have two access keys at a time so if I
hit create I'm just going to close that
notice that the button is grayed out I
can uh uh deactivate them if I feel that
I haven't used them in a while and I can
make them active again so I can bring
them back into access or what I can do
is um make them in active right and then
I can delete them and so what I
recommend right even if you do not want
to programmatically be using your
account for anything you always want to
fill up both these and the reason why
and this is for security reasons is that
if somebody wanted to come in and uh uh
get into your account what they would do
is they would try to find a user um
where they have access to them and then
they would try to generate out a key so
if both these keys are Tak up so if you
generate up both these
Keys okay and this is the one you want
to use you deactivate the other one okay
we're not going to use that one and so
now there's no way for them to fill up
that other slot okay and so that is my
strong recommendation to you but there's
again only ever two here I'm just going
to uh Delete both of these so that when
we want to uh do whatever next in a
tutorial we'll go generate that out okay
go ahead and clear that
out so hopefully that is enough for you
to understand what to do with these
access Keys okay so I'm going to go back
here there you
[Music]
go let's take a look here adus
documentation which is a large
collection of technical documentation on
how to use adus Services which we can
find at doc. adab. amazon.com uh and so
this is kind of like the landing page
where you can see all the guides and API
references if you expand them in there
uh into ec2 and you click on the user
guide you can see HTML in PDF format
Kindle and you'll notice there's a link
to GitHub and that's because all of
these docs are open source and you can
contribute to them if you choose to do
so I've done so multiple times in the
past it's quite fun so adus is very good
about providing detailed information
about every adus service and the basis
of this course and any adus
certification will derive mostly from uh
the adus documentation so I like to say
that I'm not really coming up with new
information I'm just uh taking what's in
the docs and trying to make it more
digestible and I think that's the thing
is like the docs are really good you can
read them end to end but they are very
dense um and so it can be a bit hard to
figure out what you should read and what
you should not um but uh they are a
really great resource and you should
spend some time in there
[Music]
okay so I just want to quickly show you
the adabs documentation like give you a
bit of a tour of it so if we go to ad.
amazon.com and type in docs and I'm sure
you might have seen this through other
tutorials but the idea is that you have
basically documentation for basically
any possible service that you want and a
lot of times you'll click into it and
what you'll get are these little boxes
and they'll show you different guides
and it's going to vary based on service
but a lot of times there's a user guide
there's an API reference those are the
two that you'll see there if we go to
something simpler like
S3 that might be a simpler example yeah
user guide API API reference and so all
of these are on GitHub right if you open
these up the documentation is here if
you find something you don't like you
can submit issues and uh and correct
things you can even submit your own
examples I have um I have uh committed
uh example code to the uh docs
specifically for AI services so you
might be looking at examples that I
implemented or even Ruby examples since
I really like to promote Ruby on AWS you
can download as a PDF or you can take it
as HTML a lot of times you're going to
the user guide and the way I build the
courses here is actually go through and
I read these end to end so you know if
you wanted to do that and you wanted to
be like me uh you can do that or you can
just watch my courses and save yourself
the trouble and not worry about
everything that is here but generally
the documentation is extremely extremely
good there are some exceptions like
Amazon Cognito where the content is good
but it's just not well organized so I
would say it best out of every other
provider they they have the most
complete documentation uh they generally
don't keep their examples or like
tutorials within here it's usually
pretty light they'll have some examples
um but like they like they have adus
Labs separately so you type AB Labs
GitHub right you go here and a lot of
stuff is in here instead so you have a
lot of great tutorials and examples over
there okay um but yeah pretty much
that's all there is to it is there
consistency between documentations no
they kind of vary um you know but uh
it's all there is my point and they're
always keeping up to date so yeah that's
all you need to know about the inabus
documentation
[Music]
hey this is Andrew Brown from exam Pro
and we are taking a look at the Shared
responsibility model which is a cloud
security framework that defines the
security obligations of the customer
versus the cloud service provider in
this case we're talking about AWS and
they have their own shared
responsibility model it's this big ugly
blob here um and the thing is is that
every single CSP has their own variant
on the model uh so they're generally all
the same but some visualizations make it
a little bit uh easier to understand or
they kind of uh include a little bit
more information at different parts of
it and so just to get make sure that you
have well-rounded knowledge I'm going to
go beyond the aws's shared
responsibility model and just show you
some variants uh there's also variants
not just per uh CSP but also the type of
cloud deployment model and sometimes
these are also scoped uh based on a
cloud service category like compute or
machine learning uh and these can uh
result in specialized share
responsibility models so that's what
we'll look at in this section
[Music]
okay all right so let's take a look at
the ad Shar responsibility model and so
I've reworked the graphic because it is
a bit hard to uh digest and so I'm
hoping that this way will be a little
bit easier for you I cannot include the
in and of here just because we're
limited for space but don't worry we'll
follow that up with the next slide here
so there are two people that are
responsible or two um organizations that
are responsible the customer and AWS and
on aws's side they're going to be
responsible for or anything that is
physical so we're talking about Hardware
Global infrastructure so the regions the
availability zones The Edge locations
the physical security so think of all
that Hardware that's there those data
centers um everything like that then
there's also software the services that
they're offering and so um you know this
extends to all their services but
generally it breaks down to the four
core and so we're talking about compute
storage database and networking okay and
when we say networking we're talking
about like physically setting up the wi
and also you know the software to set up
the routing and all that kind of stuff
there uh now looking at the customer
side of it they're responsible for
configuration of managed services or
thirdparty software so the platforms
they use so whether they choose to use a
particular type of os uh the
applications so if they want to use like
Ruby on Rails uh am so identity and
access management so if you uh create a
user and you grant them permissions if
you give them things they're not
supposed to have access to that's on you
right then there's configur of virtual
infrastructure and systems so that would
be choosing your OS that would be uh the
networking so there could be networking
on the um the virtual machines
themselves or we could be talking about
Cloud networking in this case then there
are firewalls so we're talking about
virtual fire walls again they could be
on the virtual machine or it could be
configuring like knackles or security
groups on AWS then there's security
configuration of data uh and so there is
client side data encryption so if you're
moving something from S3 from your local
machine to S3 you might need to encrypt
that first before you send it over then
there's server side encryption so that
might be turning on server side
encryption within S3 or turning it
encryption on your EBS volume then
there's networking traffic protection so
you know that's turning on VPC flow log
so you can monitor them turning on AIS
guard Duty so that it can detect
anomalies with your traffic or or
activities within your um adus account
and then there's customer data so that's
the data that you upload on the behalf
of your customer customers or yourself
and what you decide to um you know like
what levels of sensitivity that you want
to lock it down do you want to use
Amazon Macy to see if there's any public
facing uh personally identifi
information that's up to you so there's
a lot here and honestly it's a lot
easier than you think um instead of
thinking about this big diagram what I
do is I break it down into this and so
we have the in and the of and that's
what I said I could not fit on the um
previous slide there the idea is
customers are responsible for the
security in the cloud so that's your
data and configuration so if it's data
that's resigning on there or there
something you can configure you are
responsible for it on the adus side they
are responsible for the security of the
cloud so if it's anything physical or
Hardware the operation of managed
services or Global infrastructure that's
going to be on them and this in and of
thing is very important for the exam so
you should absolutely know the
difference between the two this is kind
of an adist concept I don't see any
other cloud service provider talking
about in and of uh so you definitely
need to know it
[Music]
okay so one variant we might see for the
uh shared responsibility model would be
on the types of cloud computing this
could also be applicable to the types of
uh deployment models but we're doing
types of cloud computing here and so we
have the customers responsibility and
then the cloud service providers
responsibility so we're seeing on
premise infrastructure as a service
platform as a service and software as as
a service and so when you are on Prem
you're basically responsible for
everything apps data runtime middleware
OS virtualization servers storage
networking basically everything and just
by adopting the cloud you're almost
cutting your responsibilities in half
here so now the cloud service provider
is going to be responsible for the
physical networking uh the physical
storage those physical servers and
because they're offering virtual
machines to you they're setting up a
hyper visor uh on your behalf so
virtualization is taking care of for you
and so um you know if you launch an ec2
instance you know you're going to have
to choose the OS so that's why you're
responsible whatever middleware there
the runtime so whatever kind of programs
you install on it uh the data that
resides on it and any kind of like major
applications okay then we have platform
as a service uh and so you know the
class service provider is going to take
even more responsibility there so when
we're talking about this we're thinking
like a elastic beant stock right so you
know the you just choose what you want
and it's all managed so you might say I
want a ruby on real server but you're
not saying what OS you need um you're
not uh saying exact you might say what
version of Ruby you want but you don't
have to manage it if it breaks uh or it
might be managed updates and things like
that the last thing here is like
software as a service and this is
something where the CSP is responsible
for everything so if you're thinking of
a of a software's a service think of
like Microsoft Word where you're just
writing
uh you know writing stuff in there and
you know you you are responsible for
where you might choose to store your
data but the data is like still handled
by the cloud service fighter because you
know it's on the cloud so on their
servers right um so yeah hopefully that
gives you kind of an idea across types
of cloud computing
[Music]
responsibilities all right so what I
want to do here is just shift the lens a
bit and look at the share responsibility
model if we were just uh observing a
subset of cloud services such as compute
and so we're going to see
infrastructures of service platform as a
service software as a service and now we
have function as a service and so that's
what I mean when we shift the lens we
get new information uh and so you can
just see that you really don't want to
look at this uh from one perspective
okay so starting at the top here we have
bare metal uh and so iTab Us's offering
is called the ec2 bare metal instance
and this is where you basically get the
whole machine uh you can configure the
entire machine with with the exception
of the physical machine itself so as a
customer you can install the host OS um
uh the host OS so the operating system
that runs on the physical machine and
then you can install your own hypervisor
um and then databas is going to be
responsible for the rest the physical
machine now normally The Next Step Up
would be dedicated but dedicated doesn't
exactly give you more responsibility it
gives you more Assurance because it's a
single tenant virtual machine and that's
why I kind of left it out here um but
we'll see it in the next slide that it
is kind of on the model and shares the
same spot as uh ec2 um but ec2 is a
virtual machine and so um here the
customer is responsible for the guest OS
so that means that you can choose what
OS you want whether it is Ubuntu or
Debian or Windows but that's not the
actual OS that is running on the
physical machine and so you're not going
to have control of that ads is going to
take care of that then there's the
container runtime so you know you you
can install Docker on this or any kind
of container layer that you want um so
that's another thing that you can do so
ads is going to be responsible for the
hypervisor uh the physical machine and
the host OS all right then looking at
containers it says more than one
offering for containers but we'll just
look at ECS here and so um this is where
you are going to uh have uh you don't
you don't install the guest OS right the
guest OS is already there for you what
you are going to do is choose your
configuration of containers you're going
to uh deploy your containers you're
going to determine where you need to
access storage for your containers or
attach storage to your containers and
adus is going to be responsible for the
guest OS it it the and there might not
even be a guest OS but there the host OS
the guest OS the hypervisor the
container runtime uh and you're just
responsible for your containers okay
then going to the next level here we
have platform as a service and so this
one also is a little bit odd where it
fits um because the thing is is that
this could be using anything underneath
it could be using containers it could be
using virtual machines um and so that's
where it doesn't exactly fit well on a
linear graph but let's just take a look
at some things here so this is where
you're just uploading your code uh you
have some configuration of the
environment you have options of
deployment strategies um the
configuration of the associated services
and then Abus is going to be responsible
for the servers the OS the networking
the storage the security so it is taking
on more responsibility than
infrastructures of service um whereas
you know adus is just going to be
responsible that so if it's a virtual
machine it's being under uh under the
use is going to be responsible for this
customer stuff okay you're not if it's
containers then Abus is going to be
responsible for this but it just depends
on how that platform is a service is set
up actually the way elastic beanock is
set up is that you actually have access
to all that infrastructure and you can
fiddle with it and so in that case um
whereas like if you were to use Heroku
which is a a third party provider um you
know they would take care of all this
stuff up here um and so you would not
have to worry about it but on AWS you
actually are responsible for uh the
underlying infrastructure because you
can you can configure it you can touch
it so that's where you know again these
do not fit perfectly you can't look at
platform as a service meaning that um
you're not responsible for certain
things it really comes down to the
service offering okay then we're taking
a look at software of service so on AWS
um this is is going to be something like
um Amazon work docs which is I believe a
competitor uh not a very popular
competitor but a competitor to Microsoft
SharePoint and this is for Content
collaboration so as the customer you're
responsible for the contents of the
document management of the files
configuration of sharing access controls
and the databas is responsible for the
servers the OS networking the the
storage the security and everything else
so you know if you use a Microsoft Word
Doc and you type stuff in it you say
where to say it that's what you're
responsible for okay the last one here
on the list is our functions here and so
's offer is it Lambda and so as the
customer all you're doing is you're
uploading your code and databus is going
to take care of the rest so deployment
container runtime networking Storage
security physical machine basically
everything um and so you're really just
left to uh develop okay so you know
hopefully that gives you kind of an idea
and again you know we could have thrown
in a few other services like what we
could not fit on this slide here was um
uh adus fargate which is a a serverless
container as a function or sorry
serverless serverless container as a
service or container as a service so uh
you know that has its own unique
properties in the model as well okay so
let's just have kind of a visualization
on a linear graph here so we have the
customers responsibility on the left
hand side and itus is responsibility on
the right and we'll look at our broad
category so we got bare metal dedicated
virtual machines containers and
functions and so no matter uh which uh
type of compute you're using using
you're always responsible for your code
for um containers you know if uh you
know like uh the functions when you're
using functions there are pre-built
containers so you say I want to use Ruby
and there's a ruby container and you
don't have to configure it but obviously
um you know when you are using container
service you are configuring that
container you are responsible for it for
um virtual machines you know you're
responsible for the runtime so you can
install a container runtime on there or
install a bunch of different packages
like Ruby and stuff like that uh the
operating system you have control over
in the virtual machines for the
dedicated and we saw with bare metal you
have both uh controls of the host OS and
the guest OS and then only bare metal
allows you to have control of the
virtualization where you can install
that hypervisor so hopefully that gives
you an idea of compute and it is
offering there and also kind of how
there's a lot of little caveats when
we're looking at the Shared
responsibility model okay
[Music]
all right so I have one more variant of
the share responsibility model and this
one is actually what is used by Google
so um we're going to apply to AWS and uh
see how it works so let's just kind of
redefine share responsibility model or
just in a slightly different way so we
fully understand it so the share
responsibility model is a simple
visualization that helps determine what
the customer is responsible for and what
the CSP is responsible for related to
AWS and so across the top we have
infrastructure as a service platform as
a service software as a service but
remember there's other ones out there
like functions and service it's just not
going to fit on here um okay so and then
uh along the side here we have content
access policies usage deployment web
application security identity operations
access and authentication network
security remember that's Cloud
networking security the guest OS data
and content audit logging now we have
the actual traditional networking or
physical networking storage and
encryption and here we're probably
talking about the physical storage
Harden kernel IPC uh the boot the
hardware and so then here we have our
bars so we have the csp's responsibility
and the customer responsibility so when
we're looking at a SAS software as a
service uh the customer is going to be
responsible for the content remember
like think of like a word processor
you're writing the content the access
policies like say I want to share this
document with someone the usage like how
you utilize it can you upgrade your plan
things like that then next on our list
here is platform as a service so
generally uh you know platform is a
services for developers to De develop
and deploy applications and so they will
generally have more than one deploy
strategy and uh you know there might be
some cost-saving measures to choose like
uh you might have to pay additional for
security uh or you it's up to you to
configure in a particular way or you
might have to integrate it with other
services uh and you know we saw that
pass is not a perfect uh definition or
fit because you know when we look at
elastic bean stock if you have access to
those resources and you can change the
underneath then you might have more
responsibility there than you think that
you would okay the next one here is
infrastructure the service and so this
is extending to Identity so who's
allowed to uh you know log into your
adabs account operations the things that
they're allowed to do in the account
access and authentication do they have
to use MFA uh things like that network
security obviously you can configure the
security of your uh Cloud infrastructure
or Cloud Network um you know so you know
do you isolate everything a single VPC
how do you set up your security groups
things like that uh we know with virtual
machines you can set up the guest OS
there's data and content but remember
that bare metal is part of the uh
infrastructure service offering and so
that's where we'd see Hardware or not
Hardware but you'd have the host o the
host Os or virtualization and so this
again is not a perfect representation uh
but it generally works okay and then
last and list there um or just looking
at what the ads is responsible for auto
logging so of course adus has cloud
trail which is for uh uh logging uh API
um events but Auto loging could be
things that are uh internally happening
with those physical servers then the
networking the physical storage uh
Harding the kernel OS has I think what's
called the Nitro system where they have
like a security chip that's uh installed
on all their servers then it's the the
boot OS uh and then the hardware itself
okay so just remember the customer is
responsible for the data
and configuration of access controls
that reside in AWS so if you can
configure it or you can put data on it
you're responsible forward okay the
customer is responsible for the
configuration of cloud services and
granting access to users via permissions
right so if you give uh one of your
employees access to do it um you know
even if it's their fault it's your fault
so remember that um and again the CSP is
generally responsible for the underlying
infrastructure we say generally because
you know there's edge cases like bare
metal and coming back to adses in the
cloud and of the cloud so in the cloud
so if you configure it or store it then
you the customer responsible for it and
of the cloud if you cannot configure it
then the CSP is probably responsible for
it
[Music]
okay hey this is Andrew Brown from exam
Pro and we are looking at the share
responsibility model from the
perspective of architecture and if
you're getting sick of share
responsibility model don't worry I think
this will be the last
slide in this section but let's take a
look here so we have uh less
responsibility more responsibility at
the bottom so what we have down here is
traditional or virtual machine
architecture so Global Workforce is most
familiar with this kind of architecture
and there's lots of documentation
Frameworks and support so maybe this
would be using elastic beanock with
platform as a service or using ec2
instances alongside with autoscaling
groups uh code deploy uh load balancers
things like that the next level here is
microservices or containers this is
where you mix and match languages better
utilization of resources so maybe you're
using fargate which is seress containers
or elastic container service or elastic
kubernetes service for containers and at
the top here we have serverless or
commonly with functions as a service so
there are no more servers you just worry
about the data or uh and the code right
so literally just functions of code and
so you could be using the amplify
serverless framework or maybe abess
Lambda for creating serverless
architecture so there you go
[Music]
hey this is Andrew Brown from exam Pro
and we are looking at Computing Services
and before we jump into uh the entire
Suite of Computing Services they was
have let's just talk about ec2 for a
moment which allows you to launch
virtual machines so what is a virtual
machine well a virtual machine or VM is
an emulation of a physical computer
using software server virtualization
allows you to easily create copy resize
or migrate your server multiple VMS can
run on the same physical servers so you
can share the cost with other customers
so imagine if your server or computer
was an executable file on your computer
okay so that's the kind of way you want
to think about it when we launch a VM uh
we call it an instance and so ec2 is
highly configurable server where you can
choose the Ami so the Amazon machine
image that affects options such as
amount of CPUs or vcpus virtual CPUs
amount of memory so Ram the amount of
network bandwidth the operating system
so whether it's Windows Ubuntu Amazon 2
uh the ability to attach multiple
virtual hard drives for storage so
elastic Block store um and so the Amazon
machine image is a predefined
configuration for AVM so just remember
that and so ec2 is also considered the
backbone of ads because the majority of
a services are using uc2 as the
underlying servers whether it's S3 RDS
10B or lambdas that is what it's using
so um what I say also it's just because
when we talk about the it Network that
is the backbone for Global
infrastructure and the networking at
large and so ec2 is for the services
[Music]
okay hey this is Andrew Brown from exam
Pro so we just looked at what ec2 is
well let's look at more of the broader
services for computing and these are the
more uh common ones that you'll come
across there's definitely more than just
what we're going to see on the single
slide here so we'll break this down with
virtual machines containers and then
serverless for for virtual machines
remember that's an UL ation of a
physical computer using software and ec2
is the main one but for our VM category
we have Amazon light sale this is a
manage virtual server service it is the
friendly version of ec2 virtual machines
so when you need to launch a Linux or
Windows server but you don't have much
adus knowledge you could launch a
WordPress here and uh you could hook up
your domain and stuff like that um so
this is a very good options for
beginners we have containers so
virtualizing an operating system or Os
to run multiple workloads on a single OS
instance so containers are generally
used in microservice architecture when
you divide your application into smaller
applications that talk to each other so
here we would have ECS elastic container
service this is a container
orchestration service that supports
Docker containers launches a cluster of
servers on these2 instances with Docker
installed so when you need Dockers a
service or you need to run containers we
have elastic container registry ECR this
is a repository of container images so
in order to launch a container you need
an image an image just means a save copy
a repository just means a storage that
has Version Control we have ECS fargate
or just fargate now people are kind of
forgetting that it's it runs on ECS
these days that's why I have it in there
it is a servess orchestration container
service is the same as ECS ex except you
pay on demand per running containers so
with ECS you have to keep a ec2 server
running even if you have no containers
is running so it manages the underlying
server so you don't have to scale or
upgrade the ec2 server so there's the
advantage over ECS okay then we have
elastic kubernetes service eks this is a
fully managed kubernetes service kuber
or so kubernetes commonly rated to K8 is
an open-source orchestration software
that was created by Google is generally
the standard for managing microservices
so when you need to run kubernetes as a
service then we have serverless category
so when the underlying servers are
managed by B to us you don't worry or
configure servers saus Lambda is a
serverless function service you can run
code without provisioning or managing
servers you upload small pieces of code
choose much uh how much memory how how
long you want the function to run is
allowed to run before timing out and you
are charged based on the runtime of the
Serv function rounded to the nearest 100
milliseconds so there you
[Music]
go hey this is Andrew Brown from exam
Pro and what I want to do is just show
you a variety of different Computing
Services on AWS so I'm going to try to
launch them and uh we're not going to do
anything with them just going to Simply
launch them okay so the first I want to
show you is ec2 and by the way we will
go more in depth in ec2 later on in this
course here um but what I'm going to do
is go ahead and launch the instance
don't worry about all this stuff but
just choose the Amazon Linux 2 so it's
in the free tier all right we're going
to choose an instance type of a T2 micro
so that's part of the free tier it's
going to be set as one all these options
are fine I want you to go ahead and
review and launch we're going to launch
and I don't want to generate on any key
pair I'm going to proceed without a key
pair I'm going to acknowledge that
because I don't want it and that's all
there is to launching an ec2 instance
and so I can go here and view my
instances and what you'll see is it's
pending okay and usually it has like a
little spinning icon maybe they've
updated it since
then so I go here it's hard to see
because there's all these terminated
ones but I don't need to do anything
with it I just wanted to show you the
actions that you'd have to do to launch
it actually we'll leave it alone maybe
we'll see it when it's launched the next
one I want to show you is e elastic
container
service um and wow this this is old
let's go let's get the new experience
please so old okay checkbox that
on and we'll hit get started and we'll
say create a
cluster and we have some options here
networking only ec2 Linux plus
networking uh for use with either ads
fargate or external windows
um uh this is if you're doing fargate
which we're not doing right now fargate
is part of elastic container service it
used it well used to be it is called ECS
fargate but it us markets it as a
separate service we'll go to next and
say my ECS
cluster um we can create an empty
cluster but that would make it a fargate
cluster which we don't want there's an
ond demand server look it's M6 I large
if you're very afraid of a lot of spend
here you don't have to do this you can
just watch me do it and just learn
well what I'm going to do is try to find
something super cheap so I want a T2
micro or a T3 micro T2 micro is part of
the free tier I don't know if we get to
choose T2 anymore in here they might not
let
you there it is you know T3 micro is
great too I just whatever says it's free
that's what I'm going to go for number
of instances one the Amazon lytic
version is fine I don't care about a key
pair uh use the existing VPC I don't
want to have to make a new one select
the existing ones
okay uh let it create a new security
group that's totally fine allow those to
be fine create a new role that's fine
create okay and so that's going to
create ourselves a
cluster um I'm going to just make a new
tab here let's just check on our ec2
instance and so if we look at our ec2
instance it is running okay great so it
has a private IP address it has a public
IP address all right um there's not much
we can do with it I can't even log into
it because we didn't generate out a key
pair L times you want to name these
things so I just go here and name it my
server okay go back to our ECS instance
and the cluster is ready so we'll go
here and oh nice we got a new UI and so
if we wanted to deploy something as a
service or a
task
um we would need to create a t template
like a task definition
file uh they don't have a new UI for
this you're being redirected to the
previous version console because this
isn't available in the new experience
yet of course it isn't so we can create
a new task definition file that's what's
used to run it it's basically like a
Docker file composed file whatever you
want um we have fargate or ec2 we are
doing ECS so we're going to have to do
ec2 so we'll say my ECS uh Tas def
file um task Ru opt optional IM roll I
don't need one network mode I I don't
care um and then this is the idea is
that because a container allows you to
use up a particular amount of the um
thing we don't have to use all of the
memory so we should look up what a T2
micro is because I don't even remember
what size it is okay T2 micro AWS so we
go here we look at the instance types
and we're going to flip over to T2 and
it says that it's one
vcpu one gab of memory so what I'll do
one yeah yeah one okay that's fine so
what we want and this is in megabytes so
we'll say 500 megabytes and um I don't
know if we can do less than one but I'm
going to do one
here
um the task CPU must be an integer
greater than or equal to 128 okay fine
128 oh I guess it's 1024 would utilize
the whole thing so I could say
512 okay and this is where we would add
our
container so
I don't do this every day so I don't
remember how to do this we'll say my
container um and I need a repository
here so I need like dockerhub Hello
World okay I don't care what it is I
just need a image that's
simple and I'm looking for the address
here
um I'm hoping that's just this
dockerhub
URL so it' be something like this right
docker.io probably Docker IO Docker
image um Docker Hub URL in
ECS okay goes to show how often I'm
launch launching these things so
repository URL Docker image so I think
that what we're going to do here
I would really just like the URL
please
reviews
tags where is
it where is it it's somewhere here
right
uh well let's just try it we'll go and
we'll type in says image and tags so
docker.io
hello world I really need an image ID
image URL hello
world Docker
Hub they're not making my life easy here
today anything I just want to see like a
single example docker.io
docker iio
URL
examples
ECS this is what it's like you know this
is what you're going to be doing if you
are um you know a cloud engineer you're
going to be Googling a lot and just
trying to find examples
here so here it says docker.io the name
the host name okay so we'll just try it
okay so I think that the the the name
here is underscore and then it's hello
world and that's what's throwing me off
here
right Docker
IO just hold on
here repository URL and then there's the
tag I don't know if like is the tag
going to be like latest view available
tags latest okay so what I'll do
here and that's the thing you got to
have a lot of confidence too so hard
limits soft limit um do I have to set
it do I have to set any of these things
can I just go to the bottom and hit
add looks like I
can okay so we'll scroll on down create
we create our task definition file which
is fine we're going to go back to our
cluster it's going to bring us back to
the new experience we're going to click
into this
cluster holy smokes uh we're going to
hit
deploy and we are going to choose
service that means it's going to
continuously run task means that when
it's done running it ends we're going to
choose our family our version that's
that's the task definition file there is
not compatible with the selected compute
strategy my task
file what if I just choose task take
that okay some maybe some you have to
like code it so that it continuously
runs I don't care we don't need to run a
service here the selected task
definition is not compatible with the
selected compute strategy
okay let's see
why uh can you double check if you're
using fargate strategy instead of the
ec2 uh blog designed for the ec2
strategy so probably what it's
suggesting is that the the strategy file
I made is not for the right one here Tas
[Music]
definitions go back over
here well what's wrong with it
task roll none my container so what I'm
going to do because I don't trust this
just going to go ahead and delete this
can I delete this how do I delete
this oh boy
actions deregister
deregister we'll create a new one and so
it us has tools like it us co-pilot um
CLI to make this a lot easier because
you can see this is very frustrating but
I chose this
so my task
def requires compatibility of
ec2
default 512
512 add
container we're going
to uh was it docker.io
underscore what's it called hello world
latest we'll just say hello world
here and we'll just say uh 512 which is
fine I don't care about any port
mappings I'm just reading it carefully
here to see what it wants we'll say 512
maybe because I didn't specify them it's
complaining this looks fine we'll hit
add
okay constraints type this all looks
fine so we'll try this
again and so we now have our file let's
see if we can just run this task from
here you
see2 this is just another way to do it
so we just choose the cluster this is
actually a lot easier to do it this is
old old old Eh this is ugly and so now
it launches so you know if you have
trouble one way then just do it another
way and uh sometimes it'll work here so
I don't expect this task to really work
in any particular way if it's pending
that that's fine if it fails that's fine
if it's successful that's fine I don't
care I just want to go through the
motion so it was successful it it ran
and then it stopped I don't know if we
could see like the output anywhere
probably what it would do is it would
log out something like into somewhere
and so I don't know if like there's logs
turned on for this if I go over to like
Cloud watch logs maybe I could see
something a lot of these services will
automatically create cloudwatch logs so
sometimes you can just go look at them
there so we'll drop down we'll go to log
groups
here there is some stuff here um there's
a couple that I created from before just
go ahead delete
those and so what I'm looking for is
like ECS so no there's no logging
happening here which is totally fine so
that is ECS um for fargate it's pretty
much uh the same the difference is that
fargate is like it has to start up and
run so it's a lot slower to
watch okay
and now let's go take a look at a
Lambda okay so this is our serverless
compute so we go ahead and create
ourselves a function uh we can start
from a blueprint that doesn't sound too
bad and I personally like Ruby so no I'm
not getting much here but we can do is
look for something like hello do we have
like a hello
world there we go hello world and we'll
click that we'll say my hello
world uh it's going to create those
permissions that's fine it's showing us
the code it's very simple okay it's
going to console log out these values
not a very good hello world function
doesn't even say hello world how can you
call it a hello world function if it
doesn't say hello world I don't
understand so we're going to go ahead
and create this function usually doesn't
take this
long okay so uh here is our function
here is our code notice that this is
Cloud9
okay and you can even move that over to
Cloud9 they didn't have this button here
before that's kind of cool I hit test
they used to have it up
here but I guess they wanted to make it
more obvious so they moved it down here
which is nice so what I can do is hit
this oops my test it's going to send a
payload here to the actual function uh
and it's going to tell us if it
worked okay so can I run my test go over
here to test
they changed it a bit so I guess I
created there it succeeded so I have my
logs okay so it's it's going to Output
those values there so there are the
three values which basically is
nothing maybe you were supposed to set
those an environment variable but you
can see you're just uploading uh some
code right it's just a bit of code it's
not like a full app or anything so we
launched an E2 container we did a a um
sorry ec2 instance a container we did a
seress function there's other things
like EKF yes but that is really really
hard to set up okay cuz you'd have to
use like kubernetes commands and stuff
like that and my kubernetes knowledge is
always very poor um I'm just taking a
peek here to see if they've updated it
so yeah you create the cluster but like
deploying it is forget it I'm just
trying to think if there's anything else
I kind of want to show you um no those
are the main three I would say so I'm
pretty happy with that um what I'm going
to do is go and kill all these things so
we're going to go over to Lambda okay
and I'm going to go ahead and delete
this as you saw ECS was the hardest and
no matter how many times I've built
things on ECS and I've deployed full
things on ECS I can't remember I always
have so much trouble with task
definition files it's unbelievable we'll
go over to our cluster
here
and ECS cluster up here make sure you're
not in the fargate cluster I know I'm
clicking really fast but there's just so
many things to click and I'm going to
click into this cluster we're going to
go hit edit because this is running an
ec2 instance right I need to destroy it
um it just took me back to the old one
here um I want to delete no I want to
delete the cluster click back
here where do I delete it up
here here I can't checkbox
anything uh how do I delete this do I
have to delete the task first maybe so
we'll go here I mean it's already
stopped there's nothing to
do
edit uh huh account
settings wow this is
confusing
okay how to delete ECS
cluster got to be kidding me I have to
actually look this up so open the USS
console from navigation in the
navigation choose clusters and the new
turn off the E uh turn off new ECS
experience and choose the old console
the delete cluster workflow is not
supported in the EC ECS console are you
serious then
why why do you have it like why even let
people use the new experience if that
you don't have all the functionality
there um oh I was going to give it
feedback but it didn't let me here it
says uh I need to delete an ECS cluster
no okay so I'm
here there's my big ugly
cluster delete
cluster okay so yeah it it's a struggle
okay like things are always changing on
me but uh you just have to have
confidence and if you've done it a few
times you know that you can do it right
um and that's one of the biggest
Hang-Ups to Cloud I would say so it's
going to take a few minutes apparently
to delete the cluster as that is going
let's let's go over to ec2 I didn't
close it I kept this tab
open and uh there's our ec2
instance we can go ahead and terminate
that instance terminate
okay and if this says it's terminating
then we're in good shape Terminator is
shutting down that's fine and notice
here that's the ECS instance just make
sure you shut down the my server not the
um the ECS instance cuz that's to stop
and so this has already terminated but
if we go back here notice that it says
that it's not done but
clearly clearly has shut
down okay so I'm going to wait here for
a bit even though I know it's been
deleted maybe it's deleting things like
the autoscaling group so we go down
below
here right so that's probably what it's
doing it's probably trying to destroy
the auto scaling
group but it doesn't show any here so it
must have already destroyed
it yeah so task Services delete so I'll
be back here in a bit but I know it's
safe it's already deleted but I'll see
you back here in a bit okay so I waited
literally a second and it's now deleted
so we deleted our Lambda we deleted our
oh did we delete our
Lambda good
question now I'm not really worried
about the Lambda because I guess we did
but I'm not really worried about it
because um you know at when it rests at
idle it's not costing us anything where
the EC s and the ec2 are backed by ec2
instances so we do have to shut those
down okay and again remember make sure
you're in the correct region sometimes
that gets flipped over and then you
think those resources are gone but
they're actually not they're just
running in another region so uh there
you
[Music]
go hey this is Andrew Brown from exam
Pro and we're taking a look at higher
performance Computing Services on AWS so
before we do we got to talk about the
Nitro system so this is a combination of
dedicated hardware and lightweight
hypervisor enabling faster Innovation
and enhanced security all new ec2
instant types use the nitro system and
the Nitro system is designed uh by AWS
okay so this is made up of a few things
we have Nitro cards these are
specialized cards for vpcs EBS instant
storage and uh controller cards you have
Nitro security chips these are
integrated into the motherboard protects
Hardware resources and we have the Nitro
hypervisor this is the lightweight hyper
visor memory and CPU allocation bare
metal like performance there's also uh
Nitro enclaves but you that's a bit out
of scope here but that's has to do with
like ec2 isolation Okay uh then we have
bare metal instances so you can launch
ec2 instances that have no hypervisor so
you can run workloads directly on the
hardware for maximum performance and
control we have the M5 the R5 um ec2
instances that can run bare metal
there's other ones I believe I've seen
as well but um you know if you are
running bare metal you can just go
investigate at the time of okay we have
bottle rocket this is a Linux based open
source operating system that is purpose
built by adus for running containers on
VMS or bare metal hosts then uh let's
just Define what HBC is so it's a
cluster of a hundred of thousands of
servers with fast connections between
each of them with the purpose of
boosting Computing capacity so when you
need a supercomputer to perform
computational problems too large to run
on a standard computer or computers or
would take too long this is where you
know HBC comes into play one solution
here is databus parallel cluster which
is uh an adus supported open source
cluster management tool that makes it
easy for you to deploy and manage higher
performance Computing HBC clusters on
AWS so hopefully that gives you an idea
of this stuff
[Music]
okay all right so let's take a look at
HPC or high performance Computing on AWS
so HPC is for uh running large complex
simulations and deep learning workloads
in the cloud with a complete Suite of
high performance Computing product
Services gains Insight faster and
quickly move from idea to Market blah
blah blah blah blah it's for ML or very
complex scientific Computing stuff these
run at least on C5 NS okay and the way
it works is that you use this um CLI
called P cluster or a parallel compute U
or a parallel cluster stuff and so let's
see if we can get this installed very
easily um so what I'm going to
do is see how hard it is to install
stall now I don't recommend you running
this cuz I don't know what it's going to
cost me and if I make a misconfiguration
I don't want you to have that spend here
but I don't think it's that dangerous so
I'm going to go back over to us East one
here I'm going to open up
cloudshell and I'm going to give it a
moment to load and so as that is loading
let's take a look at how we would go
ahead and install this so install the
current parallel um it was parallel I
think we just copy that line
okay and so we have to wait for our
environment to spin up all right so once
it has spun up we will install it and
then we will jump over to this tutorial
here okay so we'll give this a
moment and after waiting a little while
here it looks like our shell is ready it
looks like it's in bash um I'm just
going to type in ads S3 LS that's a
sanity
check okay and it works that's great so
go back over here and I'm going to go
back up to install for
Linux and what I need is that single
command where is
it so I'm certain that we already have
Linux or python installed but I just
want the command to install
it we saw it a moment ago here I'm just
going to back out till I can find
it uh one more there it is so it's under
oh it's this link here and that's what I
talk about the documentations being
tricky sometimes you have to click these
uh headings here to find stuff so this
is the first time installing it so we'll
grab that usually you're supposed to
create in Virtual environments with
python I don't care this is my cloud
shell it doesn't matter to me so we're
going to go ahead and download that and
hopefully it is fast and it was super
fast which was really nice and so what
we'll do is go check out the pcluster
version okay and that looks fine to me
I'm going to go down below here to run
our first job um the returns the it
gives outputs I don't think we need to
configure it because we already have our
CLI so what I'm going to do is go ahead
and create ourselves a new cluster um
beginning cluster creation configuration
file config not found so I guess we do
have to configure
this
configure and it's asking what region do
we want to be in um if I have us East
one I would choose it for some reason
it's all the way for number 13 that is
not a lucky number but I'm going to
choose it anyway anyway no key pair
found in Us East one region please
create one of the following um so create
an ec2 key
pairs uh no options found for ec2 key
pairs that's fine so what what I'll do
is go over
here and we'll go over to
ec2 and we will go over to key pairs key
pairs key pairs key pairs we'll create
ourselves a new new one here so we'll
say um HPC key pair or just my
HPC so we know what it it's for we have
putty or PM we're going to do pem
because we're on Linux we'll create that
and notice that it downloaded the pem
down down here and we're going to need
that for later um and so what I'll
do as I'll type in P cluster here again
configure we'll choose 13 we'll choose
number one here
uh allowed values for the scheduler I
have no idea what these are uh let's
choose the number one allowed values for
the operating system Amazon L 2 I know
what that is minimum cluster size one
maximum cluster size two head notice
instance oh T2 micro you can do that
yeah let's do it I didn't know we could
do that enter compute type uh T2 micro
sure so I thought that we'd have to use
a c5n but I guess apparently not
automate VPN uh VPC creation yes of
course network configuration so allow
values for the network configuration uh
head node in a public subnet and and
compute Fleet in a private subnet uh
head node in compute yeah we'll do it in
the both just to make our lives easier I
don't care first one sounds more secure
of course and so oh it's creating Cloud
information sack wow this is easy I
thought this was going to be super
painful okay so we'll go over here we'll
go take a look at what cloud formation's
doing all
right now I don't care if we actually
run a task on here but it was just
interesting to go through the process to
see how hard it was and we will go look
at what resources are being created so
it's creating an internet gateway so
it's literally creating a isolate VPC
for it which is totally fine I guess um
it's creating a subnet it's creating a
route table refresh
here um I'm not sure how much it wants
to create here it just looks like VPC
that's all it's creating I thought maybe
the ec2 instances would show up here but
maybe it's going to launch that on a
need be
basis okay so that's all created oh now
it's doing a VPC
Gateway I think VPC gateways cost money
let's go take a look here VPC
pricing yeah there's a uh transfer fee
so just be careful about that you know
and you just can just watch along here
you don't have to do
it default route depends on public so
now it's creating ec2
route I don't know what an ads ec2 route
is I've never seen that before sometimes
what we can do is go into ec2 and then
take a look on the left hand side you
see anything in here we don't know what
it is we just type in ec2 route cloud
formation sometimes cloud formation is
great for figuring out what a component
is not all components are represented in
the um inabus um Management console so
specify route in the route table oh it's
just a route
okay and we'll go back here we'll
refresh so that is done is the stack
done created complete good we'll go back
to our Cloud shell it says you can edit
your configuration file or simply do Etc
so now let's see if we can create the
cluster I assume this would create ec2
instances so the job schedule you are
using is sge this is deprecated in
future use parallel cluster well should
have told me okay there is a new version
of 301 parallel available I don't
understand because I just installed it
right we'll go back to cloud formation
just going to probably create nested
Stacks which that's what I thought it
would do n Stacks means that it's
Reliant so there's one main one and then
there's uh children stack so go here see
what resources it's creating oh whole
bunch of stuff wow so many things that
sqs Q
SNS uh network interface a Dynamo DB
table Yeah you you probably don't want
to run this you just want to watch me do
it and then we go into here it's
creating uh an ec2 volume so that's
going to be
EBS and then here we
have uh a log group I don't know why
they separated those out seem very
necessary we are waiting on the elastic
IP that always takes forever ever
creating elastic IP root instance
profile that is the IM Ru for
it that didn't take too long these these
take a long time I I never know why
create a roll it's really easy but
attaching an I am policy you're always
waiting for
those um so I'm going to just stop it
here I'll be back in a second because I
don't want to have to make you watch me
stare at the screen here okay all right
so after a really really long wait um
and it always takes some time there it
finally created I'm not sure what it's
made I mean we generally saw over here
in the outputs but usually the cost that
I'm worried about is whatever it's
launching under uc2 it might not even
have launched any servers here we're
going to take a look here see if there's
anything so we have a master and a
compute and they're T2 micros so seems
pretty safe here um this compute is not
running yet so I'm assuming that this is
like the machine machine that does the
Computing and maybe if you had multiple
machines here like that would be the
cluster where I could manage multiple
computes um I'm not particularly sure
but let's just keep going through the
tutorial and see what we can do the next
step is we need to get this pem key in
our Cloud shell here so this I don't
know where this is but what I'm going to
do is I'm going to move it to my desktop
I'm doing this off screen by the way so
I'm moving it to my desktop and then I'm
just going to go and upload the file
okay and there it is so we'll say open
and we'll say
upload and it's going to upload it here
onto this machine and I believe this is
on like uh I think this used as an EFS
instance like if you're wondering where
the storage for cloud shell is if we go
over here I think it's
EFS is
it h i don't know where it is okay maybe
it's just a maybe it's somewhere else
okay I can't remember where it is but
anyway um so
now it's created the cluster can I hit
enter here
here okay can I create a tab like if I
quit this is it going to kill it it
exited it which is I think it's fine I
don't think it stopped running and so
now if I do an LS there's my key and so
we can go back to our instructions just
have too many tabs open here drag this
all the way to the left here and so we
can try to use our key here to log in so
what I'm going to do is
go here and we'll say my HPC pm and see
if that works we'll say
yes and permission denied it is required
your private key is not accessible
that's because we have to chamod
it um um I never remember the command
anymore because I rarely SSH into
machines but if we go to
connect and we go to SSH client it'll
tell us what we need to
run chamad 400 okay so that's what we
need to do is we need to do a chamad 400
just wanted to grab that code
there okay and now if we hit up we
should SSH into the machine there we
are we are in the
instance we'll type in exit and so now
we want to run our job on this
machine and if we go back over to here I
guess we can go create our first job so
I'm just doing this in VI
and I'm going to paste that in
yep and I don't want the first line oh
okay that's perfect
great right
quit oh there's no file name hold on
here so I need to name this file
something so I'm going to say job
Dosh and we're going to paste that again
here we'll say
paste and I don't know if that's cut off
yeah it is okay great is that one okay
I don't trust that the first line is
there so what I'm going to
do is go back to our tutorial here it's
shebang SL bin SL
bash
uh this then that slash bin SL bash just
double check it looks good to me we're
going to quit that I'm just going to
make sure that it is what it we said it
is so job sh looks correct to me good
and so we'll try to run our job here so
I'm going to say
Q um job.
sh
LS and I guess it really depends on what
we decided to use when we set up that
thing I can't remember what we choose as
our Q we do Q
stat oh okay okay okay so I think the
thing is like you see how we have sge I
think that that's what we use to queue
up jobs and so we have to have that
install probably so
install configure surid
engine SG install um
Linux oh boy that looks like a lot of
work so I don't think we need to do
anything further here but as far as
understand the idea is that you're
choosing uh some kind of way to manage
these and so I'm not sure what cu Q sub
is let's go look up what that is what is
Q sub oh that is the sun grid engine
okay so how do we installed
that um I'm just going to see if we can
install it so I'm going to do I think
this is using
yum so if I do clear here
clear yum install Q sub let's see if I
can do
it s kud yum install Q sub no package
available Amazon Linux 2 Q sub because
that's probably what we're running in
Cloud
shell Q sub doesn't tell us how to
install
it that's
great so that's probably what it is and
so in order to use this we would have to
install that sun whatever whatever and
then we go through we do Q sub it would
queue it up um we could do qat cat hello
and destroy it that's pretty much all we
really need to know to understand this
um it would have been nice to queue up a
job and see it work but you know we're
getting kind of into a hairy territory
here and I think that we fundamentally
understand how this does work so what
I'm going to do is I'm going to go here
I'm going to remove the job Dosh here
and I want to destroy this
cluster um so I'm going to do pcluster
commands to figure out what all the
commands are and there's probably a
delete command so we'll go back up
here B
cluster where is our crate so we'll say
delete okay and so what that's going to
do is just tear down all the stuff
now so if we go over to cloud formation
okay and it looks like it's destroying
so yeah I'll see you here uh back in a
bit when it's all destroyed okay all
right so after a short little wait there
it has destroyed it been so long that I
uh my connection vanished but just make
sure if you did follow along for
whatever reason uh you know make sure
that the stuff is deleted and it looks
like it did not destroy uh this so I'm
going to go ahead and delete that that's
just VPC stuff so I'm not too worried
about it I know that's going to roll
back no problem and so I'm going to
consider this done so I'm going to make
my way back to the Management console
close this stuff up and we are good to
go uh for our next
[Music]
thing hey this is Andrew Brown from exam
Pro and we're taking a look at Edge and
hybrid Computing Services so what is
Edge Computing when you push your
Computing workloads outside of your
network to run close to the destination
location uh so an example would be
pushing Computing to run on phones iot
devices external servers not within your
Cloud Network
what is Hy Computing when you're able to
run workloads on both your on premise
Data Center and the a uh VPC okay so we
have a few Services here starting with
ads Outpost this is a physical rack of
servers that you can put into your data
center ads Outpost allows you to use
adus API and services uh such as ec2 WR
in your data center then we have adus
wavelength this allows you to build and
launch your applications in a telecom
data center by doing this your
applications will have ultra low latency
since they will be pushed over the 5G
Network and be closest as possible to
the end user um so they've partnered
with things like Verizon Vodaphone uh
business and a few others but those are
the two noticeable ones okay we have
VMware Cloud on AWS so this allows you
to manage on premise virtual machines
using VMware uh within ec2 instances the
data center must uh be using uh VMware
for virtualization for this to work okay
then we have AB local zones which are
Edge uh data centers Loc at outside of
the adus region so you can use adus
closer to the edge destination when you
need faster Computing storage databases
in populated areas that are outside of
AWS region you could do this there's
some other Edge offerings on AWS that
aren't listed here like sagemaker has
what's called like Neo stage maker let
you do Edge Computing with um ml but I
mean this is good enough
[Music]
okay all right so I wanted just to show
an example of edge computer
because we didn't cover it in our
generic uh compute and so there's a
variety of services that allow you to do
Edge Computing like wavelength and so um
I've never actually launched wavelength
before and I think that uh you have to
request it so if I go over to support
here again I've never done this before
but I'm sure we can figure it out pretty
easily I feel that if we create a
case um maybe it's like service
limit we type in wavelength here NOP not
there
so how do we get wavelength wavelength
AB us
request so that's what I'm looking for
here okay how do I use wavelength
AWS
whoops and sometimes what I'll do is go
to the docs here here opt into
wavelength zones before you specify
wavelength zone for resource or service
you must opt into it to opt in go to the
Adis console okay so we'll go to
ec2 and then it's going to say use the
region selector in the navigation bar to
select the region which supports your
wavelength so I know that there's stuff
in uh Us West because of Las Vegas right
or not Las Vegas but Los Angeles right
so if we go over here there's definitely
that over there on the navigation pane
on the ec2 dashboard under account
attributes select
zones okay do we see zones
here
zones oh ec2
dashboard zones let's go check here
again on the navigation pane choose ec2
dashboard we are there
right and under account attributes uh
settings account
attributes oh over here okay oh it's
here
zones and so there we have two zones and
we see switch regions to make uh zones a
different
region okay so under Zone groups turn on
wavelengths Zone groups
okay nothing there so I'm just going to
switch over to another one
here maybe
Oregon maybe uswest 2 oh look at all the
stuff we have here I've never seen these
before okay so here is the wavelength
one so that is the Los Angeles
one we can go ahead and enable this
before disabling The Zone group I'm not
sure what zone groups cost so wavelength
Zone pricing again you might just want
to watch me do this because it might
cost money um and so you might not want
to have to spend for
that
pricing uh provides mobile networks
wavelengths are available across
whatever learn about the data transfers
in price about ec2
instances okay so what's the
price if we go into
here all right so what I'm going to
suggest to use don't do this but I'm
going to do it and we're just going to
see what the experience is like okay so
I'm going to update my zone so now I
have this one we'll say enable I'm going
to assume that it has to do with like
data transfer
costs okay and uh we're going to go over
to
ec2 and we're going to go over to
instances
here we're going to launch an instance
and we're going to see if we we have
that available now I don't know if we're
restricted to particular
uh instances I assume we can launch a
Linux machine it' be really weird if we
couldn't you know we'll go over to
configuration and what we want to do is
choose uh the zone so how do we do it so
once it's turned on confirmation confirm
it configure your network so create a
VPC create a carrier Gateway so you can
connect your resources into the VPC to
the telecommunication Network holy
smokes This is
complicated but it's just kind of
interesting to see like the process
right
you know it's not for our use case but
uh carrier Gateway
right and as I do this I always check up
all the costs here so I say carrier
Gateway pricing AWS because maybe that's
where the price
is okay if you don't get a pricing page
then usually that's hard to say
logically isolated virtual
networks again it's not telling me
what um to use carrier you need to opt
into at least one one wavelength Zone
but I did
right and sometimes what happens is that
it just takes time for the optin to to
go so go here manage the Zone settings
that was a lot easier way so we have one
it's we're opted in right here
okay
and okay we'll we'll go here again if
that one didn't work
um we can try so I guess these are all
the regions Denver things like
that can I opt opt into this one opt
in it's not super exciting like all
we're going to do is launch an ec2
instance but you know we'll go through
the process here a
bit and I don't know why I can't create
one so we'll go back over to the
instructions
here crate so you can connect so create
a route table using the VPC to the route
table so I think that's as far as we're
going to get here because I'm not seeing
any options here but the idea was that
we would have to create a carrier
Gateway we'd update our route tables and
all we would be doing is launching an
ec2 instance so you know it's no
different than launching it you just
choose a different subnet so I think
you'd have to create a subnet for that
zone and launch it in there and that
would be Edge Computing another example
of edge Computing would be something
like via cloudfront which we have uh
these um Edge functions or not Edge
functions yeah functions here and so
these are functions that are deploy to
cloudfront
so my cloudfront
function and these would be deployed to
um Edge locations right and all you can
use here is Javascript so here's an
example of one and um I'm fine with this
development live this function is not
published we'll go to
test test the function it's
good publish publish that function and
so
the advantage of this is that you know
if you have functions that are in it was
Lambda there's a chance of cold start um
whereas if they're deployed on the edge
here there's still probably a cold start
but it's going to be a lot faster
because it's a lot closer to the edge
location so um you know it's just a
different uh different cases but yeah
there was one where we're launching ec2
workload into wavelength which we
couldn't complete which is totally fine
and then we have these functions on the
edge there's other uh Edge Computing ser
like within Sage maker you can deploy I
think it's called like Neo sagemaker and
then for iot devices those are obviously
on the edge so you can deploy those as
well uh but generally that gives you an
idea of edge Computing
[Music]
okay hey it's Andrew Brown from exam Pro
and we're looking at cost and capacity
management Computing Services so before
we talk about them let's define what is
cost management so this is how do we
save money and we have capacity
management how do we meet the demand of
traffic and use usages through adding or
upgrading servers so let's get to it the
first are the different types of EC
pricing models so you got spot instances
reserved instances saving plans these
are ways to save on Computing by paying
up in full or partially or by committing
to a yearly contract or multi-year
contract uh or by being flexible about
the availability Interruption to
Computing Services we have adus batch so
this plans schedules and executes your
batch computer workloads across the full
range of adist computing Services which
can utilize spot instances to save money
we have aist compute Optimizer so
suggest how to reduce cost and improve
performance by using machine learning to
analyze uh you uh your previous usage
history we have ec2 auto scan groups so
asgs these automatically add or remove
ec2 servers to meet the current demand
all of traffic they will save you money
and meet capacity since you only run the
amount of servers you need then we have
elb so elastic load balcer so this
distributes traffic to multiple
instances we can reroute traffic from
unhealthy instances to healthy instances
and can Route traffic to ec2 instances
running in different availability zones
and then we have elastic beant stock
here which is easy for deploying web
applications without developers having
to worry about setting up and
understanding the underlying ad Services
similar to Heroku it's a platform as a
service so not all of these are about
cost some of them are about capacity
management like elb um but yeah yeah
there you
[Music]
go hey this is Andrew Brown from exam
Pro and we are looking at the types of
storage services and no matter what
cloud service provider you're using
they're usually broken down into these
three where we have blocks file and um
uh object okay so let's take a look at
the first so this is going to be for
Block storage so for AWS this is called
elastic Block store data is split into
evenly split blocks directly accessed by
the operating system and supports only a
single right volume so imagine you have
an application over here and that
application is using a virtual machine
that has a specific operating system and
then it has a drive mounted to it uh
could be using FC or uh scuzzy here um
but the idea here is when you need a
virtual Drive attached to your VM is
when you're going to be using block okay
the next one here is for um file or it's
just basically a file system so this is
Aus elastic file storage so the file is
stored with data and metadata
multiple connections via a network share
supports multiple reads writes locks the
file so over here uh we could have an
application but it doesn't necessarily
have to be an application and so it's
using NASA exports as the means to uh
communicate and so the protocols here
can be NFS or SMB which are very common
uh file system protocols and so the idea
here is when you need a file share where
multiple users or VMS need to access the
same drive so this is pretty common
where you might have multiple virtual
machines and you just want to act as
like one uh Drive uh one example that
could be like let's say you're running a
Minecraft server you're only allowed to
have one world on a particular single
drive but you want to be able to have
multiple virtual machines to maximize
that compute that'd be a case for that
um so there you go then the last one
here is like object storage and so fors
this is called Amazon simple storage
service or also known as S3 so object is
stored with data metadata and a unique
ID scales with limmited uh uh with
limited no file limit or storage limit
so there's really very there's very
little limit to this it just basically
scales up supports multiple reads and
wrs so there are no locks and so the
protocol here we're going to be using
htps and API so when you just want to
upload files and not have to worry about
the underlying infrastructure not
intended for high uh I op so input and
outputs per seconds okay so depending on
how fast you have to do your read and
wrs are going to determine uh you know
whether you're going uh this direction
or the other way um or you know how many
need to actually connect at at the same
time and whether it has to be connected
as a mount drive to the virtual machine
[Music]
okay hey it's Andrew Brown from exam Pro
and we're going to do a short
introduction into S3 because on the
certified Cloud practitioner they ask
you a little bit more than they used to
and so we need to be a bit familiar with
S3 because it is um at least I think
that Abus considers its Flagship uh
storage
service and it really is one of the
earliest Services it was the second one
ever launched okay so what is object
storage or object based storage so data
storage architecture that manages data
as objects as opposed to other storage
architectures so file systems where uh
these are others right so which manages
data as files and a hierarchy and block
storage which manages data as blocks
with with ins sectors and tracks that
get stored on an actual uh drive and so
uh the idea here is we have S3 which
provides basically unlimited storage you
don't need to think about the underlying
infrastructure the S3 console provides
interface for you to upload and access
your data okay so we have the concept of
S3 object so objects contain your data
they are like files but objects may
consist of a key this is the name of the
object a value the data itself made up
of a sequence of bytes the version ID
when versioning enabled the version of
the object metadata additional
information attached to the object and
then you have your S3 buckets so buckets
hold objects buckets can also have
folders which in turn hold objects S3 is
a universal name space so bucket names
must be unique it's like having a domain
name okay and one other interesting
thing is an individual object can be
between Z bytes and up to 5 terabytes so
you have unlimited storage but you can't
have uh files of uh incredible size uh I
mean 5 terabytes is a lot but nothing
beyond that for a single file but just
understand that you can actually have a
zerob byte file uh and for like
associate certifications that can be a
an actual question so that's why it's
[Music]
there all right let's take a look at S3
storage classes um and so for the
certified Cloud practitioner we need to
know generally what these are for
associate levels we need more detail
than we have here but let's get through
it so adus offers a range of S3 storage
classes that trade retrieval time
accessibility durability for cheaper
storage and so the farther down we go
here the more cost effective uh it
should get uh pending uh you know
certain conditions okay so when you put
something into S3 it's going to go into
the standard uh tier the default tier
here and this is uh incredibly fast it
has
99.99% availability 119 durability and
it's replicated across 3 azs and so uh
you know we have this cheaper meter here
here on the left hand side and that
would apply this is very expensive and
it's not actually expensive but it is
expensive at scale when you can uh
better optimize it with these other
tiers so just understand that um then
you have the S3 intellig tiering so this
uses ml to analyze objects and usage and
determine the appropriate storage class
dat is moveed to the most cost effective
access tier without any performance
impact or added overhead then you have
S3 standard IIA which stands for
infrequent access this is just as fast
as S3 standard but it's cheaper if you
access the files less than once a month
there's going to be an additional
retrieval fee applied so if you do try
to retrieve data as frequently as S3
standard it's going to actually end up
costing you more so you don't want to do
that okay then you have S3 one zone IIA
so as it says it's running in a single
zone so it's as fast as S3 standard but
it's going to have lowered availability
but you're going to save money okay
there is one caveat though your data
could get destroyed because it's
remaining in a single uh a so if that a
or data centers um suffer a catastrophe
you're not going to have a duplicate of
your data to retrieve it okay um and
then you have S3 Glacier so for
long-term clothed storage retrieval of
data can take minutes to hour
but it's very very very cheap and then
you have S3 Glacier uh deep archive
which is the lowest cost storage class
but the data retrieval is 12 hours and
so you know um all of these here to here
these are all going to be in the same uh
a S3 console or Amazon S3 console S3
Glacier is basically like its own
service but it's part of S3 so kind of
lives in this weird State there's one
here that we didn't have on the list
here which is S3 outputs because it has
its own storage class and doesn't
exactly fit well into um this kind of
linear cheaper uh thing here
[Music]
okay hey it's Andrew Brown from exam Pro
and we are taking a look at the ous snow
family so this is storage and compute
devices used to physically move data in
or out of the cloud when moving data
over the Internet or Prov private
connection that is too slow difficult or
costly so we have snow cone snowball
Edge and snow mobile and so there
originally was just snowball and and
then they came out with snowball Edge uh
and Edge introduced Edge Computing
that's why there's Edge in the name but
pretty much all of these devices have
Edge Computing uh and they do
individually come with some variant so
with the snowball a snow cone it comes
in two sizes where it has 8 terabyt of
usable storage and then there's one with
14 terabytes of usable storage for
snowball Edge it technically has like
four versions but I'm going to break it
down to two for you we have storage
optimized where we have 80 terab of you
um uh of usable storage there and then
compute optimize
3.9.5 terab and even though it's not
here you get a lot of vcpus and
increased memory which could be very
important if you need to do Edge
Computing before you send that over to
AWS and then last here we have
snowmobile which can store up to 100
pedabytes of storage um in the uh
Associates I cover these in a lot more
detail because there's so much more
about these like the security of them
how they're tamper proof like how they
have networking buil in the the
connection to them but you know for this
exam that's just too much information um
you just need to know that there are
three uh three ones in the family and
generally what the sizes are and that
they're going to be all placed into
Amazon S3 what's interesting is that you
know snowmobile only does 100 pedabytes
but adabs markets it as you can move
exabytes of of um content because you
can order more than one of these devices
so they'll market it saying like
snowball EDG is when you want to move
pedabytes of data and snowball mobile is
when you want to move exabytes but you
can see that a single thing isn't in the
exabyte it's just in the petabyte
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at all the
itaba storage services in brief here so
let's get to it so the first is simple
storage service S3 this is a seress
object storage service you can upload
very large files and an unlimited amount
of files you pay for what you store you
don't worry about the unine file system
or upgrading the dis size you have S3
Glacier this is a cold storage service
it's designed as a lowcost storage
solution for archiving and long-term
backup it uses previous generation uh
HDD drives to get that low cost and it's
highly secure and durable we have
elastic Block store EBS this is a
persistent block storage service it is a
virtual hard drive in the cloud and you
attach to ec2 instances you can choose
different kinds of hard drives so SSD
iops SSD throughput HDD and um a cold
hhd okay we have elastic file storage so
EFS it is a cloud native NFS file system
service so file storage uh you can mount
to multiple ec2 instances at the same
time when you need to share files
between multiple servers we have storage
Gateway this is a hybrid cloud storage
service that extends your on premise
storage to the cloud we got three
offerings here file Gateway so extend
your local storage to Amazon S3 volume
Gateway caches your local drive to S3 so
you have a continuous backup of the
local files in the cloud tape Gateway so
stores files onto virtual tapes for
backing up your files on very
costeffective long-term storage we got
one more page here cuz there's a lot of
services here we have adab us snow
family so these are storage devices used
to physically migrate large amounts of
data to the cloud and so we have
snowball and snowball Edge these are
briefcase size data storage devices
between 50 to 80 terabytes I don't
believe snowball is available anymore
it's just snowball Edge uh but it's good
to have all of them in here so we can
see what's going on we have snowmobile
this is a cargo container filled with
racks of storage and compute that is
transported a semi trailer tractor truck
to transfer up to 100 pedabytes of data
per trailer I don't think we're going to
be ordering that anytime soon cuz that's
pretty darn expensive but that's cool we
have snow cone this is a very small
version of snowball that can transfer 8
terabytes of data we have adab us backup
a fully managed backup service that
makes it easy to centralize and automate
the backup of data across multiple a
services so ec2 EBS RDS TB EFS storage
Gateway you create the backup plans we
have Cloud endure disaster recovery so
continuously replicates your machine in
a lowcost staging area in your target
abl's account and preferred region
enabling fast and reliable recovery in
case of it data center failures we have
Amazon FSX this is a feature Rich and
highly performant file system that can
be used for uh windows so that would be
using SMB or Linux which uses luster and
so there we have the Amazon FS FSX for
Windows file server so use SMB protocol
and allow you to mount FSX to Windows
servers and then the luster one which
uses uh Linux luster file system it
allows you to mount F FSX Linux servers
are there any storage Services missing
here not really I mean you could count
elastic container repository as one but
um that's kind of something else or you
could also count maybe um uh code commit
but you know I kind of put those in a
separate category where we where those
are in our developer tools or our
containers
[Music]
okay all right so what I want to do is
show you around S3 so we'll make our way
up here and type in
S3 and we'll let it load here and what
we're going to do is create a new bucket
if you do not see the screen just click
on the side here go to buckets and we'll
create ourselves a new bucket so bucket
names are unique so let say my
bucket and we'll just pound in a bunch
of numbers I'm sure you're getting used
to making buckets in this um in this
course so
far um so if we scroll on down notice
that it says block public access
settings for this bucket this is turned
on uh like the blocking is turned on by
default because S3 buckets are the
number one thing that are a point of
entry for malicious actors where people
leave their buckets open so if we want
to uh Grant access to this bucket for
people to see this publically we'd have
to turn this off okay but for now we're
going to leave that on you can version
things in buckets which is pretty cool
you can turn on encryption which you
should turn on by default and use the
Amazon S3 key on the certified Cloud
partitioner it's going to ask you about
client side encryption and server side
encryption so you definitely want to
know what these are I'm going to turn it
off for the time being so we can kind of
explore uh here by oursel here um then
there's object lock so we can lock files
so that um you know they're you know
people aren't writing to them multiple
times so we'll go ahead and create a
bucket and it's very quick so here is
the new bucket we made and you'll notice
we have nothing here which is totally
fine if I go to
properties um you know we can see that
uh we can turn on Buck conversing turn
on encryption what I'm going to do is
I'm going to go grab some files I
remember I saved uh some files recently
here I'm just going to make a new folder
called Star Trek I just have some
graphics you can pull anything off the
internet you want to do this
yourself U but I'm just going to prepare
a folder here it'll take me a
moment
okay just a
moment okay great so now I have my
folder prepared and so what I want to do
is upload my first file so I can go here
in upload and actually I can upload
multiple files you can even add a folder
which is nice and so in here if I want
to upload these files here whoops I'll
just select multiples I'll hit open
it'll cue them up which is really nice
we can see the destination details here
if we want to turn it versioning on we
could there uh we could apply
permissions for outside access but we
have uh things turned on but what's
really important is the properties where
we have these different tiers and So
based on the tier that you use the lower
you go at least it should be the cheaper
it's going to get uh but it's going to
have some tradeoffs and we cover that
through the course then there's that
server side encryption um and I'm going
to hit upload we'll just individually
turn it on so you're going to see this
progress go across the top these have
all been uploaded I'm going to cck click
on my destination bucket and so what we
can do is we can uh open these if
they're images they'll show us right
here in the browser we can download them
so if we need to get them again all
right we can create a folder here and
just say Star Trek or Enterprise
D Enterprise D
here okay but it's not really easy it's
not like I can drag this into there um I
might be able there's no move option so
you'd actually have to copy it into the
destination and then delete the old one
it's not like using a file system you
know um there's a lot more work involved
but you know it's a great storage
solution um so let's look at concpt so I
have this selected here if I click into
it I can go to permissions I can go to
versions see that I'm looking for H
encryption here we go so if I turn it on
I can enable encryption and I can choose
whether I want to use an Amazon S3 key
so
SS S3 so an encryption key that Amazon
S3 creates manages and uses for you then
you have
ibus SS KMS and I believe this uses AES
up here which is totally fine then you
you have KMS down here and it's
interesting because they're like ads
will manage the key for you and then
this one ads will manage the key for you
it's just slightly different this one of
course is a lot simpler there's not many
reasons not to turn on encryption but U
I'm going to go turn this one so that it
is encrypted
here and just because it's encrypted
doesn't mean we can't access the file I
can still download it I can still view
it because databus is going to decrypt
it right so if I go and click on this
one and I say open okay even though it's
encrypted I can still view it right it
just means that it's encrypted on the
storage right so if somebody were to
steal that hard drive whatever hard
drive it's sitting on on ads if they
didn't figure it out it's encrypted
they're not going to be able to open up
the file right so that is the logic
there but through here um I can get it
something that's really interesting with
um um S3 is the ability to um uh have
life cycle events so I'm just kind of
looking where that is it's usually in
the bucket so if I go to management up
here I can set up a life cycle rule and
what I can do is say like move this to
deep
storage okay and then I can say what it
is that I want to filter so maybe it's
like data.
jpg I can say apply to all objects in
the bucket I acknowledge that and we say
move current versions of objects between
storage classes and I checkbox that on
and I can say move them to Glacier after
30 days I think if I go lower it'll
complain uh probably when I save there
and so the idea is that we can move
things and distort some so maybe you
have files coming in down below it's
showing you here right so a file is
uploaded and then after 30 days then
move them in the glacier so we save
money okay that's a big advantage of S3
there's a lot of things going on in S3
here like you can turn on
um uh wherever it is you can turn
on web hosting so you can turn this into
like a website down below here there's a
whole whole bunch of things that you can
do okay so uh we're not going to get
into that because that's just too much
work but uh you know we learned the
basics of S3 so what I want to do to
delete this I have to empty it first
watch it'll be like you cannot delete it
you need to empty the bucket first so go
ahead and empty it and I'll say my
bucket
empty or sorry I guess I have to type in
permanently
delete
[Music]
Perman delete no they used to oh yeah I
can copy it okay great and so once the
bucket is is emptied I can go back to
the
bucket and I'll go back one layer and
then I'll go ahead and delete my
bucket and you can only have so many
buckets I think it's like a 100 you have
like a 100
buckets how many buckets can you have in
a
WS 100 buckets yeah I was
right and I think if you wanted to know
how many you Pro there's probably like a
service limits page service limits
service
quotas so you go here you say a Services
S3 how many buckets 100 right there okay
so you know that gives you kind of an
idea what's going on there but there you
go that's
S3 all right so let's go take a look at
elastic Block store which is uh virtual
hard drives for ec2 so what I'm going to
do is make my way over to the ec2
console because that is where it's at
and on the left hand side if we scroll
on down you'll see elastic block volumes
or elastic Block store volumes and so we
can go here and the idea is we can go
ahead and create ourselves a volume and
what you'll notice is that we have a few
different options here we have general
purpose provisioned iops cold HDD
throughput optimized magnetic magnetic
being um basically like uh physical tape
that you can use to back up like the old
school stuff and so you have all these
options here and you can choose the size
so when you change these options you're
going to notice that some things are
going to change like the through uh
throughput or iops so notice that
general purpose is fixed at between 300
to 3,000 and notice that it goes from 1
Gigabyte to how many ever that is that's
a lot there and so it's not too
complicated but in practicality I don't
really create volumes this way what I do
is I'll just go launch an ec2 instance
so I'll say launch ec2 instance and
we'll choose Amazon lytic 2 and again
you know if we haven't done the uc2 uh
follow along we'll cover all this stuff
in more detail don't worry about about
it um we go to configure instance then
we go to add storage and this is what
you're going to be doing when adding EBS
volumes um to your ec2 instances and
you'll notice we always have a root
volume that's attached to the ec2
instance that we cannot remove we can
change the size up here I believe the oh
it shows us right here that we have up
to 30 gigabytes so sometimes you might
want to Max that out to take advantage
of the free tier you notice we can also
change uh this there might be some
limitations in terms of the root volume
so notice that we have a few more
options here we can have a cold HDD or
HDD as our root volume uh notice we have
a delete on termination so EBS volume
persists independently from the running
life so you can choose to automatically
delete uh EBS volume when the associated
instance is terminated so if you take
this off if the ec2 instance is deleted
the volume will still remain which could
be something that's important to you uh
for encryption here um you might want to
turn it on and so generally adus always
has a KMS manage key which is free so
you checkbox that on it will be
encrypted uh you can turn it on later um
but you can never turn encryption off
but you should always uh turn encryption
on and so just be aware to turn that on
you can also add file systems down below
here but maybe we'll talk about that
later because I think that gets
into um e EFS okay so that is a
different type of file storage there but
that's pretty much all there is to it uh
you just go ahead and create uh your
volume there and then it would show up
under EBS we could take snapshots of
them to back them up that goes to S3 but
that's all we really need to know here
[Music]
okay all right let's take a look at
elastic file uh system or EFS uh storage
manage file storage what does EFS stand
for EFS system elastic file system okay
sorry and so what we can do is go ahead
and create a file system here so I'm
going to say my EFS and the great thing
is that it basically a serverless so
it's only going to be white you consume
right so what you store and what you
consume um and I think that's what it's
going to be based on we have to choose a
VPC I want to launch it in my default
VPC and we have the choice of regional
or one zone um I guess this is going to
be based on what gets backed up to S3
possibly so onezone probably is more
cost effective but I'm going to choose
Regional and that's a new Option I never
noticed before I just opened it up to
see a few more things here we have
General Max IO bursting provision things
like that we'll hit next
we'll choose our
azs and uh then you might have to set up
a policy so I'm going to hit next here
you'll go ahead and hit create so you
know this is really interesting but the
trick to it is really mounting it to a
dc2 instance and that's kind of the pain
okay so if we go into this um you have
to mount it and there are commands for
it so like EFS mounting Linux
commands Okay
I've done this in my Solutions architect
associate uh but you know again I'm not
doing on a regular basis so I don't
remember and so if we go here I'm just
trying to see if we can see some code
that tells us how to mount it so
mounting on an E2 uh uh ec2 Linux
instance with the EFS Mount helper um so
I don't know if they had that before but
that sounds interesting so pseudo Mount
hyphen T the file system the EFS
mounting
Point yeah this looks a lot easier I
than what we had before okay so before I
had to enter a bunch of weird commands
but now looks like they've boiled it
down to single command but once you have
your EFS
instance
um I'm going to assume that there is an
entry point here just clicking around
here seeing what we can see I would
imagine we have to create an access
point so my access
point sure I don't know if it's going to
let me just do that it did and so I
would imagine that you probably use an
access point let's go back here ifs
Mount point I think that's the same
thing I think the mount point and the
access point you create access points
and that's what you use uh we can go
here we can attach it so oh yeah here's
the command
so um Mount via DNS or Mount via IP
address
so it doesn't look too hard we can try
to give it a go I haven't done it in a
while it looks like they've made it
easier so maybe we'll try it out okay so
we go to ec2 here
and I'm going to launch an instance I'm
going to choose Amazon
2 okay we're going to go and choose that
and then we want to choose a file
system and
so it's going to mount to here okay and
storage is fine all this is fine and I'm
going to go ahead and launch
this and and I need a new key pair so
create a new key pair um this will be
for EFS example
okay we're going to download that key
pair there we're going to launch this
instance okay and then we're going to go
view this and as that is launching what
I'm going to do is open up my cloud
shell and I'm going to want to upload
this pen so again like before I'm going
to drag it to my desktop off screen and
then what I'm going to do is upload this
file so I have
it EFS example okay we're going to
upload
it because I just want to see if we can
access that EFS volume and so if I do
LS that's our old one which I can delete
by the way I'm never going to use that
anytime soon
yes LS and I'm going just delete the
hello text there so it's a bit cleaner
for what we're doing and so we need to
chod that
400 uh EFS
example and we saw that's how like if
you want to try to connect to a server
remotely that's what you do right so I
believe that the drive is
mounted if I go to storage does it show
up
here doesn't show up under
here but
um what we're waiting for are these two
status checks to pass and then we can
SSH into this machine
and I'm just going to go back here and
take a look here so using the EFS Mount
helper so pseudo Mount hyphen T EFS TLS
this volume to EFS and so I imagine it's
going to mount it to EFS here using the
NFS client so I guess it just depends on
what we're going to have available to us
even if the status checks haven't passed
I'm going to try to get into this
anyway um so what we can do is click on
this grab the public IP address we'll
type in
SSH uh ec2 hyphen user at sign paste
this in hyphen I EFS example pem I
usually don't log in Via
SSH um but you know just for this
example I will and so I want to see if
this drive
exists usually be under mount right
there it is okay so it already mounted
for us so I can do touch hello
world. text
say pseudo
here I can say pseudo VI I'm going to
open up the file and say hello from
another computer
okay and so I've saved that file and
what I want to do
now
oops oh okay sorry I'm in the cloud
shell here but what I want to do now is
I want to kill this
machine okay and what I'm going to do is
spin up another ec2 instance I'm going
to see if when I mount that if that file
is there if it actually worked but wow
that is so much easier than before I
can't tell you how hard it was to attach
an EFS volume the last time I did it um
so we'll go ahead We'll add that and the
storage is fine we're going to go to
review here we're going to say launch
and I'm just going to stick with the
same key pair
there we're going to give that moment to
launch and we're going to go to view
instances and so now this one is
launching as it's launching let's just
go peek around and see what we can see
so you know I imagine if we didn't add
that file system during the the boot um
and we were we're adding it after the
fact we probably could just ran that
line and added it really easily um I'm
not going to bother testing that because
I just don't want to go through that
trouble to do that um I still can't
remember what these access points are
for um but uh it's okay let kind of out
of the scope for the certified Cloud
practitioner and then so I'm just
curious so we get some nice monitoring
here right so that's kind of nice
um I guess they're trying to suggest
here like anabis backup data sync
transfer so that would just be backing
up simplify uh automates accelerates
moving data okay that's pretty
straightforward transfer family fully
managed F SFTP okay so nothing exciting
there and we're going to refresh that
there and this is initializing so let's
go see if we can connect to this one so
I'm going to go ahead grab that public
IP address I'm going to hit up
okay I'm going to swap out that IP
address and we're going to see if we can
connect to that machine yet so we'll say
yes and we got into it so that's great
and so what I'm going to do is go again
into the mount directory EFS FS1 LS and
there it is I'm going to do cat hello
world and so it works and so that's the
cool thing about DFS is that you have a
a file system that you can share among
other um uh ec2 instances I'm sure users
could connect to it using the NFS
protocol I'm not the best at like
Network or storage networking so I'm not
going to show that here to you today but
that gives you a general idea of how EFS
works again you only pay for what you
store it is serverless so we'll go here
and type delete CU I'm done with this
I'll probably uh destroy the instance
first it doesn't get mixed
up and just so we clean up a little bit
better here I'm going to delete these
Keys
here
delete okay and we'll go ahead and
delete this one as
well delete since I'm done with
that uh we'll make sure that that is
tearing down that is good and we'll make
our way back over here and it says enter
probably the ID's name in so we'll enter
that in and we hit
confirm and we'll see is it deleting I'm
not confident with it I'm going to do it
one more time confirm it by entering the
the file systems ID so we'll put it in
again
is it destroying I cannot tell there we
go so it's destroying we are in good
shape it is gone our data is gone um but
yeah that is
[Music]
EFS all right let's take a look at um
the snow family in ads so if we type in
snow up here and we click into ads snow
family this is where we can probably
order ourselves a device um I might not
be able to order them at least when I
originally looked at this like way back
in the day uh it wasn't available in
Canada so I'm kind of curious to see
what there is but the idea is that
you're going to go here and Order and
you have some options so you can import
into S3 or export from S3 and then down
below we have local compute storage so
perform local comput storage workloads
without transferring data you can order
multiple devices and clusters for
increased durability and storage
capacity so it sounds like you're not
you're not um transferring data you're
just using it uh locally on uh to um
it's like basically buying renting
temporary computers was just kind of
interesting I never saw that option
before but we're going to choose import
into ads3 and we're just going to read
through this stuff and it's not my
expectation that we're going to even be
able to submit a job here and you
probably don't want to because it's
going to cost money but I just want to
show you the process so we can see what
there is here so snow job assistance if
you're new to snow family run a pilot of
one to two devices so batch file smaller
than 1 Megabyte Benchmark and optimize
deploy St uh staging
workstations discover remediate
environment m al uh issues early files
and folders name must conform to Amazon
S3 prepare your Ami once the pilot is
completed confirm the number of snow
family devices that you can copy devices
to simultaneously follow the best
practices use the following resources to
manage your snow devices so we have iTab
us openhub and then there's the edge
client
CLI so openhub is a graphical user
interface you can use to manage snow
devices so that's kind of cool and then
we have the CLI which I imagine is
something that's very useful to use so
just close those off here and then we
have other things so I'm going to say I
acknowledge I know what I'm doing which
I don't really but that's okay and then
here we are going to enter in our
address so we say Andrew Brown and I'm
not going to I'm not going to enter this
in for real just whatever so it would be
Toronto exam Pro um Canada oh see so
there's there's the thing you can only
ship it to the US and so that's as far
as I can get okay um and that's the
thing is like if you really want to know
ad us inside and not you got to be in
the US but let's pretend that we do have
an address in the states what's a very
famous address so what is the address of
the White House
okay there it
is so I'm just going to copy that
in because again we're not going to
submit this for real I just want to see
what's farther down the line here
okay uh what's NW
is that the state it's in Washington
right is is this part of it NW Northwest
is that a thing I'm from Canada so I
couldn't tell you um so we'll go down
here and we have Washington do we have a
second address line it doesn't look like
it
um we have a zip code I believe this is
the zip
code and do we need a phone number looks
like we do
416 uh 111 11111 okay okay we have one
day or two day shipping why not just
have one right and so then we can choose
our type of device so we have snow cone
snow cone SSD snow cone optimized I'm
surprised I never took a screenshot of
this earlier um compute optimized things
like that so you can choose which one
you want it looks like we're going to
see some different options but we'll go
with snow cone my snow
cone and snow cones do not ship with a
power supply or ethernet cable snow cone
devices are powered by by 45 wat CB C uh
USBC power supply I'll provide my own
power supply and cable do not ship with
a power supply re cable that's fine uh
snow con Wireless snow con connect to
your wireless connection connect the
buckets you want there's a bucket we
created
earlier Computing use comp using ec2
instances use a device as a mobile data
center by loading ec2 Ami so here's an
Ami that I might want to
use uh ad iot green validated Ami not
interested in Remote device management
you can use Ops Hub or Etc to monitor
reboot your device that's fine and so
then we need to choose our security
key I don't know if we'll have to set
the service R we'll see what happens
here and uh we'll let it update that's
fine and so then I guess we just hit
create job and so I don't really want to
order one um so I'm not going to hit
that button and also it's going to go to
the White House and they're going to be
like Andrew Brown why did you do that so
that's not something I feel like doing
today but at least that gives you an
idea of that process there and I imagine
that uh if you go the other way it's
going to be pretty similar you know it's
just like same stuff I think uh so you
it saved that address it's not a real
address and the the options are a little
bit uh limited here and it's like NFS
Bas S3 base so it's slightly different
but it's basically the same process just
curious we'll take a look at the last
one
there since there are three options just
curious okay Sim similar thing okay so
yeah that's pretty much all I want you
to know about um the snow family and
that's about it
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at what is
a database so a database is a data store
that stores semi-structured and
structured data and just to emphasize a
bit more a database stores more complex
data stores because it requires using
formal design and modeling techniques so
databases can generally be categorized
as either being relational so structured
data that strongly represents tabular
data so we're talking about tables rows
and columns so there's a concept of row
oriented or colum oriented and then we
have non relational databases so these
are semi-structured that may or may not
distinctly resemble tabular data so here
is a very uh simple example the idea is
that you might use some kind of language
like SQL put in your database and you'll
get back out tables for relational
databases let's just talk about some of
the functionality that these databases
have so they can be uh using a special
specialized language to uh query so
retrieve data so in this case SQL
specialized modeling strategies to
optimize retrieval for different use
cases uh more fine-tune control over the
transformation of the data into useful
data structures or reports and normally
a database infers uh someone is usually
using a a relational row oriented data
dat store so um you know just understand
that when people say database that's
usually what they're talking about like
postgress MySQL relational row store is
usually the default but uh obviously
there's a lot more broader terms there
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at what is
a data warehouse so it's a relational
data store designed for analytical
workloads which is generally column
oriented data store okay so companies
will have terab and millions of rows of
data and they'll need a fast way to be
able to produce analytics reports so
data warehouses generally perform
aggregation so aggregation is the idea
of grouping data together so find a
total or an average uh and data
warehouses are optimized around columns
since they need to quickly aggregate
column data and so here is kind of a
diagram of um a data warehouse and so
the idea is that it could be ingesting
data uh from a regular database here I'm
just getting out my pen tool so it could
be regular database or it be coming from
a different data source that isn't
compatible in terms of the schema and
you use like ETL or El or ETL to get
that data into uh that data warehouse so
data warehouses are generally designed
uh to be hot so hot means that they can
return queries very fast even though
they have vast amounts of data data
warehouses are infrequently accessed
meaning they aren't intended for
real-time reporting but maybe once or
twice a day uh or once a week to
generate business and uh user reports of
course it's going to vary based B on the
um the service that is offering the data
warehouse a data warehouse needs to
consume data from a relational database
on a regular basis and again it can
consume it from other places but you'll
have to transform it to get it in there
[Music]
okay hey this is Andrew Brown from exam
Pro and we're taking a look at a key
value store so a key value store or
database is a type of non-relational
database or nosql that uses a simple key
Value method to store data and so key
value stores are dumb and fast uh but
they generally lack features like
relationships indexes aggregation of
course there are going to be providers
out there have managed solutions that
might uh poly fill some of those uh
issues there but I want to show you the
underlying way that key value stores
work to kind to kind of distinguish them
between document stores so a key value
stores literally a unique key alongside
a value and the reason I'm representing
that as zeros and ones is because I want
you to understand that that's what it is
it's basically just some kind of of
there and how the key value uh store
interprets it is going to determine what
it is so when you look at a document
database that is just a key value store
that uh uh interprets the value as being
documents right and so key value stores
can and do commonly store um uh multiple
uh like an associate array that's pretty
common so even for Dynamo DB that's how
it does it and so that's why when you
look at a key Value Store it looks like
it uh a a table but it's not actually a
table it's schema list because
underneath it's really just um you know
that associative array and so that's why
you can have uh columns or sorry rows
that have uh different amounts of
columns okay so due to the design they
are able to scale very well beyond a
relational database and they can kind of
work like a relational database without
all the bells and whistles so hopefully
you know that makes sense okay
[Music]
all right let's take a look at document
stores so a document store is a nosql
database that stores documents as its
primary data structure and a document
could be an XML uh type of uh structure
but it also could be something like Json
or Json like document stores are
subclasses of key value stores uh and
the components of a document store are
very uh comparable to relational
databases so just kind of an example
here where in a relational database
that' be called tables now you have
collections they were called rows now
they're called documents you had columns
they had Fields they may have indexes
and then joins might be called embedding
and linking so you can translate that
knowledge over uh you know they they're
not as um they don't have the same kind
of feature set as a relational database
but you have better scalability and
honestly document stores are just key
value stores with some additional
features built on top of it
[Music]
okay hey it's Andre Brown from exam Pro
and we're going to take a look at the
nosql database services that are
available on AWS so we have Dynamo DB
which is a serverless nosql key value
and document database it is designed to
scale to billions of records with
guaranteed consistent data returned in
at least a second you do not have to
worry about managing shards and Dynamo
DB is ad's Flagship database service
meaning whenever we think of a database
service that just scales is cost
effective and very fast we should think
of Dynamo DB and in 2019 Amazon the
online shopping retail shut down their
last Oracle database and completed their
migration to Dynamo DB so they had 7,500
Oracle databases with 75 pedabytes of
data and with Dynamo DB they reduce that
cost by 60% and reduced the latency by
40% so that's kind of to be like a
testimonial between relational and a no
escal database so when we want a
massively scalable database that is what
we want Dynamo db4 and I really just
want to put that there because it if you
remember that you're going to always be
able to pass uh or get those questions
right on the exam okay then we have
document DB so this is a nol document
database that is mongod DB compatible uh
so mongodb is very popular no SC among
developers there were open source
licensing issues around using open
source mongodb so ad us got around it by
just building their own mongodb database
basically so when you want a mongod DB
like database you're going to be using
document DB we have Amazon key spaces
this is a fully managed apoe Cassandra
database so Cassandra is an open source
nosql key value database similar to
Dynamo DB that is column or store
database but has some additional
functionality so when you want to use
Apache Cassandra you're using Amazon key
[Music]
spaces hey this is Andrew Brown from
exam Pro and we are taking a look at
relational database Services starting
with relational database service RDS and
this is a relational database service
that supports multiple SQL engines so
relational is synomous with SQL and
online transactional processing
oltp and relational databases are the
most commonly used type of database
among tech companies and startups just
because they're so easy to use I use
them I love them um RDS supports the
following SQL engines we first have
MySQL so this is the most popular open
source SQL database uh and it was
purchased and is now owned by Oracle uh
and there's an interesting story there
because when Oracle purchased it they
weren't supposed to have it Mario DB was
or sorry myell was sold to Oracle Sun
systems and then within the year um uh
Oracle purchased it from them and the
original creators never wanted it to go
to Oracle um just because of their uh
the way they do licensing and things
like that and so um the original
creators came back and they decided to
fork myql and then maintain it as Mario
DB just so that uh you know oracle never
kind of pushed away the most popular um
database so that everyone had to go to a
p solution then you have postest so psql
as it's commonly known is the most
popular open source SQL database among
developers this is the one I like to use
because it has so many Rich features
over my SQL uh but but it does come with
added complexity then Oracle has its own
SQL proprietary database which is well
used by Enterprise companies but you
have to buy a license to use it then you
have Microsoft SQL so Microsoft's
proprietary SQL database and with this
one you have to buy a license to use it
uh then you have Aurora so this is a
fully managed database uh and there's a
lot more to uh going on here with Aurora
so we'll talk about it it almost acts as
a separate service but it is powered by
RDS so aora is a fully managed database
of either myell so five times faster or
postest SQL three times faster database
so when you want a highly available
durable and scalable and secure
relational database for post mqu you
want to use Aurora uh then you have
Aurora serverless so this is a ser on
demand version of Aurora so when you
want the most of the benefits of Aurora
but you can trade uh off to have cold
starts or you don't have lots of traffic
or demand uh this is a way you can use
Aurora in a serverless way then you have
RDS on VMware so this allows you to
deploy RDS supported engines to on
premise data centers uh the data center
must be using VMware for Server
virtualization so when you want
databases managed by RDS on your own
database Center uh and yeah I realize
that this is a small spelling mistake so
say just on here but yeah there you
[Music]
go hey this is Andrew Brown from exam
Pro and we're looking at the other
database services that abos has because
there's just a few loose ones here so
let's talk about redshift so it is a
petabyte size data warehouse and data
warehouses uh are for online analytical
processing oap and data warehouses can
be expensive because they are keeping
data hot meaning that they can run a
very complex query and a large amount of
data and get that data back very fast so
when you need to quickly generate
analytics or reports from a large amount
of data you're going to be using red
shift then you have elastic cache so
this is a managed database of an
inmemory and caching open source
databases such as reddis or memcache so
when you need to improve the performance
of an application by adding a caching
layer in front of your web servers or
database you're going to be using
elastic cache then you have Neptune this
is a managed graph database the data is
represented as interconnected nodes I
believe that it uses gremlin is the way
to interface with it which is no
surprise because that's what it looks
like most class providers are using so
when you need to understand the
connections between data so mapping
fraud Rings or social media
relationships uh very relational
database heavy information you're going
to want to use Neptune we have Amazon
time streams it's a fully managed time
series database so think of devices that
send lots of data that are
time-sensitive such as iot devices so
when you need to measure how things
change over time we have Amazon Quantum
Ledger database this is a fully managed
uh Ledger database that provides
transparent immutable cryptographically
variable transaction logs so when you
need to record a history of financial
activities that can be trusted and the
last one here is database migration
service DMS it's not a database per se
but it's a migration service so you can
migrate from on premise database to
adabs from two databases in different or
same adus accounts using different SQL
engines and from an SQL to a nosql
database and I'm pretty sure we cover
this in a bit uh greater detail in this
course
[Music]
okay all right let's go take a look at
Dynamo DB uh which is ad's nosql
database so we'll go over to Dynamo
DB and what we'll do is create ourselves
a new table and we'll just say my Dynamo
DB table and you always have to choose a
partition key you don't necessarily have
to have a sort key but it could be
something
like um like you want it to be really
unique so it could be like email and
this one could be created at
right and so we have string binary
notice that the the types are very
simple then for settings we have default
settings or customized settings so the
default is use provision capacity mode
rewrite five rules Etc custom no
secondary indexes use KMS so I'm going
to just expand that to see what I'm
looking at we have two options here on
demand uh so simplify billing by paying
the actual reads and rights you use or
provisioned which is this is where you
get a guarantee of performance so if you
want to be able to do you know whatever
it is a thousand I don't know what it
goes up to but like a thousand read
writes per second then that's what
you're paying for okay you're paying for
being a having a guarantee of that um of
that capacity okay I'm not going to
create any secondary indexes but that's
just like another way to uh look at data
notice down below that we have a cost of
$2.1 uh then we have encryption at rest
so you can do owned by Amazon Dynamo DB
that's pretty much the same as like adab
us has or S3
has ssse S3 there you could use uh CM
actually I guess both of these are
probably KMS I would imagine we'll go
ahead and create the table
here and let going to create the table
this is usually really really
fast we'll go here and what we can do is
is insert some data so as it's just
starting up here we can go over
to our tables they recently changed this
UI so that's why I look a bit
confused U view items up here okay and
then from here we can create an item so
I can add something to say so Andrew
exampro doco and
2021
uh well we'll just do the future so
let's say 20
25 505 I don't want to have to think too
hard here but we can add additional
information so I can say like uh today
true we could say
um make like a
list uh you know food and then I could
go here and then add a
string it is not working oh there we go
there we are so we could say like um
banana and then we could say pizza right
we can go ahead and create that
item and so now that item is in our
database uh we can do a scan that will
return all items we can query we can
actually have uh some limitations of
what we're choosing there's the party Q
editor so we can use SQL to select it um
I have not used this
before party Q um AWS or party Q Dynamo
DB
examples I'm hoping I can just find like
an example of some of the language
getting started here I don't need to I
don't need an explanation I just show me
an example query here and I will I'll
get to it
here okay so here's some examples right
so maybe we can give this a
go um so we have our table here so my
Dynamo DB
table and I just want the email that
back we don't need a
wear we'll run this see if it
works there we go I'm not sure if we
could select additional data there so I
know that we had some other things like
uh
food there it is okay so that's really
nice um addition to it dynb can stream
things into a Dynamo DB stream to go to
and do a lot of fun things so there all
sorts of things you can do with Dynamo
DB but um I'm pretty much done with this
so I'm going to go ahead and delete this
table and notice that it also created
some cloudwatch alarms so we want to
delete those as well create a backup no
we do not care go ahead and delete
that and that is Dynamo
[Music]
DB okay so now I want to show you uh RDS
or relational database service so go to
the top here type in RDS and we'll make
our way over there and so RDS is great
because it allows us to launch
relational databases um sometimes the UI
is slow I'm not sure why it's taking so
long to load today but every day is a
bit different and so what we're going to
do is go ahead and create a new database
uh you're going to notice that we're
going to have the option between
creating a standard or easy I stick with
standard just because I don't like how
easy hides a lot of stuff from us even
here like it says two cents per hour but
it's not giving us the full cost so I
really don't trust it because if you go
down here and you chose their Dev test
here look it's like $100 it's not
showing the the cost preview right now
maybe because we didn't choose the
database type sorry I wanted to choose
postgress but before we do that let's
look at the engine types we have Amazon
Aurora so we have between MySQL and
postgress MySQL Marb postgress Oracle
microsoftsql notice for Microsoft SQL it
comes with a license you don't have to
do anything with that it might change
based on the addition
here uh nope comes with a license for
all them which is great if you want to
bring your own license that's where you
need a dedicated host right running uh
Microsoft SQL for Oracle uh you have to
bring your own license that's going to
be based on um importing with the Abus
license manager if we go over to postest
which is what I like to use uh we're
going to set it to Dev test to try to
get the cheapest cost scroll down look
$118 we can get it cheaper we get super
cheap so here are the password going to
be testing 1 2 3 a capital on the T so
and an explanation mark on the end okay
because it has a bunch of requirements
of what it wants here I want a T2 micro
so I'm just going to scroll down
here what is going on here standard oh
look M classes I don't want an m class I
want a burstable class that's the cheap
ones and so we go here can we still do a
T2 micro or is it now
T3 so I don't see
T2 so I imagine a T3 micro must be the
new itus free tier so we go it free tier
here right and if I go
to
databases um RDS on the t2 micro 750
hours but I can't select
it
so I'm going to assume that the T3 micro
must be the new tier if it's not there
right unless it's saying include
previous
generations and then maybe I can see it
then okay so I don't see it
there I really don't like how they've
changed this on
me okay so the oldest I can choose is a
T3 micro which is fine I just I just
know T2 being the free tier that's all
uh this is fine we don't want Auto
scaling turned on for our example here
we do not want a multi-az so do not
create a standby that's going to really
jump up our cost we don't need Public
Access it will create a VPC that is fine
password authentication is fine we have
to go in here which I don't know why
they just don't keep that expanded
because you always have to come in here
name your database so my database we
choose our postest version here I'm
going to turn backups off uh because if
we
don't if we don't it's going to take
forever to launch this thing encryption
is turned on you can turn it off but
generally it's not recom commended we
can have performance insights turned on
I'm going to turn the retention oh we'll
leave it to 7 days cuz we can't turn
that off we don't need enhanced
monitoring so I'm just going to turn
that
off and uh that's fine we're not going
to enable delete protection here and so
we are good we can now go ahead and
create our
database and what we'll do here is wait
for that database to be created so the
thing is is like
if we're doing the solutions architect
or the developer social stuff I'd
actually show you how to connect to the
database um it's not that hard to do
like you just have to connect uh grab
all the database information so it's
going to have an endpoint a port stuff
like that and you use something like
table Plus or something to connect to
the database but that's out of scope of
the certified Cloud partitioner I'm just
going through the motions to show you
that you can create an RDS database very
easily but not how to connect to it and
actually utilize it okay and so that
would spin up and we would have a server
and after that we can just go ahead and
delete the server here so I just say
delete me
okay and that's all there really is to
it there is the special type of um
database like Aurora doesn't have its
own like console page it's part of RDS
so if you want to spend up Aurora you
just choose the compatibility you want
you can choose between provisioned or
serverless um and serverless is supposed
to be really good for um scaling to zero
cost so that's something there so you
fill that all out but the initial cost
is a lot more expensive you can't choose
a T2 micro here um unless it lets you
now it is
for provision it's
uh oh T2 T3 medium is the smallest you
can go okay so if you reach to the point
where using a a mediumsized database
then you might consider moving over to
Aurora just because it's going to be
highly scalable Etc like that um so
that's a consider there there's also
something called Babble fish um that us
announced last year when I when I shot
this um or when I'm shooting this as of
now and the idea was to make it
compatible with myql SQL Server to
migrate over to Aurora post SQL which is
kind of interesting um but that's about
it so if our database is destroying I
think it is just going to go back over
here to
RDS it's taking a long time to load
today
and uh I think it's already deleted
maybe we go to databases here it's
deleting so I'm confident it's going to
delete so there we
[Music]
go all right let's take a look at Red
shift so red shift is a data warehouse
and it's generally really expensive so
it's not something that you're going to
want to launch uh dayto day here but
let's see how far we can get with it um
just by running through it so what we'll
do is go ahead and create a cluster and
again you can just watch me do this you
don't have to create uh you don't have
to create one yourself uh so free trial
configure for learning that sounds good
to me uh is free for limited time if
your organization has never created a
cluster I rarely ever create these so
when the trial ends delete your cluster
to avoid the charges of on demand okay
that sounds fair um so here we're going
to have two vpcu it's going to launch a
d a
dc2
large so it's look that up for
pricing show me prices please please
please
um I think it's loading right here okay
so I don't know how much it is but I
know it is not cheap and down below we
have sample data is loaded into your red
shift cluster that sounds good to me
ticket is the sample data
okay ticket sample
data red shift I just imagine they
probably have like a tutorial for it
here
they do right
here and so because I want to know what
we need to do to query it right if we
can even query it via the interface here
so the admin user is adus user um and
the password is going to be capital T
testing 1 2 3 4 5 6 exclamation and
we'll hit create
cluster oh cool we can create the data
right in here so that's what I wasn't
sure about whether we would be able to
just query it in line because before
you'd have to use Java with j jdbc or
odbc driver and download the jar and
it's not as fun as it sounds of course
but it looks like we can query data once
the data is
loaded so that looks really good I guess
we can pull data in from um the
marketplace so that's looks pretty nice
too and I guess we could probably
integrate into other things like quick
site because you probably want to adjust
your data over
there again I usually don't spend a lot
of time in red shift um but looks like
it's a lot easier to use very impressed
with this so I don't know how long it
takes to uh launch a red shift cluster I
mean it is 160 GB uh of of of storage
there it's uh even at the smallest it's
pretty large so what I'm going to do is
to stop the video and I'll be back when
this is done
okay okay so after a short little wait
here um it was a lot faster than I was
expecting but uh it's available and so
looks like here it says to query the
sample data use red shift version two so
I'm going to click that and I'm sure
there's tons of buttons to get here and
it'd be great if it just populated the
query for me um it doesn't but this
looks really nice really nice UI I
wonder if it has like some existing
queries no that's okay so what I'm going
to do here is I'm going to go ahead and
pull out this query and see if we can
get this to work here never found out
what those prices were
though okay and what we'll do is hit run
I like how there's like a limit of 100
but here it has that so we'll go ahead
and hit run and see what data we get so
relation sales does not
exist okay
so what's going on
here um we'll go up here so most of the
examples in the red shift documentation
uses uh a sample database called ticket
this sample this small database consists
of seven tables you can load the ticket
data set by following the this
here okay so to load the sample data
from Amazon S
3 okay
so I would have thought it already had
the data in there I could have swore it
would
have
Dev
public
tables zero
tables okay so I don't think there's any
data in here and so we're going to have
to load it ourselves
I really thought it would have added it
for us uh but let's go ahead and create
these tables and see if this is as easy
as we think so run that create that
table cool okay we got it down
here we'll run that we'll just run each
at a
time I think there's seven of them so
date already exists okay that's fine
event already exists saying all these
tables
exist maybe I just wasn't
patient
okay
um interesting all right so maybe we'll
go back and uh run that query maybe we
just had to wait a little while for that
data to
load run
okay so you know what I think it was
doing this for us like if if if it did
not create it for us we would have to go
through all these steps which is fine
because we're learning a little bit
about um uh red shift but um uh it looks
like we just had to wait there so it
looks like you would run those you
download that you use the copy command
to bring it over there um it looks like
you can do all of this via the uh this
interface here and we've done a queries
that's kind of
cool um I imagine you probably could
like save it or export it what if we
chart it what happens okay you can chart
it that's kind of
fun can we export it out to oh just we
can save it I thought maybe it could
export out to Quick site but I I suppose
you'd rebuild it in quick site a but
yeah I guess that's it right there so
that's pretty darn simple so what I'm
going to do is make my way back over to
Red shift because we are done for this
example and we will go over to clusters
here and I'm going to go ahead
and delete my
cluster
delete create file snap shot
nope
delete delete the cluster there we go so
I'm pretty sure that will succeed no
problem there and we are done with red
shift and red shift is super expensive
so just make sure that thing deletes
okay
[Music]
hey this is Andrew Brown from exam Pro
and we are taking a look here at Cloud
native networking Services um and so I
have this architectural diagram I
created which has a lot of networking
components uh when people create
networking diagrams for AWS they don't
always include all these things here
even though they're there so we're just
being a little bit verbo so you can see
okay the first thing is our VPC our
virtual private Cloud this is a
logically isolated section of the aabus
cloud where you can launch adus
resources that's where your uh resources
are going to reside not all services uh
require you to select a VPC uh because
they're managed by AWS but I wouldn't be
surprised if under the hood they are in
their own VPC Okay then if you want uh
the internet to reach your services
you're going to need an internet gateway
um then you need to figure out a way to
Route things to your uh various subnets
and that's where route tables uh come in
then we need to Def Define a region that
it's going to be which is a geographical
location on your network then you have
your availability zones which are
basically your data centers where your
A's resources are going to reside then
you have subnets which is a logical
partition of an IP network into multiple
smaller Network segments um and these
pretty much map to your uh availability
zones if you're making one per a and
then we have knackles these act as a
firewall at the subnet level then we
have security groups that act as a
firewall at the instance level so
hopefully that gives you a good overview
[Music]
okay all right so now let's take a look
at Enterprise or hybrid networking so we
have our on premise uh environment or
your private cloud and then we have our
ads account or our public Cloud so
there's a couple Services here that we
can Bridge them together the first is
ADS virtual private Network VPN it's a
secure connection between on premise
remote offices and mobile employees then
you have direct connect this is a
dedicated gigabit connection from on
premise data center to adabs so it's a
very fast connection a lot of times the
direct we say it's a a private
connection but doesn't necessarily mean
secure it's not encrypting uh the data
in transit so very commonly these
services are used together not just
singular okay um and then uh we have
private links and so this is where you
already uh are using ads but you want to
keep it all within ads never going out
to the internet okay so these are
generally called VPC interface endpoints
and then the marketing Pages call them
private links which is a bit confusing
but you know it just keeps traffic
within the aabus network so it does not
transverse out to the internet okay
[Music]
hey this is Andrew Brown from exam Pro
and we are taking a look at vpcs and
subnets so a VPC is a logically isolated
section of the adus network where you
launch your adus resources and you
choose a range of ips using a cider
range so a cider range is an IP address
followed by uh this uh net mask or sub
submask that's going to determine how
many IP addresses there are um and
there's a bunch of math behind that
which we're not going to get into um but
anyway so here is an architectural
diagram just showing a VPC with a couple
subnets so subnets is a logical
partition of an IP network into multiple
uh smaller Network segments and so
you're essentially breaking up your IP
ranges for vpcs into smaller networks so
just thinking about cutting up a pie
okay so subnets need to have a smaller
cider range uh to uh the vpcs represent
for their portion so uh 424 is actually
smaller which is interesting the the
higher the gets the smaller it gets and
so this would allocate 256 IP addresses
and so that's well smaller than 16 okay
we have the concept of a public subnet
so this is one that can reach the
internet and a private subnet the one
that cannot reach the internet and um
these are not uh strictly enforced by
AWS so the idea is that when you have a
subnet you can just say don't by default
assign publicly assignable IP addresses
but it's totally possible to launch an
ec2 instance into your priv private
subnet and then turn on um uh the IP
address so you got to do other things to
ensure that they stay private or public
[Music]
okay hey it's Andrew Brown from exam Pro
and we are comparing security groups
versus knackles so I have this nice
architectural diagram that has both
knackles and security groups in them and
we'll just kind of talk about these two
so knackles stand for network access
control list and they act as a virtual
firewall at the subnet level and so here
you can create an allow uh and deny
rules and this is really useful if you
want to block a specific IP address
known for abuse and and I'm going to
just kind of um compare that against
security groups because that's going to
be a very important difference okay so
secur security groups act as a firewall
at the instance level and they
implicitly deny all traffic so you
create only allow rules so you can allow
an E2 instance to access port on uh Port
22 for SSH but you cannot block a single
IP address and the reason I say that is
because in order for you to block a
single IP address in Security Group you
would literally have to block or you
literally have to allow everything but
that IP address and that's just not
feasible okay and so if you can remember
that one particular example you'll
always be able to remember the
difference between these two one other
thing that um adab us likes to do is is
ask which ones are stateless which ones
are stateful but at the uh Cloud
partitioner level they're not going to
be asking that
[Music]
okay all right all right let's learn a
bit about U networking with AWS so what
I want you to do is go to the top and
type in VPC which stands for virtual
private cloud and what we'll do is set
up our own VPC it's not so important
that you remember all the little bit of
details but you get through this so that
you can remember the major components so
what I'll do is create a new VPC I'm
going to call this my
VPC uh tutorial and here I'm going to
say
10.0.0.0 for sl16 the reason you're
wondering why I'm doing that if we go to
xyx
y z here um this tells you the size of
it so I go here and I put 16 so you can
see we have a lot of room if we do
24 it takes up it it it's smaller see so
this is basically the size of it right
the empty blocks over here so we're
going to have a lot of room so we do 10
006 we don't need IPv6 we're going to go
ahead and create that and once we have
that we can go ahead and create a subnet
which we will need so we're going to
choose our VPC we'll go down here and
say my Subnet tutorial
and we'll choose the first a z you can
leave it blank and'll choose it random
and then we need to choose a block that
is smaller than the current one so 16
would be definitely um uh well 16 is the
size that we have now so we can match
that size but 10.0.0.0 424 would be
absolutely smaller okay so we go ahead
and create that
subnet and so that is all set up now um
let's see if our route tables hooked up
so our route table says where it links
to and it says to local so it's not
going anywhere and that's because we
need to attach a u internet gateway that
allows us to reach the internet so if we
go over here and create a new internet
gateway we'll say myig
GW and we'll go ahead and create
that and what we'll do is associate that
with our VPC we created here okay and so
now that we have the internet gateway
attached we want that subnet to make its
way out to the Internet so if we go to
the route table we can edit the uh route
table Association here I like how it
keeps on showing me this as if I don't
know what I'm doing um but I do and
so this would change that particular
Association but I want to add to that
route table so I thought when I clicked
that it would allow me to add more but
apparently I got to go to Route tables
over
here and I'm looking for the one that is
ours we can see that it's over here we
could even name it if we wanted to like
my rote
table Noti then we apply uh uh U names
it's actually just applying a tag see
over here it's always what that
is so we'll go over to routes and we
want to edit the routes and we want to
add a route and we want this to go to 00
and we're going to choose the internet
gateway
okay we're going to say save
changes and what that's going to allow
us to do is to reach the internet
um and so what I want to do is go back
to subnet I was just curious about this
I've never used this
before um so looks like we can just
choose some options here I'm not too
concerned about that but I assume like
that's used for debugging azure's had
those kind of services for a long time
and so it was has been starting to add
those so you can easily debug your
network which is nice so we have a
subnet the subnet uh can reach the
internet because there's a there's um
uh internet gateway and it's hooked up
via the route table one thing that
matters is will it assign a public IP
address um so that is something that we
might want to look into it's not the
default subnet which is totally fine so
it says Auto assign is no so that might
be something that you might want to
change so here we go to edit the r table
Association no it's not there they
changed it on me used to be part of the
uh setup instructions us to just
checkbox it now they moved it modify the
auto assign so we'll say enable so that
means it's always going to give it a
public IP address on
launch and while we're here I'm just
going to double check if I have any
elastic IPS I did not release okay just
double checking here and
so this is all set up and we should be
able to launch a um ec2 now within our
our new VPC so I'll go over here to
ec2
okay and I'm going to launch a new
instance
say Amazon elix
2 we're going to choose this tier
Here and Now what we should be able to
do is Select
that and that is our subnet there
okay go ahead and launch that I don't
care if we use a key whatsoever so I'm
going to go ahead and launch that
there okay we'll go
back and so there you go it is launching
so we created our VPC and we launched uh
in it no problem
whatsoever so hopefully that is pretty
darn
clear um so yeah uh what I'm going to do
is I'm going to let that launch because
I want to show you security groups So
within AWS you can set security groups
and
knackles and that's going to allow or
deny access based on stuff and when we
launch this eccu instance it has a
default security group that was assigned
we could have created a new one but what
I might want to do is create myself a
new Security Group
here okay and you can end up with a lot
really fast like here is a bunch and I
can't even tell what's what so like
there's Bunch for load balancers and
things like that and so I might just go
ahead and delete a bunch of these
because I cannot tell what is going on
here and um we'll delete these security
groups and sometimes they won't let you
delete them because they're associated
with something like a network interface
or
something all right but um we need to
find out which one we're using right now
so the one that we are using is the
launch wizard 4 so we'll go into
here and I don't know if you can rename
them after they've been created I don't
think so which is kind of frustrating
because if you want to rename it it's
like I don't want that to be the name so
what's interesting is you can go here
and you can edit the
routes uh the rules sorry the inbound
rules and the outbound rules and so here
it's open on Port 22 so that allows us
to ssh in we could drop this down and
choose different things so if we want
people to access a website we go Port 80
and we say from anywhere ipv 46 so now
anyone can access it um you might want
to do something like give it access to
postgress that runs on Port 5432 things
like that um could be something else
like maybe you need to connect to Red
shift that's on that Port you can go
ahead and save those rules we're just
going to say uh from anywhere you can
say my IP so maybe only I'm allowed to
connect to it right so you added inbound
rules you don't really ever have to
touch outbound rules it's set for all
traffic so it's stuff that's
leaving uh the that there one
interesting thing to note about uh
security groups is
that you don't have a deny option right
so let's say you only wanted a
particular IP address you only wanted um
let's say what's my IP my IP
address so that is my IP address and
let's
say I wanted to block it right so I go
here and I say okay I want to
block on all TCP I want to block this
number right but I can't do that all I
can say is I allow this number so in
order to do it I would have to enter
everything but this number in here and
you can enter ranges in with like these
forward slashes and stuff like that but
You' imagine that'd be really hard
because you have to start and go like
you'd have to start and go through every
single IP address in the world to get it
out of here and that's almost impossible
and that's the key thing I want you to
remember about security
groups um so that's security groups and
there's also
knackles knackles um they're associated
with subnets so they probably show up
under VPC I rarely touch knackles rarely
ever have
to um I mean they're great tools but you
know for me I I just don't ever need
them so knackles are associated with
subnets so we can go here and try to see
my Subnet tutorial so we created our
subnet we got a knle for free and we can
set inbound and outbound rules and so
here here is where we could say Okay I
want to add a new rule and I want to and
I want to make the rule number
150 you always do these in hundreds okay
or the power of tens so that you can
move them around easily and I can say
all traffic that comes from this IP
address I'm going to put the forward SL
Z that just means a single IP address I
say deny right and so now
uh this my address I can't access that
ec2 instance okay if I try to go there's
nothing running on the server but if I
was to try to use it I wouldn't be able
to do it and and this applies to
anything for that subnet it's not for a
particular instance it's for anything in
that subnet so hopefully that is is
pretty clear there um but that's pretty
much all you really need to know I mean
there's lots of other stuff like Network
firewalls all these other things it gets
pretty
complicated um it's well beyond what we
need to learn here but uh what we'll do
is tear down that ec2 instance
okay we'll terminate
that and once that instance is destroyed
we can get rid of our security group and
a bunch of other
stuff and there's always a bunch of
these darn
things so we'll say
delete one Security Group
Associated so we go here this is the one
we are using but I want to get rid of
all these other ones
okay if I go here it could be because
like of inbound
rules so see this one because you can
reference another Security Group within
a security group so I'm just going to go
save that there say any my IP there
whoops it's set to n uh NFS so that
might have been set up for our access
point or I can just delete delet it that
would probably be
easier okay so that's one that's kind of
of a
pain so I'm just looking for rules that
might be referencing other security
groups to get rid of
them okay let's try this
again we go ahead and delete I'm leaving
the um
I'm leaving the uh the defaults alone
because those come with your vpcs and
you don't want to get rid of
those so it won't let me delete this one
so I'm going to go edit that
rule delete it save it you might not
have this kind of clean up to do it's
just might be me here you
know um outbound
inbound let's try this again here
delete and I'll open this one
up must be this one that is referencing
the other
one just going to delete the
rule and this is something that's just
kind of frustrating with AWS but it's
just just how it is where sometimes it's
hard to get rid of resources because you
have to click through stuff so it's not
always a clean you might have like
lingering resources and this isn't going
to cost us anything but it's just the
fact that
um that it just makes things harder to
see what you're doing you
know this last one really doesn't want
to go
away so I'm just trying to delete all
the rules out of here get rid of it
can I delete this one
now one group Associated it will not
show me what it's talking about okay
here it
is
um okay this is referencing
it I think it was the one there was an
old one I don't know what this
is we'll go down here
and we'll go here and delete that and
while I've been cleaning all these up
now we can go over to our inst instance
make sure that it's terminated it is
good because if our instance is not
terminated we cannot destroy the VPC uh
prior the VPC could not be destroyed
unless you detach the internet gateway I
wonder if it's going to still complain
about
that we'll say yes it actually looks
like it includes it in the
cleanup type delete here
there we go so we're all good we're all
cleaned up there you
[Music]
are hey this is angre Brown from exam
Pro and in this video I just want to
show you cloudfront so let's make our
way over to cloudfront cloudfront is a
Content delivery Network and it's used
to cash your data all over the place as
you can see I have some older ones here
if you have a splash screen what you can
do is just look for the left hand side
there might be hamburger menu open that
up and then click on distributions and
what we're going to do is create a new
distribution if you don't want to create
one cuz these do take forever to create
um you can just kind of watch along I
don't even feel like I'm going to hit
the um the create distribution button
because I just hate waiting for so long
but the idea is that you have to choose
an origin and so the origin could be
something like an S3 bucket load bouncer
media store this is where um the the
content distribution network is going to
Source its content right so if I say
this bucket here um and I just it will
probably default to the root path the
idea is that it's going to be able to
pull content from there and then cach it
everywhere and then down below you can
say okay set the type of protocol
redirect to here you can set up uh
caching rules or like how often do you
want it to uh cash like cash a lot don't
cash a lot the great thing is like you
have these Edge or these um Lambda Edge
functions so you can uh read and modify
the request and response to the CDN
which is very powerful but what I'm
going to do is I'm just going to go look
at what we already have cuz again I said
said they take forever to spin up and
we're not going to see too much if we do
so once it's spun up um this is what it
looks like so you'll have an origin it
says where it's pointed to you can
create multiple Origins group them uh
you can modify your behavior so that was
basically what we're looking at before
as you can see we have our Behavior
there nothing super exciting we can set
up error Pages you can restrict based on
geographical location so if you're for
whatever reason if you if you're not
allowed to serve content in UK you could
say exclude this geographical region
right so you have an allow list or a
block list saying like Okay we can't do
UK because like let's say you just don't
want to do um say England you don't want
to do um uh gdpr for whatever reason you
could block out I don't know why I'm
having a hard time here Britain England
it's England right United Kingdom there
we go so you just say okay forget United
Kingdom I don't have to do GDP now uh
for invalidations the idea is that you
know it is a cash so things can get
stale or just persist and so here you
can just type in say I want to get rid
of image.jpg and then you create that in
validation and then it will go delete it
out of the cache and so the next time
someone requests they'll get the the
fresh content this usually doesn't take
that long but that's pretty much
cloudfront in a nutshell
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at ec2 also
known as elastic cloud and so this is a
highly uh configurable virtual server or
it's also known as a virtual machine and
that's what we're going to generally
refer to it uh ec2 is resizable compute
capacity it takes minutes to launch new
instances and anything and everything on
adus uses ec2 instances underneath
that's why we generally call it the
backbone to all the adus services and uh
you're going to just have to choose a
few options here so the first thing
you'll need to do is choose your OS via
your Amazon machine image so that's
where you get red hat Ubuntu Windows
Amazon Linux Seuss it might also come
with pre-installed libraries and things
like that then you're going to choose
your instance type that's going to
determine things like your vcpus your
memory so here you can see how many
there are and you'll have like a monthly
cost and that's the name of the instance
type then you have to add storage so
very commonly you're attaching elastic
block storage or elastic files system or
service uh and so you know if you do
choose your EBS uh you are going to have
to determine what type it is so whether
it's a solid state drive a hard disk
drive a Magnetic Tape or even attaching
multiple volumes not just a single one
and the last thing is configuring your
instance so this is configuring the
security groups the key pairs user data
IM roles placement groups all sorts of
things so we will experience in that
because we will show you how to launch
it easy to instance and it'll make a lot
of sense if it does not make sense right
now
[Music]
okay all right let's take a look here at
ec2 instance families so what are
instance families well instance families
are different combinations of CPU memory
storage and networking capacity and
instance families allow you to choose
the appropriate combination of capacity
to meet your application's unique
requirements different instance families
are different because of the varying
Hardware used to give them their unique
properties and we do talk about this
thing about uh capacity reservation
where adus can actually run out of a
particular type of instance family
because they just don't have enough
Hardware in that data center and so you
have to reserve it but let's go through
the different types of instance families
the first is general purpose and these
are the names of the different families
uh very popular ones is the t2 um the t2
and one that's really interesting is the
Mac which actually allows you to run um
a a Mac server so these are great
balance of compute memory and network
resources so you're going to be using
these most of the time the use cases
here would be web servers code
repositories things like that then you
have compute optimize so um they all
start with C no surprise there they're
ideal for compute bound applications
that benefit from high performance
processor their edge cases here are
scientific modeling dedicated gaming
servers ad server engines things like
that then you have memory optimized um
and so there's a variety here these are
fast performance for workloads that
process large data sets in memory um
they're great for in-memory caches
in-memory databases real-time big data
analytics then you have accelerated
optimize so this is your P2 P3 P4 things
like that these are Hardware
accelerators or co-processors these are
great for machine learning computational
Finance seismic analysis speech
recognition if you're doing um uh ML on
AWS you're you'll start coming across
these types ads technically has a
separate page on sagemaker ML machines
but they're all pulling from these
instance families okay then you have
storage optimize so I3 i3en things like
that these are highly High sequential
read and write access to very large data
sets on local storage the use cases here
would be nosql in memory or
transactional databases data warehousing
for the certified Cloud practitioner you
just need to generally know these five
categories not the names of the instance
families if you're doing um Associates
or above you definitely want to know
these things in a bit more detail and I
want to say that commonly instance
families are called instance types but
an instance type is a combination of
size and family but even aws's
documentation doesn't make this family
distinction clear but I know this
because you know an Azure they make that
very clear and and gcp and so I'm bring
that language over here to just kind of
normalize it for you okay
[Music]
let's take a look at what ec2 instance
types are so an instance type is a
particular instance size and instance
family and a common pattern for instance
sizes you'll see is things like Nano
micro small uh medium large x large 2x
large 4X large 8X large and you know
generally they're to the power of twos
but sometimes it'll be like 12 14 16 or
it's even uh and so when you you go to
launch your ec2 instance you're going to
have to choose that instance type and so
here you can see you know here is our T2
micro and then we have um the small the
medium the large the x large okay but
there are exceptions to this pattern for
sizes so you know there is one
particular one called uh metal and so
that's going to indicate that this is a
bare metal machine and then sometimes
you get these Oddball ones like 9x large
so you know the rule of power of two or
even numbers is not always the case uh
but generally it'll be pretty even for
you know the start here okay uh just
talking about instant sizes so the E2
instant sizes generally double in price
and attribute so uh just bringing up
these numbers a little bit closer
starting at the small here you're going
to notice one two doesn't maybe double
there but four and here we see 12 24 uh
almost doubles there almost doubles
there but I want to I show you that the
price is generally almost double so 16
33 67 135 and so a lot of times like you
always have the option to say okay do I
want to go to the next instance size up
or have uh an additional instance of the
same size and sometimes it's a better
approach to get an additional instance
because then you can distribute it
across another a uh but then you also
meet additional capacity so there you
[Music]
go so we talked about dedicated
instances and hosts a little bit but
let's just make that distinction very
clear so dedicated hosts are single
tenant easy to instances designed to let
you bring your own license so Bol based
on machine characteristics and so we'll
compare the dedicated instance to the
dedicated host across isolation billing
uh physical characteristics visibility
Affinity between a host and instance
targeted instance placement automatic
instance placement and add capacity
using allocation request so for
isolation for dedicated instance you're
going to get instance isolation so you
can have the same customer on the same
physical machine but there is
virtualization there for them and
there's a guarantee of that um for a
dedicated host you have physical server
isolation so you get the whole server
for billing uh on a dedicated instance
it's per instance billing and it's going
to have an additional fee of $2 per
region and for dedicated host it's per
host building so it's a lot more
expensive but you get the whole machine
uh for visibility of physical
characteristics you're not going to get
any of that information for dedicated
instance for dedicated host you are such
as sockets core host host ID and this is
really important when you have a bring
your own license and they're saying this
license is for x amount of cores or x
amount of sockets then we have Affinity
so there's no affinity for dedicated
instance for dedicated hosts you'll have
consistency with deploys to the same
instance to the same physical server uh
there's no control of Target instance
placement for dedicated instance you do
have control on a dedicated host for
auto automatic instance placements you
have it for both and to add capacity
using allocation requests it's a no for
dedic at instance and it's a yes for
dedicated host so I want to come back to
the main point that's what's highlighted
here is that on a dedicated host you
have visibility of sockets core host ID
and this is really really important when
you're bringing your own licensed byol
such as um you know Microsoft SQL
servers where you have to specify the
manacor and things like that
[Music]
okay so we've been talking about uh
tendency and I just want to make it very
clear uh the difference between the
different levels of tendency on AWS so
we have three okay so we got dedicated
host so your server lives here and you
have control of the physical attribute
so basically the whole server okay uh
then we have dedicated instances so your
server is on the same uh physical
machine as other customers but the
actual slot that you have the dedicated
instance will always be the same uh and
then we have uh the default so your
instance will live somewhere on the
server uh and when you reboot it it's
going to be somewhere else so there's no
guarantee that it's going to be in the
same place every single time
[Music]
okay hey this is Andrew Brown from exam
Pro and in this follow along we're going
to be looking at ec2 and also um
services that are adjacent to it so like
autoscaling groups load balancers
elastic IPS things like that so we fully
understand ec2 um you don't have to know
tons for the exam but you should be able
to go through the motions of this with
me me so that you can cement that
knowledge um for some of those deeper
Concepts like working with key Pairs and
things like that so let's make our way
over to the ec2 console and learn what
we can learn um and generally when you
go to the ec2 console it'll bring you to
the dashboard for whatever reason it
didn't bring me there and then the idea
here is that on the left hand side we
can make our way over to
instances okay and this is where we can
launch our first instance so if we go
here and launch our instance the first
thing we're going to be presented with
is to choose our Ami or Amazon machine
image and so that is a template that
contains the software configuration so
the operating system applications and
other binaries that would be installed
on that OS by default all right and so
we have a variety that we can choose
from in the quick starts and generally
the ones that you're going to see first
are the ones that ad support so there
are uh um Amis or operating systems that
a ads will support when you contact them
and then there's ones that are outside
that where uh they'll still help you
with but they might not have the
knowledge on so just understand that if
you pick from these core ones you're
going to be in good shape uh the most
popular is the Amazon Linux 2 because
it's part of the free tier and it is is
very minimal and well hardened by AWS so
it's a very good choice there but you
can see you can install a bunch of
things so like if you want to launch a
Mac OS server you can absolutely do that
a red hat uh Suzie Ubuntu a Windows
Server you name it they have it um if
you wanted something more farther out
there you can go to the market
Marketplace and uh subscribe to one that
is managed by company basically
everything exists Under the Sun here or
you could get a community Ami so these
are ones that are contributed by the
community for free but we're going to go
back to quick start here and what I want
you to notice is that there is this Ami
ID that's how we can uniquely identify
what we are using if we were to change
region even with the same Amazon L 2
instance this thing will change so just
understand that it is regional based and
it comes in a 64-bit variant and a arm
variant and so we're going to be using
the x86 here you can notice here you can
change it on the right hand side we're
going to stick with x86 I'm going to go
ahead and hit next so now we're going to
choose our instance type and so this is
going to decide um uh greatly how much
we're going to be spending because the
larger it is the more we're going to
spend so see this T2 micro if we want to
know the pricing for that we go to
ec2 pricing
AWS and once we get to ec2
pricing we want to go to on
demand and from here this will
load and so down below we can kind of go
find our price it should show
us should show us the list ah here it is
okay so I can say a T2
micro and we can see the On Demand is
this so it seems really cheap what you
got to do is do the math so if you do
time 7:30 that's how many hours there
are in a month if we launch a T2 micro
and let's say we didn't have the free
tier we you do if you first made your
account you're going to have 7 750 hours
for free with a free tier but if you
didn't it would only cost you $8 and 46
USD okay so just be aware of that and if
you ever need to figure something out go
there copy it do the math 730 it's
pretty easy so here we have a T2 micro
and the t2 family it's going to have one
V vcpu notice it has a V for virtual so
there could be more than a single CPU on
the underlying Hardware but we're only
going to have access to one virtual CPU
we have 1 GB of memory it's for low to
moderate Network performance so that's a
factor that can change if you need like
uh uh gigabit stuff like really fast
connections for on Prem hybrid
connections and you have specialized
servers for that but for this this is
fine the TT micro is great uh if you
want you can also search uh this way to
see all the instance families and things
like that you can filter for current
Generations all generations so this is
fine okay so from there we're going to
go to configure our instance type you
can say let's launch multiple of these
instances let's turn on spot to save
money and try to bid for a particular
price we can change our VPC it's going
to default to the default VPC um if you
have no subnets just going to pick one
at random here which is fine um whether
to autoassign a public IP address if you
do not have an IP address you cannot
reach the internet so generally you want
this to be enabled this is dependent on
the subnet whether it will default
enabled but doesn't matter if you have
an ec2 instance in a private or public
subnet you can always override this and
give it a public IP address you have
placement groups which allows you to
place servers together closely not
something for the certified Cloud
partitioner there's capacity
reservations so if you're worried about
any us running out of this you can
reserve capacity so that's kind of
interesting domain join directory this
isn't something that I've done much with
but I imagine that has something to do
with um direct active directory or
something like that to join information
then you need to uh uh have an IM roll
and we absolutely do need an IM roll
here so what I want you to do is create
a new rooll just going to close off
these other tabs here and we will go
wait a moment create a new roll here and
we want to do this for ec2 so we say ec2
is what we're creating the rule for
we'll hit next and um I don't know if I
have a policy but I'm going to go ahead
and um well I don't need to make a new
policy but I just want SSM and the
reason I want SSM is so that I can um uh
use sessions manager to log in so we
don't have to use key pairs we will use
key pairs but if we didn't want to use
it that's what we could do and this used
to be the old rle and it'll tell you hey
go use this new one here so just want to
make sure I know which one it is and so
we'll just checkbox that on we'll hit
next we can add tags right here it' be
uh well actually we don't need to add
any tags here so that's fine we'll sit
next and then I'll just say U my SSM ec2
roll okay and we'll create that
roll and now that we have created that
roll we can go back to our first tab
here and and give this a refresh and
then drop down and it should show up
here if we go down here a little bit we
could turn on extra monitoring there is
monitoring built in but if you wanted to
uh monitor it to a lower uh like it more
frequently you could do that as well we
want share tendency right this is where
you change to Dedicated instance or
dedicated host obviously these cost more
but we're going to stick with shared
elastic inference so this is for um uh
attaching a a fractional GPU great for
ML not something that we want there's
credit specification I don't remember
seeing this before selecting unlimited
for credit specification allows for to
burst beyond the Baseline so as for
bursting here you can attach an uh EFS
uh so if you need a file system that you
want to mount or attach um then there's
the Enclave option so Nitro Enclave
enables you to create isolated compute
environments to further protect your uh
and securely process highly sensitive
data so it might be something you might
want to checkbox on um based on your use
case and then down below are we have the
ability to enter our user data and this
is something we want to do because we
want to install aachi so that we have
something to work with here so what I'm
going to do is make a shebang so that is
a pound and an exclamation mark I know
that's really small so I'll try to bump
up my font here so you can see what I'm
doing and we're going to do a forward SL
bin and a for SL bash on the next line
here we're going to do yum install
hyphen y
httpd um that's going to install apachi
and why it's not called Apache I don't
know why but they call it http D there's
no Apachi in the name there and so we'll
say system CTL start httpd system CTL
enable htpd so we're saying start up
Apachi and then make sure that it stays
running if we restart our machine very
simple so from there we will go to our
storage we'll say add our storage and
this is at 8 gigabytes by default we
could uh uh turn that up to 30 if we
like so you can go all the way up to 30
if you like um and you might want to do
that but I'm going to leave at 8 we
could change our volume type I'm fine
with gp2 because it's very cost
effective and if we want to turn on
encryption and you should always turn on
encryption there's no reason not to and
so we'll turn that on it's not like it's
going to cost you more it's going to be
the same cost it's just your choice
there if do want to add a tag yes we're
going to add a name and we're going to
say my ec2
instance
okay and so that's going to give us a
name which is something we would really
like to have then we have our security
group I'm going to just create a new
security group called my um um ec2 SG
here and just say my ec2 SG something
you cannot do is rename a security group
once you've made it so make sure you
don't make a spelling mistake up here
and we want to be uh accessing that httt
HTT or it's going to launch a website so
in order to do that we need to make sure
we have HTTP as a type with the port ad
open and we want it from anywhere so
we'll say anywhere and that will be
0.0.0.0 forze 0 and that that's for the
ipv4 this is for the IPv6 okay so we'll
just say
internet and this is for SSH right and
for this um I would probably suggest to
say my IP but since we might be using a
cloud shell to do that we're going to
leave it as anywhere so that we don't
have any issues connecting so from here
we'll review and launch and you can
review what it is that's going on here
it's going to say here hey you have an
open port that's okay we we want the
internet to see our website cuz that's
the whole point there and we'll go ahead
and launch it it's going to ask for a
key pair we can go down and say proceed
without key pair but what I'm going to
do is I'm going to create a new key pair
because I want to show you how those
work and I'm sure we've already done in
this course once but we'll do it again
and so I'm going to just name this as my
ec2 instance here and then we're going
to go download that key pair it's going
to download a PM file there and so now
we can go ahead and launch that
instance and while that is launching so
I'm going to just close this other t
here we're going to click on The View
instances and so here is that instance
that's why we put the tag so we could
have a name there we're going to wait
for that to start but as that's going
I'm going to make a new tab by just
right clicking here on the logo click
anywhere pretty much to do that and uh
once we do that we'll click on cloud
shell and as that is going what I want
to do is take this pen down below I'm
going to move it to my desktop to make
it easier for me to upload I'm doing
this off screen okay
and uh once this environment is running
I'm going to go ahead and upload that
okay so we'll just give it a moment to
do that we're also waiting for the
server to spin up as you'll notice there
is a public IP address here it says it's
running so if we want we can copy it
we're looking for those two checks to
pass so the server could be available
but generally you want to wait for those
two system checks because one says Hey
the Hardware's fine the Network's fine
things like that okay if I take that IP
address paste it on it up here we have
the web page so that is working uh no
problem there so that's great and we'll
go over to Cloud shell and that is still
starting uh it's not the fastest but
that's just how it is and um you know
we'll get going here in a second as soon
as this decides to
load there we go so it's loaded I can
type clear here just to clear that
screen out and so what I want to do is
upload that pen file so I'm going to go
and upload that file we're going to go
ahead and select it I'm going to go to
my desktop here whoops my desktop and we
are going to choose my ec2 instance pen
all right and from there we'll hit
upload and that's going to upload that
pem
file once that is uploaded we're going
to do
LS okay and so uh this is from a
previous tutorial so I'm going to go
ahead and just delete that other one
there we'll say remove EFS example pem
yes okay we'll type clear
and then what we can do here is Type in
chamod and um I believe it's
400 and what do we call this my ec2
instance pen if you hit tab it will
autocomplete which is nice and if you do
lsen la we can take a look at that file
and see it should look like this it
should have only one R here so the idea
is you're locking it down so it's not
writable or executable it's just
readable because that's what you have to
have it if you want to SSH and so if we
want to ssh what we'll do is hit the
connect button here
and we have four options they just give
you too many options it's going to be a
fifth one for sure soon but right now
we're talking about SSH so for SSH um we
had to chamod our file which we did and
then we need to use this DNS to connect
to it and so this is the full line here
if you click on this copy that over and
paste it
in that should be everything and noce
we're doing ec2 user followed by this
you could put the IP address in here
instead if you preferred so if you were
over here
you could go and take that IP address
which is I think shorter nicer but um
you know if you just click that one
button it works that's fine you always
have to accept the uh the fingerprint
then you'll be inside the instance you
can type who am I to see which user you
are you're the ec2 user that's the user
that ads creates for their Amazon Linux
instances um it's going to vary per um
Ami so not all Amis have an ec2 user it
might be something else but that's
generally the ones that Aus uses for
their supported ones and so if we do um
an LS again we're in the server right
now we can tell because it says right
here or if we do a PWD we can kind of
just kind of look around so I think it's
going to be at VAR ww that's where HT
httpd or Apachi always uh puts their
files here so if I go in here whoops I'm
just looking for um the index file so I
thought the index file was
in cdar
WW H
HTML well where the heck is it so I'm
going to just touch a file here and see
if it overrides
it oh I don't care I'll just type
pseudo and what we can do is just try to
restart this system CTL um there's a
very similar command that's like uh
service and so I always forget the order
of it so I think it'd be I'm just
checking um probably uh restart httpd
and so failed to restart the policy was
not provided as the name service um
Service uh maybe
pseudo there we go and so if we go back
here I'm going to see if it changed
because it will take whatever is in the
index HTML file so if there's no file
there it's going to uh show that there
and so what I can do is I can edit this
file so going type VI index HTML and um
I'm going to hit I for insert mode oh
says it's readon so what we have to do Q
uh colon Q
quit oops uh clear LS and so what we
need to do is do pseudo VI index HTML
and so Vim every single key is a hotkey
okay um and I'm not teaching Vim here
but I'm going to teach you the basics
but the idea is that when you're here
notice that the cursor is blinking when
I hit I it enters insert mode now I can
type normally so I'd say hello uh hello
Cloud okay and I'm going to hit escape
to go back to um navigation mode
whatever you want to call it I'm going
to hit colon so it brings up the command
I'm going to type in uh write and quit
Okay and hit enter and so I'll type
clear and so whoops clear and so we'll
hit up till we get that command pseudo
system CTL restart hbd we'll hit that
hit enter
okay and it should restart pretty fast
there it is so it says hello Cloud I
probably didn't even have to restart it
to do that but anyway so now that
instance uh you can see how we're
updating that so what I want to do is
just do a sanity check and make sure
that if we restart this instance that
we're going to be able to um have aachi
running that's something you should
always do if you have an app and you or
anything you install it restart your
server make sure that everything works
so what I'm going to do is uh just hit
hit exit here so we go back to the top
level cloudshell type clear I'm going to
go back over to my ec2
instance going have to click around to
find it here and what I want to do is
reboot it okay and if I reboot the
machine the IP address is going to stay
the same okay so if we reboot it the IP
address is going to stay the same and
the reboot is going to happen really
fast if we want to observe that reboot
we could go over to um here on the right
hand side go to the system log and it
would show us that it it had
rebooted I think so yeah it does a cloud
in it there I think it
rebooted not sure um but anyway if it's
rebooted then we can go ahead and
connect and make sure everything's fine
so let's just go here and hit enter and
let's see if the what the web page is
here notice that it's hanging right so
it's probably because it's still
restarting even though it doesn't look
like it is and that's something that you
have to understand about the cloud is
that you have to think about what you're
doing and have confidence that it is
happening and also just double check it
but uh that's something that can be kind
of frustrating because these are
globally available Services uh uh
they're massively scalable and so one of
the trade-offs is that you don't always
have the most uh responsive uh uis ads
has one of the most responsive uis out
of all the major providers but even
still like sometimes I have to second
guess myself but the page uh right now
it was not working now it is so it's
fine so it just took time for that to
reboot and so um what I want to do is
connect a different way so we're going
to go here and we're going to hit um
we're going to checkbox that on we're
going to hit connect and instead of
using SSH client we're just going to go
to sessions manager and hit
connect and this is the preferred way of
connecting because you don't have to
have this this SSH key and that's a lot
more secure because if someone has that
key and you you know you hand it to
someone they could hand it to somebody
else and then you have a big problem on
your hands so here this looks very
similar but if you type who am I it
actually logs in as the SSM user which
is kind of annoying so I type in P sudo
Su I have to do this hyphen here and
then I'm going to say the user I want to
be which is ec2 user and then if I type
who am I we are the correct user you
can't do anything in that SSM hyphen
user or SSM user so you got to switch
that over and I can bump this up to make
it a bit larger so this is obviously not
as nice as working over here or even in
your own terminal but it's a lot more
secure and it's tracked and all these
other things so we really should be
using it
okay and um I really don't like having
to bump this up with my HTML I'm going
to just go back to zero there there's
probably like a way to configure that
but anyway uh let's just go and take a
look at our
file I'm going to type VI again and
we're going to do VAR www HTML index
HTML I'm going to put pseudo in front of
there and again remember you have to hit
I to go into insert
mode and uh what I'm going to do is just
take capitalize that hello Cloud give
that exclamation mark colon WQ to quit
right quick
going to go back here refresh okay so we
don't have to restart our server which
is nice all right so um that's that
that's pretty clear so I'll hit
terminate
here and I don't think we need Cloud
shell for anything so I'm just going to
close that and so that's pretty much it
when it when it comes to working with an
an ec2 instance and so the next thing I
want to show you is elastic IP
[Music]
okay okay so now I want to show you
elastic IP uh commonly abbreviated to
EIP and so all that is it's just a um a
static IP an IP that does not change
because this ec2 instance here notice
that it's 54 163
4104 and what would happen if we were to
stop this instance not reboot it but
stop it because for whatever reason we
had to or or um for whatever reason and
if we were to stop this instance and we
were to restart
it
okay uh and we have to wait for to stop
but that IP address is going to change
okay so 54 1634 104 hopefully we can
observe that I'm just going to write
that down so we do not forget so I can
prove to you that it does
change and now that it it's still
stopping here so as that's stopping
we're just going to go ahead and get our
elastic IP and I will prove that as we
go here so I'm going to go over to here
and so what I want to do is Reserve or
allocate an elastic IP address and so
I'm going to say Us East one and it's
going to say from the Amazon Pool of
ipv4 addresses so adabs has a bunch of
IP addresses they're holding on to and
so you can just allocate one and once
you've allocated that's your IP address
so coming back to here okay this is
stopped notice there is no public IP
address we're going to start it
again okay and will'll just checkbox it
on and we just have to wait a little
while to see what the IP address is
going to be I'm going to tell you it's
going to be something
else so if I go back here this is 54
2352 1110 and our original one was 54
1634 104 so the reason why it's
important to have the same address is
that if uh you have a load balancer well
not a load balcer but if you have a
domain pointing to your I uh your server
and you reboot then and the you have a
dang a dangling um path or route where
Revue 3 was going to be pointing to
nothing and so adus does have things to
mitigate that like aliases and things
like that but um in general you know
there's cases where you just have to
have a static IP address and so we had
allocated one over here and if we want
to assign it we're going to associate
that elastic IP address we're going to
drop it down choose the cc2 instance um
I suppose the private IP as well and
then we're going to go ahead and hit
allocate or
associate and once it's Associated it
should now have 34 199 121 116 so we go
over
here and we're going to take a look here
and that's its IP address we can pull it
up okay and that's that so yeah that's
thetic
[Music]
IP okay so now that we um have our
lastic IP we have our ec2 instance
running let's say um you know we lose
the server we terminate it so we would
lose all of our configuration so if we
wanted to bake this Ami to save it for
later what we'd have to do is go and
create an image so to do that we go to
the top here and we go to images and
templates and we can create an image or
we can create a a template which is a
lot better but for the time being we're
going just go ahead and create an image
and when you create an image you're
basically creating an Ami and so here
I'm just going to say uh my
ec2 and I'm going to go 0 0 0 to just
kind of like number it so that's a very
common numbering just do three zeros and
then Inc by one and so here I can just
say my Apachi server and so it's going
to save some settings like the fact that
there is a a volume you could uh save
some tags there and so I might go ahead
and add a tag and just say name and
we'll just say my ec2 server or so that
it remembers
that okay and then what we'll do is go
ahead and create our image and so this
can take a little bit of time if we go
over to uh images
here it's going to to be spinning for a
while and uh we'll just wait until it's
done okay all right so after waiting a
little while here our Ami is ready so
we're just waiting for it to go
available if you do not see it just make
sure you hit the refresh um because
sometimes ads will just spin forever um
and so that's just something you'll have
to do so you know hopefully that makes
sense what we'll do is go make our way
back over to instances here and we can
launch one this way well actually we can
do it over from um the Ami page so what
I'm going to do is just terminate this
instance we're all done with it okay and
we'll hit terminate it's totally fine
and it had a message about elastic IPS
about releasing them so when it does
that the elastic IP is still over here
so it did not release it so what we're
going to do is go ahead and disassociate
the elastic
IP okay and then we're also going to
release the IP address because if we
don't we're going to have this IP
address and sticking around that we're
not using it is going to charge us a
dollar month over month so just be aware
of those because that's just kind of
like a hidden cost there but what we're
going to do is go over to
Ami and we're going to select it here
we're going to go to actions we're going
to go ahead and
launch and what it's going to do is make
us fill out all this other stuff again
so if you had made a launch template uh
we wouldn't have to fill out all this
stuff it'd be part of it but that's what
I'm trying to show you with this Ami
stuff so um instead of filling out all
this what I'm going to do is now go
create a launch template just to kind of
show you that that would be a much
easier way to
work so we go over to E2 instance es and
then left hand side we're looking for a
launch template launch launch
configurations is the old thing um
launch templates here we go so what
we'll do is create ourselves a launch
template we'll just say my Apachi
server and then down below we need to
choose our Ami so we're going to go here
and we need to type it in so what do we
call it my
ec2 I really don't like this uh search
here it's very slow frustrating but once
we find it whoops that's why I don't
like it because a lot of times it'll be
loading and you'll end up clicking the
wrong
thing okay
so I don't like this okay we'll type in
my give it a
second there it is just wait because it
will keep loading and then once it's
loaded hit
enter and so it has that instance
selected and then from there uh don't
include in the launch template so here
we could be explicit I would say I want
this to be 22 T2 micro but we could
exclude it if we wanted to we could
specify the key pair here um not that we
really want to use key pairs we'll say
my ec2 instance then down down here for
the networking we can specify uh that
security group we created so we created
one here called my ec2
SG um storage is fine it's going to be
encrypted network interface is fine
Advanced details what I want to do is
set the IM instance profile that's
really important because we don't want
to have to figure out that roll every
single time so we put that there and
that should be everything and we could
put user data in there but it's already
baked into our Ami so we don't have to
worry about anything so what I'm going
to do here is go ahead and create this
launch template and then we're going to
view this launch template and so now
what we can do is then use it to launch
an instance okay and so we're going to
look here and it's very similar to dc2
but except it's vertical so we're going
to have one instance it's going to use
that Ami that instance type so you can
see how you can override them which is
nice we're going to check the advanced
details make sure that I profile is set
and we'll go ahead and launch this from
a
template so from there we can go ahead
and click the instance value there and
just be aware that when you do click
through links like that you'll end up
with the search so I always just
checkbox that off so I can see what I'm
doing and so we're just waiting for this
instance to show up and the only thing I
noticed is it didn't set our darn tags
so I wanted the name and there and I
think it's because we said it in the Ami
but it didn't carry over to the launch
template so I'd have to go back to the
launch template and update it probably
so if I go into here into the launch
template um we can probably modify
create a new
version and then add tags there so we'
say
name uh my uh Apachi
server I realize I'm changing it between
them and so that should allow us to have
a version two so we'll create that and
but anyway that will be for the next
time we launch it okay and so this
instance is running I'm going to go grab
the IP
address the server may or may not be
ready we'll take a look here and so it's
just spinning if it's spinning it's
either the server is not ready or um our
ports not open so it was just getting
ready to work there so it is working now
so that is our launch template so now
you know we don't have to worry about
losing our stuff and if we need to make
new versions We can just B new Amis and
increment them as uh Inc and attach them
as new versions of that launch template
[Music]
okay all right so what I want to show
you in this follow along is to set up an
autoscaling group for our ec2 instance
and the idea behind this is that um
we'll be able to always ensure that a
single server is running or uh increase
the capacity if the demand requires it
so in order to create an autoscaling
group we can go all all the way down
below to here um and so you know I
really don't like the Autos scaling
group form but it's okay we'll work our
way through it so the first thing is
we'll have to create our or name our
autoc scan group so we'll just say my
ASG and then we'll have to select a
launch template which is great because
we already have one and then we'll have
to select the version I'm going to
select version two so that it applies
that tag name and we'll go to next and
so here um it's going to need to select
a VPC and then we need some subnets so
we're going to choose three just because
to have high availability you have to be
running in at least three different
availability zones so that's why we have
three different subnets and then down
below we have the instance type
requirements so uh T2 micro launch
template looks good to me so we'll go
ahead and hit
next and then from here we can choose to
do a load balancer and so I want to do
the load balancer separate so we won't
do it as of yet but very often if you're
going to have an on group you're going
to usually have a load balancer but
we'll talk about that when we get to
that point there so we'll just go to the
bottom here and hit next and so this is
what's important so how many do you want
to be always running and so we always
want to have one and maybe the maximum
capacity is two and you want the desired
C capacity to be around a particular
number so if you had three and you said
the desired is two um there are things
that could try to work to always make
sure there's two but we just want to
have one for this example we can set up
a scaling policy so I do Target tracking
scaling policy and so here we could do
it based on a bunch of different things
so if the CP utilization went over 50%
it would launch another server so that
might be something we might want to set
so I'll we're not going to uh try to
trigger the scaling policy but we might
as well just apply because it's not too
hard then you can also do a scaling uh
scale in protection policy so if you
want to make sure it does not um uh
reduce the amount of servers that's
something you could do we can add a
notification to say hey there's a
scaling policy happening here which is
fine we don't have to worry about that
um and there's tags so add tags to help
you search filter Etc um so I'm going to
put a tag here I'm going to say name I'm
just wondering if this is going to
attach to the ec2 and or this is for the
Autos scaling group you can optionally
choose to add tags to instances by
specifying tags in your launch templates
so we already did that so I don't need
to put a tag here and so we can review
our um Auto scaling group and go ahead
and create that auto scaling
group okay and so that auto scaling
group expects there to be a single
instance so what it's going to do is
it's going to start launching an
instance and so what I'm going to do is
just get rid of this old server because
we don't need it anymore this old one
here
okay and you can already see okay that
the load balancer is launching this new
one here and remember we updated our
version two to have that name so that's
how we know that it is so if we go back
over to our autoscaling
group okay it's now saying there is an
instance we don't have a status as of
yet
and so there are ways of doing uh status
checks to for it to determine whether or
not the server is
working um because if the server is
unhealthy what it would do is it would
actually kill it and then start up a new
one right so if I go down below it's
right now doing an ec2 health check and
the ec2 health check just means that is
the server working right um is it
running it doesn't necessarily mean like
hey can I load this web app um but you
know it's very simple so we'll give it a
moment here to start up and just make
sure that it's working
okay and I think it's ready so if I take
that public IP address here and paste it
in there it is okay so if we were to
tell it to increase the capacity to
three then what it would do is it would
launch three and then it should probably
launch it all evenly to those other it
should evenly launch it to all those
other uh availability zones and then
we'll have something that is highly
available okay so that's pretty much it
for this and then we'll move on to autos
scaling
[Music]
groups all right so we have our uh ec2
instance now managed by an Autos scaling
group and the great thing is that if we
terminate this instance this Auto
scaling group will launch another uh
instance to meet our particular capacity
um the only thing though is that if we
were to have multiple E2 instances
running like three of them um how would
you distribute traffic to the mall right
so you know you have an IP address
coming in from the internet uh but let's
say you want to evenly distribute it and
that's where a load bouncer comes into
play and even if you have a single
server you should always have a load
bouncer because it just makes it a lot
easier for you to scale when you need to
and you it acts as an intermediate layer
where you can attach a web application
firewall you can attach an SSL
certificate for free so there's a lot of
reasons to have a load balancer so what
we'll do is go down below on the left
hand side and we're going to make our
way over to load bouncers and we're
going to create ourselves a new load
boun bcer so I'm going to hit create
load balcer here and you're going to see
we have a lot of options application
load balcer Network load balcer Gateway
load balcer and then the classic load
Bouncer and so we are uh running an
application so I'm going to create an
application load balcer and here I'm
going to say my ALB um for an
application load balancer this is going
to be internet facing it's going to be
ipv4 um we're going to let it launch in
the default um subnet and we're going to
choose the same the same uh uh azs right
so that we get the same subnets as our
that are in our autosan group and that's
really important okay and then here um
you know we need to have a security
group and I just feel like selecting the
same one here because that should work
no problem there and we want to make
sure that we can listen on Port 80 and
that it's going to forward it to a a um
a Target group and it looks like I might
have a Target group there from before so
just to reduce that confusion you won't
have this problem I'm just going to
double check if that's true so do I have
a Target group from there from before
yes I do that came
from I'm not sure it might have been
created by um elastic beanock and wasn't
deleted okay so I'll go back over to
here just so there's less confusion
and we were selecting our Target group
so we're going to have to create a new
Target group so we go over here and here
you can choose whether it's instance IP
Lambda application load balancer so you
could point it specifically to an IP
address and so if it was a static IP
address that would make sense uh
apparently you can Port uh point it
directly to instances I don't remember
seeing that option before I guess that
makes sense yeah no sorry that makes
sense because that would go to uh vpcs
okay or sorry uh asgs Autos scaling
groups it's just that you are pointing
them to Auto scaling groups you're not
pointing them to instances so that's why
that's confusing so I'm going to say my
um Target group it'll be for Port 80
here um protocol http1 is fine we want
to be in the same um VPC so that's fine
as well and down below we have our
health check and so the for slash means
that it's going to hit the index HTML
page and so if it gets back um something
healthy and that that something healthy
is going to be um uh Port 80 then it's
going to be considered good and then we
can say the threshold of check so I'm
just going to reduce this so it's not so
crazy so we'll say three uh two and then
10 okay and then it expects back a 200
which I think that's what we'll get back
so we'll go ahead and hit next and so
now we have our Target group and it
should register instances so it's saying
hey we detected this and this fits the
requirements for this so this is now uh
this E2 instance is now in this target
group okay so we can go back over here
and we can now drop down and choose
whoops hit the refresh
button and choose our Target group
so I'm not seeing it here so I'm going
to go back over here oh we didn't create
it
okay and now we can go back hit refresh
and there it
is and yeah that looks all good so we'll
go ahead and hit create load
balcer we can view the load balcers and
these create really fast if we scroll on
up what we can do is now access our
server through this DNS name okay so we
copy that paste that in
there does it
work not as of yet so if it's not
working there because we did say look at
these instances another way is to
directly associate your Autos scaling
group with the load balancer so if I go
into here and we hit uh
edit there is a way aha load bouncer
so we want to associate this way and we
want to say this Target group
here and also while we're here we might
as well set it to elb so it's going to
use the elb tech so that makes it so the
autoscaling group if it wants to uh
restart server it's going to use the
elb's check which is a lot more
sophisticated and then what we'll do is
go hit
update
okay and now if we go back over to our
load balancer just going to close some
of these tabs so it's a less
confusing uh load balcer here
I think we should be able to see through
here whether it is seeing it let's go
down below listeners monitoring
integrated Services no it's going to be
through the target
group
okay I mean it already had it there so
maybe it's just that it hasn't finished
the check so over here it has a health
status check oh now it's healthy okay so
if it's healthy in the Target group and
the load bouncer is point to it then it
should technically work so we're going
to go ahead
and uh copy the DNS again here make a
new tab paste it
in and there it is okay so that's how
you're going to access um all your all
your instances that are within your
autoc groups you're going to always go
through the DNS and so if you had a row
53 uh domain like you your domain
managed by AWS you just point to the
load balancer and that's how you hook it
up so that's pretty much it so yeah
there you
[Music]
go all right so there you go we learned
everything we wanted to know about ec2
so the the last thing to do is to tear
everything down so we have a load
balancer we have an autoc scanner group
um and those are the two things we'll
have to pull on down so the first thing
would be to take down the autoscaling
group and when you delete an autoscaling
group it's going to delete all the ec2
instances so we'll do it that way if you
tried to delete the ec2 it would just
keep on spinning up so you have to
delete that first and so as that's
deleting then we'll be able to delete
our load balancer I'm going to try
anyway to see if I can delete it at the
same
time and so I'll go up here I'm going to
go ahead and delete that uh load
balancer actually it did work no
problem going to make sure I don't have
any elastic
IPS I'm going to also make sure I don't
have any key pairs you can keep your key
pairs around but like I just want to
kind of clean this up so
okay okay and that instance should be
terminating got to go back to the Autos
scan group
here if we click into it we can check um
its activity
here so it's just saying successful so
it is waiting on elb connection draining
which is kind of annoying because we
deleted at elb so there's nothing to
drain um draining is just to make sure
that uh you know there's no
interruptions when terminating services
so just trying to be smart about
it and all I want to see is that it's
just saying terminating over here and
then I think we're done
okay so we'll just have to wait a little
while here okay and I'll see you back in
a moment okay all right so after waiting
a very long time it did destroy so if I
go down over to uh my load balancer here
we're going to see that it does not
exist so there was that connection
draining thing which was kind of
annoying it's probably because I deleted
the load balancer first and then the um
the uh the Autos SC group second and
probably connection draining was turned
on but it's not a big deal we just
waited and it did eventually delete so
we're pretty pretty much all done here
so there you
[Music]
go hey this is Andrew Brown from exam
Pro and we are taking a look at ec2
pricing models and there are five
different ways to pay with ec2 remember
ec2 are virtual machines so we have on
demand spot uh reserved dedicated and
adus savings plans so what we'll do is
look at these in summary here and then
we'll dive deep onto each of these
different pricing models so for on
demand you are paying the a low cost and
also you have a lot of flexibility with
this plan uh you are paying per hour so
this is a pay as you go model uh or you
could be paying down to the second which
we'll talk about uh the caveats there
when we get to the on demand section
this is suitable for workloads that are
going to be short-term spiky
unpredictable workloads uh that cannot
be interrupted and it's great for
first-time applications and the ond
demand uh pricing model is great when
you need the least amount of commitment
for spot pricing you can see we can save
up to 90% which is the greatest Savings
of out of all these models here uh the
idea here is you're requesting spare
Computing capacity that adus is not
using and that's where you're going to
get that savings you have flexible start
and end times uh but your workloads have
to be able to handle interruptions
because these servers can be stopped at
any time to be giving to more priority
customers uh and this is great for
non-critical background jobs very common
for like scientific Computing uh where
jobs can be started and stopped at any
given time this has the greatest amount
of savings then you have Reserve or
reserved instances this allows you to
save up to 75% this is great for a
steady state or pred ible usage you're
committing uh with AWS uh for ec2 usage
over a period of 1 or 3E terms you can
resell on uh unused reserved instances
so you're not totally stuck with this if
you buy them this is great for the best
long-term savings then you have
dedicated so these are just dedicated
servers and technically not a pricing
model but more so that the fact that it
can be utilized with pricing models um
but the idea here is it can be used with
on demand reserved or even spot this is
great when you need to uh have a
guarantee of isolate hardware for
Enterprise requirements and this is
going to be the most expensive uh so
yeah there you go and we'll dive deep
here
[Music]
okay so the on demand pricing model is a
pay as you go model where you consume
compute and then you pay later so when
you launch an ec2 instance by default
you are using that on demand pricing and
On Demand has no upfront payment and no
long long-term commitment you are
charged by the second up to a minimum of
60 seconds so technically a minute or
the hour so let's just talk about the
difference between those uh per second
billing and those per hour billing so
per second are for Linux windows windows
with SQL Enterprise windows with SQL
standard windows with SQL web instances
that do not have a separate hourly
charge and then everything else is going
to be um per hour and so you know when
I'm launching ec2 instance I can't can't
even tell when something's per second or
per hour you just have to know that it
has a separate hourly charge but
generally you know if you're just
launching things it's going to probably
be the perc billing when you look up the
hourly or the uh the pricing it's always
shown in the hourly rate so even if it
is using uh per second billing when you
uh look up that pricing it's always
going to show it to you like that but on
your bill you'll see it down to the
second okay up to the first 60 seconds
uh an on demand is great for workloads
that are shortterm spiky or
unpredictable
uh but when you have a new app
development this is where you want to
experiment and then when you're ready to
uh start saving because you know exactly
what that workload is going to be over
the span of a year or three that's where
we're going to get into reserved
instances which we'll cover
[Music]
next hey this is Andrew Brown from exam
Pro and we are taking a look at reserved
instances also known as RI and this is
um a bit of a complex topic but uh you
know if we do get through it it's going
to serve you well through uh multiple ad
certifications so let's give it a bit of
attention here so RI is designed for
applications that have a steady state
predictable usage or required Reserve
capacity so the idea is that you were
saying to ads I'm going to make a
guaranteed commitment uh saying this is
what I'm going to use and I'm going to
get savings because adus knows that
you're going to be spending that money
okay so the idea here is that the
reduced pricing is based on this kind of
formula where we have term class
offering the ra attributes and payment
options technically the ra tributes
don't exactly factor into it other the
fact that they an RA tribute could be
like the instance type size uh but I'm
going to put that in the formula there
just because it is an important
component so let's take a look at each
of these uh components of the formula to
understand how we're going to save so
the first is the term so the term uh the
idea here is the longer the term the
greater the savings so you're committing
to a one-year or three-year contract
with AWS um and one thing you need to
know is that these do not renew so at
the end of the year the idea is that you
have to purchase again and when they do
expire your instances are just going to
flip back over to On Demand with no
interruptions to service then you have
class offerings and so the idea here is
the less flexible the offering the
greater the savings so the first is
standard and this is up to a 75
reduction in the price compared to on
demand and the idea here is you can
modify some ra attributes which we'll
we'll talk about when we get to the um
ra tribute section there then you have
convertible so you save up to 54%
reduced pricing compared to on demand
and you can exchange RIS based on the r
attributes if the value is greater or
equal in value and there used to be a
third class called schedule but this no
longer exists so if you do come across
it just know that ads is not planning on
offering this uh again for whatever
reason I'm not sure why uh then there
are the payment options so the greater
upfront the greater the savings so here
we have all upfront so full payment is
made at the start of the term partial
upfront so a portion of the cost must be
paid upfront and the remaining hours in
the terms are built at a discounted rate
and then there's no upfront so you are
build at a discounted hourly rate for
every hour within the term regardless of
whether the reserv is being used and
this is really great this last option
here because basically you're saying to
AWS you're saying like I'm just going to
pay my bill as usual but I'm going to
just tell you what it's going to be and
I'm going to save money so if you know
uh that you're going to be using a T2
medium for the next year uh you can do
that and you're just going to save money
okay okay so RIS can be shared between
multiple accounts within an organization
and unused RIS can be sold in the
reserved instance Marketplace but we'll
talk about the limitations around that
when we get a bit deeper in here just to
kind of show you what it would look like
in inabus console and they updated it I
love this new uh UI here the idea here
is you're going to filter based on your
requirements and that's going to show
you RIS that are available and then
you'll just choose the desired quantity
you can see the pricing stuff there
you're going to add it to cart you're
going to check out and that's how you're
going to purchase it okay
[Music]
so another factor to that formula were
RI attributes and sometimes the
documentation calls them R attributes
sometimes they call them instance
attributes but these are limited based
on class offering and can be uh uh can
affect the final price of the ra
instance and there are four ra
attributes so the first is the instance
type so this could be like an M4 large
and this is composed of an instance
family so the M4 and the instance size
so large okay then you have region so
this is where the reserved instance is
purchased then you have the tency
whether your instance runs on shared so
the default which uh would be
multi-tenant or a single tenant which
would be dedicated hardware and then you
have the platform whether you're using
Windows or Linux even if you're using on
demand of course this would just affect
your pricing but there are some
limitations around here which we'll get
into as we dive a bit deeper here with
RI
[Music]
okay all right let's compare Regional
and and zonal Ri so when you purchase an
RI you have to determine the scope uh
for it okay so this is not going to
affect your price but it's going to
affect the flexibility of the instance
uh so this is something you have to
decide so we're going to talk about
Regional RI which is when you purchase
it for a regional and zonal RI when you
purchase it for an availability zone so
when you purchase it for regional RI it
does not Reserve capacity meaning that
there's no guarantee that those servers
will be available so if ad us runs out
of those servers uh you're just not
going to have them but when when it's
zonal uh you are reserving capacity so
there's a guarantee that those will be
there when you need them um in terms of
uh AZ flexibility uh you can use the
regional RI for any a within that region
but for the zonal ri you can only use it
for that particular region we're talking
about instance flexibility um you can
apply the discount to uh any instance in
the family regardless of the size uh but
then when we're looking at AZ there is
no instance flexibility Okay so just
going to use it for exactly what you
defined you can cue purchases for
regional R you cannot cue purchases for
zonal Ri so there you
[Music]
go let's talk about some ra limits here
so there's a limit to the number of
reserved instances that you can purchase
per month and so uh the idea here is
that you can purchase 20 Regional
reserved instances per region and then
20 zonal reserved instances per a so if
you have a region that has three A's you
can have uh 60 um zonal reserved
instances in that region okay there are
some other limitations here so for
regional limits you cannot exceed the
running on demand instance limit by
purchasing Regional reserved instances
the default for on demand limit is 20 so
before purchasing your RI ensure on
demand limit is equal to or greater than
your RI you intend to purchase you might
even want to open up a service uh limit
increase just to make sure you don't hit
that wall for zonal limits you can
exceed your running on demand uh
instance limit by purchasing zonal
reserved instances if you're already uh
have 20 ond demand instances and you
purchase 20 zal reserved instances you
can launch a further 20 ond demand
instances that match the specification
of your zonal reserved instances so
there you
[Music]
go let's talk about capacity reservation
so ec2 instances are backed by different
kinds of hardware and so there is a
finite amount of servers of available
within an availability Zone per instance
type of family remember an availability
zone is just a data center or a
collection of data centers and they only
have so many servers in there so if they
run out because the demand is too great
you just cannot spin anything up and so
that's what's happening you go to launch
specific ec2 instant type but AB is like
sorry we don't have any right now and so
the solution to that is capacity
reservation so it is a service of ec2
that allows you to request uh a reserve
of ECC instance type for a specific
region and a so here you would see that
you just select the instance type
platform a tendency the quantity and
then here you might manually do it
specify time or you might say okay I
can't get exactly what I want but can
give me something generally around uh
that kind of stuff or that type that I
want so the reserve capacity is charged
at the selected instance type on demand
rate whether an instance is running in
it or not and you can also use Regional
Reserve instances With Your Capacity
reservations to benefit from billing
discounts so there you go
[Music]
so there are some key differences
between standard and convertible Ri so
let's take a look at it here so the
first is that with standard RI you can
modify your tributes so you can change
the a within the same region you can
change the scope uh from a zonal r to
original RI or vice versa you can change
the instance size uh as long as it's a
Linux and it has the default tendency
you can change the network from ec2
classic to VPC and vice versa but when
you're looking at convertible you you
don't modify ra tributes you perform in
exchange okay and so standard RIS cannot
do exchanges where convertible RI you
can uh exchange during the term for
another convertible RI with new R
attributes and this includes the
instance family instant type platform
scope and tency um in terms of the
marketplace you uh they can be bought in
standard RI uh in the marketplace or you
can sell your RI if you uh don't need
them anymore uh but for convertible R
they cannot be sold or bought in the
marketplace you're just dealing with ads
directly
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at the
reserved instance Marketplace we had
mentioned it prior so let's give it a
little more attention here so it allows
you to sell your unused standard RI to
recoup your spend for R right you do not
intend or cannot use so reserved
instances can be sold after they have
been active for at least 30 days and
once databus has received the upfront
payment you must have a US bank account
to sell RI on the ri Marketplace there
must be at least one month remaining in
the term for the ri you are listing you
will retain the pricing and capacity
benefit of your reservation until it's
sold and the transaction is complete
your company name and address upon
request will be shared with the buyer
for tax purposes a seller can set Only
The Upfront price of an RI the usage
price and other configurations such as
instance type availability Zone platform
will remain the same as when the ri was
initially purchased the term length will
be rounded down to the nearest month for
example a reservation with 9 months and
15 days remaining will appear as 9
months on the R Market you can sell up
to 20,000 USD in reserved instances per
year if you need to sell more RI
reserved instances in the gov Cloud uh
region cannot be sold on the ri
Marketplace so there you
[Music]
go hey it's Andrew Brown from examp Pro
and we are taking a look at spot
instances so adus has unused compute
capacity that they want to maximize the
utility of their idle servers all right
so the idea is just like when a hotel
offers booking discounts to fill vacant
Suites or planes offer discounts to fill
vacant seats all right so spot instances
provide a discount of 90% compared to On
Demand pricing spot instances can be
terminated if the Computing capacity is
needed by other on demand customers but
from what I hear rarely rarely does spot
instances ever get terminated um it's
designed for applications that have
flexible start and end times or
applications that are only feasible at
very low compute cost so you see some
options here like load balancing
workloads flexible workloads Big Data
workloads things like that um there is
another service called Aus batch which
is for doing batch processing and this
is very common what you use um spot with
and so you know if you find the spot
interface too complicated you're doing
batch processing you want to use this
service instead um there are some
termination conditions so instances can
be terminated by adabs at any time if
your instance is terminated by ads you
don't get charged for a partial hour of
usage if you terminate an instance you
will be still charged for an hour uh
that it ran so there you
[Music]
go hey this is Andrew Brown from exam
Pro and we are taking a look here at
dedicated instances so dedicate
instances is designed to help meet
regulatory requirements inabus also has
this concept called dedicated hosts and
this is more for when you have strict
server bound licensing that won't
support multi- tendency or cloud
deployments and we'll definitely
distinguish that in this course but just
not in this slide in particular um and
so to understand uh dedicated instances
or hosts we need to understand the
difference between multi- tendency and
single tendency so multi- tendency you
can think of it like everyone living in
the same apartment and single tendency
you can think of it everyone having
their own house so the idea here is that
you have a server I'm just going to get
my uh cursor or my pen out here to say
server and you have have multiple
customers running workloads on the same
hardware and the idea is that they are
separated via virtual isolation so
they're using the same server but it's
just software that might be separating
them okay and then we have the idea of
single tency so we have a single
customer that has dedicated Hardware so
the physical location is what separates
customers um and the idea here is that
dedicated can be offered via on demand
reserved and spot so that's why we're
talking about dedicated here in the
pricing model just so you know that you
know even though these are a lot more
expensive than on demand uh you can
still save by using reserved and also
spot which I was very surprised about um
and when you want to choose dedicated
you're just going to launch your ec2 and
you'll have a drop down where you have
that shared so that's the default
dedicated so you have dedicated instance
and dedicated host and again we'll talk
about dedicated host uh later when we
need to here um and so again the reason
why um you know Enterprises or large
organizations may want to use dedicated
instances is because they have a sec uh
a security concern or obligation about
uh against sharing the same Hardware
with other adus customers
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at ABA
savings plans and this is similar to
reserved instances but simplifies the
purchasing process so it's going to look
a lot like RI at the start here but I'll
tell you how it's a bit different okay
so there are three types of saving plans
you have compute Savings Plan ec2
instance saving plan plans and sagemaker
saving plans uh and so you just go ahead
and choose that you can choose two
different terms so one year threee so
would be simple as that and then you
choose the following payment options so
you have all upfront partial payment and
no upfront and then you're going to
choose that hour of the commitment
you're not having to think about
standard versus convertible uh uh
Regional versus zonal RI attributes it's
a lot simpler uh and let's just talk
about the three different saving plans
or types in a bit more detail so you
have compute so compute savings plans
provides the most flexibility and helps
to reduce your cost by 66% these plans
automatically apply to ec2 instances
usage ads fargate 8s Lambda service
usage regardless of the instance family
size AZ region Os or tency then you have
ec2 instances so this provides the
lowest prices offering saving up to 72%
in exchange for commitment to usage of
instance uh individual instance families
in a region so automatically reduce uh
your cost on the selected instance
family in the region regardless of a
size OS tendency gives you the
flexibility to change your usage between
instances with a within a family in that
region and the last is sagemaker so
helps you reduce Sage maker cost by up
to 64% automatically apply to Sage maker
usage regardless of instance family size
component adus region if you don't know
what sagemaker is that's adab Us's ml
service and it uses ec2 instances or
specifically ml ec2 instances so
everything's basically using ec2 here um
but there you
[Music]
go all right let's take a look at the
zero trust model and the Zer trust model
is a security uh model which operates on
the principle of trust no one and verify
everything so what I mean by that is
malicious actors being able to bypass
conventional access controls
demonstrates traditional security
measures are no longer sufficient and
that's where the zero trust model comes
into play so with the zero trust model
identity becomes the primary security
perimeter
uh and so you might be asking what do we
mean by primary security perimeter the
primary or new security perimeter
defines the first line of defense and
it's security controls that protect a
company's Cloud resources and assets um
if it still doesn't make sense we do
cover a part of the defense in depth
where you see the layers of Defense from
data all the way to physical and so you
can kind of see you know what we're
talking about in that model there but
the old way that we used to do things is
Network Centric so we had traditional
focused on firewalls and VPN since there
were few employees or workstations
outside the office or they were in a
specific remote office so we treat the
network uh the network as kind of like
the the boundary so if you're in in
office there's nothing to worry about
but we don't think like that anymore
because everything is identity Centric
so this is where we have bring your own
device remote workstations which are
becoming more common uh we can't always
trust that the employee is in a secure
location we have uh identity based
security controls like MFA providing
provisional access based on the level
risk from where when and what a user
wants to access and identity Centric
does not replace uh but it augments
Network Centric security so it's just an
additional layer of consideration for uh
security when we're thinking about our
Aus Cloud workloads
[Music]
okay all right so we just Loosely
defined what the zero trust model is so
let's talk about how we would do zero
trust on AWS and so zero trust has to do
a lot with identity security controls
and so let's talk about what is at our
disposal on AWS so on AWS we have
identity and access management IM this
is where we create users or groups or
policies so I policy is a set of
permissions that allow you to say okay
this user is allowed to use uh these
services with these particular actions
uh then you have the concept of
permission boundaries and so these are
saying okay um these aren't the
permissions the user has currently but
these are the boundaries to which we
want them to have so they should never
have access to um uh ml services and if
someone's to uh apply them uh uh
permissions it'll always be within these
boundaries then you have service control
policies and these are organization-wide
policies so if you have a policy where
you don't want anyone to run anything in
the Canada region you can apply that
policy at the organizational level and
it will be enforced then within an
policy there are the concept of
conditions and so these are all the kind
of like uh little knobs you can uh tweak
to say how do I uh control based on a
bunch of different factors so there's
Source IP so restrict where the IP
address is coming from a requested
region so restrict based on the region
as we just mentioned as an example uh
multiactor off presence so restrict if
MFA is turned off uh current time so
restrict access based on time of day
maybe you know your employees should
never be really using things at night
and so that could be an indicator that
someone is doing something malicious so
you know only give them access during a
certain time a day and so that's where
we're going to figure out you know based
on all these type of controls security
controls uh to our adus resources we can
kind of enforce the zero trust model
adus adus does not have a ready to use
identity controls that are intelligent
which is why adus is considered not to
have a zero trust offering for customers
and thirdparty services need to be used
so what I'm saying is that technically
you know this checkbox is this thing
saying okay we can kind of do zero trust
on AWS but there's a lot of manual work
and you know if I was to say okay um I
don't want anyone using this at
nighttime that doesn't really detect you
know what I'm saying it's not going to
say oh I think this time is suspicious
or malicious so then restrict access
only to these core services and anything
outside of the services can't be used it
just can't exactly do that without a lot
of U work yourself and that's what I'm
talking about here where we have a
collection of aable services that can be
set up in an intelligence intelligent is
detection way for identity concerns but
requires expert knowledge so the way you
might do on AWS is that everything all
the API calls go through ad's cloud
trail and so what you could do is feed
those into Amazon guard Duty and guard
duty is an intrusion uh uh intrusion
detection and protection system so it
could detect suspicious or malicious
activity on those cloud trail logs and
you could follow that up with
remediation or you could pass that on to
Amazon detective that could analyze
investigate and quickly identify
security issues uh that it could ingest
from guard duty but I'm telling you that
this stuff here is not as easy um for
the consumer and so you of course you
can do zero trust model but it's going
to take a lot of work here and there are
some limitations which we'll talk about
next
[Music]
here so now let's see how we would do
zero trust on OS with third Pary so os
does does technically Implement a zero
trust model but does not allow for
intelligent identity security controls
which you know you can do it but it's a
lot of work so uh let's kind of compare
it against kind of a third party where
we would get the controls that we would
not necessarily get with AWS so for
example aure active directory has a real
time and calculated risk detection Based
on data points than AWS and this is
based on device and application time of
day location whether MFA is turned on
what is being accessed and the security
controls verification or logic
restriction is much more robust so you
know just as one particular example like
device and application is not something
that ads factors in uh with the existing
controls or at least not in a way that
is consumer friendly and you know I
can't say onus okay when you think that
this is the type of threat only allow
them access to these things or if you
think they're in a risky area or risky
uh location only give them access to you
know these things where there's not
sensitive data you can't exactly do that
on itus very easily and so this is where
third party Solutions are going to come
into play so you have Azure active
directory Google Beyond Corp jump Cloud
all these have more intelligent security
controls for realtime detection um and
so way you would use these is these
would be your primary directories uh for
Google beond Corp is just a zero trust
framework so I guess you'd use uh
Google's uh Cloud directory but the idea
anyway here is that you'd use single
sign on to connect those directories to
your adus account and that's how You'
access access those uh itus resources
and you get this more robust
functionality
okay hey it's Andrew Brown from exam Pro
and we're looking at ident now we need
to know a bunch of Concepts before we
talk about identity on AWS so let's jump
into it the first is a directory service
so what is directory service well it's a
service that Maps the names of network
resources to network addresses and a
directory service is shared uh
infrastructure or information in
infrastructure for locating managing
administrating and organizing resources
such as volumes folders files printers
users groups devices telephone numbers
and other objects a directory service is
a critical component of a network
operating system and a directory server
or a name server is a server which
provides a directory service so each
resource on the network is considered an
object by the directory server
information about a particular resource
is stored as a collection of attributes
associated with that resource or object
uh well-known directory Services would
be a domain name service um so the
directory service for the internet
Microsoft active directory and uh they
have a cloud hosted one called Azure
active directory we have aachi directory
service Oracle inter internet directory
so o ID uh open ldap uh cloud and
identity and jump Cloud
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at active
directory now you might say well we're
doing adab best why are we looking at
this well no matter what cloud service
provider you're using you should know
what active directory is uh especially
when it comes to Identity you can use it
with AWS um so let's talk about it so
Microsoft introduced active directory
domain services in Windows 2000 to give
organizations's ability to manage
multiple on- premise infrastructure
components and systems using a single
identity per user and since then it's uh
involved uh evolved obviously it's uh
running Beyond Windows 2000 as of today
and uh they even have a managed one
called Azure ad which is on Microsoft
Azure but just to kind of give you an
architectural diagram here the idea is
that you would have your domain servers
here uh and they might have child
domains and the idea is that you have
these running on multiple machines so
that you have redundant ability to log
in from various places when you have a
bunch of domains it's called a forest
and then within a domain you actually
have organizational units and when then
within organizational units you have all
your objects like your users your
printers your computers your servers uh
all things like that
[Music]
okay hey it's Andrew Brown from xam Pro
and we're talking about identity
providers
ipds so
hey this is Andrew Brown from exam Pro
and we are talking about identity
providers also known as
idps so an identity provider is a system
entity that creates maintains and
manages identity information for
principles and also provides
authentication services to Applications
with a federation or distributor Network
a trusted provider of your user identity
that lets you use authent uh lets you
authenticate to access other service
identity providers so this could be like
Facebook Amazon Google Twitter GitHub
LinkedIn uh Federate identity is a
method of linking a user's identity
across multiple separate identity
management systems and so some things
that uh we can use for that is like open
ID so this is an open standard and
decentralized Authentication Protocol
allows you to be able to log in to
different social media platforms using
Google or Facebook account open ideas
about providing who you are then we have
ooth 2.0 this is an industry standard
protocol for authorization oath doesn't
share password data but instead uses
authorization tokens to prove an
identity between consumers and service
providers oath is about granting access
to functionality and then we have saml
the security assertion markup language
which is an open standard for exchanging
authentication and authorization between
an identity provider and a service
provider and this is important use for
samle which we use for single sign on
Via the browser
[Music]
okay hey this is Andrew Brown from exam
Pro we're looking at the concept of
single sign on so SSO is an
authentication scheme that allows a user
to log in with a single ID and password
to different systems and software SSO
allows it departments to administer a
single identity that can access many
machines and cloud services so the idea
is you have Azure active directory this
is just an example of a very popular one
You' use samle to do SSO and you can
connect to all things bu Google
workspaces Salesforce or your computer
uh the idea here is uh once you uh log
in um you don't have to log in multiple
times so you log into your primary
directory and then after that you're not
going to be presented with a login
screen some Services might show an
intermediate screen but the idea is
you're not entering your credentials in
multiple times so it's
[Music]
seamless all right let's talk about ldap
so lightweight directory access protocol
is an open vendor neutral industry
standard application protocol for
accessing and maintaining distributed
directory information Services over uh
IP network so a common use of ldap is to
provide a central place to store
usernames and passwords ldap enables for
same signon so same sign on allows users
to uh use a single ID and password but
they have to enter it every single time
they want to log in so maybe you have
your on premise active directory and
then it's going to store it in that ldap
directory and so the idea is that um you
know all these services like Google
kubernetes um jenkings is going to uh
deal with that ldap server so why would
you use ldap over SSO which is more
convenient or seamless so most SSO
systems are using ldap under the hood
but ldap was not designed neighly to
work with web applications so some
systems only support integration with
ldap and not SSO so you got to take what
you can get
[Music]
okay let's let's take a look here at
multifactor authentication also known as
MFA and this is a security control where
after you fill in your user's name and
email password you have to use a second
device such as a phone to confirm that
it's you that is logging in so MFA
protects against people who have stolen
your password MFA is an option in most
Cloud providers and even social media
websites such as Facebook so the idea is
I have my uh username or email and
password I'm going to try to log in this
is the first factor and the second
Factor multiactor is I'm going to use a
secondary device so maybe my phone we're
going to enter in different codes or
maybe it's passwordless so I just have
to press a button to confirm that it's
me and then I'll get access so in the
context to AWS it's strongly recommended
that you turn on MFA for all your
accounts especially the adus root
account uh we'll see that when we do the
follow
[Music]
alongs let's take a look at security
keys so a security key is a second
device used as a second step in
authentication process to gain access to
a device workstation or application a
security key can resemble a memory stick
and when your finger makes contact with
a button of exposed metal on the device
it will generate and autofill a security
token a popular brand of security Keys
is the UB key and this is the one I use
and is looks exactly like the one that's
right beside my desk it works out of the
box with Gmail Facebook and hundreds
more supports pho2 web o n uh u2f it's
waterproof and crust resistance it uh
has Vari like usba us NFC dual
connectors on a single key can do a
variety of things so when you turn on
MFA on your ads account you'll have
virtual MFA device so that's when you're
using something like a phone or using
software on your phone to do that then
there's the u2f security key so this is
what we're talking about right now and
there's even other kinds of Hardware MFA
devices um which we're not really going
to talk about but um you know just
security Keys tie into MFA and this is a
lot better way than using a phone
because you know you can have it on your
desk and press it um and you know you
don't have to worry about your phone
being not charged
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at itus
identity and access management also
known as IM am and you can use this
service to create and manage adus users
groups uh use permissions to allow and
deny their access to adus resources so
there's quite a few components here
let's get to it so the first is I am
policies so these are Json documents
which Grant permissions for specific
users groups or a role to access
services and policies are attached to IM
IM identities then you have I am
permissions or a permission and this is
an API action that can or cannot be
performed and they represented in the IM
policy document then there's the IM
identity so we have IM users these are
end users who log into the console or
interact with ad resources
programmatically or via clicking UI
interfaces you have IM groups so these
these uh group up your users so they all
share the same permission levels so
maybe it's admins developers or Auditors
then you have IM roles so these roles
Grant adus resources uh permissions to
specific adus API actions and Associate
policies to a role and then assign it to
an adus resource just understand that
roles are when you're attaching these to
resources so like if you have an ec2
instance and you say it has to access S3
you're going to be attaching a a role
not a policy directly okay
[Music]
hey this is Andrew Brown from exam Pro
and we are looking at I impulses a
little bit closer here and they are
written in Json and contain the
permissions which determine the API
actions that are allowed or denied um
and rarely do I write these out by hand
uh because they have a a little wizard
that you can use to write out the code
for you but if you want to you
absolutely can write it out by hand but
we should know the contents of it and
how these Json files work so the first
thing is the version um which is the
policy language version and it's been
2012 for a very long time I don't see
that changing anytime soon if they
happen to
change what or what the structure of the
Json is then you have the statements and
these are for policy elements uh and
you're allowed to have multiples of them
so the idea is that this is the the
policies or permissions we should say uh
that you uh plan on applying then you
have the Sid this is a way of labeling
your statements um this is useful for
like visualization or for ref
referencing it for later on but a lot of
times you don't have to have a sid um
then there's the effect it's either
allow or deny then you have the action
so here we're saying give access to S3
for all actions under it there's another
action down below where it's saying give
access I'm going to get my pen tool out
here just to create a service link rle
so that's a cross account rule there
then there's the principal so this is
the account user role or Federated user
to which you would like to allow access
or deny so we're specifically saying uh
this IM user named Barkley um in our
adus account there uh then there are the
resources so the resources to which the
action applies um so in this one up here
we are specifying a specific adus bucket
here we're saying all possible resources
in in anus account and then the
condition so there's all sorts of
different kinds of conditions so this is
a string like one and it's saying look
at the service name and if it starts
with this or that then they'll have
access to that so this person even
though it says all resources they're
really only going to have access to RDS
okay
[Music]
so in this follow along we're going to
take a closer look at I am policies so
go to the top and type in I am and what
we'll do is make our way over here uh
all the way over to policies and what I
want to do is create a new policy that
only has access to uh um limited
resources
so um let's say we want to create an
Amazon ec2 instance and that ec2
instance has access to a very particular
S3 bucket so what I want you to do is
make your way over to S3 and we're going
to create ourselves a new
bucket and I'm going to go ahead and
create a bucket here we're going to call
this
um policy
tutorial and I'm going to just put a
bunch of numbers here you'll have to
randomize it for your use
case and so now that we have our bucket
what we're going to do is go ahead and
create a
policy and the policy is going to choose
a service we're going to say S3 and what
I want to do is only be able to list out
actions I'm going to expand this so I
don't want everything so we're just
going to say list
buckets okay and then what we'll do is
uh expand this here and I want to save
for a particular bucket so we'll go back
over here click into our
bucket and uh we're going to go ahead
and set those
permissions by finding that Arn we're
going to paste that
we're going to paste that RN up there
sometimes it's a bit tricky it vanishes
on
you and we could set other conditions if
we wanted to but this is pretty simple
as it
is and so that's our rule here right so
we're saying this policy allows us to
list this bucket for that
okay so what we'll do is go ahead and
hit next we'll hit review and we'll just
say my bucket
policy and we'll create that policy
okay so there's a few other things I
think that I'd like to do with this
policy I'm going to pull it back up here
so if we want to find it uh they used to
be able to filter these based on the
ones that you
created but
um yeah these are like the little icons
so these are ones that I've created up
here and so there's my bucket
policy and I feel like I want to update
this policy to have a bit of extra
information here so I'm going to go edit
this
policy no you know what I think this is
fine so what I want to do is now create
a
ro and we're going to create a new role
and I'm going to call this
um well before I do I need to choose
what it's for so it's going to be for
ec2 so we're going to go ahead and hit
next we're going to choose our policy so
my bucket policy there it is and I want
to add another one here because I want
to be able to use sessions manager
because I really don't want to use an
SSH key to check that this works and
so um for this I I need to use SSM so
I'm going to type in SSM
here and I'm going just make sure this
is the new one so this policy will soon
be deprecated use Amazon SSM manag for
instance should always open these up and
read them to see what they do and so
that's the one that's going to allow us
to access uh Simpson manager so we can
use um sessions manager okay and so I'm
going to say my ec2 roll for S3
we going go ahead and create ourselves a
roll so now that we have our roll I'm
going to go over to
ec2 and I'm going to go ahead and launch
myself a new
instance we're going to choose Amazon L
2 we're going to stick with T2 micro I'm
going to go over to configuration here
everything is fine here um I'm fine with
all that storage is fine we'll go to
Security Group and I don't want any
ports open because I'm not going to be
using
SSH we're going to launch this instance
I don't even want to key
pair
okay and then we're going to go over
here and so what we're waiting for is
this instance to launch as that is going
what I want to do is go over to my S3
bucket and I want to place something in
this bucket so I do have some files here
um so what I'm going to
do is create a new folder here whoops uh
I'm going to go back
and I'm just going to create a folder
first create a folder Enterprise
D and I'm going to click into this and
then I'm going to upload all my images
here so you'll have to find your own
images off the internet this is just the
ones I have and we'll go ahead and
upload
those give that a
moment okay and so we don't have access
to read those files we'll adjust our
policy as we go so that we can do that
okay so this instance should be running
um it does doesn't have the two status
checks pass we should be able to uh
connect to it so click on connect here
and so we have options like E2 instance
connect sessions manager SSH client I
want you to go to sessions manager it
says we weren't able to connect to your
instance common reasons SSM agent wasn't
installed we absolutely have it
installed the required IM profile oh
right so we were supposed to attach I
forgot we were supposed to attach an ion
profile right so an ion profile is the
RO uh it or the it holds the role uh
that's going to give the permissions to
that instance and since we didn't add it
we have to go retroactively added after
the
fact and so I'm going to have to modify
the IM roll and we're going to choose my
ec2 roll for S3 and we're going to save
that and actually when that happens you
have to reboot the machine you only have
to do that if you have no Ro attached
like prior no profile attached and
you're attaching for the first time but
after that you never have to reboot the
machine this is the only case where
you'd have to do
that that's why when I launch an ec2
instance I always at least have the SSM
R attached the managed one that gets
sessions manager so that I don't ever
have to do a reboot in case I have to
update the
policy and so we will give that a moment
there it says initializing so I'm going
to try again to connect to it okay
and we still don't have that option
there um so I'm going to go back to my
instances I'm going to check to see if
the RO the RO or policy is
attached or profile I should
say so I'm just looking for it
here there it
is and so if I click into this into the
r we can see that we have the Amazon SSM
managed instance core there so that's
set up and the my uh bucket policy so
this has everything that it should be
able to do it no
problem okay so I'm going to try that
again okay so now the connect shows up
OS is finicky like that you just have to
have confidence in knowing what you're
doing is correct okay we'll go ahead and
hit
connect and I didn't have to use SSH
keys or anything and this is a lot more
secure way to connect your instances
when it logs Us in it's going to set us
as the SSM user but we want to be
the um the ec2 user
okay that's uh ads always makes their uh
am like their Linux versions as the ec2
user and that's what you're supposed to
use but it's just you just that's how
you have to get to that you just have to
type that pseudo Su hyphen ec2 user okay
just once and if you type who am I
that's who you are if you type type exit
you'll go back to that user so if I type
exit and I type who am I I'm now this
person so I'm going to go back hit up go
back in there type clear so now I want
to see if I have access to S3 so I have
to do a S3 LS want to see if I can list
buckets it says access
denied
so I mean that kind of makes sense
because if you have list buckets and
we're just saying only that bucket that
might not make a whole lot of
sense so I'm GNA go back to my policy I
might just written a a crummy policy but
we'll say I am here if we have that one
open we should just go
here and click on this policy
here I'm going to edit that
policy so I'm going to do is I'm just
going to change it and we say all
resources review the policy save changes
and we'll see how fast that
propagates okay
because I'm pretty sure I don't have to
do anything here it should just now give
me full access to
S3 just going to keep on hitting up
here so what I'm going to do is I'm just
going to take like a three four minute
break going to get a drink I'm going to
go back here and see if this propagates
I'm pretty sure I don't have to do
anything for that to propagate and I
think that I've attached everything
correctly here
okay okay so I haven't had much luck
here it's still having the same issue so
if that is happening what I'm going to
do um is I'm just going to reboot it
because maybe I didn't give it a good
opportunity to reboot there again I
don't think we should have to reboot it
every time when we're we're changing um
uh things there but we will give it
another go here and see if that fixes
that problem there so no sessions manner
is going to time out here which is
totally
fine it's going to kill that session
there um and so what we'll have to do is
close this out because there's not much
we can do with
that and we're going to go ahead and go
back to connect and so we're waiting for
this button to appear because it is
rebooting so if we want to monitor that
stuff usually there is an option here to
monitor where it'll show us the system
logs of what it's doing so here it's
just like restarting the
machine I'm not sure if we expect to see
something after
this so I can click that
there and uh yeah it's easy to get
turned around this so I can connect to
it again
now we'll type in pseudo Su hyphen ec2
user ads S3
LS and we still
have access deny for list buckets so if
that's the case it could be that um
sometimes you need other permissions
when doing list policy like uh list
buckets so if that's the case we're
going to do a sanity check I'm just
going to say all permissions here okay
and this way there's no way that I've
set this incorrectly um it just has to
work now so type this
in there we go okay so there has to be
something more to it so just because you
say list buckets you know like means
there must be more to it right so if I
go here to this
right and I say
whoops and I say uh list buckets here
we'll say
copy paste
okay here it's saying maybe I need get
object as well so I just know from using
it us a long time that that's the case
that it could be more than one thing so
you know that was in the back of my mind
that that could be happening and I guess
that is but notice that didn't have to
restart my uh my server boot my server
to get those to work um uh but anyway
let's go lock that down and see if we
can just kind of make this uh more
focused so let's say um all resources
I'm going to
specify the
condition so I might want to just say
for particular
buckets we say
specific when you checkbox everything
then you have to do this so for storage
accounts these are fine any for object
objects that could be something we'll
say multi- region access bucket any
bucket but what I'm going to say is I
want to only allow them to access things
in a particular bucket and so if I go to
Arn
here um what is our bucket
name our bucket name is policy tutorial
34141
whatever right and so we can actually
give it a wild card or we can say
Enterprise
D and we learned this in the course that
uh you can provide arms with
randomize things there I don't know if I
spelled it wrong over here so I should
really double check I should probably
just copy
it
oops I just don't want to type it wrong
and so
this
okay means that we should only be able
to get stuff from there I'm going to
review the policy let see if it takes
save the
changes and if I just view the Json
here notice it says anything from here
right so allow S3 anything as long as
it's within here and then it also broke
it up into sub1 4 here okay um so anyway
what I want to see is what
happens if I upload something into the
loose area here so I'm say
upload and I'm going to just say add a
file we're just going to grab data here
and upload
it go back to our bucket there's our
file we have that stuff in there and so
if I go back over to my ec2 instance
which I'm still connected
to uh who am I okay great clear um so
I'm going to say ads S3 LS see if that
works still it does good and so what I
want to do is see if I can copy a file
locally so I'm going to do Abus S3
copy I think it was S3 a no it's just S3
copy POC uh S3 SL SL
policy
tutorial
34 141 whoops
34 tutorial
hyphen
34141
slash Enterprise
D data.jpg I think it's a JPG let's go
double check yeah it is okay and then I
just want to say data.
jpg it downloaded it right so I'm going
to remove that one and so now what I'm
going to do is I'm just going to see if
my policy is working or maybe my
permissions aren't exactly what I think
they are and I was able to download it
so
it's these policies can get kind of
tricky because like this one says allow
all actions for these but then these say
all actions and
so that makes it hard because I want get
object so another thing we can
do and if that one doesn't work really
well I'm just going to write one by hand
it's not that scary to write these by
hand you just get used to it so I'm
going to say effect
um is it disallow or maybe it's
deny
deny
action S3 get object I believe that's
what it
is
resource and then I'm going to specify
exactly the resource I don't want it to
allow so we're going to say
AR AWS s three three
colons policy
tutorial
34141 uh and just say data.jpg
now if this is not valid it's going to
complain and say hey you didn't write
this right and it and it's fine okay
so we'll save those
changes and so that should deny access
to that right
hopefully I got the policy
right okay so that one doesn't work
which is
fine and that one's fine so that worked
we were able to deny that but you can
see there's a little bit of an art to
creating these policies uh as you make
more of them it becomes a lot easier so
hopefully it's not too scary but uh
that's all there really is uh to it that
I want to show you today so what we're
going to do is clear out this bucket
we're done with this bucket here so
we'll say delete whoops we got to empty
it first
and we'll just say permanently delete
here okay and we will exit that out
we're going to go ahead and delete that
bucket grab its name
here and uh we'll go back over
here I think I forgot to delete this
Bucket from earlier I'm just going to
delete that because I don't need that
bucket so that's okay with you just
going to go ahead and delete that
and we have that ec2 instance running so
we want to stop
that go ahead and we're going to
terminate that yes
please and then we'll go to IM and do
some
cleanup I have some custom roles I've
been creating um you know from prior
things a lot of those usually there's a
way to uh We've redesigned it okay
where's the redesign this is the
redesign that can't be it because
there'll be like rolls that ads makes I
think these are all rolls that I've
made um I don't want to delete service
rolls but I want to get rid of some of
these because I just have too many you
know it's getting out of hand for me and
I'm going to just see if it will let
me
delete all of these let's delete those
there we go just clean up a bit I still
have a lot here but there's like service
roles that OS creates once and you
really don't want to delete those
because you
don't um and then I have a bunch of
these like I'm never going to use these
so I might as well detach them delete
detach you really don't want to keep
like rolls that you're never going to
use
around things like that like gauze we're
going to be using that
again
delete there's that bucket we just
created anyway you get the idea so uh
yeah that's uh that's I am
[Music]
okay principle of least privilege PP is
the computer computer security concept
of providing a user role or application
the least amount of permissions to
perform an operation or an action and
the way we can look at it is that we
have just enough AIS so Jaa permitting
only the exact actions for the identity
perform a task and then we have just in
time jit permitting the smallest length
of duration an identity can use
permission so usually when we're talking
about PLP it's usually a focus on here U
but now these days uh there's a larger
focus on jit as well and so J is the
difference between having longlived um
uh permissions or access Keys versus
short-lived ones and the most
Progressive thing in PP is now
risk-based adaptive policies so each
attempt to access a resource generates a
risk score of How likely the request is
to be from a compromized source so the
risk score could be based on many
factors such as device user location IP
address what service is being accessed
and when did they use MFA did they use
Biometrics things like that and right
now as of this time itus does not have a
risk-based adaptive policies built into
I am you can roll your own um what's
interesting is Cognito has risk-based
adaptive policies they call like um
adaptive authentication but that's for
user pools and not identity pools user
pools is for getting access to an app uh
that you have built through an ipd where
identity pools in cognito is about
getting access to aabus resources so uh
you know I'm sure abos will get it
eventually but they just don't have it
right now and you have to rely on third
part party um identity Solutions uh to
get risk-based adaptive policies now
talking about just enough access and
just in time just in time is like you
think how would you do that with the ads
you just add and remove permissions
manually well one thing you could do is
use something like console me so this is
an open- Source Netflix project to self-
serve short-lived IM policies so an end
user can access ad resources while
enforcing Jaa and jit and so there's a
repo there as well um the idea is they
have like this self- sered wizard so you
say I want these things and then the
machine decides okay you can have them
or you you don't need them and it just
freezes you up asking people and
worrying about the length and stuff like
that
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at the
idibus root user uh and this gets
confusing because there's an account
root user and regular user so let's
distinguish what those three things are
so here we have an adus account and the
account which holds all the adus
resources including the different types
of user
then you have the root user this is a
special account with full access that
cannot be deleted and then you have just
a user and this is a user for common
tasks that is assigned permissions so
just understand that sometimes people
say it was account they're actually
referring to the user and sometimes when
they're saying account they're actually
referring to the AES account that holds
the users I know it's confusing it just
it's based on what people decide the
context is when they're speaking so the
ads account user is a special user who's
created at the time of the ABS account
creation
and they can do uh they have a lot of
conditions around them so the re user
account uses an email and password to
log in as opposed to the regular user
who's going to provide their account ID
Alias username and password the root
user account cannot be deleted the root
user account has full permissions to the
account and its permissions and cannot
be limited and when we say it cannot be
limited we're saying that if you take an
IM am policy to explicitly deny the user
access resources it's not something you
can do however you can do it in the case
of adab organizations service control
policies because a service control
policy applies to a bunch of accounts so
it just it's one level above and so that
is a way of limiting root users but
generally you can't limit them within
their own account uh there can only be
one root user uh per ad of us account
the root user is instead for very spe
specific and specialized tasks that are
infrequently or rarely performed and
there's a big list and we'll get into
that here in a moment and the root uh
account should uh not be used for daily
or common tasks it's strong strongly
recommended to to never use the root
users access keys because you can
generate those and use them it's
strongly recommended to turn on MFA for
the root user and AD us will bug you to
no ends to tell you to turn it on so
let's talk about the uh tasks that you
should be performing with a root user
and only the root user can perform so
changing your account settings this
includes account name email address root
user password root user access Keys
other account settings such as contact
information payment currency preference
regions do not require the root user
credentials so not everything um restore
IM user permissions so if there's an i
IM admin so it's just a user that has
admin access who actually revokes their
own permissions you can sign into the
root user to edit policies and restore
those permissions um so you can also
activate IM access to the billing and
cost Management console you can view
certain tax invoices you can close your
ads account you can change or cancel
your adus support plan register as a
seller in the reserved instance
Marketplace enable MFA delete on S3
buckets edit or delete an Amazon S3
bucket policy that includes an invalid
VPC ID or VPC endpoint ID sign up for
govcloud and something that's not in
here which this I took this from the
documentation but uh you can use the
adus uh account user to create the
organization you can't create that with
any other user so um you know the ones I
highlighted in red are very likely to
show up your exam and that's uh why I
highlighted them there for you but there
you go
[Music]
hey this is Andre Brown from exam Pro
and we are taking a look at adus single
sign on also known as adus SSO and so
this is where you create or connect your
Workforce identities in adabs once and
manage access centrally across your adus
organization so the idea here is you're
going to choose your identity Source
whether it's it SSO itself active
directory samle 2.0 IDP you're going to
M manage user permission centrally to
ads accounts applications samle
application
and it uses it can you get single click
access to all these things so you know
just to kind of zoom in on this graphic
here uh you know you have your on
premise active directory it's
establishing a ad trust connection over
to Able single sign on you're going to
be able to apply permissions to access
resources within your adus account so
via adus organizations in your
organizational units down to your
resources you can also use ads SSO to
access custom samle based application so
you know if I built a web app and I uh
like the exam Pro platform and I wanted
to use sample based uh connections for
single sign on there I could do that as
well and you can connect out SSO access
to your business Cloud application so
Office 365 Dropbox slack things like
that so there you
[Music]
go let's take a look here at application
integration so this is the process of
letting to Independent applications to
community Comm unicate and work with
each other commonly facilitated by an
intermediate system so Cloud workloads
uh strongly encourage systems and
services to be Loosely coupled and so
inabus has many services for the
specific purpose of application
integration and these are based around
common systems or design patterns that
utilize application integration and this
would be things like queuing streaming
Pub sub API gateways State machines
event buses and I'm sure there are more
but that's what I could uh think about
that are the most common ones
[Music]
okay so to understand queuing we need to
know what is a messaging system so this
is used to provide asynchronous
communication and decouple processes via
messages and events from a sender
receiver or a producer and a consumer so
a queing system is a messaging system
that generally will delete messages once
they are consumed it's for simple
communication it's not real time you
have to pull the data it's not reactive
and uh a good analogy would be imagine
people that are queuing in a line to go
do something so fre TOS it's called
Simple queuing service sqs it's a fully
managed queuing service that enables you
to decouple and scale microservices
distributed systems and serverless
applications so a very common use case
in a web application would be to queue
up transactional emails uh to be sent
like sign up reset password and the
reason why we have queing to decouple uh
those kind of actions is that if you had
a long running task um and you had too
many of them it could hang your
applications so by decoupling them and
letting a separate compute uh service
take care of that um that would be
something that would be very useful
[Music]
okay let's take a look here at streaming
and so this is a different kind of
messaging system um but the idea here is
you have multiple cons consumers that
can react to events and so in streaming
we call messages events and then in a
queing system we just call them messages
but events live in the Stream for long
periods of time so complex operations
can be applied and generally streaming
is used for Real Time stuff whereas
queuing is not necessarily real time and
so ad's solution here is Amazon Kinesis
you could also use Kafka but we'll focus
on Kinesis here so Amazon Kinesis is the
adist fully managed solution for
collecting processing and analyzing
streaming data in the cloud so the idea
is that you have these producers so that
are producing events could be ec2
instances mobile devices could be a
computer or traditional server they're
going to go into the data stream there's
a bunch of shards that scale and there's
consumers on the other side so maybe red
shift wants that data Dynamo DB S3 or
EMR okay but the thing you have to
remember is that streaming Is For Real
Time data and as you can imagine because
it's real time and it's doing a lot more
work than um a queuing system it's going
to cost more okay
so we have another type of messaging
system known as pubsub so this stands
for publish subscribe pattern commonly
implemented in messaging systems and a
pub sub system the sender of messages
the Publishers do not send their message
directly to receivers they instead send
their messages to an event bus the event
bus categorizes their messages into
groups then receivers of messages
subscribers subscribe to these groups
whenever new messages appear within
their subscriptions the messages are
immediately delivered to them so the
idea is you have Publishers event bus
subscribers and event buses appear more
than once so it actually appears in
streaming appears in this Pub sub model
and then it can appear in other
variations so you're going to hear it
more than once the word event bus um so
the idea here is the publisher has no
knowledge of who the subscribers are
subscribers do not pull for messages
messages are instead automatically
immediately pushed to the subscribers
and messages and events are
interchangeable terms in Pub sub all
right and so you know the idea here with
Publisher subscribers just imagine
getting like a um a magazine
subscription right if you think of that
you kind of think of the mechanisms that
are going here in terms of practicality
it's very common to use these as a
real-time chat system or a web hook
system so you know hopefully that gives
you an idea there in terms of aws's
solution we're using simple notification
service SNS this is a highly available
durable secure fully managed Pub sub
messaging service that enables you to
decouple micros Services distributed
systems and serverless applications so
here we have a variety of Publishers
like the SDK the CLI cloudwatch a with
Services you'll have your SNS topic you
can uh filter things fan them out and
then you have your subscribers so Lambda
sqs emails HPS looks very similar to
streaming but again you know um you know
there's not a lot of communication going
back between it it's just Publishers and
subscribers and it's limited to you know
these things here so it's a very managed
service right whereas uh Kinesis you can
do a lot more with it
[Music]
okay so what is API Gateway well it is a
program that sits between a single entry
point and a and multiple backends API
Gateway allows for throttling logging
routing logic or formatting of the
requests and response when we say
request and response we're talking about
https uh requests and responses and so
the service for ads is called Amazon API
Gateway so API Gateway is just a type of
pattern and this is the few cases where
ADS has named the thing after what it is
and so we have Amazon API Gateway which
is a solution for creating secure apis
in your Cloud environment at any scale
create apis that act as a front door for
applications to Access Data Business
logic or functionality from backend
services so the idea is that you have
data coming in from uh mobile apps web
apps iot devices and you Define the API
calls and then you say where do you want
them to go so maybe tasks are going to
go to your lambdas um and then other
routes are going to go to RDS Kinesis
ec2 uh or your web application and so
these are really great for having um
this uh being able to Define your API
routes and change them on the Fly and
then and always route them to the same
place
[Music]
okay so what is a state machine it is an
abstract model which decides how one
state moves to another based on a series
ofad conditions think of a state machine
like a flowchart and for AWS the
solution here is adus Step function so
coordinate multiple adus Services into a
serverless workflow a graphical console
to visualize the components of your
application as a series of steps
automatically trigger and track each
step and retries when there are errors
so your application executes in order as
expected every time logs the state of
each step so when things go wrong you
can diagnose and debug problems quickly
and so here's the example of using a
bunch of um uh steps together on the uh
the adus step functions service and so
you know this is generally applied for
servess workflows but it is something
that is very useful in application
integration
[Music]
okay so what is an event bus an event
bus receives events from a source and
routes events to a Target based on rules
so I'll get my pen tool out here so we
have an event it enters the event bus we
have a rules tell it to go to the Target
it's that simple and we have been seeing
event buses in other things like uh
streaming and uh Pub sub but adus has
this kind of event bus offering uh that
is kind of uh high level it's called
event bridge and it's a service event
bus service that is used for application
integration by streaming realtime data
to your applications the service was
formerly known as event Amazon
cloudwatch events they gave give it a
renaming to give it a a better um
opportunity for users to know that it's
there to use uh and they also extended
its
capabilities and so the thing is is that
a lot of AD services are always
admitting events and they're already
going into this bus and so if you
utilize this service um it's a lot
easier than having to roll your own
thing uh with other services so Amazon
event bridge will just Define an event
bus so there is an event bus holds event
data defines the rules on event bus to
react to events you always get a default
event for every single adus account you
can create custom event buses scope to
multiple accounts or other adus accounts
you have a SAS event bus scope to
thirdparty SAS providers you have
producers these are ad services that
emit events you have events these are
data emitted by Services they're jent
objects that uh travel the stream within
the event bus you have partnered sources
these are third-party apps that can emit
events to event buses you have rules
these determine what events to capture
and pass to targets and then targets
which are a services that consume events
so yeah it's all just this great
built-in um uh uh stuff that's going on
here and so you know there there might
be a case where you can use event bridge
and save your time uh a lot of time and
effort uh doing application integration
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at
application integration services at a
glance here so let's get through them so
the first is simple notification service
SNS this is a pub sub messaging system
sends notific ation via various formats
such as plain text email htps web hooks
SMS text messages sqs and Lambda pushes
messages which are then sent to
subscribers you have sqs this is a
queuing messaging system or service that
sends events to a queue other
applications pull the queue for messages
commonly used for background jobs we
have step functions this is a state
machine service it is it coordinates
multiple a Services into a servess
workflow easily share data among lambdas
have a group of lambdas wait for each
other create logical steps also works
with fargate tasks we have a rent Bridge
formerly known as cloudwatch events it
is a service event bus that makes it
easy to connect applications together
from your own application thirdparty
services and adus services then there's
Kinesis a real-time streaming data
service creates producers which send
data to a stream multiple consumers can
consume data within a stream used for
realtime analytics click streams
ingesting data from a fleet of iot
devices you have Amazon mq this is is a
manage message broker service that uses
Apachi active mq so if you want to use
Apachi active mq there it is manage
kofka service and this gets me every
time because it says
msk and that is the proper
initialization but you'd think it'd be
MKS it is a fully managed Apachi Kafka
service kofka is an open source platform
for building realtime streaming data
pipelines and applications similar to
Kinesis but more robust very popular by
the way we have API Gateway a fully
managed service for developers to create
publish maintain Monitor and secure apis
you can create API endpoints and wrote
them to ad Services we have appsync this
is a fully managed graphql service
graphql is an open- Source agnostic
query adapter that allows you to query
data from many different data sources so
there you
[Music]
go hey this is Andrew Brown from exam
Pro and we are comparing virtual
machines to Containers so I know we
covered this prior but I just want to do
it one more time just to make sure that
we fundamentally understand the
difference before we jump into
containers so the idea is that if you
were to request an ec2 instance it has a
host operating system that we don't
really know much about but we don't
really need to know uh and then the idea
is you have a hypervisor which allows
you to deploy virtual
machines and so when you launch an ec2
instance you're actually launching a VM
on top of a hypervisor on a server uh
with on uh within the adabs uh data
centers servers there and you're going
to choose an operating system so like
fun to and it might come with some
pre-installed packages or you're going
to install your own libraries packages
and binaries and then you're going to
decide what kind of workloads you want
to run on there so it could be D Jango
uh mongodb so your database and some
kind of queuing system like rabbit mq
the difficulties with virtual machines
you're always going to end up with some
unused space because you're going to
want to have some Headroom uh to make
sure that uh you know if you know Dango
needs more memory or or mongod DB needs
more storage that you have that room
that you can grow into
but the idea is that you're always
paying for that even when you're not
utilizing it and so you know that can be
uh not as cost effective as you'd like
it to be so when we're looking at um
doing this again and we are using
containers um instead of the hypervisor
we have container virtualization a very
common one would be called Docker Damon
for Docker of course and so now you're
launching containers and so maybe you
have Alpine and this is for your web app
and then you install exactly the
libraries packages and binaries you need
for that and then for mongodb you want
to have a different OS different
packages and same thing with Rabbid mq
maybe you want to run it on FreeBSD and
the idea is that uh you know you're not
going to have this waste because it it's
kind of changed in the sense that these
containers are flexible so they can
expand or decrease based on the the use
case of what they need uh and you know
if you use particular services like it
fargate you know you're paying like for
running the containers not necessarily
uh for over provisioning okay so VMS do
not make best use of space apps are not
isolated which could cause config
conflict security problems or resource
hogging containers allow you to run
multiple apps which are virtually
isolated from each other launch new
containers configure OS uh dependencies
per container
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at the
concept of microservices and to
understand microservices we first need
to understand monoliths or monolithic
architecture and the idea here is that
we have one app which is responsible for
everything and the functionality is
tightly coupled so I'm going to get my
pen tool out here and just to highlight
notice that there is a server and
everything is running on a single server
whether it's load balancing caching the
database um maybe the marketing website
the front-end JavaScript framework the
back end with its API uh the uh
om connected to background tasks things
like that and that's the idea of a
monolith and that's what um a lot of
people are used to doing but the idea
with microservice architecture is that
you have multiple apps which are
responsible for one uh one thing and the
functionality is isolate and stateless
and so just by uh leveraging um various
cloud services or bolting it onto your
service um you know you are technically
using microservice architecture so maybe
your web app is all hosted um in
containers so you have your apis your or
your orm your reports maybe you've
abstracted out some particular functions
into Lambda functions you have your um
marketing website hosted on S3 you have
your frontend JavaScript hosted on S3
You're Now using elastic load balancer
uh elastic cache
RDS sqs and that's the idea between
monoliths and microservices
[Music]
okay let's take a look here at
kubernetes which is an open-source
container orchestration system for
automating deployment scaling and
management of containers it was
originally created by Google and now
maintained by the cloud native Computing
foundation so the
cncf kubernetes is commonly called K8
the8 represents the remaining letters
for kuti which is odd because everyone
calls it kubernetes with the S on there
but that's just what it is the advantage
of kubernetes over Docker is the ability
to run containers distributed across
multiple VMS a unique component of
kubernetes are pods a pod is a group of
one or more containers with with shared
storage network resources and other
shared settings so here is kind of an
example where you have your kubernetes
master it has a schedule controller etcd
you might be using it uses an API server
to run nodes within the nodes we have
pods and within the pods we have
containers kubernetes is ideally for
microservice architectures where company
has tens to hundreds of services they
need to manage I need to really
emphasize that tens to hundreds of
services all right so you know crimin is
great but just understand that it is
really designed uh to be used for
massive amounts of microservices if you
don't have that need you might want to
look at something just easier to use
[Music]
okay all right let's take a look here at
Docker which is a set of platforms of
service products that use OS level
virtualization to deliver software in
packages called containers so Docker was
the earliest popularized open source
container platform meaning there's lots
of tutorials there's a lot of services
that uh integrate with Docker or make it
really easy to use and so when people
think of containers they generally think
of Docker there's of course a lot more
options out there than Docker to run
containers but this is what people think
of and so we said it's a suite of tools
so the idea is you have this Docker CLI
so these are CLI commands to download
upload build run and debug containers a
Docker file a configuration file on how
to provision a container Docker compose
uh which is a tool and configuration
file when working with multiple
containers Docker swarm an orchestration
tool for managing deployed
multicontainer architectures Docker Hub
a public online repository for
containers published by the community
for download and one really interesting
thing uh that came out of Docker was the
open container initiative oci which is
an open government structure for
creating open industry standards around
container formats and runtimes so Docker
established the OC oci and it is now
maintained by the Linux foundation and
so the idea is that you can write a
Docker file or or do things very
similarly and use different types of um
technologies that can use containers as
long as they're oci compatible you can
use them so Docker has been losing favor
with developers due to their handling of
introducing a paid open source model and
Alternatives like podman are growing and
that's why we're going to talk about
podman next
[Music]
okay so let's take a quick look here at
podman which is a container engine that
is oci compliant and is a drop in
replacement for Docker I just just want
to get you exposure here because I want
you to know about this um and that you
can uh use it as opposed to using Docker
um there are a few differences or
advantages that podman has so podman is
Damon list where Docker uses a container
D Damon podman allows you to create pods
like KU brunetes where Docker does not
have pods podman only replaces one part
of Docker podman is is to be used
alongside builda and uh scopio so you
know Docker is an all-in-one kind of
tool uh everything is done via single
CLI and everything is there but you know
they just wanted to make it more module
and so uh these other tools anytime you
say podman it usually means we're
talking about podman builda and scopio
so builda is a tool used to build the
oci images and scopio is a tool for
moving container images between
different types of container storages p
is not going to show up in your exam but
you should practically know it um just
for your own benefit
[Music]
okay let's take a look here at the
container services offered on AWS
so we have primary services that
actually run containers provisioning and
deployment on you know tooling around
provisioning and deployment and
supporting services so the first here is
a lock stick container service ECS um
and the advantage of this service is
that it has no cold starts but it is a
self-managed dc2 so that means that
you're going to be always paying for the
resource as it is running all right then
you have ads fargate so this is more
robust than uh using a Lambda it can
scale to zero cost um and it's uh being
managed by adus managed ec2 however it
does have cold starts so you know if you
need containers launching really fast
you might be wanting to use ECS then you
have elastic kubernetes service eks this
is uh open source it runs kubernetes um
and this is really useful if you want to
avoid vendor lockin um which is not
really a problem but uh that or it's
just you want to run kubernetes then you
have itus Lambda so you only think about
the code uh it's designed for short
running tasks uh if you need something
that runs longer You' want to use that
is serverless you'd use aess fargate
which is serverless containers you can
deploy custom containers so prior it us
Lambda just had um pre-built runtimes
which were containers but now you can
create any kind of container and uh use
that uh on it was Lambda for
provisioning deployment you can use
elastic Bean sock so um it can uh deploy
elastic container service for you um
which is very useful there now there's
app Runner which kind of overlaps on
what elastic beanock does but it
specializes it specializes for
containers um and I believe that it can
actually I don't know what it uses
underneath because it is a managed
service so elastic beanock is um open
you can see what is running underneath
and app Runner I don't believe you can
see what is running underneath it's just
taken care of by AWS then there's AWS uh
co-pilot CLI so this allows you to build
release operate production ready
containerized applications on app Runner
ECS and Abus fargate for supporting
services you have elastic container Reg
indry this is reple for your containers
not necessarily just Docker containers
but containers in general probably oci
compliant containers x-ray so analyze
and debug between microservices so you
know it's distributed tracing then you
have step functions so stitch together
lambas and ECS tasks to uh create um um
a state machine and the only thing I
don't have on here would be you know
being able to launch an ec2 instance
from the marketplace that has um a uh a
container runtime installed like doer U
I just don't feel that that's very
relevant for the exam but it is another
option for containers not something that
people do very often but there you
[Music]
go hey this is Angie Brown from exam Pro
and we are taking a look here at
organizations and accounts so adus
organizations allow the creation of new
adus accounts and allows you to
centrally manage building control access
compliance security and share resources
across your adus accounts so here's kind
of a bit of a structure of um
the architecture of adus organizations
and we'll just kind of walk through the
components so the first thing you have
is a root account user this is a single
signin identity that has complete access
to all adus services and resources in an
account and each account has a root
account user so generally you will have
a master or root account and even within
that you'll have a root account user and
for every additional account that you
have you'll notice over here we have a
root account
user then there's the concept of
organizational unit
uh these are commonly abbreviated to OU
so they are a group of adus accounts
within an organization which can contain
other organizational units creating a
hierarchy so here is one where we have
called Starfleet and here's one called
Federation planets and underneath we
have multiple uh accounts it was
accounts within that organizational unit
and even though it does not show it here
you can create an organizational unit
within an organizational unit then we
have service control policies scps and
these give uh Central control over the
allowed permissions for all adus
accounts in your organization helping to
ensure your accounts stay within your
organization's guidelines what they're
trying to say here is that um there's
this concept of adus I am policies and
all you're doing is you're creating a
policy that's going to be uh
organizational uniwide or organizational
wide or for select accounts so it's just
a way of applying I am policies across
multiple accounts AIS organizations must
be turned on and once it's turned on it
cannot be turned off it's generally
recommended that you do turn it on um
because basically when if you're going
to run any kind of serious workload
you're going to be using adus
organizations to uh isolate your adus
accounts based on workloads you can
create as many adus accounts as you like
One account will be the master or root
account um and I say root account here
because this is the new language here
and some of the documentation still
calls it master account so just
understand this is the root account not
to be confused with the root account
user so another clarification I want to
make is an ad account is not the same as
a user account which is another thing
that is confusing so when you sign up
for AWS you get um an adus account and
then it creates you a user account which
happens to be a root user account so
hopefully that is
[Music]
clear so adus control tower helps
Enterprises quickly set up a secure ads
multi account it provides you with a
baseline environment to get started with
a multi-count architecture so it does
this a few uh a few different ways the
first thing is it provides you a landing
Zone this is a baseline environment
following well architected and best
practices to start launching production
ready workloads so imagine you wanted to
go have um you know the perfect
environment that you know secure um is
correctly configured and has good
logging in place that's what a landing
zone is and so os's Landing zone for
control tower is going to have SSO
enabled by default so it's very easy to
move between it accounts it will have
centralized logging for ad cloud trail
so that you know they're going to be
tamper evident or tamper proof away from
your workloads where they can't be
affected it'll have cross account
security auditing um so yeah Landing
zones are really great to have then
there's the account Factory they used to
call this um uh a vending machine but uh
they changed it to account Factory the
idea is that it automates provisioning
of new accounts in your organization it
standardizes the provisioning of new
accounts with pre-approved account
configuration you can configure account
Factory with pre-approved Network
configuration and region selections uh
enable sell service for your Builders to
configure and provision to accounts
using AA service catalog AA service
catalog is just pre-approved uh
workloads uh via Cloud information
templates you created to say okay you're
allowed to launch This Server these
resources um and the third and most
important thing that a control tower
comes with is guard rails so these are
prepackaged governance rules for
security operations compliance the
customers can select and apply
enterprise-wide or to specific groups of
accounts
so adus control tower is the replacement
of the retired adus Landing zone so if
you remember adus Landing zones which
was never a selfs serve easy thing to
sign up for it required a lot of money
and uh stuff to go in there they just
don't really have it anymore and it was
control tower is the new offering um
there
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at 's
config and to understand adus config we
need to know what compliance as code is
and to understand compliance as code we
need to understand what change
management is so change management in
the context of cloud infrastructure is
when we have a formal process to monitor
changes enforce changes and remediate
changes and compliances code also known
as CAC is when we utilize programming to
automate the monitoring enforcing and
remediating changes to stay compliant
with the compliance program or expected
configuration so what is adus config
well it's a compliances code framework
that allows us to manage change in your
it accounts on a per region basis
meaning that you have to turn this on
for every region that you need it for
and so here is a very simple example
where let's say we create a config Rule
and we have an ec2 instance and we
expect it to be in a particular State
and then in the other case we have a an
RDS instance and it's in a state that we
do not like so the idea is that we try
to remediate it to put it in the state
that we want it to be and those config
rules are just powered by lambdas as you
can see based on the Lambda icon there
so when should you use Adis config well
this is when I want this resource to
stay configured a specific way for
compliance I want to keep track of
configuration changes to resources I
want a list of all resources within a
region and I want to use uh uh analyze
potential security weaknesses and you
need detailed historical information so
there you go
[Music]
hey this is Andrew Brown from exam Pro
and in this follow along we're going to
take a look at adus config so adus
config is a tool that allows you to
ensure that your services are configured
as expected so I've already activated it
in my North Virginia region so what I'm
going to do is just go over to Ohio here
uh because it is per region activated
and I'll go over to config and then what
we'll have to do is set it up so there
is this oneclick setup and it did Skip
me to the review step because it's kind
of piggybacking on the configuration of
my original one here but the idea is
that you'll just say uh record all
resources in this region or things like
that you'll have to create a service
roll link if you have not done so so
this will look a little bit different
but here it's using the existing one
you'll have to choose a bucket so or
create a bucket uh it's not super
complicated so you get through there you
hit confirm and basically you're going
to end up with this so the inventory um
lets you see all the the resources that
or not all of them but most resources
that are in your account in this
particular region it this will not
populate right away so you will have to
wait a little bit of time for that to
appear one really nice thing are
conformance packs I really love these
things when ad of us first brought these
out there was only like a couple but now
they have tons and tons and tons of
conformance packs so you can go deploy a
conformance pack and you can open up the
templates I just want to show you look
at how many they have so there some of
you might recognize like
nist uh CIS things like that well
detected uh stuff and all these are um
and I'm not sure if it's easy to open
these up but all these are if we open
them up they're on GitHub is these are
just Cloud foration templates to set up
configuration rules so there's a variety
of suggested rules uh like around IM
best practices and things like that that
we can load in um but the idea is that
you're just going to create rules so you
go here and you add a rule and they have
a bunch of manage rules here um that we
can look at but I think it might be fun
to actually run a um a conformance pack
I'll just show you what it looks like to
add a rule first so let's say we wanted
to do something for
S3 um and it was making sure that we are
blocking Public Access so we go next
here generally you'll have a trigger
type you can choose whether it's uh
configured when it happens or it's
periodic this is disabled in this case
here and you just scroll on down um and
then once you've added the rule what you
can
do is also manage remediation so if this
rule said hey this thing is
non-compliant we want you to take a
particular action and you have all these
adus actions that you can perform and
you can notify the right people to
correct it or have it auto correct if
you choose to do so um for rules you can
also make your own custom one so that's
just you providing your own Lambda
functions you're providing that Lambda
Arn and so basically you can have it do
anything that you want whatever you want
to put in a Lambda you can make adist
config check for okay so it's not super
complic at here but um this one here is
just going to go ahead and check and so
if we go and
reevaluate it might just take some time
to show up it's either going to say that
it's compliant or non-compliant okay and
I it should be compliant but while we're
waiting for that to happen let's just
see how hard it is to deploy a
conformance pack because I feel like
that's something that's really important
oh you can just drop them down and
choose them that's great so we might
want to go to I am here oops identity
and access
management and hit next and say uh my my
um uh IM best practices and you might
not want to do this because it does have
spend and I want I say spend it's not
going to happen instantly but the idea
is that if you turn this on and forget
to remove it uh you will see some kind
of charges over time because it does
check based on the rules it's not super
expensive but it is something to
consider about um but anyway so it looks
like we created that conformance pack so
if I refresh it looks like it's in
progress I wonder if that's going to set
up a cloud formation template I'm kind
of curious about that so we'll make our
way over to cloud
formation and it is so that's really
nice because once that is done what we
can do is just tear it down by deleting
the stack so I'm going to go back over
to our conformance pack
here let's take a look here and so it
still says it's in progress but it is
completed and we can click into
it and we can see all the things that
it's doing so it says item groups have
user check informance pack um and so it
looks like there's a bunch of of uh cool
rules uh here so what we'll do is we'll
just wait a little while and we'll come
back here and then just see if um this
updates and see how compliant we are
from a uh a basic account okay all right
so after waiting a little while there it
looks like some of them are being set so
I just gave it a hard refresh here uh
and here you can see that it's saying is
root account um whoops we give it a
moment here to refresh but uh is the
root account MFA applied yes have we
done a password policy no and actually I
never did a password policy which is
something I forgot to do but here
they're just talking about the minimums
and maximums of things that you can
do okay so that's a conformance pack um
but if we go to rules actually I guess
it's all the rules here I can't really
tell the difference between the
conformance pack rules and our plane
rules kind it's kind of all mixed
together here I
think yeah so it's a bit hard to see
what's going on there if we go to the
performance pack and click in again it
might show the rules yeah there we go so
here's the rules there just see a little
bit more information so use a hardware
MFA so you know how they're talking
about using a security key like what I
showed you that I had earlier in the
course things like that um I am password
policy things like that so you know not
too complicated but um I think I'm all
done here so what I'm going to do is I'm
going to go over to cloud formation and
tear that on down but you get the
idea well I might want to show you uh
drift so there used to be a way it's go
like keep changing things on me here but
there's a way to see uh history over
time and so that was
something that they used to show and I'm
just trying to like find where they put
it because it is like somewhere else
resources
maybe ah resource timeline okay so they
moved it over into the resource
inventory and so if we were to take a
look at something anything maybe this
here resource timeline um and there
might not be much here but the idea is
it will show you over time how things
have changed so the idea is that not
only can you say with a config is
something compliant but when was it
compliant and that is something that is
really important to know okay so very
simple example maybe not the best but
the idea is that we can see when it was
and was not compliant based on uh
changes to our stuff but uh anyway that
looks all good to me here so I'm going
to make my way over to cloud formation
actually already already have it open
over here we're going to go ahead and
delete that stack um um termination
protection is enabled you must first
disable it so we'll edit it disable it
whatever okay we'll hit delete there and
as that's deleting I'm going to go look
for and config my
original rule there again I'm not really
worried about it I don't think it's
going to really cost me anything but uh
I'm also just kind of clear the house
here just so you're you're okay as well
and so if we go over to our rules um the
one that I spun up that was custom um I
think was this one here because these
are all grayed out right so I can go
ahead there delete that rule type in
delete and we are good so there you
go that
is it all
[Music]
right adabs quick starts are pre-built
templates by adabs and adus partners to
help deploy a wide range of stacks it
reduces hundreds of manual uh procedures
into just a few steps the uh Quick Start
is composed of three parts it has a
reference architecture for the
deployment an itus cloud formation
templates that automate and configure
the deployment a deployment guide
explain the architecture implementation
in detail so here's an example of one
that you might want to launch like the
adus Q&A bot and then you will get an
architectural diagram and a lot of
information about it and from there you
can just go press the button and launch
this infrastructure most quick start
reference deployments enable you to spin
up a fully functional architecture in
less than an hour and there is a lot as
we will see here when we take a look for
[Music]
ourselves all right so here is uh adus
quick starts where we have a bunch of
cloudformation templates uh built by
adabs or Amazon or adus partner networks
APM partners and uh there's a variety of
different things here so I'm just going
to try to find something like Q&A
bot Q&A bot just type in bot here and I
don't know why it was here the other day
now it's not showing up which is totally
fine but um you know I just want
anything to deploy just to kind of show
you what we can do with it so you scroll
on down we have uh this graphic here
that's representing what will get
deployed so we have cloudfront S3 Dynamo
DB assistance manager Lex paully all
these kind of fun stuff um and there's
some information about how it is
architected and the idea is you can go
ahead and launch in the console or view
the implementation guide let's go take a
look here um and there's a bunch of
stuff so we have Solutions and things
like that conversational things like
that but what I'm going to do is go
ahead and see how far I can get to
launching with this it doesn't really
matter if we do launch it it's it's just
the fact that um I want to just show you
what you can do with it so if we go to
the designer it's always fun to look at
it in there because then we can kind of
visualize all the resources that are
available and I thought that that would
populate over there but maybe we did the
wrong thing so I'm just going to go back
and
click I'm just going to click out of
this oops cancel let's close that leave
yes and we will launch that
again and so this oh viewing the
designer I hit the wrong button
okay so now this should show us the
template might just be
loading there we go so this is what it's
going to launch and you can see there's
a lot going on here just going to shrink
that there
uh and I don't know if you can make any
sense of it but clearly it's doing a lot
and so if we were happy with this and we
wanted to launch it I know I keep
backing out of this but we're going to
go back into it one more
time we can go here and we can go next
and then we would just fill in what we
want so you name it put the language in
and this is stuff that they set up so
maybe you want a mail voice set the ab
in and stuff like that and it's that
simple really um and every stack is
going to be different so they're all
going to have different configurations
ation options but hopefully that gives
you kind of an idea of what you can do
with quick starts
[Music]
okay let's take a look at the concept of
tagging within AWS so a tag is a key and
value pair that you can assign to anus
resource so as you are creating a
resource it's going to prompt you to say
hey what tags do you want to add you're
going to give a key you're going to give
a value and so some examples could be
something like based on Department the
status
the team the environment uh the project
as we have the example here the location
um and so tags allow you to organize
your resources in the following way for
resource management so specific
workloads so you can say you know
developer environments cost management
and optimization so cost tracking
budgets and alerts operations management
so business commitments SLA operations
Mission critical Services security so
classification of data security impact
governance and Regulatory Compliance
automation workload Automation and so
it's important to understand that
tagging can be used in Junction with um
IM policy so that you can restrict
access or things like that based on
those tags
[Music]
okay all right I just want to show you
one interesting thing about tags um and
it's just the fact that it's used as the
name for some services so when you go to
ec2 and you launch an instance uh the
way you set the name is by giving it a
tag called name and I just want to prove
prove that to you just like one of those
little exceptions here so we choose an
instance
here we go to configure storage and then
what we do is we add a tag and we say
name um and my server name okay and then
we go ahead and review and launch we're
going to launch this I don't need a key
pair so we'll just say proceed without
key pair I
acknowledge
okay and we will go view the instances
and you'll see that is the name so um
that's just like one of those exceptions
or things that you can do with tags if
there's other things with tags I have no
idea that's just like a a basic one that
everybody should know and that's why I'm
shown to you with the tags but there you
[Music]
go so we just looked at tags now let's
see what we can do with resource groups
which are a collection of resources that
share one or more tags or another way to
look at it it it's a way for you to take
multiple tags and organize them uh into
resource groups so it helps you organize
and consolidate information based on
your project and the resources that you
use resource groups can display details
about a group of resources based on
metrics alarms configuration settings
and at any time you can modify the
settings of your resource groups to
change what resources appear resource
groups appear in the global console
header uh which is over here and under
the systems manager so technically it's
part of an simple system assists manager
or System Manager interface but it's
also part of the global interface so
sometimes that's a bit confusing but uh
that's where you can find it
[Music]
okay all right so what I want to do is
explore resource groups and also um
tagging so what I want you to do is type
in resource groups at the top here and
it used to be
accessible not sure where they put it
but it used to be accessible here at the
top but they might have moved it over to
systems manager so I'm going to go to
SSA here not sure why I can't seem to
find it today and on the left hand side
we're going to look
for resource
groups
for for
all right so what I want to do is take a
look at resource groups and I'm really
surprised because it used to be
somewhere in the global now but I think
they might have changed it um and what's
also frustrating is if I go over to
systems manager it was over here as well
and so on the left hand side I'm looking
for resource groups it's not showing up
so I don't know best you keep moving
things around on me and I'm I can only
update things so quickly in my courses
but if you type in resource groups and
tag editor it's actually over here um I
guess it's its own Standalone service
now why they keep changing things I
don't know but uh the idea is we want to
create a resource Group so you can
create unlimited single region groups in
your ab account use the group to view
related insights things like that so I'm
going to go ahead and create a resource
Group you can see it can be tag based or
cloud formation based but I don't have
any tags I don't really have anything
tags so what I'm going to do is make my
way over to S3 we're just going to
create some resources or a couple
resources here with some tags so that we
can do some filtration so I can go ahead
and create a bucket I'm going to say my
bucket uh this like that
whoops and then down below I'm going to
go down to tags and we're going to say
project and we're going to say um RG for
Resource
Group okay and then I can go back over
here and then I'm going to just say I
can say exactly what type I want I'm
going to support all resource
types and I'm going to say
project RG see how it aut completes and
we'll go down below we'll just
say my
RG a test
RG we'll create
that and so now we have a resource Group
and we can see them all in one place uh
resource groups are probably useful for
using in um policies so I can say say
like Resource Group IM
policies that's probably what they're
used for
okay so before you use I am managed to
access resour groups you should
understand I am features things like
that and so administrators can use Json
policies to specify who has access to
what and so a policy action or Resource
Group is used following the prefix
resource groups so my thought process
there is that if you want to say okay
you have access to a resource you can
just specify a resource Group and it
will include all the resources within
there and so that might be um a better
way to apply permissions at a per
project basis um and that could save you
a lot of time writing out IM policies so
that's basically all there really is to
it also you kind of get an overview of
of the U resources that are there so
that can be kind of useful as well
there's the tag editor here I can't
remember what you use this for you can
set up tag
policies um tag policies help you
standardize tags on resource groups in
your accounts use uh to Define Tech
policies and Abus org to attach them to
the entire organization um we're not in
the OR account so I'm not going to show
you this and it's not that important um
but just understand that resource groups
can be created and they are used within
I policies in order to um uh Grant or
deny access to stuff I'm go ahead and
delete that Resource Group and really
stop moving that on me if you move it
one more time I'm just never going to
talk about resource groups again
[Music]
okay hey hey this is Andrew Brown from
exam Pro and we are taking a look at
business Centric services and you might
say well why because in the exam guide
It's explicitly says that these are not
covered but the thing is is that when
you're taking the exam some of the uh
choices might be some of these Services
as distractors and if you know what they
are it's going to help make sure that
you um uh guess correctly and the thing
is that some of these services are
useful you should know about them so
that's another reason why I'm talking
about them here so first one is Amazon
connction this is a virtual call center
you can create workflows to Route
callers you can record phone calls
manage a que of callers based on the
same proven system used by Amazon
customer service teams we have
workspaces this is a virtual Remote
Desktop Service secure manage service
for provision either windows or Linux
desktops in just a few minutes which
quickly scales up to thousands of
desktops we have work docs which is a
shared collaboration service
essentialized storage to share content
in files it is similar to Microsoft
SharePoint think of it as a shared
folder where the company has ownership
we have chime which is a video
conference service it is similar to zoom
or Skype you can screen share have
multiple people on the on the same call
it is secure by default and can show you
a calendar of upcoming calls we have
workmail this is a manag business uh
email contacts calendar service with
support of existing desktop and mobile
email client applications that can
handle things like IMAP similar to Gmail
or exchange we have pinpoint this is a
marketing campaign Management Service
pinpoint is for sending targeted emails
Via SMS push notifications voice
messages you can perform um A to B
testing or create Journey so complex
email response workflows we have sces
this is a transactional email service
you can integrate sces into your
application to send emails you can
create common templates track open rates
keep track of your reputation we have
quick site this is a business
intelligence uh service connect multiple
data sources and quickly visualize data
in the form of graphs with little to no
knowledge definitely you want to
remember quick site SCS pinpoint Point
uh cuz those definitely will show up in
the exam the rest probably not but they
could show up as distractors
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at
provisioning services so let's first
Define what is provisioning so
provisioning is the allocation or
creation of resources and services to a
customer and IT provisioning services
are responsible for setting up and
managing those adaus Services we have a
lot of services that do provisioning
most of them are just using cloud
formation underneath which we will
mention here but let's get to it the
first is elastic beanock this is a
platform as a service to easily deploy
web apps eie will provision various a
with services like ec2 S3 SNS cloudwatch
ec2 Auto scaling groups load balancers
uh and you can think of it as the Heroku
equivalent to AWS then you have opsworks
this is a configuration Management
Service that also provides managed
instances of Open Source configuration
managed software such as chef and p
puppet so you'll say I want to have a
load balcer or I want to have servers
and it will provision those for you uh
indirectly then you have Cloud
information itself this is an
infrastructure modeling and provisioning
service it automates the provisioning of
AD Services by writing Cloud information
templates in either Json or yaml and
this is known as IAC or infrastructures
of code you have quick starts these are
pre-made packages that can uh be
launched and configure your Aus compute
network storage and other services
required to deploy a workload ons we do
cover this in this course but quick
starts is basically just Cloud
information templates that are authored
by the community or um buy um Amazon
partner Network okay then we have abis
Marketplace this is a digital catalog
for thousands of software listings of
independent software vendors that you
can use toine by in tes and deploy
software so the idea is that um you know
you can go there and provision whatever
kind of resource you want we have Abus
amplify this is a mobile web app
framework that will provision multiple
Aus Services as your backend it's
specifically for serverless services I
don't know why I didn't write that in
there um but you know like Dynamo DB um
things like uh whatever the graphql
service is called API Gateway things
like that uh then we have ads app Runner
this is a fully managed service that
makes it easy for developers to quickly
deploy containerized web apps and apis
at scale with no prior information
experience required it's basically a
platform as a service but for containers
we have AIS co-pilot this is a command
line interface that enables customers to
quickly launch and manage containerized
applications any bus it basically is a a
CLI tool that sets up a bunch of scripts
to set up pipelines for you makes things
super easy we have Adis code start this
provides a unified user interface
enabling you to manage your software
development activities in one place
usually launch common types of stacks
like lamp then we have cdk and so this
is infrastructure as a code tool allows
you to use your favorite programming
language generates out Cloud information
templates as a means of I so there you
[Music]
go hey this is Andrew Brown from exam
Pro and we are taking a look at a
elastic bean stock before we do let's
just Define what passes so platform as a
service allows customers to develop run
and manage applications without uh the
complexity of building and maintaining
the infrastructure typically associated
with developing and launching an app and
so elastic beanock is a pass for
deploying web apps with little to know
uh knowledge of the underlying
infrastructure so you can focus on
writing application code instead of
setting up an automated deployment
pipeline or devops tasks the idea here
is you choose a platform upload your
code and it runs with little uh
knowledge of the infrastructure and adab
us will say that it's generally not
recommended for production apps but just
understand that they are saying this for
Enterprises and large companies if
you're a small to medium company you can
run elastic beanock for quite a long
time it'll work out great elastic
beanock is powered by cloud formation
templates and it sets up for you elastic
load balancer asgs RDS ec2 instances
preconfigured for particular platforms
uh monitoring integration with
cloudwatch SNS uh deployment strategies
like in place blue green green uh
deployment has security built in so it
could rotate out your passwords for your
databases and it could run dockerized
environments and so when we talk about
platforms you can see we have Docker
multicontainer Docker uh go.net Java
nodejs Ruby PHP python Tomcat go a bunch
of stuff and just to kind of give you
that architectural diagram to show you
that it it can launch of multiple things
[Music]
okay hey it's Andrew Brown from exam Pro
and this fall along we're going to learn
all about elastic beanock maybe not
everything but we're going to definitely
know how to at least um use the service
so elastic beanock is a platform as a
service and what it does is it allows
you to uh deploy web applications very
easily so here I've made my way over to
elastic beanock that we an environment
and app and then we set up our
application we have two tiers a web
server environment a worker environment
worker environment is great for long
running
workloads performing uh background jobs
and things like that and then you have
your web server which is your web server
and you can have both and it's generally
recommended to do so um but anyway what
we'll do is create a new application so
let's say my app here and uh there's
some tags we can do and then it will
name based on the environment then we
need to choose an environment name so
I'll say my environment and just put a
bunch of numbers in there hit the check
availability scroll on down and we have
two options manage platform custom
platform and I'm not sure why custom is
blanked out but it would allow you to um
it would allow you to I think use your
own containers so I'm a big fan of Ruby
so I'm going to drop down to Ruby and
here we have a bunch of different
versions and so 2.7 is pretty pretty new
which is pretty good and then there's
the platform version which is fine and
the great thing is it comes with a
sample application now you could hit
create environment but you'd be missing
on a lot if you don't hit this configure
more options I don't know why they put
it there it's a not a very good UI but
um if you click here you actually get to
see everything possible and so up here
we can have some presets where we can
have a single instance so this is where
it's literally running a single E2
instance so it's very cost effective you
can have it with spot uh spot pricing so
you save money um there's High
availability so you know if you want it
set up with a load balcer an auto
scaling group it will scale very well or
you can do custom configuration we
scroll on down
here you can enable Amazon x-ray you can
rotate out logs you can do log
streaming um there's a lot of stuff here
and basically it's just like it sets up
most for you but you can pretty much
configure what you want as well if we
had the load balcer set if I go here go
to High availability now we'll be able
to change our load balancer options you
have different ways of deploying so you
can go here and then change it from all
at once rolling immutable traffic
splitting depends on what your use case
is um we can set up a key pair to be
able to log into the
machine there's a whole variety of
things you can connect your database as
well so it can create the database
alongside with it and then it can
actually rotate out the key so you don't
have to worry about it which is really
nice what I'm going to do is go to the
top here and just choose a single
instance because I want this to be very
cost effective we're going to go ahead
and hit create
environment and so we are just going to
wait for that to start up and I'll see
you back when it's done
okay okay so it's been uh quite a while
here and it says a few minutes so if it
does do this what you can do is just
give it a hard refresh I have feeling
that it's already done is it done yeah
it's already done so and here it says on
September 2020 elastic talku Etc default
default I don't care um but anyway so
this application I guess it's in a
pending State um I'm not sure why let's
go take a look here causes instance has
not sent any data since launch uh none
of the instances are sending data so
that's kind of interesting because um I
shouldn't have any problems you know
what I mean so what I'm going to do is
just reboot this Miss machine and see if
that fixes the issue there but usually
it's not that difficult cuz it's the
sample application it's not up to me um
as to how to fix this you know what I
mean
so I'm not sure but um what we'll do is
we will let the machine reboot and see
if that makes any difference okay all
right so after rebooting that machine
now it looks like the server is healthy
so it's not all that bad right if you do
run issues that is something that you
can do and so uh let's go see if there's
is actually working so at the top here
we have a link and so I can just right
click here it says congratulations your
first 8us elastic uh beanock Ruby
application is now running so it's all
in good shape um there's a lot of stuff
that's going on here in elastic beanock
that we can do uh we can go back to our
configuration and change any of our
options here so there's a lot of stuff
as you can see uh we get logging uh so
click the request log so if we click on
this and say last 100
lines should be able to get uh logging
data we have to download it I wish it
was kind of in line but here you can
kind of see what's going on so we have
STD access logs error logs Puma logs
elastic beam stock engine so you could
use that to debug very common to take
that over to uh support if you do have
issues uh for Health it monitors the
health of the instances which is great
then we have some uh monitoring uh data
here so gives you uh like a built
dashboard so that's kind of nice you can
set up alarms uh you have not defined
alarms you can add them via the
monitoring dashboard so I guess you'd
have
to you'd have to somehow add them um I
don't think I've ever added alarms for
um Classic Bean but it's nice to know
that they have them you can set up
schedules for managed events then this
is event data so it's just kind of
telling you it's kind of like logs it
just tells you of things that have
changed so there's stuff like that what
I'm looking for is to see how I can
download the existing
application because there's a version
uploaded here oh the source is over here
okay
so I think it's probably over here the
one that's running so that's
it if it was easy to find what I
probably would do is just modify it and
oh yeah it's over here so if we go here
and download the
zip I wonder if it'd be even worth um
playing with this so let's I'm just
going to see if we can go over to
Cloud9 and give this a go quickly
so if we go over and launch a Cloud9
environment maybe we can tweak it and
upload a revised version so we say
create new we'll say EB
um uh environment for elastic beanock
we'll set it all the defaults that's all
fine it's all within the free tier we'll
create that environment what I'm going
to do is just take this uh Ruby zip file
and move it to my
desktop and as that is loading we'll
give it a moment here I'm just going to
go back and I was just curious ious does
it let you download it directly from
here no so the only thing is that you
know if you download that application
elastic beanock usually has a
configuration file with it and so I
don't know if they would have given that
to us but if they did that would be
really great but we just have to wait
for that to uh launch there as well I
guess you can save configurations and
roll back on those as
well um but we will just wait a moment
here
while it's going I might just peek
inside of this file to see what it is
this ZIP
contains just going to go to my desktop
here open up that
zip so it looks pretty simple it doesn't
even look like a rails app it looks like
maybe it's a Sinatra app I thought
before that it would it would have
deployed a Ruby on Rails application but
maybe they keep it really
simple
um I don't see usually it's like yamell
files they use for configuration I don't
see that there
so it might be that the default settings
will work fine uh there's a king fig. Ru
and stuff like that but once Cloud9 is
up here we will upload this and see what
we can do with it okay so there we go uh
Cloud9 is ready to go and so if we right
click here whoops right click here we
should be up be able to upload a file if
not we can go up here to the
top or it's here or
there where is the upload I've I've
uploaded things in here so I absolutely
know we can I just got to find it
is that the
upload upload files
Cloud9 oh boy that's not helpful that's
not helpful at all so let me just click
around a little bit here I mean worst
case I can always just bring it in Via a
curl oh upload local files there it is I
was is just not um being patient okay so
we'll drag that on in there and we
will did it upload yep it's right there
okay great so we need to unzip it so
what I'll do is just drag this on up
here I'll do an LS and we'll say
unzip
rubyzip and so that unzipped the
contents there I think the readme was
part of Cloud9 so I'm going to go ahead
and delete that out not that it's going
to hurt anything and so now what we can
do and we'll delete the original
the original zip
there um and let's see if we can make a
change here so I'm just going to open up
see what it is so it's yeah it's running
Sinatra so that's pretty clear there we
have a proc file to say how it runs we
have a worker sample so that's just
tells how the requests go you don't need
to know any of this I'm just kind of
clicking through it because I know Ruby
very well we have a cron yamel file so
that could be something that gets loaded
in here so I think basically a Sinatra
app probably just works off the bat here
but if we want to make a change we
probably just make a change over to here
so I'll go down here and this is
your second 8s elastic beanock
application so the next thing we need to
do is actually zip the contents here I
don't know if it would let us zip it
with in here but I'll have to look like
Zip the contents of a
directory Linux this goes to
show Google is
everything so the easiest way to zip a
full
folder
um
zip everything in the current
directory
Linux okay that's easy so we'll go back
over here and we will type in
zip and it wants hyphen R for recursive
which makes sense and then the name of
the zip so
um uh Ruby 2.zip and we'll do
period zip warning found is who is
zip
oh uh yum install zip maybe we have to
install uh ZIP but maybe it's not
installed pseudo yum install
zip since it's Amazon 2 uses
yum and so package already installed so
I'm going to type zip again so zip is
there now great oops don't need to
install
twice zip warning Ruby to zip not found
or
empty okay so install zip and use zip
pyph R you can use the flag to best
compensate so if that's not working what
I'm going to do is just go up a
directory why is it saying not found or
empty
[Music]
maybe I need to
use okay so I think the problem was is I
was using the wrong flag so I put F
instead of R I don't know why I did that
so I probably should have done this okay
and so that should have copied all the
contents of that file so what I'm going
to do is go ahead whoops make sure I
have that selected and download that
file and once I have downloaded that
file I'm going to just open the contents
to make sure it is what I expect it to
be so we're going to open that up and
whoops get out of here when and it looks
like everything I want so what I'm going
to do is go back over to here I'm going
to make sure I have my Ruby 2 on my
desktop we're going to see if we can to
upload another version here so upload
and deploy choose the file we're going
to go all the way to my desktop here and
we're going to choose Ruby 2 and um like
Ruby 2 will be the version name or we'll
just say two and we'll deploy and we'll
see if that works okay but there are
like uh elastic beanock configuration
files like yaml files that can sit in
the root directory and so generally
you're used to seeing them there but you
know I imagine that eight of us probably
engineered these examples so that it
uses all the default settings but uh
once this is deployed I'll see you back
here in a moment okay after a short
little wait it looks like it has
deployed so what I'm going to do is just
close my other tabs here and open this
up and see if it's worked it says your
second ad elastic being stck Ruby
application so uh we were successful uh
deploying that out which is really great
so what we can do now is just close that
tab there and uh since we have that
cloud environment it will shut down on
its own but you know just for your
benefit I think that we should shut it
off right now so go ahead and delete
that I'm going to go back over to
elastic beanock here and I just want to
destroy all of it so we'll see if we can
just do that from here terminate the
application enter the name so I think we
probably have to enter that in
there and so I think that oh a problem
occurred rate
exceeded what that's AWS for you so it's
not a big deal I would just go and check
it
again and maybe what we'll do is just
delete the application
first okay so that one is possibly
deleting
let's go in here is anything
changing can't even tell we'll go ahead
oh can't take that one
out delete application
again if it takes you a couple times
it's not a big
deal it's AWS 4 yes so there's a lot of
moving parts so it looks like it is
terminating the instance and so we just
have to wait for that to complete uh
once that is done we might have to just
tear down the environment so I'll see
you back here when it has finished
tearing this down okay all right and so
after a short little wait here I think
it's been destroyed we'll just double
check by going to the applications going
to the environments yeah and it's all
gone probably because I initially
deleted that environment and then took
the application with it so I probably
didn't have to delete the app separately
um but uh yeah so there you go just make
sure your Cloud9 environment is gone and
you are a okay there'll probably be some
like lingering S3 buckets so if you do
want to get rid of those you can it's
not going to hurt anything having those
around uh but they do tend to stack up
after a while which is kind of annoying
so if you don't like them you can just
empty them
out as I am doing here whoops oh just
permanently
delete copy that text
there we go back
to here and then just go take out that
bucket delete that
there oh if you get this this is kind of
annoying but uh elastic beanock lights
to put in an I am permission or policy
in here so if you go down here there's a
bucket policy you just have to delete it
out it prevents it from being
deleted and we'll go back over here and
then we will delete
it okay and yeah there we go that's
[Music]
it so let's take a look at serverless
services on AWS and this is not
including all of them because we're
looking at the most purely serverless
Services uh if we try to include all the
serverless services it would just be too
long of a list uh but let's take a look
here so um before we do let's just
redefine what is serverless so when the
underlying servers infrastructure and
operating system is taken care of by the
CSP serverless is generally by default
highly available scalable cost effective
you pay for what you use the first one
is Dynamo DB which is a serverless nosql
key value and document database is
designed to scale to billions of records
with guaranteed consistent data return
in at least a second you do not have to
worry about managing chars you have
simple storage service S3 which is a
serverless object storage service you
can upload very large and unlimited
amounts of files you can pay for what
you store you don't worry about the
underlying file system or upgrading the
dis size we have e CS fargate which is a
seress orchestration container service
is the same as ECS except you pay on
demand per running container with ECS
you have to keep a ec2 server running
even if you have no containers running
where adus manages the underlying server
so you don't have to scale or upgrade
the ec2 server we have adus Lambda which
is a serverless function service you can
run code without provisioning or
managing servers you upload a small
piece of code choose uh how much memory
you want how long want the function is
allowed to run before timing out your
charge based on the runtime of the
service function rounded to the nearest
100 milliseconds we have step functions
this is the state machine service it
coordinates multiple Services into
serverless workflows easily shared data
among lambdas have a group of lambdas
wait for each other create logical steps
also work with fargate tasks we have
Aurora servus this is a serous on demand
version of Aurora so when you want most
of the benefits of Aurora but uh trade
you have to trade off those cold starts
or you don't have lots of traffic or
demand so things Ser the services that
we could have put in here as well is
like API Gateway appsync it amplify um
and those are like the the first two
were application Integrations you could
say sqs SNS those are all serous
services but you know again we'd be here
all day if I I I listed them all
[Music]
right all right let's take a look at
what is is serverless and we did look at
it from a server perspective earlier in
the course but let's just try to
abstractly Define it and talk about the
architecture so serverless architecture
generally describes fully managed cloud
services and the classification of a
cloud service being serverless is not a
Boolean answer it's it's not a yes or no
but an answer on a scale where a cloud
service has a degree of serverless and I
do have to point out that this
definition might not be accepted by um
everybody because calist is one of those
uh terms where um we've had a bunch of
different cloud service providers Define
it differently and then we have thought
leaders that have a particular concept
of what it is so you know I just do my
best to try to make this practical here
for you but a seress service could have
all or most of the following
characteristics and so it could be
highly elastic and scalable highly
available highly durable secure by
default it abstracts away the underlying
infrastructure and our build based on
the execution of your business tasks a
lot of times that that cost is not uh is
not always represented as something that
is like I'm paying X for compute it
could be abstracted out into some kind
of um credit that uh doesn't necessarily
maap to something physical then we have
serverless can scale to zero meaning
when it's not in use the serverless
resources cost nothing uh and these two
last topics basically pull into pay for
Value so you don't pay for Idol servers
you're paying for the value uh that your
service
provides and uh my friend Daniel who
runs the servus Toronto group he likes
to describe servus as being similar to
like energy efficient rating so an
analogy of servus could be similar to
energy rating labels which allows
consumers to compare the Energy
Efficiency of a product so some services
are more servoless than others and again
you know some people might not agree
with that where there's a a definitive
yes or no answer but I think that's the
best way to look at it
[Music]
okay hey it's Andrew Brown from exam Pro
and we're taking a look at windows on
ads so ads has multiple cloud services
and tools to make it easy for you to run
window workloads on ads so let's get to
it so the first is Windows servers on
ec2 so you can select from a number of
Windows Server versions including the
latest version uh like Windows Server
2019 uh for uh databases we have SQL
server on RDS you can select from a
number of SQL database versions then we
have adabs directory service which lets
you run Microsoft active directory ad as
a managed service we have ads license
manager which makes it easier to manage
your software licenses from software
vendors such as Microsoft we have Amazon
FSX for Windows file server which is a
fully managed scalable storage built for
Windows we have the ads SDK which allows
you to write code in your favorite
language to interact with adus API but
it specifically has support for net a
language favorite for Windows developers
we have Amazon workspaces so this allows
you to run a virtual desktop you can
launch a Windows 10 desktop to provide
secure and durable workstations that is
accessible from wherever you have an
internet connection AOS Lambda supports
poers shells a programming language to
write your serverless functions and we
have adus migration acceleration program
map for Windows is a migration
methodology for moving large Enterprises
it us has Amazon partners that
specialize in providing professional
services for map this is not just
everything for Windows on AWS like if
you want to move your SQL Server over to
RDS postest I believe they've like they
created an adapter to do that um but
yeah hopefully that gives you an idea
what you can do with Windows on AWS
[Music]
okay hey this is Andrew Brown from exam
Pro and I want to show you how you can
launch a Windows uh server on AWS so
what you're going to do is go to the top
here and we are going to type in ec2 and
from here uh what we'll do is we'll go
ahead and launch ourselves a new ec2
instance and we are going to have um a
selection of instances that we can
launch and so we're looking for the
Microsoft Windows uh server and this is
interesting there's actually a free tier
uh eligible that is crazy because if you
go over to Azure they don't have a free
tier Windows Server us does so that's
pretty crazy um and it runs on a T2
micro no that can't be right there's no
way it can run a T2 micro that seems
like that's too small let's try it okay
I just don't believe it because when you
use Azure you have to choose a
particular size of instance by default
and it's a lot more expensive and there
is no free tier so we'll go here there
are free tiar just not really for
Windows in particular so we'll go here
this looks good security groups this
opens up RDP so we can get into that
machine we're gon to go next here
and launch this
machine says if you plan to use Ami the
benefits the Microsoft license Mobility
check out this form that's not something
we're worried about today and I mean I
guess we can create a key pair I'm not
sure what we would use a key pair for
here um for Windows Amis the private key
file is required to obtain the password
used to log into the instance okay so I
guess we're going to need it
so Windows key
great we'll launch that
instance and uh I'll see you back here
when it launches but I just don't
believe that it would launch that fast
you know all right so after a short
little wait here the server is ready and
so let's see if we can actually go ahead
and connect to this so I'm going to hit
connect here and we'll go over to rdb
client so you connect to your windows
instance using a remote desktop client
of your choice and downloading and
running the RDP shortcut below so I'm
gon to go ahead and download this and
you're GNA have to be on a um Windows
machine to be able to do this or have an
rdb client installed I think there's one
for Mac that you can get from the Apple
Store um but all I'm going to do is just
double click the file
so you probably can't see it here I'm
just going to expand this try to oh my
computer is being silly but anyway there
we go we moved it over there I'm just
going to drag over here and just double
click this image so you can see that I'm
doing it I'm saying
connect okay
and it's going to ask for a password so
I'm going to hope that I can just click
that and get the password so to decrypt
the password you will need your key PA
instance you'll have to upload that and
I don't know if I remember having to do
that before but it's a great security
measure so I'm fine with it I'm going to
drag my key to my desktop so I can see
what's going on there as
well and we're going to go grab that and
decrypt the password and so
now um where's our password oh it's
right here okay so we're going to grab
that password
there we will paste that in said
okay say yes and see if we can connect
to this instance and if this is running
on a T2 micro I'm going to lose it
because that is just
cheap it just just doesn't seem possible
to me because again on Azure you have to
launch an instance with a lot of stuff
and it just uh seems uh crazy what's
also interesting is that adabs uh on
Windows like launches so fast it's
unbelievable how fast these servers spin
up and it's just very unusual but yeah
so we are in
here
um it's not asking me to activate or
anything so I guess there's already a
Windows license
here and um I'm not sure if there's any
kind of like games installed like do we
have mind sweeper can I play M sweeper
on here
it's a data center server so I'm
assuming not um but yeah so this is a
Windows server and it's pretty
impressive that this works I'm not sure
if this is going to have an outbound
connection here um just because we
probably would have to configure it just
say okay I just I really don't think
it's going to go out to the Internet by
default yeah so you'd probably have
to do some stuff you
know oh no we go so yeah we got to the
Internet so it's totally possible but uh
yeah that's about it that's all I really
wanted to show you so what I'm going to
do is just go back to ec2 and we're
going to shut down the server here just
expand that
there and we will go here and we will
terminate that
instance good we'll give that a refresh
that's shutting down and we are
[Music]
done hey this is Andre Brown from exam
Pro and we are taking a look at Abus
license manager and before we do let's
talk about what b y l or bring your own
license mean so this is the process of
reusing an existing software license to
run vendor software on a cloud vendor
Computing service Bol allows companies
to save money since they may have
purchased the license in bulk or the
time that provided a greater discount
than if purchased again and so the
example of this could be the license
Mobility provided by Microsoft volume
licensing to customers with eligible
server applications covered by the
Microsoft software Assurance program uh
and I don't know what I was trying to do
there I guess maybe it's just essay and
I missed the parentheses there on the
end no big big deal um but Aus license
manager is a service that makes it
easier for you to manage your software
licenses from software vendors centrally
across ads in your on premise
environments ads license manager
software uh that is licensed based on
Virtual cores uh physical cores sockets
or a number of machines this includes a
variety of software products for
Microsoft IBM sap Oracle and other
vendors so that's the idea you say what
is my license type it's it's bound to
this amount of vcpus aabus license
manager works with ec2 with dedicated
instances dedicated hosts and even spot
instances and for RDS there's only for
Oracle databases so you can import that
license for your Oracle server um just
understand that um if you're doing
Microsoft Windows servers or Microsoft
SQL Server license you're generally
going to need a dedicated host because
of the insurance program uh and this can
really show up on your exam so even
though AIS license manager works on
dedicated instances and spot instances
just try to gravitate towards dedicated
hosts on the server or on the exam
[Music]
okay all right let's take a look at the
logging services that we have available
in AWS so the first one here is cloud
trail and this logs all API calls
whether it's SDK or the CLI so if it's
making a call to the API it's going to
get tracked between AD services and this
is really useful to say who can we blame
who was the person that did this so who
created this bucket who spent up that
expensive ec2 instance who launched the
stagemaker notebook um and the idea here
is you can detect developer
misconfigurations detect malicious
actors or automate responses through the
system then you have cloudwatch which is
a collection of multiple Services I
commonly say this is like an umbrella
service because it has so many things
underneath it so we have cloudwatch logs
which is a centralized place to store
your cloud services log data and
application logs metrics which
represents a Time ordered set of data
points a variable uh to monitor uh event
Bridge or previously known as cloudwatch
events triggers an event based on a
condition so every hour take a snapshot
of the server alarms triggers
notifications based on metrics
dashboards creates visualizations based
on metrics and that's not all of the
things that are under cloudwatch but
those are the core five ones you should
always know um absolutely there then we
have adus x-ray this is for distributed
tracing system so you can use it to
pinpoint issues within your
microservices so you see how data moves
from one app to another how long it took
to move and if it failed uh to move
forward
[Music]
okay let's take a closer look here at
ABA cloud trail because it's a very
important service so it's a service that
enables governance compliance
operational auditing and risk auditing
of your A's account and the idea is that
every time you make an API call it's
going to show up as some kind of
structured data that you can uh interact
with or read through so AB cloud trail
is used to monitor API calls and actions
made on ads account easily identify
which users and accounts made the call
to AWS so you might have the WHERE so
the source IP address the when the event
time the who the user agent uh and the
what the region resource and action so
I'm just going to get my pen tool out
here for a moment and just notice you
have the event time so when it happened
the source the name the region The
Source IP address the user agent uh who
was doing it so here was leforge the
response element so you know it's very
clear what is going on here um and then
you know cloud trail is already logging
by default and will collect logs for the
uh for the last 90 days via event
history if you need more than 90 days
you need to create a trail which is very
common you'll go into AWS and make one
right away trails are outputed to S3 and
do not have like event history to
analyze a trail you have to use Amazon
Athena and I'm sure there are other ways
to analyze it within AWS but here's just
what the event history looks like so
right off the bat you can already see
that there are information there there
I'm not sure if they've updated the UI
there they might have uh as even when
I'm recording this I kind of feel like
if we go into the follow along which we
will um I bet they might have updated
that the idea here is that you know you
can browse the last 90 days but anything
outside of that you're going to have to
do a little bit of work yourself
[Music]
okay so we're not going to cover all the
cloudwatch services there's just too
many but let's look at the most
important ones and one of that those
important ones is cloudwatch alarms so
cloudwatch alarms monitors a cloud watch
metric based on a defined threshold uh
so here you can see there's kind of a
condition being set there so if the
network in is greater than 300 for one
data points within five minutes it's
going to breach an alarm so uh that's
when it goes outside is defined
threshold and so the state's going to
either be something like okay so the
metric or expression is within the
defined threshold so do nothing alarm
the metric or expression is outside of
the defined threshold so do something or
insufficient data the alarm has just
started the metric is not available none
enough data is available and so when the
state has changed you can Define actions
that it should take and so that could be
doing a notification autoscaling group
or any C2 action um so cloudwatch alarms
are really useful for a variety of
reasons the one that we will come across
right away will be setting up a billing
[Music]
alarm so let's take a look here at the
autonomy of an alarm and so I have this
nice graphic here to kind of explain
that there and so the first thing is we
have our threshold condition uh and so
here you can just set a value and say
okay the value is a th000 or 100
whatever you want it to be and this is
going to be for a particular metric the
actual data we are measuring so maybe in
this case we're measuring Network in so
the volume of incoming Network traffic
measured in bytes so when using 5 minute
monitoring divide by 300 we get bytes
per second if you're trying to figure
out that calculation there you have data
points so these represent the metrics
measurement at a given point then you
have the period how often it checks to
evaluate the alarm so we could say every
five minutes uh you have the evaluation
period so the number of previous periods
and the data points to alarm so you can
say one data point is breach and
evaluation period going back four
periods so this is what triggers the
alarm uh the thing I just want you to
know is that you can set a value right
and that it's based on a particular
metric and there is a bit of logic here
in terms of uh the alarm so it's not as
simple as just it's breached but there's
this period thing happening
[Music]
okay let's take a look at cloudwatch log
so to understand that we have log
streams and log groups so a log stream
is a stream that represents a sequence
of events from an application or
instance being monitored so imagine you
have an ec2 instance running a web
application and you want those logs to
be streamed to cloudwatch logs that's
what we're talking about here so you can
create log streams manually uh but
generally this is automatically done by
the service you using uh unless you
collecting application logs on an ec2
instance as I just described here is a
log group of a Lambda function you can
see the log streams are named after the
running instance lambda's fre uh
frequency run on New instances so the
stream contains timestamps so what I'm
trying to say here is that there's a
variety of different Services Lambda RDS
what have you and they already send
their logs to cloudwatch logs and
they're and they're going to vary okay
so here's a log group of an application
log running on uc2 you can see here the
log streams are named after the running
instance ID here is the log group for
Adis glue you can see that the log
streams are named after the glue jobs um
and so you know we have the streams but
let's talk about the actual data that's
made up of it the log events so this
represents a single event in a log file
log events can be seen within the log
stream and so here's an example of you
would open this up in cloudwatch logs
and you can actually see what what was
being reported back by your server you
can filter these events to filter out uh
logs based on simple or pattern matching
uh syntax so here I'm just typing in
saying give me all those debug stuff and
you know this isn't very robust but 8 of
us does have a better way of analyzing
your logs which is log insights which
we'll look at here in a moment
so we were just looking at uh cloudwatch
log events and how those are collected
but there's an easier way to analyze
them and that's with log insights so you
can interactively search and analyze
your cloudwatch log data and it has the
following advantages more robust
filtering than using the simple filter
in the in a log stream less burdensome
than having to export logs to S3 and
analyze them via Athena cloudwatch log
Insight supports all types of logs so
cloudwatch log insights is commonly used
via the console to do ad hoc queries
against log groups
so that's just kind of an example of
someone writing a query and cloudwatch
log insights uses a query syntax so a
single request can query up to 20 logs
query time out after 50 minutes if not
completed and queries results are
available for seven days so abis provide
sample queries that you can get started
for common tasks and uh and ease the
learning into the query syntax a good
example is filtering VPC flow logs so
you go there you click it and you start
to getting some data you can create and
save your own queries uh to make future
repetitive tasks easier on the certified
Cloud prer they're not going to ask you
all these details about this stuff but I
just conceptually want you to understand
that in login sites you can use it to uh
robustly filter your logs based on this
query syntax language you get this kind
of visual and it's really really
useful let's take a look here at
cloudwatch metric which represents a
Time ordered set of data points it's a
variable that is monitored over time so
cloudwatch comes with many predefined
metrics that are generally names spaced
by Services uh so the idea is that like
we were to look at the ec2 it has these
particular metric so we have CPU
utilization discre Ops dis write Ops
disre bytes dis write bytes Network in
network out Network packet in uh Network
packets out and the idea is that you can
just like click there into ec2 and then
kind of get that data there and so Cloud
metrics are leveraged by other things
like Cloud watch events Cloud watch
alarms cloudwatch dashboards so just
understand that
[Music]
okay all right so what I want to do in
this follow along is show you a bit
about cloud trail so we're going to go
to the top here and type in cloud trail
the great thing about cloud trail is
it's already turned on by default so
it's already kind of collecting some
information so it's here it says now use
I am access analyzer on cloud trail
trails that sounds pretty cool to me but
uh we shouldn't have to create a trail
right off the bat because we'll have
some event history and the event history
allows us to see
things that are happening within our
account in the last 90 days um but the
thing is if you want something Beyond 90
days you're going to have to create a
trail uh but if we just take a look here
we can kind of see uh as we've been
doing a lot of things all the kind of
actions that's been happening so here we
have an instance that I terminated so if
I go in here and and look at it I can
kind of see uh more information about it
so we can see when it terminated who had
done that what access key they had used
the Event Source the request
ID um the Source IP what whether it was
readon what was the event type that was
called the resource there and this is
the actual raw record so this is
generally how I would look at it or this
is how you had to look at it back in the
day um but the idea is that you'd have
that user identity described the event
time the source the event name the
region The Source IP the uh the agent
all the information there okay and so
this is a great way to kind of find
stuff so you can go through here and try
to debug things this way so you can go
to the event name and so if you if you
go here you can kind of get uh see a bit
of stuff here so if I was just trying to
say like maybe create I'm just trying to
find something that I know that I've
been doing like create access keys I can
see the access keys that going to be
created within this uh sandbox account
here for the user and things like that
so it's a great way to kind of find
things but generally you're going to
always want to turn on uh or create your
own trail so if you go here and hit
create Trail say my new Trail and um
you're going to need an bucket for that
you'll probably want encryption turned
on which sounds good to me you'll
absolutely want log file validation and
generally you don't want to store your
your Cloud tra logs within the existing
account you want to have a isolated
hardened account that's that is uh
infrequently accessed or only by your
your Cloud security Engineers um away
from here because you don't want people
tampering with it deleting it or
changing stuff but um we'll just take an
existing one here I don't want a c
customer manage don't I have one that is
managed bys here new
custom um we'll just choose that one I
don't know which one that is we'll just
hit next usually adus gives you a manage
key there so I was kind of surprised um
you can also include additional data so
if you do data events this would collect
information from S3 um but the thing is
you might not want to track everything
because if you track everything it can
get very expensive very quickly uh but
if you don't you just leave on
management events it'll save you more
money there's inside events uh this is
new I haven't seen this yet so ident I
identify unusual activity errors users
behavior that sounds really good but
these can come also add additional
charges but I'm going to hit next anyway
for fun I'm going to create that
trail
okay and uh the key policy does not
Grant sufficient access to etc etc so
I'm going to go turn that off even
though I should really have it turned on
but I just want to be able to show you
this okay so we have this new
Trail and so this Trail is being dumped
to S3 so we might not be able to see
anything in here as of yet but I'm just
going to pop over here and just see
right I probably have one in my other
account but it's not um it's not that
important we basically saw what the data
would look like so we go into here
there's a digest I don't remember there
being a digest so that's nice so there's
no data yet but when there is it will
pop into there um I'm not sure if we're
going to be able to do anything with
insights here at least not in this
account
insights are events that are show
unusual API activity and things like
that so that's kind of cool I don't know
what cloudwatch insights looks
like uh inside events are shown in the
table for 90 days okay so I'm just
curious if we can see kind of a
screenshot of what that looks like
whoops well we're at least on the
article here so I guess you could kind
of get like some kind of graphs or
something saying like hey this looks
unusual and they might select it so not
pretty clear in terms of what that looks
like but I mean sounds like a cool
feature and I'm sure when I when working
on my uh security certification course I
will definitely include in there but
that's pretty much all there is to it um
I'm going to go ahead and delete um that
trail because I I just don't really need
in this account but uh generally you
always want to go in and create a trail
um and what you can do is if you're in
your root account I'm not this is
actually a an account that's part of an
organization but if you're at that
organization level you can create a
trail that that spans all the regions
that spans all the ad accounts with an
organization and that's what you should
be doing okay but uh that's about
[Music]
it hey this is Andrew Brown from exam
Pro we're looking at ML and AI services
on AWS but let's first just Define what
is AI ML and deep learning so AI also
known as artificial intelligence is when
machines that perform jobs that may make
human behavior ml or machine learning
are machines that get better a task
without explicit programming and deep
learning or DL are machines that are
have an artificial neural network
inspired by the human brain to solve
complex problems and a lot of times
you'll see this kind of onion where
they're showing you that um you know AI
uh can be using ml or deep learning and
then deep learning is definitely using
machine learning but it's using neural
networks and so for AWS the flexship
product here is Amazon sagemaker it is a
fully managed service to build train
deploy machine learning models at scale
um and there's a bunch of different kind
of Open Source Frameworks you can use
with it like Apachi mxnet on us which is
an open source deep learning framework
that is the one that it decided to say
hey we are going to back this one and so
you'll see a lot of example code for
that one we have tensorflow that you can
use pie torch uh hugging face other
things as well okay um and so there's a
lot of services underneath some that
might be of interest to mention right
away is like Amazon sagemaker ground
truth which is a data labeling service
where you have humans that label a data
set that will be used to train machine
learning models or maybe something like
Amazon uh augmented AI so human
intervention review Services when
sagemaker uses machine learning to make
a prediction that is not confident uh it
has the right answer cue up to the
predict for a human review and these are
all about just labeling data um you know
when you're using supervised um
supervised learning but there are a lot
of Services Under sagemaker itself and
just AI services in general so we'll
look at that next
[Music]
okay all right let's take a look at all
the ML and AI services and there's a lot
on AWS so the first is Amazon code Guru
this is a machine learning code analysis
service and code Guru performs code
reviews and will suggest to improve the
code quality of your code it can show
visual code profiles so show the
internals of your code to pinpoint
performance next we have Amazon Lex this
is a conversation interface service with
Lex you can build voice and text chat
Bots we have Amazon personalized this is
a real-time recommendation service it's
the same technology used to make product
recommendations to customer shopping on
the Amazon platform then we have Amazon
poly this is a text to speech service
upload your text and an audio file
spoken by synthe synthesized voice uh
and that will be generated you have
Amazon recognition this is an image and
video recognition Service uh analyze
image and videos to detect and label
objects peoples and celebrities then we
have Amazon transcribe this is a speech
to text service so you upload your audio
and that'll be converted into text we
have Amazon text extract this is an OCR
tool so it extracts text from scan
documents when you have uh paper forms
and you want to digitally extract that
data you have Amazon translate this is a
neural machine learning translation
service so use deep learning mod models
to deliver more accurate and natural
sounding translations we have Amazon
comprehend this is an NLP so natural
language processing service find
relationships between text to produce
insights looks at data such as customer
email support tickets social media and
makes
predictions then we have Amazon forecast
this is a Time series forecasting
service and it's you know uh I mean
technically I guess it's a bit of a
database but the idea here is that it
can forecast business outcome such as
product demand resource needs or
financial uh performance and it's
powered by ml or AI if you want to call
it we have ads deep learning Ami so
these are Amazon ec2 instances they're
pre-installed with popular deep learning
Frameworks and interfaces such as
tensorflow pytorch Apachi mxnet chainer
GL uh gluon uh horovod and
kiras we have adus deep learning
containers so Docker images instances
pre-installed with popular deep learning
Frameworks interfaces such as tensorflow
pytorch apachi mxnet uh we have adsd
composer this is machine learning
enabled musical keyboard uh I don't know
many people using this but it sounds
like fun it was deep lens is a video
camera that uses deep learning it's more
of like a learning tool so again we
don't see many people using this adus
deep racer is a toy race card that can
be powered with machine learning to
perform autonomous driving again this is
another learning tool for learning ml
they like to do these at reinvent to
have like these racing
competitions Amazon elastic interface so
this allows you to attach lowcost GPU
perform uh powered acceleration to ec2
instances to red the cost of running
deep learning interfaces by
75% we have Amazon fraud detector so
this is a fully managed fraud detection
uh as a service uh it identifies
potentially fraudulent uh online
activities such as online payment fraud
and the creation of fake accounts Amazon
Kendra so this is an Enterprise machine
learning uh search engine service it
uses natural language to suggest answers
to questions instead of just simple
keyword matching so there you
[Music]
go hey hey it's Andrew Brown from exam
Pro and we're going to do a quick review
here of the big data and analytic
services that are on AWS but before we
do let's just to find what big data is
so it's a term used to describe massive
volumes of structured or unstructured
data that is so large it is difficult to
move and process using traditional
database and software techniques so the
first here we have is Amazon Athena this
is a serverless interactive query
service it can take a bunch of CSV or
Json files in an S3 bucket and load them
into a temporary SQL table and so you
can run SQL queries so it's when you
want to create CSV or Json files if
you've ever heard of um Apachi Presto
it's basically that okay then we have
Amazon Cloud search so this is a fully
managed full teex search service so when
you want to add search to your website
we have Amazon elastic search service um
commonly abbreviated to es and this is a
manage elastic search cluster and
elastic search is an open source full
Tech search engine it is more robust
than Cloud search but requires more
server and operational maintenance then
we have Amazon elastic map produce
commonly known as EMR and this is for
data processing and Analysis it can be
used for creating reports just like red
shift but is more suited when you need
to transform unstructured data into
structured data on the Fly and it
leverages open-source um technology so
like spark um Hive Pig things like
that then we have Kines is data Stream
So This is a real-time streaming data
service it creates producers uh which
sends data to a stream it has multiple
consumers that can consume data within a
stream and use uh it for real-time
analytics click streams ingestion data
from a fleet of iot
devices then we have Kinesis fire hose
this is a serverless and a simple
version of a data stream and you pay on
demand based on how much data is
consumed through the stream and you
don't worry about the underlying
servers then you have Amazon Kinesis
data analytics this allows you to run
queries against data that is flowing
through your real-time stream so you can
create reports and Analysis on emerging
data and last on the Kinesis side here
we have Amazon Kinesis video streams
this allows you to analyze or apply
processing on real-time streaming videos
onto the second page here we have manage
kofka service
msk um and it might be MKS um now that
I'm looking at it here so just be aware
that that might be incorrect but a fully
manage aachi kofka service kofka is an
open-source platform for building
real-time streaming data pipelines and
applications it is similar to Kinesis
but with more robust functionality then
we have red shift which is um a this
Flagship uh Big Data tool it's a
petabyte size data warehouse the data
warehouses are for online uh online
analytical processing olap so data
warehouses can be expensive because they
are keeping data hot meaning that we can
run a very complex query in a large
amount of data and get that data back
very fast but this is great when you
need to quickly generate analytics or
reports from a large amount of data we
have Amazon quick site this is a
business intelligence tool or a business
intelligence dashboard bi for short you
can use it to create business dashboards
to power business decisions it requires
little to know programming and connect
and adjust to many different types of
databases if you ever heard of Tableau
or powerbi this is just the adus
equivalent we have adus data pipelines
this automates the movement of data you
can reliably move data between compute
storage and services we have Abus glue
this is an ETL service so it allows you
to move data from one location to
another where you need to perform
Transformations before the Final
Destination it's simar similar to DMS
but it's more robust we have Aus Lake
formation this is a centralized curated
and secured repository that stores all
your data so it's a data Lake it is a
storage repository that holds a vast
amount of raw data in its native format
until it is needed and then last on here
we have adab state exchange this is a
catalog of third-party data sets you can
download for free uh or subscribe or
purchase data sets so they might have
like the covid-19 foot traffic data the
IMDb TV movie data historical weather
data and sometimes this is really great
if you're just trying to learn how to
work with these tools
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look here at
Amazon quick site which is a business
intelligence dashboard or bi dashboard
that allows you to ingest data from
various databus storage or database
services to quickly visualize business
data with minimal programming or data
formula knowledge so here's an example
of a quick site dashboard um and so the
way quick site is able to make these
dashboards super fast is via spice the
super fast parallel in memory
calculation engine um and the thing is
you don't have to use spice um but
generally it is good to use it uh and
there are some caveats when getting your
data into Quick site sometimes it can't
ingest data directly from a particular
uh data store so you might have to dump
it to S3 first but it's not too bad
because you can use Adis glue to
transform that data over um there are
additional features sometimes Market as
services but we have quick site ml
insights this detects anomalies perform
accurate uh forecasting it can generate
natural language narrative so basically
like you know describe it as if you're
going to read it out as a business
report you know then there's Amazon
quick site Q this allows you to ask
questions using natural language on all
your data and receive answers in seconds
so there you go
[Music]
hey this is Andrew Brown from exam Pro
and let's go take a look at Amazon quick
sites which is a or quick site which is
um a business intelligence tool so when
you go here you have to uh sign up
because it's kind of part of ads but on
its own separate thing and then you have
to choose what you want so we have
Enterprise and standard um I do not want
to pay that much so I'm going to go to
standard over here I'm not really sure
what the difference is it's not really
telling me what
um between standard and
Enterprise but I'm going to assume
standard is more cost effective but here
we it says user use I am Federated
identities which is fine use I am
Federate identities only um we can stick
with the top one there that seems fine
to me we need to enter a name so we'll
just say my quick site
account and we probably have to fill
something in there so let's say Andrew
example . Co and these are the services
that are going to integrate with it
Athena S3 RDS things like that I guess
we could select some of those buckets
I'm not too worried about doing that
right now the provided account name is
not available that is a terrible UI but
that's AWS for you so I'm just going to
dump some numbers there going put my
email in here
again um we probably want some S3
buckets I'm going
to make a new
bucket because I think that's how we're
going to do this we're gonna have to
make a bucket here and say uh quick
site
data okay and we're going to create
ourselves a bucket here I'm going to go
back and hopefully that shows
up uh it does not so what I'll have to
do is just back
out and I'm just going to give it a hard
refresh here and we're hit quick sign up
for quicksite again and we'll choose
standard and we'll say my quick site
account account a bunch of numbers there
Andrew exam. I don't really care about
adjusting data from everywhere else I
just want it from
S3 there's my
data uh sure we'll give it right
permissions even though I don't plan to
do anything with Athena here
today and we'll give it a moment to
load so what I'm thinking is
so what I'm thinking is just making like
an Excel spreadsheet here and just
filling in some data so oh it says our
account is set up here so we'll go to
Quick site because I bet it can import
like a CSV or
something um I'm more of a tableau or
powerbi kind of person um but uh you
know for the purpose of the cloud
practitioner I am going to show you this
Amazon quick set lets you easily
visualize data and Etc that sounds great
next next next I know what I'm doing oh
do we have some examp great so I don't
even have to make a spreadsheet okay so
what we'll do is just click on
that and we have stuff it looks like
they've really improved this since the
last time I've seen it which is quite
nice um but I could try and make my
own I'm just trying to think how do we
do this
again yeah we have the spice there so
it's a lot easier from starting from
scratch I'm just going to say
close and
user analysis we want data sets in here
oh we already have some data sets these
are coming from S3 I think that's the
old S3 logo I'm not sure why they're
using that one we can go here and create
a new data set oh we can upload directly
so I don't even have to use S3 that's
great so what I'm going to do is just
have some values in here so I'm going to
just say
um uh type value so we'll say
banana 125 123 we'll say
Apple
11
orange nobody likes
oranges I shouldn't say I'm sure it's
like lots of people like
oranges oh we got to put pears on
there I actually really like pears
people think I like bananas which is not
true I actually like pears that's what I
like so I'm going to go ahead and save
this save
as and I'm just going to save this to my
desktop here so just give me a moment
just doing this soft
screen and I'm just save this uh data
set quick
site CSV it can even take an XLS so I
don't have to save it as a uh I'll just
save it as an
XLS okay and so we're going to just
upload that so there is that data
set it's going to scan that file it's
going to see that sheet you even preview
it there's the information we're going
to add that data
uh I get added as a data data
set well how do I or where do I it's
like it says add the data I just want to
add it as a data set so data set up here
maybe save and visualize up
here and is it autographing yet maybe if
I drag in is it working is it thinking
okay it's at 100% so I'm going to just
drag that onto
there and it says pear orange
banana just kind of trying to make sense
of this here is it taking in count the
value maybe put the value down there wow
that's so much easier I've been used
this for like a year and um I'm going to
tell you this has gotten a lot easier to
use so I'm quite impressed with this but
yeah I mean this is pretty much what
quick site is if you want to visualize
things in different types you can drag
them out you can probably like click on
the the wheel here and change
it again I'm not sure
exactly how all the uh the dials and
knobs work here but I mean another thing
we could do is just drag out like
another object and do the same thing so
maybe I'd want a pie
chart um
so add a
visual yeah it's not as nice as powerbi
but like it's still great that it's here
you know type
value so we got a nice pie chart
there uh let's try something weird let's
give this one a go
doesn't color it which is not very nice
um there's probably some kind of way to
color it but focus on banana only I
don't know I don't know what the point
of there but anyway that's quick site so
um I really don't want to pay for this
so what I'm going to do let's go up
here um there's you have to deactivate
I'm just trying to remember
how because they Chang the interface
again they change everything on
you so maybe we go I'm on a trial for 4
days here maybe quantity for just the
four 29 day trial so if I want to get
out of this trial what do I do I don't
want to use it anymore um
so how to delete ads quick
site canceling your subscription so
before you can unsubscribe uh you're
signed in the IM am account your
quicksite administrator you're the root
I am administrator sure uh you deleted
any secondary name spaces to find the
existing name space Etc so choose your
username in the application bars manage
quick site account settings
unsubscribe so I was almost there I
thought I was in the right
place uh this one
no I was just
there manage quick
site your
subscriptions
edit there's no unsubscribe
option so I'm not
sure can I
cancel
unsubscribe button does not appear in
quick site
and it could just be because we're on
trial and so maybe after the end of the
trial it will uh it will vanish
there they are not making this easy for
me account settings ah delete account so
this is what we probably want to do
permanently delete the account
yes I mean that has to get rid of the
subcription because it gets rid of
everything there we
go we'll say
confirm delete
account unless you're using them in the
services blah blah blah blah blah um
successful okay great so now I should go
back ads. amazon.com and just to confirm
that it's gone I'm going
to go to quicksite again and just see if
it's trying to ask me to sign again so
it is so I've gotten R of my account so
we're all in good shape and uh yeah that
is that is quick
site all right let's take a look at some
more machine learning AI services
because adab us won't stop making these
things um and basically last time I made
uh the videos all this generative stuff
did not exist so we need to cover it the
first is Amazon Bedrock so the uh this
uses large language models and makes it
a cloud service offering to generate
text and images responses if you know
what chat GPT is you know what Bedrock
is we have Amazon code Whisperer it's an
AI code generator that will predict code
to meet your use case uh so if you've
heard ever heard of GitHub co-pilot it's
the same thing basically it's going to
write code for you or along with you I
should say we have Amazon devops Guru
this uses ml or machine learning to
analyze your operational data and
application metrics and the events to
detect operational
abnormalities um imagine if you had kind
of like a junior devops person digging
into your metrics to figure out if
there's something wrong then we have
Amazon Lookout this is actually three
different um offerings we have Amazon
lookout for equipment Amazon uh lookout
for metrics and Amazon look out for
vision they all seem to have to do
something with quality control and
Performing automated inspection so
vision of course would use Vision to
detect anomalies uh one would be for
equipment to detect if there's anything
wrong with operational equipment uh and
then metrics would be you know with
metric data so something probably more
for um the hard Industries uh to utilize
and you have Amazon monotron so this
uses machine learning models to predict
unplanned equipment downtime and so the
way they do that is they have these uh
iot sensors that's going to capture
vibrations and sensor data from your
Hardware then we also have adus neuron
this is an ad SDK used to run deep
learning workloads on adus uh infer I
can't say that word but I know what it
is it's basically um it's a machine
learning acceleration on gpus that you
can attach and ads train
trainum so yeah I wish the words weren't
so hard there's actually more um stuff
that Aus has for machine learning I
didn't include them because they were
just too far out there and they're
definitely not going to show up in your
exam you'll definitely never see them
but we now have better coverage what I
really wanted to show was Bedrock hod
Whisperer because I feel like those two
uh will show up on future exams I'm just
trying to get those in front of you now
even if they're not on the exam uh at
the time of this recording okay
[Music]
ciao all right so you probably are
already know what generative AI is but
just in case you don't I want to just
quickly cover it and show a very tiny
example uh so generative AI which also
can be shorten to gen AI though most
people don't say that uh is a type of
artificial intelligence capable capable
of generating new content such as text
images music or other forms of media so
an example would be something like a
software that I like to use called mid
Journey uh where you can put in a prompt
and so it will then go ahead and
generate out an image um so all the
cloud service providers have some kind
of offering with both image and text um
but yeah hopefully that makes sense the
idea is that you can plug stuff in you
get stuff out
[Music]
okay let's us take a look here at
machine learning and deep learning
Frameworks and so these are Frameworks
that uh can be used with sagemaker or
have direct support for them I just want
to get you some uh exposure and to uh
get you some context in terms of these
because machine learning and Ai and all
this stuff is becoming more popular so
you should at least have heard of these
things so I have all the logos on the
left hand side and we'll go through them
the first is Apachi mxnet so this is a
machine learning framework adopted by
ads basically um every single cloud
service provider backs their own kind of
open- source framework and they try to
make that the one that they suggest you
to use but in practice uh there's ones
that are good and there's ones that
people just don't want to use and Apachi
mxnet is not fun to use whatsoever um
and so you'll see it all over in the
marketing and pushed everywhere but
really people want to use things like
curus tensor flow but anyway I just
wanted to point that out that it was has
a bias because they've invested energy
into uh their team of machine learning
Frameworks you got pytorch optimized for
tensor Library uh for deep learning
using GPU and CPU it's created by
Facebook Facebook does not necessarily
um have its own cloud service provider
offering so it's kind of out there and
so you'll see good support for pytorch
and all the major providers U the next
is tensorflow this is made by Google
what's interesting with tensor flow is
Google made uh their own um GPU or TPU
they call it a tensor Processing Unit so
tensor is a a unit of thing in
tensorflow and it they have optimized
hardware for it I personally find
tensorflow the easy to use or I should
say cires so um CES is a highlevel
machine learning framework built on top
of tensorflow because these lower level
ones were just really hard to use and so
basically pytorch came along and it was
much easier to use and then everyone
noticed how easier py torch was and so
that's where curus came from was to be
competitive with pytorch and be easier
to use then you have a poy spark which
is a unified analytics engine for large
scale data processing but they do have
ml offerings within it called spark ml
um so there's definitely things you can
do there uh there's a piece of software
called chainer um and it's for it's a
deep learning framework that supports
Cuda then there's hugging face which is
not exactly a framework or tool it's
just a way of accessing a lot of models
online and data sets and quickly
launching them for whatever reason I uh
adus has uh strong synergies with
hugging face I've seen like developer
Advocates and other uh folks that worked
at AOS go over to hugging face and so
there seems to be strong uh
relationships between hugging face and
adabs for whatever reason there's a lot
of ml Frameworks out there but because
uh ml is uh just uh
progressively um or rapidly innovating
you'll see Frameworks come and go and so
I remember when I researched this and I
was just trying to understand all the
Frameworks out there there was just a
lot and I just kept digging into them
finding oh they're not active anymore
they're not active anymore so I just
want to point out that we have all these
ones up on screen if they become active
tomorrow I would not be surprised but uh
for the most part all of these seem to
be very popular uh and uh they're being
well supported uh but yeah hopefully
that gives you an idea of these
Frameworks okay
[Music]
ciao all right let's take a look here at
Apachi mxnet a little bit more in detail
because this is the framework that aabus
wants you to use whether you want to use
it or not is a different story uh but
you'll see it all over in their
marketing pages and things like that so
apachi mxnet is a deep learning machine
learning framework which supports many
many different programming languages so
that is one advantage of it uh the key
features uh is that it's scalable it's
flexible it's portable it's it supports
multiple programming language inabus has
made Apachi mxet their framework of
choice so there's lots of support for it
within ad sagemaker and the ad ml
containers but I have noticed that
they've been increasing support for p
torch so maybe you know they're just
trying to meet the customer where they
are but but anyway um there is a lot of
stuff for mxnet mxnet has two highlevel
interfaces uh one's called glue on and
there is module API so uh depending on
which one you use one is imperative
programming one's symbolic programming
uh this is more of a deeper concept for
machine learning but I'm going to tell
you one is really easy one is really
hard um but uh let's look at a very
simple example of uh some code for using
the gluon API so it kind of looks like
that you can see that they are using
python so hopefully that gives you an
idea of uh mxnet and its offering the
key thing is that it offers it in a lot
of different programming languages will
this appear on your exam absolutely not
but should you know it you absolutely
should um just so you have good context
with adus and ml so there you
[Music]
go I want to talk a little bit about
Intel because I think it's very
important to remember the hardware that
is running with these um cloud service
providers because it really does matter
um and there's a couple terms you might
see when using a compute that you're not
aware of and I want to make sure you
know what they are so let's talk about
what is Intel so Intel is a
multinational corporation is one of the
world's largest semiconductor chip
manufacturers Intel is the inventor of
the
x86 instruction set so basically uh they
released this chip back in 1978 this
one's called the Intel 8086 chip and the
idea is that um they came up with an
instruction set um it's basically a
bunch of words that you can use um to
program the chip and it's a lower level
language so um that lower level language
would be in assembly um if if that makes
any sense so the idea is that you have
this um instruction set and you have to
write an assembly and so basically most
modern programs like when you use uh
programming languages like uh C it will
actually compile down to assembly um or
other languages will compile down to
assembly because that is what the chip
understands and then assembly is turned
into machine code like the zeros and
ones and the reason I'm mentioning this
is that when you go and you uh launch uh
a compute uh instance let's say on AWS
uh you're launching a ec2 instance you
have to choose uh whether it's x86 or a
different instruction set or
architecture and so the other one is arm
and they're both really really good it
just depends on whether uh uh your stuff
can support it but for the most part
Intel has arm chips as well so so um
there is no company that produces arm
chips per se it's just an architecture
and uh the way it works is that it just
has fewer instruction sets so there's
fewer uh rules that you can write in so
it's a more limited writing it in
assembly but at the end of the day it
doesn't matter because your programming
language is going to compile it down so
you don't have to worry about those
fewer instructions but because it has
fewer instructions it generally results
in a better uh Power efficiency and so
it can have better performance or better
or better cost to to you the customer so
when I can I try to run arm and for the
most part it's always great to run arm
but uh it really depends on if your
software is going to be able to run on
arm um and stuff like that so I just
wanted to point out those two things
there about uh at least a Intel and then
instruction sets
[Music]
okay all right I want to talk about two
things um that Intel has with ads and
the first is Intel Zeon scalable
processor and the second is Intel Gotti
um so it us of course does work with or
purchases um Hardware from other um uh
other companies like they use AMD and
Nvidia but I think it's worth mentioning
Intel in a little bit more detail here
because every time I go to reinvent
Intel has a big giant booth and you can
go scour the ads website and it just
looks like ADS works more closely with
Intel as opposed to the other uh
providers not to say that Intel is not
being utilized on gcp and Azure and
others but uh I just noticed something
more going on there with AWS but let's
first talk about Intel xon scalable
processors these are high performance
CPUs designed for Enterprise and server
applications commonly used in a
instances um that scalable part makes
them very good for machine learning so
you often are going to be be using Intel
Zeon processors whether you know or not
on
ads the Intel is the Intel uh Habana
Gotti processor so this is a a processor
specialized for AI training uh you could
say that this is a direct competitor to
Nvidia or a similar competitor because
uh they uh they uh do something very
similar um I believe that Intel Gotti
has their own SDK called synapse AI uh
that you can use to interact with it so
you launch up Sage maker and then use uh
that uh that API or SDK in order to best
utilize uh that Hardware there but both
of these um pieces of Hardware are
offered uh on ads and I think it's just
good to know them at least to name uh
what they are
[Music]
okay hey this is angrew brown and let's
talk about gpus I'm sure most people
know what gpus are here but I'm going to
talk about it anyway because I want to
talk about cudas so a GPU stands for
General processing unit and it's a
processor that is specialized to quickly
render high resolution images and videos
concurrently if youve ever played video
games you know you need a good GPU
because it's all about those images
however gpus can perform parallel
operations on multiple sets of data so
they can also be used for non-graphical
tasks and this makes it really good for
machine learning and scientific uh
computation so if you're trying to uh
convince your significant other that you
need a better graphics card you can just
tell them it's for work I need it for
machine learning and scientific comp
computation it's not your fault that you
can also play video games with it and so
we have like a graphic there on the
right hand side I think I got that from
Nvidia and so they're kind of trying to
demonstrate the difference between uh
the paralyzation with GPU versus serial
tasks with CPU but let's go and just
read a little bit more so CPUs can have
an average of four to 16 processor cores
gpus can have thousands of processor
cores how that works I have no idea but
I just know that that's how it works uh
so we have 48 gpus can provide as many
as 40,000 C
so that is a lot gpus are best suited
for repetitive and highly parallel
Computing tasks such as rendering
Graphics cryptocurrency mining if people
are even still doing that and deep
learning and machine learning so you
know there you go that's
[Music]
gpus all right let's take a look here at
Cuda but before we do let's talk about
Nvidia so Nvidia is a company that
manufactures graphical processing units
for gaming and professional markets if
you have ever played video games and you
build your own rig um a lot of people
like to choose Nvidia but Nvidia can do
things other than video games and this
is due to their framework uh called cuda
which stands for compute unified device
architecture so it's a parallel
Computing platform and API I said
framework but I guess it's an API bu in
video that allows developers to use Cuda
enable gpus for general purpose
Computing gpus and it says GP GPU
because it's saying general purpose gpus
I know that's a mouthful there um so
over on AWS they have a bunch of
instances that um can utilize uh Nvidia
GPU so I adus is always changing the
instances so these could be old but you
can see we have a P3 which has the Tesla
Tesla V100 you have the G3 with a Tesla
M M60 the G4 with a T4 uh the P4 with
the Tesla a 100 so there's probably
these are probably old ones there's new
instances with newer Nvidia cards but my
point is is that adus has uh gpus that
you can utilize another thing I want to
point out with Cuda is that all major
deep learning Frameworks are integrated
with Nvidia deep learning sdks there's a
big fight or War over um uh these
companies that make uh gpus and CPS
because they really want the uh Theirs
to be used for machine learning so you
can definitely be sure that AMD probably
has some kind of similar offering or
something uh and definitely Intel as
well um but Nvidia has done a very good
job in uh making sure that theirs is the
most popular um so Nvidia deep learning
SDK is a collection of En uh Nvidia
libraries for deep learning so this is
something that this is the SDK you can
use with Cuda to interact with their API
uh so one of those libraries are called
cuda deep neural network library so
that's something you can use with it and
it's uh tuned for a bunch of stuff if it
looks like it's getting a little bit too
um uh technical it's because this slide
was was for my machine learning uh
inabus specialty and I didn't do a whole
lot to change it and brought it over uh
so you don't don't really need to know
that last part there but just understand
what Cuda is and that it's uh very
important uh for working with machine
learning and adus has uh good offerings
uh for instances with it okay
[Music]
hey this is Andrew Brown from exam Pro
and we are taking a look at the ads well
architectur framework so this is a white
paper created by ads to help customers
build using best practices defined by
AWS you can find this at aws.amazon.com
architecture forwell architected this
idea is not unique to AWS the other
providers have it but I believe AWS was
the first one to Define this and they
have a really good uh a good approach to
this and this is pretty much Essential
Knowledge that you have to have uh four
certifications when we're looking at the
cloud practitioner the soci architect
associate and professional because um
there's a lot of principles here are
best practices that adus uses themselves
to architect their infrastructure okay
so the framework is divided into five
sections called pillars which address
different aspects or lenses that can be
applied to a cloud workload so imagine
you have your Cloud workload you're
going to want to adopt an a architect
framework some things that you know
people don't consider outside the Five
Pillars is that you need to know en
definitions uh General design principles
and the review process um and then from
there you have your five pillars so you
have operational excellence security
reliability performance efficiency and
cost optimization and all these have
major sections in this uh white paper
but outside of just the main white paper
each of these have their own white
papers that go even into farther detail
so if you really want to uh really focus
on security and get a lot more
information they have that as well okay
[Music]
let's take a look at the general
definitions for the well architecture
framework starting with the pillars so
the operational excellent pillar is
there to run and monitor systems the
security pillar is to protect data and
systems to mitigate risk the reliability
pillar is to mitigate and recover from
uh disruptions the performance
efficiency pillar is about using
Computing resources efficiently or
effectively and the cost optimization
pillar is about getting the lowest price
and this is where you're going to find
all the business value and I put an
aster there because uh you know you
might obsess saying we need to meet the
requirements for all these pillars and
that's not the case you can trade off
pillars based on the business context so
you know don't take it as literally
Implement every single thing but just
consider that uh you know you might have
to adapt it based on your workloads then
we have some general definitions that we
will come across so there's components
so code configuration itless resources
against the requirement a workload so a
set of components that work together to
deliver business value mileston so key
changes of your architecture through the
product life cycle then there's
architecture itself so how components
work together in a workload and then we
have technology portfolio so a
collection of workloads required for the
business to operate
[Music]
okay so the well architected framework
is designed around a different kind of
team structure so when you're looking at
Enterprises they generally have a
centralized team with specific roles
where ADS structures their teams as
being distributed with flexible roles
and so this new kind of methodology of
distributed teams uh has some major
advantages but it does come with some
risks and so it us has baked in some uh
practices or uh things that they do to
mitigate these issues okay so let's
compare on premise Enterprise uh to what
itus is proposing for your team
structure so on premise what we'd see is
a centralized team consisting of
technical Architects solution AR
Architects data Architects Network
Architects security Architects and you
kind of see that they all have a
specialized vertical and they are
usually managed by either TF or Zack man
framework so those are just ways of
structuring your teams those are very
popular and so what adus is proposing
here is that you have a distribute team
and uh the way you're going to make that
team work because obviously just
thinking about distribute team they're
going to be a lot more agile but to make
sure that they effectively work you have
practices like team experts who raise
the the bar uh making sure that you know
uh in any areas we can always say how
can we do this better uh then there are
mechanisms in place for automated checks
for standards so that's the great thing
about Cloud can all be automated to say
hey does it meet our Regulatory
Compliance or or what have you and then
there's the concept of the Amazon
leadership principles which we will
cover on in the next slide in detail and
so um you know iTab us is not obviously
using uh these other Frameworks because
it has its own which is this one here
but the the mechanism to which they stay
organ oriz and up to date is they are
supported by a virtual community of
subject matter experts principal
Engineers so that what they'll do is
they'll engineer things like lunchtime
talks and then recycle that into their
onboarding material or into this
framework itself
[Music]
okay so we're taking a look here at
Amazon's leadership principles and these
are a set of principles used during the
company's decision- making problem
solving simple brainstorming and hiring
all right um and so I can't say I like
all of these but definitely some of them
really stand out as being great
especially the first one which is
customer Obsession so instead of
worrying about what your competitors are
doing think about what the customer
wants work your way back and uh you know
really focus on the customer's needs
then there's ownership so if you're
going to go do something uh you know try
to be your own mini boss uh and take
responsibility for whatever it is you're
building event and simplify so you know
always look for the simplest solution
don't try to engineer something super
complicated if it's not necessary uh or
right a lot so you know try to be right
uh learn and be curious so that's pretty
self-explanatory hire and develop the
best insist on the high standards adus
always refers to this as raising the bar
think big buys for Action frugality and
adus is really Frugal if you didn't know
that but not just for like themselves
but also for their customers they want
customers to uh spend the least amount
of money possible when using their
infrastructure earn trust uh dive deep
have a backbone disagree and commit
deliver results strive to be the earth's
best employer success and scale bring
broad responsibility and if you want to
read these in detail because they have a
big block of text for each of these uh
you can go to amazon. jobs
uhen principles and read all about it
[Music]
okay all right let's talk about some
general design principles uh that you
should be considering when you are
designing your infrastructure no matter
what pillar that you are looking to
adopt the first is stop guessing your
capacity need so the great thing with
cloud computing is you use as little or
much based on demand whereas on premise
you would have to purchase a machine and
you'd have to make sure you have
additional capacity so that you could
grow into it right and so here with uh
Cloud you do not have to worry about
that uh test systems at production scale
so be able to clone your production
environment to testing tear down testing
while not in use to save money so a lot
of people will have a staging server
that they run all the time but the great
thing here is that with Cloud you know
it's you can just spin it up and have it
right away and then tear it down and
save money um there's automating to make
architectural experimentation easier
this is talking about using
infrastructure as a code so for ad ofs
this would be using cloud formation
creating change sets which kind of um uh
say exactly what is going to change
stack updates drift detection to see if
your stuff is uh uh being changed over
time by developers through manual
configuration things like that then we
have allow for evolutionary
architectures so this is about adapting
cicd um doing nightly releases or if
you're using serverless if you adopted
lambdas they deprecate over time forcing
you to use the latest version uh and so
that is evolutionary architectures then
we have drive architectures using data
so um when you're using Cloud there's a
lot of Tooling in there to automatically
start collecting data so cloudwatch will
be collecting some things by default and
cloud trail will as well so you know
that is another thing and then improving
things through game days so this is
about stimulating traffic on production
or purposely killing ec2 instances or or
messing with your services to see how
well they recover all
[Music]
right before we jump into each of the
pillars let's go open them up and take a
look at what structure we should expect
to see so we have design principles
definition best practices and resources
all the pillars follow this to a t so
let's just talk about what these are so
the design principles are a list of
design principles that needs to be
considered during implementation and
that's where we're going to focus a lot
of our energy then you have definition
so this is an overview of the best
practice categories then you have the
best practices themselves these are
detailed information about each practice
with uh various a services and then you
have resources these are additional
documentation white papers uh and videos
to implement this pillar and I just want
to tell you that if you're doing the
certified Cloud practitioner we're
really just going to cover the design
principles but for the solutions
architect associate or anything uh
that's associate or above that's where
we're going to actually dive deep into
to the implementation of the best
practices because there is a lot of
stuff there so uh yeah there we
[Music]
go let's take a look here at the design
principles for operational excellence so
the first here is perform operations as
code so apply the same engineering
discipline you would to application code
to your infrastructure so by trating
your operations as code you can limit
human error and enable consistent
responses to events generally we're
talking about infrast infrastructure as
a code here so this would probably like
things like cloud formation there's
other things you could do like policy as
a code and a bunch of other ones then we
have make frequent small reversible
changes so design your workloads to
allow components to be updated regularly
uh this could be talking about doing
rollbacks incremental changes Blu green
deployments having a cicd pipeline
refine operations procedures frequently
so look for continuous opportunities to
improve your operations uh here you use
game days to simulate traffic or event
failure on your production workloads
anticipate failure so perform post
modems on system failures to better
improve write test code kill production
servers um there's a small spelling
mistake it should have an R here so
servers to test recovery learn from all
operational failure so share Lessons
Learned in a knowledge base for
operational events and failures across
your entire organization but you know if
you can just remember these headings
here uh and be able to categorize what
would be under operational excellence
you'll be okay all right
[Music]
all right let's take a look at the
design principles for the security
pillar so the first here is Implement a
strong identity foundation so implement
the principle of lease privilege or PP
that's a very uh popular concept meaning
giving people only the permissions that
they need use centralized identity so
that would be using adus am avoid Long
Live credentials then we have enable
traceability so monitor alerts and audit
actions and changes to your environment
in real time integrate log and Metric
collection and automate investigations
and Remediation then we have apply
security at all layers so take defense
in depth approach with multiple security
controls for everything from Edge
networks vbcs load balancing instances
OS application code uh we might have a
slide in this course on defense and uh
depth where basically you see like a
ring of things and you can kind of see
how like there's layers that go from
outward to Inward and that's what
they're talking about when they're
listing out all these things here
automate security best practices uh
protect prot your data in transit at
rest uh keep people away from your data
the reason I don't have descriptions
there is because those are pretty
self-evident prepare for security events
so Incident Management systems and
investigation policies and processes
tools to detect investigate and recovery
from incidences and uh there are a lot
of security tools out there and they all
have funny uh initialisms I didn't put
any of them in here but I'm sure there
are some there um but yeah there you go
for
[Music]
security all right let's take a look at
the design principles for reliability
and the first here is automatically
recover from failure so Monitor kpis and
Trigger automations when the threshold
is breach test recovery procedures so
test how your workload fails and you
validate your recovery procedures you
can use automation to simulate different
failures or to recreate scenarios that
led to failures before scale
horizontally to increase aggregate
system availability so replace one large
resource with multiple small resources
to reduce the impact of a single failure
on the over overall workload to
distribute requests across multiple
smaller resources to ensure that they
don't share a common point of failure so
we're talking about multi-az uh High
availability okay stop guessing capacity
we've seen this multiple times so in on
premise it takes a lot of guess work to
determine the elasticity of your
workloads uh workload demands with Cloud
you don't need to guess how much you
need because you can request the right
size of resources on demand that's going
to give you better reliability okay
manage change in automation so making
changes via infrastructure as a code
will allow for a formal process to track
and review infrastructure you're going
to see IC show up a lot in this
framework
[Music]
okay let's take a look at design
principles for performance efficiency so
the first here is democratize advanced
technology so focus on product
development rather than procurement
provisioning and management of services
because if you're on Prem you'd have to
order those machines set them up and so
take advantage of advanced technology
specialize in optimize for your use case
with on demand cloud services because
again if you're using on Prem uh you you
know you might not have the option to
have Sage maker right it's just going to
be a VM and you're going to have to do
all the work yourselves whereas ads has
all these specialized things so you can
move quickly uh Go Global in minutes so
deploying your workload in multiple Abus
regions around the world allows you to
provide lower latency and a better
experience for your customers at a
minimal cost we have used serverless
architecture so serverless architecture
removes the need for you to run and
maintain physical servers for
traditional Computing activities removes
the operational burden of managing
physical servers and can lower
transactional costs because manag
Services operate at Cloud scale and can
be a lot better at um running them
efficiently than you will uh experiment
more often so with virtual and
automatable uh resources you can quickly
carry out comparative testing using
different types of instances storage or
configurations to make the best choice
we call this right sizing choosing the
right size consider mechanical sympathy
so understand how cloud services are
consumed and always use technology
approach that aligns best with your
workload goals for example consider data
access patterns when you select database
or storage
[Music]
approaches let's take a look here at
design principles for cost optimization
so the first one here is Implement Cloud
financial management so dedicate time
and resources to build capacity uh via
Cloud financial management and cost
optimization tooling soab us is saying
hey take advantage of all our tooling
that makes it easy for you to know
exactly what you're spending adopt a
consumption model so pay only for
computing resources that you require uh
an increase or decrease using uh
depending on the business requirements
we're talking about on demand pricing
measure overall efficiency so measure
the business output of the workload and
the cost associ associated with
delivering use this measure to know the
gains you make from increasing output
and reducing costs so stop spending
money on
undifferentiated that's a hard word to
say
undifferentiated heavy lifting so adus
does the heavy lifting of the data
center operations like racking stacking
and power servers it also removes the
operational burden of managing op
operating systems and applications with
managed services this allows you to
focus on your customers and business
projects rather than your it
infrastructure and the last one here is
analyze and attribute expenditure so the
cloud makes it easier to uh accurately
identify the usage and cost of systems
which then allow transparent uh
attribution of it costs to individualize
workload owners this helps measure
return on investment and gives workload
owners an opportunity to optimize their
resources and reduce costs so there you
go
[Music]
hey this is Andrew Brown from exam Pro
and we are taking a look at the adus
well architected tool so this is an
auditing tool to be used to assess your
Cloud workloads for alignment with the
AWS well architected framework and so
what it is it's essentially a checklist
uh but it also has nearby references so
you know as you're reading through it it
will show you information uh and
resources so that it can help you with
this checklist here and the idea is when
you're done you can generate out report
and then you can provide that report to
your Executives and key stakeholders to
prove uh you know how well architected
your workload is on AWS
[Music]
okay hey this is Andrew Brown from exam
Pro and in this video I want to show you
two things the well architected
framework and the well architected tool
so first let's go look for the well
architected framework so we're going to
look up white papers uh AWS and so if we
go here to a amazon.com white papers we
have a bunch of pages here and so I'm
going to just checkbox on white papers
so that we can kind of reduce the amount
there and then I'm going to checkbox
well architector framework if we scroll
all the way top here one of these you
think it'd be right at the top but one
of these is the well architected
framework and here it is and so if we
open it up I used to just directly open
up as a PDF I'm sure you can still
download it as is but generally you're
going to open up as this HTML page and
you can basically read through it see
all the stuff see the multiple pillars
we can click into here see the design
principles read the definitions and then
start reading about uh the best
practices and they have these things at
the bottom of each one uh very boring
very very boring but um you know when
you get to the solutions architect and
things like that you're going to need to
know this stuff inside and out it's
going to really help you out at this
Cloud practitioner we only need to know
surface level
information um but that's the architect
framework let's take a look at the well
architected tool so we going type in
well here we'll get the well architected
tool and if we go here you can see that
I've created a couple before probably
demos for um our videos and so I'm going
to go Define a new workload I'm going to
say my my workload here uh my
workload whoops my workload it is
messing up because I probably have
grammarly installed so it does not like
grammarly so I'm just going to turn it
off for now so my workload
and it's still not typing correctly so I
have to kill out kill out grammarly here
which is kind of frustrating so that's a
bug that that's not grammarly's fault
that's adab Us's fault for not playing
well with grammarly and that's something
I will definitely report to them because
it's very annoying so I'm going to go
ahead and refresh this
page my workload my
workload um and this is Andrew Brown
production or pre-production doesn't
matter pick your regions Us East or Us
East 2
sure I'm selecting
it there we go uh optional optional
optional optional you go to next and
then you can choose your lens servus
lens FTR lens so that's the foundational
technical review SAS lens we can go with
architected framework and then once that
is there we can start
reviewing okay and then we get this big
checklist and so we can go through this
and read each one so we say Ops one how
do you determine what your priorities
are and all these things like Ops and
stuff like that these are all the
summaries in each of the well
architected framework sections so you
pretty much don't need to really read
the dock you just go through this so
everyone needs to understand their part
in enabling business success have shared
goals in order to set priorities of
resources this will maximize the benefit
of your efforts so select from the
following evaluate the customer's
external needs external customer needs
evaluate internal customer needs if you
click info it's going to highlight each
one here so evolve key stakeholders
including Business Development
operations teams this will ensure Etc
and so you just go through this and uh
you know once you have that and you save
and
exit Okay uh you'll have uh the
questions that are answered it'll say
what's high risk what's not things like
that very simplistic it's really just a
way of making a very organized report or
checklist and proving that you went
through it uh to the executive level or
to the management level there so
hopefully that makes sense to you um
it's not too complicated but there you
[Music]
go hey it's Andrew Brown from exam Pro
and we are looking at the Adas
architecture Center so the architecture
Center is a web portal that contains
best practices and reference
architectures for a variety of different
workloads and you can find this at adab.
amazon.com architecture so if you're
looking for Best Practices inter terms
of security they have a huge section on
that and they have it for pretty much
every kind of category on AWS or if
you're looking for practical examples
you can view the large library of
reference architectures so here's one to
make an ads Q&A bot and it will have an
architectural diagram but you can also
uh deploy it via cloud formation or
possibly cdk um and this way you can get
a working example and then tweak it for
your use case so this is a really great
tool um when you are done the it well
architect framework and you're saying
okay how do we apply it can we get more
concrete examples and I wouldn't be
surprised if a lot of the resources
within the well architectured framework
white paper are just pointing to the
center
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at the
concept of total cost of ownership also
known as TCO so what is TCO well it is a
financial estimate intended to help
buyers and owners determine the direct
and indirect cost of a product or
service so here is is an example of you
know TCO for maybe like a data center so
we have Hardware monitoring installation
IT personnel training software uh
security licensing and taxes but that's
not just the limit of it it's just kind
of the examples we show here uh the idea
of creating TCO is useful when your
company's looking to migrate from on
Prem to cloud and we will have a better
uh kind of visual here to kind of
understand how you would contrast
against on premise to Cloud but let's
just talk about how it actually works in
practicality which I think gets kind of
of overlooked when cloud service
providers are selling you on TCO so the
idea is that Gardener um you know they
uh they were they wrote this article
based on This research where an
organization had moved uh 2,500 virtual
machines over to Amazon dc2 and so what
you're seeing here is that there is a an
additional cost that we're not
considering which is the migration cost
See This Bar up here um so the idea is
that the company was paying around
400,000 and so they started to move over
and as you see uh the cost initially
went up for a short period of time here
uh but then once that migration cost was
over uh you can notice that they had a
55% reduction so it's uh totally
possible to save money uh and clearly
there is great savings uh now is it
exactly what AWS promises probably not
and that's that could be the reason why
they updated their TCO calculator but
let's now just do that contrast against
the two so we have on premise on the
left and ads on the right or any cloud
service provider and what I want to do
is help you think about what costs do
people generally think about because if
we have like Iceberg the idea here is
that these are the costs that we always
think about above the iceberg and then
there's these hidden costs that we just
don't consider when factoring in our
move and that's the idea of T TCO is to
consider all the cost not just the
superficial ones and so people say these
look like teeth and that's why I add
penguins and a whale here um and so when
we're talking about on premise what we
generally think are software license
fees and subscription fees but when you
compare those against each other they
might look the same um ad us might just
look slightly cheaper or even more and
so the idea is you need to then factor
in everything so on on premise there's
implementation configuration training
physical security Hardware IT personnel
maintenance and on the adab side you
know you are you don't have to do as
much of that stuff so you just have
implementation configuration and
training and so adab us with their TCO
calculator their old one used to make a
promise of 75% in savings um again you
know this is going to really vary based
on what your migration strategy looks
like um but you know it's totally
possible you could save 75% or you could
save 50% over a third year threeyear
period And there's a an initial Spike so
that's just something you have to
consider but the nice thing though is
that once you've moved over all the
stuff over here on the left hand side
will be ad Us's responsibility
[Music]
okay all right so let's take a look at
Capital versus operational expenditure
so there's capex and Opex so on the
capex side the idea here is you're
spending money upfront on physical
infrastructure deducting that expenses
from your tax bill over time uh a lot of
companies that are running their own
data centers uh or have a lot of on-
premise stuff understand what capex is
because um it's something that a lot of
times they get tax breakes on and that's
why we see a lot of people that have a
hard time moving away from the cloud
because you know they keep on thinking
about that money they save from the
government but capex costs would be
things like server costs storage Network
costs backups and archives Disaster
Recovery costs data center costs
technical Personnel so the idea is with
capital exp expenses you have to guess
up front what you plan to spend okay
with operational expenditure the idea
here is the cost associated with an on-
premise data center that has shifted the
cost to the service provider the
customer only has to be concerned with
non-physical costs so leasing software
and customizing features uh training
employees and cloud services paying for
cloud support uh billing based on cloud
metrics so compute usage storage usage
and so the idea here is with operational
expenses you can try a product or
service without investing in equipment
so basically kex is what we think about
when we think of on premise and then
Opex is what we think about um you know
when we're thinking about cloud or AWS
[Music]
okay all right let's ask a very
important question about Cloud migration
so does cloud make it Personnel
redundant so a company is considering
migrating their workloads from on
premise to the cloud to take advantage
of the savings there is a concern among
the staff that there will be Mass
layoffs does cloud make it Personnel
redundant and that's a very important
question to to have an answer to and
this all talks about shifting your it
team into different responsibilities so
a company needs it Personnel during the
migration phase as we saw with that
Gardener research report that there was
a period at least like a year where they
needed that for you know depending on
the size of your company so you're still
going to need those people around a
company can transition some roles to new
Cloud roles so a very traditional
example would be you have your
traditional networking roles or people
have like their CCNA and now they're
moving over to Cloud networking uh they
have a reduced workload but there's
other things uh that they could be doing
in the cloud um a company may decide to
take a hybrid approach so they'll always
need to have a traditional it team and a
cloud uh it team um and the last one and
this would you'd actually see on the
exam which is a company can change
employees AC ities from managing
infrastructure to re Revenue generating
activities okay so the idea is that you
know if you a company why would you get
rid of all your staff when you can just
put them all into Revenue generation I
suppose you know you could uh you know
uh lay them off and some companies might
do that um or you know you could just
retrain them because if that IT
personnel team has uh technical
expertise I'm sure they can translate
that to the
[Music]
cloud let's talk about the adus pricing
calculator and this this is a free cost
estimate tool that can be used within
your web browser without the need of an
adus account to estimate the cost of a
various adus services and this is um
available at calculator. AWS and the
reason we're bringing this up is because
there used to be a TCO calculator but
now this is the calculator that you use
so the adabs pricing calculator contains
100 plus services that you configure for
cost estimate and so you can just click
through a bunch of knobs and uh boxes to
uh you know uh exactly figure out a very
accurate cost so the idea here is that
to calculate your TCO an organization
needs to compare that existing costs
against their adus costs and so the adus
pricing calculator can be used to
determine uh you know the adus costs and
obviously the organization knows its
cost so we can compare it against that
um and the way you can get data out of
this is you can export it as a final
estimate to a CSV
[Music]
okay hey this is this is Andre Brown
from exam Pro and we are taking a look
at the AWS pricing calculator so to get
there it's calculator. AWS what you're
going to do is hit create estimate and
then here you have a bunch of services
so you just choose what you want so you
type in ec2 we're going to configure
that and from there we can do a quick
estimate or an advanced estimate so
choose this option for fast and easy
route to Ballpark and estimate choose
this option for detailed estimate for
accounts workloads and stuff so notice
down below very simplistic we hit
Advanced and we get all sort sorts of
stuff okay so you know it's really up to
you I'm very comfortable with the
advanced options so I might be running a
Linux machine what is my usage it's
going to have uh daily spikes of traffic
because of the use cases you could say
it's not busy on Saturday and Sunday
that it has a baseline of one a peak of
two eight things like that then you can
choose what you're using um T4 G I don't
even know what that is uh but we'll just
say like
t uh T2 micro which is not that big 23
micro and you can say we're doing on
demand cuz a lot of people would be
doing that and you see like $7 a month
it's not a lot of money then you're
looking at your storage data in data
out okay so we can add that another
thing that we might see is something
like
RDS so we go to RDS and we add post
Crest and not all of them have the
simple and complex sometimes they're
simple so production database
we'll have one here and which're just
going to be say a dbt2 micro T T3 micro
there we go uh 100 that's fine we're not
going to have multi-az we'll have single
a on demand show the calculation $13 a
month add that to our estimate so you're
kind of getting the idea there
right um and so you know we have our
summary that's our monthly
$391 um oh sorry over $12 months our
monthly cost is
$32 okay you can go back there clone the
service edit it stuff like that you can
export the estimate I think it goes out
as a CSV you can also hit
share um and then hit agree and so then
you have a public link and if I have
that link we can just see what happens
if I paste it okay and it just brings
them to the same estimate so there you
[Music]
go hey this is Brown from exam Pro and
we are taking a look at migration
evaluator so it was formerly known as
TCL logic and then abos acquired the
company and it is an estimate tool used
to determine an organization existing on
premise costs so it can compare it
against its aabus cost for Planned Cloud
migration uh so the idea is that you can
get uh very very detailed information
and the way it collects information is
via an agentless collector to collect
data from your on- premise
infrastructure to extract from your own
on premise costs I don't know if you can
see there but you can see that it works
with a lot of different kinds of on-
premise technology like VMware Microsoft
uh tsql all sorts of things
[Music]
okay one migration tool that we can use
with AWS is the VM import export and
this allows us to import virtual
machines into ec2 so inabus has import
instructions for VMware Citrix Microsoft
hyperv windows vhd from Azure and also
Linux vhd from Azure and so the way this
works is that you prepare your virtual
image for upload and adus has a bunch of
instructions for that once it is ready
you're going to upload that to an S3
bucket and once it's uploaded to an S3
bucket then what you can do is use the
adab CLI to import your image um and so
that is the CLI command down below and
once it is produced it will generate out
an Amazon machine image and so from an
Ami you can then go your ec2
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at the
database migration service which allows
you to quickly and securely migrate one
database to another DMS can be used to
migrate your on premise database to ads
and that's why we're talking about it uh
and so here's a general diagram where
you have your Source database which
connects to a source endpoint goes
through a replication instance so that's
a ec2 instance that's going to replicate
the data to the Target endpoint onto the
the target database uh and so we have a
bunch of possible sources so we have
Oracle database Microsoft SQL MySQL Mari
DB post SQL mongodb sap
ASC IMDb db2 AZ your SQL database Amazon
RDS Amazon S3 and I'm assuming these are
database dumps Amazon Aurora Amazon
document DB and so for possible targets
it's very similar we got Oracle database
Microsoft SQL MySQL Mario DB post SQL
reddis saps SE Amazon red shift Amazon
RDS Amazon Dynamo DB Amazon S3 Amazon
Aurora Amazon open search service Amazon
elasticache for reddis Amazon document
DB Amazon Neptune Apachi Kafka I'm just
showing you the list to give you an idea
of how flexible this service is uh but
you can tell that these are very
different databases so how can it uh
move them over right and so so in not
all cases can it easily do it like it's
very easy to go from myql to postest um
but you know for ones that are like
relational to uh nosql uh this is where
the adaba schema conversion tool comes
into play it's used in many cases to
automatically convert a source database
schema to a Target database schema or
semi-automate it so that you can kind of
like uh you know uh figure out how to
map the new schema uh each migration
path requires a bit of research since
not all combinations of sources and
targets are possible and it really comes
down to even versions of these things so
but I just want you to know about that
it's an option as a database migration
service and I've migrated a very large
database before and it's super fast uh
so and it's not that hard to use so
something you definitely want to
remember when you're
[Music]
migrating hey this is Andrew Brown from
exam Pro and we are taking a look at the
cloud adoption framework so this is a
white paper to help you plan your
migration from on premise to AWS at the
highest level the AWS CAF organizes
guidance into six Focus areas we got
business people governance platform
security and operations and this white
paper is pretty high level uh so you
know it doesn't get into uh granular
details on how that migration should
work uh but gives you kind of a holistic
approach and I believe that probably
through the adus uh Amazon partner
Network there's people that specialize
in using this particular framework to
help organizations move over and I
believe that Abus has Professional
Services through the APN but let's just
kind of break down what these six
categories are we're not going to go too
deep into this um but let's do it so the
first is the business perspective so
these are business managers Finance
managers budget owners strategy
stakeholders so it's how to update the
staff skills and organizational
processes to optimize business value as
they move Ops to the cloud you have
people perspectives so Human Resources
Staffing people managers so how to
update the staff skills and
organizational processes to optimize and
maintain the workforce and ensure
competencies are in place at the
appropriate time you have governance
perspective so cios program managers
project managers Enterprise Architects
business analysts so how to update the
staff skills and organizational
processes that are necessary to ensure
business governance in the cloud and
manage uh and measure Cloud Investments
to evaluate the business outcomes we
have platform perspectives so CTO it
managers solution Architects so how to
update the staff skills and
organizational processes that are
necessary to deliver and optimize Cloud
Solutions and services security
perspectives so ciso it security
managers it security analysts so how to
update the staff skills and
organizational processes that are
necessary to ensure that the
architecture deployed in in the cloud
aligns to the organization security
control requirements resilience and
compliance requirements we have
operational or operations perspective so
it operations managers it support
managers so how to update the staff
skills and organizational processes that
are necessary to ensure system health
and reliability during the move of
operations to the cloud and then to
operate operate using agile ongoing
cloud computing best practices so this
just Taps the surface of what the CAF is
uh and I think for each of these they
actually have a more detailed breakdown
so you know business is going to break
down to even more uh uh finite things
there
[Music]
okay so itus has free services that are
free forever unlike the free tier that
are up to a point of usage or time um
and so there are a lot here this is not
even the full list there's definitely
more and we have am Amazon VPC Auto
scaling cloud formation elastic bean
stock opsw Works amplify Apps sync code
star organizations Consolidated building
it with cost Explorer Sage maker systems
manager there's a lot of them okay um
but the thing is is that uh these
services are free but some of these um
can spin up other resources so the
services are free themselves however
ones that provision Services May cost
you money so cloud formation which is an
infrastructure is a code tool could
launch virtual machines those virtual
machines will cost money right opsworks
can launch servers that can cost money
amply can launch um lambdas that can
cost money so that's something you just
have to consider um but uh yeah there
you
[Music]
go hey this is Andrew Brown from exam
Pro and we are taking a look at the adus
support plans so we got basic developer
business and Enterprise and you
absolutely absolutely need to know this
stuff inside out for exam they will ask
you questions on this okay so basic is
for email support only uh such as
billing and account so if you think it
got over bu and that's something you
should do if if you've uh uh
misconfigured something and you end up
with a big Bill just go Um open up a
support uh ticket under basic for
billing and they're likely to refund you
but if you do have questions about
billing accounts that's what we're going
to be using for everything else that is
for tech support um and so for developer
business Enterprise you're going to get
email support which they'll uh roughly
reply within 24 hours I believe this is
business hours so if you message them on
Friday um or sorry Saturday you might be
waiting till Monday for it okay um in
terms of thirdparty support uh the only
one that doesn't have third party
support is developer so if you are using
something like Ruby on Rails or Azure or
something that has interruptibility
between AWS and something else business
Enterprise will absolutely help you out
with it same with Enterprise but the
developer one not so much uh if you like
to use the phone or you like to chat
with people um that's available at the
business Enterprise tier this is the way
I end up talking to people if you are um
you know like if you're in North America
and you're calling between 9 to5 and a
Monday and Friday you're likely to get
somebody that is within North America if
not it'll be one of uh one of the
supports from some other area so just be
aware of that that can also affect the
time they pick up uh sometimes it's 5
minutes times it's 30 minutes to to an
hour uh you know it just depends on what
service you're asking for and you know
what time a day okay um in terms of
responsiveness uh for General guidance
everything is 24 hours or less for
developer business Enterprise if your
system is impaired it's within 12 hours
or less with production system impaired
it's four hours or less with production
system down it's 1 hour or less and if
you're for Enterprise um it's going to
be business critical system
less than 50 minutes so just notice who
has what for these things um I've
definitely waited like 3 days on General
guidance before so just take these with
a grain of salt that they're not you
know they don't really stick to these
that or maybe I'm just not paying enough
for them to care okay um in terms of uh
getting actual people assigned to you
this only happens at the Enterprise
level where they have their coner team
so they uh help your um organization uh
learn how to use ad asking them any
questions personally and then you have a
tam a technical account manager that is
somebody that knows um a side inside and
out and they'll help you architect
things and make correct choices or
they'll check your bill and help you try
to reduce that bill things like that
okay in terms of trust advisory checks
at the basic developer you get seven
advisor checks once you're paying for
business you get all the checks the cost
here for business is zero um for
developer it's starting at $29 a month
for businesses starting at $100 a month
and then for Enterprise it's 15,000 uh a
month so I said starting at because it's
dependent on your usage okay so let's
just look at developer business
Enterprise here cuz basic's not going to
be applicable here so for developers $29
USD a month or 3% of the monthly adist
usage which whichever is greater on the
exam they're only going to ask you like
is it $2,900 like generally do you know
the tier of expensiveness but they're
not going to ask you the percentage of
usage okay there's not going to be
formulas here um when you get into
business it's a little bit uh different
where they have it in different brackets
so it's going to be 10% for the first uh
10,000 and the next is going to be the
next 7,000 stuff like that similar for
Enterprise as well so let's just do some
math so we know that we understand how
this works so if you if you had a
monthly spend of $500 at the developer
tier that's 3% of $500 is $15 so they go
okay what is greater $29 or $15 so
you're paying $29 if your spent is
$1,000 that comes up to $30 uh so you're
you're going to end up paying $30
because that's greater than 29 okay for
business uh if your monthly spend is
,000 that's 10% of ,000 that's $100 if
your spend is $5,000 then you're going
to be paying $500 if your monthly spend
is 12,000 then the first 10% of 10,000
is a th000 and then the next is 7% of
2,000 so your total bill is 140 USD
we're not going to do a calculation for
Enterprise because it's the same for
business but hopefully that gives you an
idea there okay
[Music]
hey it's Andrew Brown from exam Pro and
we are taking a look at a technical
account manager also known as a tam and
these provide both proactive guidance
and reactive support to help you succeed
with your adus journey so what does a
tam do and this is straight from an adus
job
posting what they would do is build
Solutions provide technical guidance and
advocate for the customer ensure Adas
environments remain operationally
healthy while reducing cost and
complexity
develop trusting relationship with
customers understanding their business
needs and Technical challenges using
your technical uh acumin and customer
Obsession you'll drive technical
discussions regarding incidents
tradeoffs risk management consult with a
range of Partners from developers
through the SE Suite Executives
collaborat with a Solutions architect
business developers Professional Service
consultants and sales account managers
proactively find opportunities for
customers to gain additional value from
ads provide detailed reviews of service
disruptions metrics detailed pre-launch
planning being uh part of a wider
Enterprise support team providing post
scale cons uh uh consultative expertise
solve a variety of problems across
different customers as they migrate
their workloads to the cloud uplift
customer uh capabilities by running
workshops Brown Bag sessions Brown Bag
sessions being sessions that occur at
lunchtime something you can learn in 30
minutes an hour and so one thing that's
really important to understand is that
Tams follow the Amazon leadership
principles especially about customer uh
being customer obsessed and we do cover
the Amazon leadership principle
somewhere in this course and Tams are
only available at the Enterprise support
tier so hopefully that gives you an idea
what a tam
[Music]
does hey this is Andrew Brown from exam
Pro and this follow along I'm going to
show you um a support and in order to
use ad support or to change your level
of support you're going to need to be
logged into the rout account I should
say you can use support with IM users
but if you want to change the support
plan you're going to have to be the root
user so in the top right corner I'm
going to support and notice here on left
hand side right now I have a basic
plan and so uh before we look at
changing our plan I'm just going to go
create a case and we're going to uh just
take a look at some of the options that
are open to us so we have account
billing support service limit increase
technical support notice this is grayed
out so we cannot select anything here
I can go to here and increase our
service limit and this is something that
you might have to do uh pretty soon
early in your account you might say hey
I need more of something like ec2 or um
a very common thing is SC so for SCS you
might say hey um I need to have this
amount of emails for ETC okay so um if
we go over to count and billing support
uh we can go here and ask anything we
want so if it's about the free tier I
could say ask a general question getting
started and saying uh what is free on
AWS um I want to know what is free on
AWS and you can attach uh three
attachments there you can choose via web
and phone which is really nice um but
today I'm just going to do web here and
submit that just to kind of show you
that as an example and so what that is
going to do is open a case and then we
will see probably respond in 24 hours to
48 hours just depends on on um whether
it's the weekend or not because it's
based on business hours of course so now
that we have an understanding of basic
let's go take a look at what the other
tiers look like so we have basic
developer business and enterprise
Enterprise being extremely expensive
developer being affordable and then
business being um you know affordable
for businesses so I would say developer
is okay it gives you um uh it gives you
uh better support but it's all via email
and so you know if you really want good
support you're going to going to have to
pay the business one and that's the one
that I use quite a bit so if I change my
plan I'm going to go over to business
and this is going to cost me 93 bucks
just to do to show you here today so I'm
going to go ahead and click that and so
it's now processing it and so what's
going to happen is I'm going to have to
wait for this basic to switch to
business so if I go to the case here it
hasn't happened as of yet so no I cannot
select this so I'm going to see you back
here in maybe like four five minutes or
however long it takes and we'll take a
look then okay okay great so after a few
minutes it says my plan is now business
and what I can do is go ahead and create
a new case and so I can go over to
technical support and ask a question so
if I was having issues with anything it
doesn't matter what I could go over to
ec2 Linux and then I could choose my
category so I could say I'm having an
issue with um systems
manager and a lot of times they like you
to provide the instance ID it's going to
change based on what service you choose
here um but you'll get different
information I'll just say I need
help with um logging into my ec2
instance managed by SSM so I can say I
created an ec2 instance and I am
attempting to access uh the instance
via sessions
manager but it is not
working I think I have a ro issue and
then I'm just going to go down here and
say this is not got a real
question I am filming a demo video for
tutorial
video on how to use support okay and so
once we do that we have the option of
web chat and phone so if you use phone
you're going to enter your phone number
in and they're going to call you back uh
usually you will be on hold for anywhere
for 5 minutes to an hour it just depends
usually it's within 15 minutes so it's
very very good of course it depends on
the time of day and your location things
like that and the service because
there's different support Engineers for
different types of services and the the
balance of those are different but
generally chat is pretty good so I can
go here and I'm just going to hit submit
and it's going to open a chat box and so
you just wait okay and sometimes it's
super fast and sometimes it takes uh
minutes okay so we are going to just sit
here for a bit and um you know I'll just
pop back here when there is somebody to
talk to
okay okay so after waiting a little
while looks like uh we've been connected
here so it took a bit of time so we're
just going to say hello hi um uh this is
Andrew
Brown um I am recording a video to teach
people how to use
AWS and I wanted to show them how it
support
works so I'm just showing them how the
chat system
works say
hello and hopefully they'll appreciate
or they won't it just doesn't really
matter we'll give them a moment
there we
go that's
it thanks for your
help okay and so that's pretty much it
um so you know there's nothing really uh
uh special about that but the idea is
when you are typing with them it will
appear in the ch respondence there so
I'm just going to end the chat okay uh
and then I'm just going to mark that
case as resolve sometimes they will ask
you to resolve it if I go to cases I
probably have some previous ones here um
and I have a lot but I don't know why
they don't all show up here so you can
see this one is pending this one is
resolved I go back to this one you can
kind of see that the uh history of a
conversation is kept and you can go back
and forth uh with the people there um
yeah that's pretty much it uh you can
also do screen sharing so they might
send you request to go on Zoom or
download this piece of software that
shares your screen and so that is
another option as well so they can get
pretty handson to help you uh with your
problems there but that's pretty much
all I wanted to show you with support
I'm going to downgrade this and I'm not
sure if they're going to give you back
my money sometimes it'll PR rate it for
you but I'm go here and go back to basic
um so we will also refund your credit
card directly in the month's remaining
fees on your old plan which you
previously paid you're obligated to pay
a minimum of 30 days of support each
time you register so I'm not going to
get any money back which is totally fine
because I just wanted to show you how
that works but business support is
definitely worth it and uh you know
that's
[Music]
it so the anabis marketpl is a curated
digital catalog with thousands of
software listings from independent
software vendors uh easily find buy test
and deploy software that already runs an
8s the product can be free to use or can
have an Associated charge the charge
becomes part of your adus bill and once
you pay adus Market pays the provider
the sales channel for isv and Consulting
Partners allow you to sell your
solutions to other adus customers
products can be offered such as Amis Aus
CL information templates software of
service offerings web acl's it WAFF and
rules so it sounds great um if you want
to sell here I think you need like a US
bank account to do it um and you know
sometimes zus Marketplace is just part
of AWS so like when you're using the ec2
marketplace you are technically using
the itus marketplace um but they also
have like a dedicated page for it so
it's integrated with some services and
it's also stand alone
[Music]
okay hey this is Andrew Brown from exam
Pro and this follow along we're going to
take a look at the adus marketplace so
what I want you to do is go on the top
and type in Marketplace and that'll
bring us over to here the marketplace
can be found in a variety of different
places on the platform here you can see
that uh previously I was using something
called guaca Bastian host to launch a
server um but the idea is that um you
can discover products and subscriptions
that you might want to utilize so if I
go over here there's a variety of
different
things and so it could be like I want to
have something like a firewall that
might be something that we might be
interested in so we could search there
and there's like bring your own license
firewall so maybe you have a license
with this and you want to run it on an
ec2 instance something like that again
it's not like super complicated what's
going on here but a lot of times you
know when you're using Services you're
accessing the marketplace anyway so like
when I'm launching an ec2
instance notice on the left hand side
says ABS Marketplace and so I don't have
to go to the marketplace there I can
just kind of like check out the thing I
want um and that's pretty much all there
really is to it okay so you know
hopefully that makes
sense let's take a look here at
Consolidated billing so this is a
feature of Abus organizations that
allows you to pay for for multiple
accounts via one bill so the idea here
is we have a master account and we have
member accounts and I'm pretty sure that
we probably call this root account now I
don't think uh master account might be a
data term but it's still showing up in
the documentation the idea is that if
you have member accounts within your
organization they're all going to be
Consolidated under the single account if
you have an account outside of your
organization um you know this is not
going to give you uh this is going to be
basically a separate bill um as if it's
like a standalone organization or what
have you okay
so uh for billing adus treats all
accounts in an organization as if they
were one account you can designate one
uh uh master or root account that pays
the charges for all the other member
accounts consolidate billing is offered
at no additional cost you can use uh
cost Explorer to visualize usage for
Consolidated billing which we can see I
have the icon here uh you can combine
the usage across all accounts in the
organization to uh to share the volume
pricing discount which we did Cover in
this course separately if you want an
account to be able to leave the
organization you do have to attach it to
a new payment method so if let's say you
had an account and you want to give it
to your friend or whatever they're have
to hook up their uh their credit card
but you can totally have uh an account
leave an organization but you have to
deal with that billing aspect
[Music]
okay all right so there's a really cool
way to save an ads and that's through
volume discounts and it's available for
many services the more you use the more
you save is the IDE aide behind it um
and so consolidating building lets you
take advantage of volume discounts this
is a particular feature of AIS
organization so if you do not have the
or turn on you're not going to be able
to take advantage of that okay so one
example would be something like data
transfer where it is build uh for the
first 10 terabytes at at 17 cents or
sorry 17 cents and then the next 40
terabytes it will be AT3 cents okay so
if we had two accounts um such as Odo
and Dax and they're not within an abl
organization we can calculate those and
see what they are unconsolidated and
just so you know 1 terab equals 1024
gigabytes and that's what you're going
to see in these calculations so for Odo
uh you know if he has four terabytes and
that is uh we calculate the gigabytes
there we times it by uh the um scent
value there we're going to get
$696 okay for Dax we're going to end up
with uh about 1392 there and so if we
were to add those up the bill would come
out to
$2,088 okay so the idea is that there's
an organization and they like a your
company and they created two accounts
but they're just not within an
organization by having them in the
organization you're going to save um
about almost $80 there so um that is a
reason why you'd want to use volume
discounts
[Music]
okay hey this is Andrew Brown from
exampro and we're taking a look at IIs
trusted advisor so trusted advisor is a
recommendation tool which automatically
and actively monitors your adus accounts
to provide acual recommendations across
a series of categories so this is what
it looks like I personally prefer the
older dashboard but this is what they
have now and you can see along the side
we have a bunch of categories and then
we have some checks here saying uh you
know what are we meeting what are we not
and you can go in and read each one and
they'll tell you so much information
they'll even show you like what things
are not meeting that requirements in
some case you can easily remediate by
pressing a button not in all cases but
the thing with adus trust advisor is
think of adus trust advisor like an
automated checklist of best practices on
AWS and they kind of map to the pillars
of the well architecture framework not
exactly but pretty close but there are
five categories of adus trusted advisor
so we have cost optimization how much
money can we save performance so how can
uh we improve performance security how
can we improve security fall tolerance
how we can we prevent disaster or data
loss and service limit so are we going
to hit the maximum limit for a service
and so uh the next thing we need to
discuss is um there's a variation of the
amount of checks that are available to
you based on your support plan so you
know if you're using basic or developer
you have seven trusted advisor checks
and if you have business Enterprise you
have all the trusted advisor checks so
uh if we're talking about just the ones
that are available to you the ones that
come for free is MFA on root account
security specified ports of unrestricted
Amazon S3 bucket permissions Amazon EBS
public snapshots Amazon RDS public
snapshots IMU so this is just about
alerting you about discouraging the use
of the root account service limits so
all service limit checks are free um
it's weird because they call it the like
seven Security checks but if you counted
all the service limits it obviously be
too large of a number but notice that 1
through six are all Security checks so
you're not getting anything from the
other tiers just the security tier and
what I want to do is just go over a
bunch of available checks out there it's
probably not the full list because I
couldn't even be bothered to update it
if they've added more but it will give
you the general idea of what you could
expect under each category so for cost
optimization um it could be things like
looking at idle load bouncers so you
know if you have load bouncers you're
not using you're paying for them so get
rid of them unassociate elastic IP
addresses so for every IP that's not
associated you're paying for as well
maybe under performance you have um High
utilization of Amazon ec2 instances so
maybe you can save money by switching to
smaller instances under security we saw
MFA on rout account very popular one
making sure you turn on key rotation
could be something as well there under
fault tolerance um it could be making
sure that you're using backups on your
Amazon RDS database maybe that's turned
off uh for service limits there's just a
ton of them and so uh one that that you
know might be PR to use vpcs or ec2
limits so there you go
[Music]
go hey this is Andrew Brown from exam
Pro and we're going to take a look at
trusted advisor so what I want you to do
is go to the top and type in trusted
advisor and once you're there you're
going to notice on the left hand side we
have cost optimization performance
security fault tolerance and service
limits right now there are no
recommended actions because there's not
much going on this account and when you
uh have the uh Free level of support the
basic support you're not going to have
all these checks but if we go in here we
can still see kind of what they do um so
we have like performance security things
like that so these are the ones that we
actually can see and they generally work
all the same way if you expand here it's
going to say Amazon EBS public snapshot
so check the permission settings for the
EBS volume snapshots and alert you if
the any snapshots are marked as public
and so if you scroll on down if there
were ones that were an issue it would
tell you right here okay then down below
here we see like check buckets in Amazon
S3 that have open access permissions or
allow access to authenticated aabus
users so yellow the ACL allows uh list
access for everyone uh a bucket policy
allows for any kind of Open Access
bucket policy statements have public
Grant access so maybe what we can do is
see if we can get this to
trigger and so what I'm going to do here
is go over to S3 and what we're going to
do is make a B bucket that has full
access okay so I'm going to create a new
bucket and it'll say my exposed
bucket we'll scroll on down here and
we'll just checkbox that off and create
the bucket I say I acknowledge that is
totally
fine okay so now I have a bucket that is
100% exposed if we go back to trusted
advisor give this a
refresh I'm not sure how fast it will
show up here but if I
expand so it says the bucket ACL allows
upload delete for everyone the trusted
advisor does not have permissions to
check the policy uh bucket policy has
statements that Grant Public
Access so what we could try to do is
make a
policy and try to Grant all access here
so I'm not writing these every single
day but I'm sure we could try to figure
this
out
um we'll say S3 bucket policy Public
Access public
read and so that one might be a good
example so I'm going to go ahead and
copy this one granting read only
permission to anomymous
users I don't recommend you doing this
I'm just doing this to show you to see
if we can get the trusted advisor to
check because I don't want you to uh do
this and forget about it and then have a
serious issue but the principal is set
to anybody so anyone can read it and
here it saying get object Etc then it's
saying what particular resource so this
one is going to be for uh the bucket in
question here which is my
exposed
bucket we're going to scroll on down
save the
changes okay so this bucket is publicly
accessible we're going to go back over
here refresh and see what we can
see okay so checks buckets in S3 Etc so
it should appear under
here and it could be that it's just
going to take some time so what I'm
going to do is I'm just going to hang
tight for a little bit oh there we go
okay so it's showing up and I guess it
just took some time to populate and so
here we can see we have a a yellow
symbol it's a warning saying hey there's
a problem here if we go back to the
dashboard I wonder if that shows up so
this one's for investigation and
recommendation so you know hopefully
that kind of makes sense to you I think
in some cases you can do remediation
from from here or at least you can go
and check box and say okay um
ignore could of swore there's
remediation for some of
these but in any case you know that's
generally what trusted adviser does um I
think that you probably can have it so
that gives you alerts so yeah you could
set recipients for particular things
like if there's a security issue then I
could email a particular person on your
team and they could deal with it but
that's pretty much it so what I'm going
to do is go ahead and delete this bucket
I'm all done with it
we'll go
delete and say my delete uh my exposed
bucket here to delete it and that is it
[Music]
okay let's cover the concepts of service
level agreements also known as SLA so an
SLA is a formal commitment about the
expected level of service between a
customer provider when a service level
is not met and if customer meets its
obligation under the SLA customer will
be eligible to receive compensation so
Financial or service credits and so when
we talk about slas then we talk about
SLI so SLI service level indicator is a
metric or measurement that indicates
what measure of performance the customer
is receiving at a given time a SLI
metric could be uptime performance
availability throughput latency error
rate durability correctness and if we're
talking about sis then we're talking
about slos service level objectives so
the objective that that the provider has
agreed to meet slos are represented as a
specific Target percentage over a period
of time and so an example of a Target
percentage would be something that says
an availability SLA of
99.99% in a period of 3 months all right
and let's just talk about Target
percentages and the way they can be
represented very common ones we will see
is
99.95%
99.99% uh then we have 99 followed by
99 and so commonly we just say we call
this 99 okay and then there's one 911s
so if somebody says we have an SLA
guarantee of of 911s it's going to be
the 99 followed by 911s all
[Music]
right let's take a look at Abus service
level agreements and so there are a lot
of them and I just wanted to show you a
few services to give you an idea how
they work uh on the exam they're not
going to ask you like oh what's Dynamo
DB's SLA for Global tables um but
generally we should just go through this
because it's good practice so let's take
a look at dynamodb SLA so adus will use
commercially reasonable efforts to make
dynb available with a monthly uptime
percentage of each adus region during
any monthly billing cycle uh so for a at
least
99.999% if Global tables slas applies or
99.99% if the standard SLA applies in
the event Dynamo DB does not meet the
service commitment you'll be eligible to
receive service credits described below
so we have monthly uptime percentage and
the service credit percentage we get
Global tables standard tables so let's
take a look here so if less than
99.999% but equal to or greater than
99.0% is met so if if the service ends
up being this you'll get 10% back of
what you spent as service credits if it
drops between U 99.0 and 95.0 you get
25% back if it's less than 95
uh% um then it's 100% back okay and you
get the general idea here SLA is going
to be slightly different with their
drops now let's take a look at um
compute and so compute is going to apply
across a bunch of compute
Services probably because they're all
using ec2 underneath so that's probably
the reason for it so we have ec2 EBS ECS
eks and ab uh makes two SLA commitments
uh for the included services so we have
a region level SLA that uh governs
included Services deployed across
multiple A's or regions and an instance
level SLA that governs Amazon ec2
instances individually and again we have
our monthly up up time percentage our
service CED percentage region and
instance level so you can just see the
same thing it's like it's going to
change based on uh what it can meet then
we'll take a look at one more like RDS
so relational database service so it was
we'll use commercially reasonable
efforts to make multi-az instances
available with monthly uptime percentage
of 99.95% during any monthly billing
cycle and again you know if if they
don't meet those requirements you're
going to get service credits back which
basically equal USD dollars on the
platform and so for this it looks like
that so just notice that you know with
like compute it was for a a bunch of
services for Dynamo DB it was based on
uh particular features like global
standard tables SLA it's very
straightforward uh we didn't do S3
because I just did not want to show you
that one it's just too complicated but
my point is is that it's going to vary
so you have to look up per service okay
[Music]
hey this is Andrew Brown from exam Pro
and we are taking a look at Amazon's
service level agreements and so the way
you find slas is you pretty much just
type in SLA for whatever it is so if
you're looking for compute you type in
SLA or you look for a particular service
so maybe you say sage maker SLA AWS I
don't think there's like a generic SLA
page at least I don't know where it is I
always just type in SLA to find what it
is and through that you can just kind of
read through and try to find out uh the
things that that matter to you for your
business
[Music]
okay let's take a look here at the
service Health dashboard and so the
service Health dashboard shows General
status of agus services and it's really
simple the idea is that you can uh check
based on the geographic area so you'd
say North America Europe Etc and what
you'll see is an icon that says whether
the service is in in good standing and
the details the service is operating
normally Etc notice they also have an
RSS feed the reason I'm talking about
service Health dashboards is because I
want to talk about personal health
dashboards and because they're both
called Health dashboards it's confusing
so I wanted to tell you about this one
first so now we'll jump into the adabs
personal health
dashboard so we saw the service Health
dashboard now let's take a look at the
adus personal health dashboard so this
is what it looks like and it provides
alerts and guidance for adus events that
might affect your environment all Abus
customers can access the personal health
dashboard the personal health dashboard
shows recent events to help you manage
active events and show proactive
notifications so that you can plan for
scheduled activities you uh you can use
these alerts to get notified about
changes that can affect your aess
resources and then follow the guidance
to diagnose and resolve the issue so
this is very similar to the service
Health dashboard but it's personalized
for you um and it's uh you know I I
don't see crop up very often but if you
had to create alerts or be reactive to
uh things that are happening within us
this is where You' do it
[Music]
okay so there's a team called adus trust
and safety that specifically deals with
abuses occurring on the adus platform
and so I'm going to just list of all the
cases where you'd want to be contacting
them as opposed to support so the first
is Spam so you're receiving unwanted
emails from an adus owned IP address or
adus resources are used to spam websites
or forms Port scanning your log show
that one or more adus owned IP addresses
are sending packets to multiple ports on
your server uh you also believe uh this
is an attempt to discover unsecured
ports uh dos attack so your logs show
that one or more itus owned IP addresses
are used to flood ports on your
resources with packets you also believe
this is an attempt to overwhelm or crash
your server or the software running on
your server intrusion attempts so your
logs show that one or more ad of owned
IP addresses are used to attempt to log
into your resources hosting prohibited
content so you have evidence that Abus
resources are used to host distribute
prohibited content such as illegal
content or copyrighted content without
the consent of the copyright holder
Distributing malware so you have
evidence that abis resources are used to
distribute software that was knowingly
created to compromise or cause harm to
computers machines that it's installed
on and so in any of these cases you're
not going to Ed support you're going to
open up an abuse ticket and so you got
to contact abuse at amazon.com
or fill out the uh Amazon abuse uh form
so and this is whether it's coming from
uh an outside AOS account or even your
internally if you think that some
someone has compromised your account and
it's being used in any of these ways uh
this is what you're going to do
[Music]
okay hey this is Andrew Brown from exam
Pro and we're looking at AWS so uh we're
saying that adus has the adus trust and
safety team and what you'll want to do
is if you uh find that there's an issue
you're going to report it to this email
at abuse Amazon.com or you're going to
use this form which is the report Amazon
a abuse so you'll go down here you'll
sign in you'll put your email in your
first name last name or phone number um
Source IP the the details uh uh in uh
here you can even select the type of
abuse so you say if it's this kind or
that kind things like that it's very
straightforward um and that's pretty
much it okay
[Music]
hey this is Andrew Brown from exam Pro
and we are taking a look at the adus
free tier and this allows you to use
adus at no cost um and when we say free
tier there there there's the idea of the
first 12 months of sign up there's going
to be special offerings or it's free
usage up to a certain monthly Limit
Forever um and then there's just
services that are inherently free which
we have a total separate slide on but
let's talk about just the free tier
stuff and this is absolutely not the
full list um but uh it's a good idea
like it gives you a good um overview of
stuff that is free so for ec2 which you
use a web server you get a T2 micro for
750 hours per month for one year and so
there's about 730 hours um in a month
and so that means you could have a
server
running uh the entire month for free uh
and an additional server for a bit as
well so for RDS which is a relational
database service for either my schol or
postgress we can do it T2 DB micro for
750 hours for free so there we get our
free database and you would be surprised
how far you can get with a uh a T2 DB
micro um you know even for a mediumsized
startup you can run it on uh a T2 DB
micro with no problems then you have
your lassic load balancer you get 750
hours per month for one year um so that
is a really good thing uh load balancers
usually cost $150 a month so that's
great actually all these pretty much
cost $15 a month so that's about um 1530
$45
month over month for a year that's uh
free then you have Amazon cloudfront
this is where you'd have your homepage
caching your videos things like that so
you get 50 gigabyt data transfer out for
the total year then there's Amazon
connect you get your total free number
there 90 minutes of a call time per
month for one month or for one year
sorry Amazon elasticache so you could
launch a redis or elasticache server you
get 70 hours on a Cash3 micro for a year
um elastic search service so this full
Tech search so again 7 50 hours per
month for one year pinpoint campaign
marketing email so you can send out
5,000 targeted users per month for one
year sces so um simple email uh service
so this is for um transactional emails
um so that you send up from your web app
so 62,000 emails per month forever it
code pipeline so one pipeline free it
code build so uh this is for building
out uh projects or things like that so
100 build minutes per month forever it
was Lambda service compute 1 Mill ion
free requests per month 3.2 million uh
million seconds of compute time per
month for free uh and you know I like to
highlight these ones because for
traditional architecture you're always
going to have a web server a database a
load balancer um and you might even have
cloudfront in there as well but uh yeah
again there's a huge list and this does
not even tap the surface of what's free
on
[Music]
AWS hey this is Andrew Brown from exam
Pro and we are taking a look at a
promotional credits and these are the
equivalent to USD dollars on adus
platform ad credits can be earned
several ways this could be joining adus
activate startup program winning a
hackathon participating surveys and any
other reason that Adis wants to give
credits out uh once you uh have um a
promotional code you click the redeem
credit button in the billing console you
enter it in and then your credits will
be shown there you can monitor them via
adus budgets or uh via cost Explorer and
probably even building alarms it credits
generally have an expired dat tax maum
could be a few months to a year itus
credits can be used for most services
but there are exceptions where itus
credits cannot be used like purchasing a
domain via row 53 because uh that domain
costs money outside of ad's cost like
for their infrastructure and virtual
stuff and so for things like that uh you
know they're not going to be you're not
going to be able to use credits for that
[Music]
okay the adabs partner Network also know
as APN is a global partner program for
ads so joining the APN will open your
organization up to business
opportunities and allow exclusive
training and marketing events so when
joining the APN you can either be a
Consulting partner so you help companies
utilize adabs or a technology partner
you build technology on top of adabs as
a service offering and a partner belongs
to a specific tier so it's either going
to be select advance or Premiere when
you sign up it's free to sign up but
you're not going to be able to do much
until you start uh committing to an
annual fee so that's it's like a certain
amount of money to uh be able to be part
of that tier and it starts in the
thousands okay so I think the first tier
is like something like a, or $2,000 and
it gets uh more expensive as you go up
as a tier and you also have to have
particular knowledge requirements so
this could be holding uh particular ad
certifications at the at the
foundational level at the associate
level things like that um or it could be
adus APN exclusive certifications so
training that um is not a with
certifications but there're
certifications that are only available
to Partners saying like how do you it
could be like something like how do you
uh talk to customers or communication
things like that you can get back
promotional Abus credits so you know if
you say oh man I spent uh
$2,000 on just being able to uh get into
the APN at least the idea is that you
can generally get back that uh that
spend on AWS so it's like you committing
if you give like $2,000 it's like you're
going to commit to keep using ads I'm
not showing the annual fee commitment
here and the promotional credits that
you get back just because they've
changed it a couple times on me and I
just don't want this slide to go stale
in case they happen to change it again
so you'll have to look that up to find
out what they actually are right now uh
you can have unique speaking
opportunities in the official adus
marketing channels like the blogs or
webinars being part of the APN is a
requirement to be a sponsor with a
vendor booth at adus events so when you
when you go to reinvent or any ads um
event all the vendors are part of the
APN all right so they've paid their fee
and now they paid an additional fee to
get their Booth but um yeah the ab
partner network uh is very good for uh
uh helping you find new business and
connecting with other people that are
building workloads onws but hopefully
that gives you an idea of how that works
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at adus
budgets so adus budgets gives you the
ability to set up alerts if you exceed
or approaching your defined budget
create cost usage or reservation budgets
it can be tracked at the monthly
quarterly or yearly levels with
customizable start and end dates alert
support ec2 RDS red shift elastic cast
reservations uh and so the idea here is
you can choose your budget amount so it
could be like $100 it'll even show you
what was the last amount if you're uh
resetting the budget is something new
you can choose based on a different kind
of unit so if you wanted to be based on
running hours on ec2 you could totally
do that is budgets can be used to
forecast costs but is limited compared
to cost Explorer or doing your own
analysis related with cost and usage
reports along with business intelligence
tools budgets uh based on a fixed cost
or or you can plan your cost uh upfront
based on your chosen level can be easily
managed from the adus budgets dashboard
via the ad budgets API get notified by
providing email or chatbot and threshold
uh how close to the current or
forecasted budget um so you'd see a list
of budgets here uh current versus
forecasted the amount used things like
that you can see your budget history you
can download a CSV uh it'll show you the
cost history right in line there which I
can't show you it it's hard to see there
you get the first two budgets are free
so there's no reason not to set a budget
when you first get into AWS and each
budget costs about uh 002 cents a day so
it's like 60 cents um uh USD per month
for a budget so they're very cheap to
use and you got a limit of 20,000
budgets they're going to be in good
shape
[Music]
okay let's take a look here it was
budget reports which is use alongside
abos budgets to create and send daily
weekly or monthly reports to monitor the
performance of your abos budgets that
will be email to specific emails so it's
not too complicated here you say create
the report budget choose your frequency
uh the emails you want um and ab budget
report serves as a more convenient way
of staying on top of reports since
they're delivered to your email instead
of logging into the abis Management
console so it's just for those people
that just can't be bothered to log in
okay
let's take a look here at adus cost and
use as report so generate a detailed
spreadsheet enabling you to better
analyze and understand your adus cost so
this is kind of what it looks like and
when you turn this feature on it will
place it into an S3 bucket you could use
something like Athena to turn the report
into a queriable database since it's
very easy to consume S3 csvs into Athena
you could use Quick site to visualize
your building data as grass so quick
site is a business intelligence tool
similar to Tableau or powerbi you can
also ingest this into red shift um but
the idea here is when you turn it on you
can choose how granular you want the
data to be hourly daily or monthly if
you turn on daily you'll be able to even
say spikes of uh of of of costs for uc2
instances which is kind of nice the
report will contain cost allocation tags
um which I think we have a separate
slide on that type of tags and the data
is stored in e as either a CSV it'll be
zipped or it will be a parket format it
just depends on how you want it um uh
for that
[Music]
okay let's talk about cost allocation
tags so these are optional metadata that
can be attached to adus resources so
when you generate a a cost and uses
report you can use that data to better
analyze your data so what you'd have to
do is make your way over to cost
allocation tags and need to activate the
tags you want to show up there are two
types of tags so we have user defin so
whatever you've previously tagged will
show up probably there
you turn it on so if you made one with
project you turn on project and there's
a lot of Adis generated ones that you
can turn on so there's a huge list there
but uh yeah that's particular with cost
um usage and reports if it says like
cost allocation reports it's just that's
what cost and usage reports used to be
called um and some of the documentation
is a bit old there but yep there you
[Music]
go so you can create your own alarms in
cloudwatch alarms to monitor spend and
they are commonly called building alarms
uh and so it's just a regular alarm but
it's just focused on spend but in order
to do this you have to turn on building
alerts first in order to uh be able to
use it uh and then you'll go to
cloudwatch alarms and you can choose
billing as your metric and then you just
set your alarm however you'd want
bilding alarms are much more flexible
than abess budgets and are ideal for
more complex use cases for monitoring
spend and usage in terms of alerting so
you just have to decide what you want to
do uh before this was the only way to do
it and so this is the way I'm used to
doing it and I still do it this way
today but you know both options are
valid you just have to decide what is
your use case
[Music]
okay let's take a look at abos Cost
Explorer which lets you visualize
understand and manage your adus cost and
usage over time so uh here's a big
graphic of Adis cost Explorer and you
can specify time and range and
aggregation and it has a lot of robust
filtering um what's really nice is that
they have a bunch of default reports for
you so I'm just going to get my pen tool
just to show you where that button is
it's over uh here uh if you can see my
marker there but but you know you can
look at things like monthly cost by
service monthly cost by linked account
daily cost a Marketplace R utilization
so there's a bunch there you can also
notice you can create your own report so
if you do find something that you like
you can save it for later um you can you
could have access to forecasting here so
you get an idea of the future cost and
whether it's been it's gone up or down
just to kind of zoom in on some of those
filtration options you can choose um
either monthly or daily level of of how
you want the data to be grouped together
and you have a lot of filter control so
if I want to just have ec2 instances for
a particular region then I can get that
filtered information over here and you
can see you have a breakdown of the
different types so it's very detailed
and class Explorer shows up in Us East
one I'm pretty sure if you click on
class Explorer it will just switch you
over to that region but just understand
that's where it lives okay
[Music]
hey this is Andrew Brown from exam Pro
and in this video I want to show you ad
cost Explorer so what we'll do is go to
the top here and actually on the right
hand side we're going to click on the
right and go to my billing dashboard and
from there on the left hand side we're
going to look for cost Explorer and then
click launch cost Explorer and this is
where we're going to get to the aist
cost management dashboard where this is
where we find savings plans reservations
things like that on the left hand side
click on cost Explorer and you can get
this nice chart and so the idea is you
can change it from monthly to daily if
you if you uh
prefer okay you can change the scope
here maybe we don't need six months we
can just go
back
um three months here so there's less
data it is a bit delayed when I'm
clicking here so it also could be just
because I'm doing the daily instead of
monthly so you just have to be a little
bit patient when uh using this interface
you can change it to stack line graph
you can kind of see the details there
it's not always clear like what others
is or things like that and so uh you can
drill down and there's like ways of
applying filters and things like
that I always forget how to uh do this
it's because it's it's bringing
everything in so you have to hit clear
all first I
think and
um oh you have to click into it so like
if you wanted to click into it and pick
a particular service we could go here
and type in
ec2 and say ec2 instances and then apply
that filter so now we can just see
exactly that cost or if we want to
choose like maybe just
RDS okay so you know that could be
useful for you to see but yeah sometimes
it's not always clear and so what I
recommend is just go back to your
billing dashboard and from there just go
to bills okay bills is really really
useful because here it shows you exactly
every single little service that you're
being built for you can expand it and
see exactly where if you have other
accounts you can go into this side here
as well and find spend that way um but
cost Explorer is very useful just it's
useful in a different way okay so there
you
[Music]
go hey this is Andrew Brown from exam
Pro and we are taking a look at the adus
pricing API so with adabs you can
programmatically access pricing
information to get the latest price
pricing offerings for services this
makes sense because abos can change them
at any time and so uh you know you might
want to know exactly what the current
price is uh there are two versions of
this API so we have the career API known
as the pricing service API and you
access this via Json and then there's
the batch API also known as the price uh
list API via HTML what's odd is that um
the batch API returns Json but you're
accessing it via HTML so you can
literally paste those links in your
browser for the API you're actually
sending an an application Json request
so you'd have to use something like
Postman or something uh you can also
subscribe to SNS uh notifications to get
alerts when pricing for the services
change ads prices change periodically
such as when ads Cuts prices when new
instance types are launched or when new
services are introduced so there you
[Music]
go hey this is Angie Brown from exam Pro
and what I want to do here is show you
savings plans and so savings is going to
be found under the it cost Explorer so
just type in cost Explorer at the top
here or if you want you can type in
savings plan as well and once we are
here on the left hand side we are going
to have uh savings plans options so
we're going to go to the overview and
here it just describes um what our
savings plans if you want to read
through it but down below if you have
already some spend happening it's going
to make some suggestions and in this
particular account it's saying that I
could save some money on compute before
we take a look here I'm just going to go
to the form here and see what we can see
so up here we can say uh commitment
through 3 years by the way you have
compute savings which applies to ec2
fargate or Lambda then you have the ec2
specific one where uh we can select a
very particular type of instance family
and then there's the sage maker savings
plans um but if we go here and we just
enter in like
$2 all up front uh I don't really
understand it from here because it
doesn't make it clear what the savings
are um but uh I what it does make it
very easy is probably if we go over here
and then click down on the compute so I
kind of feel like here it would autofill
it in for you and so here I filled it in
uh or sorry it's filled it in for me and
so here it's saying with a one-year plan
all Upfront for uh based on the past 30
days that it's going to see that I'm
going to see a monthly savings of
$25. 36 and then I can add it to the
cart that way and I kind of feel like
that is the easiest way to um figure
that out where with um with how it was
going to that form I just configured out
myself what the savings were uh there
are some utilization reports and
coverage reports honestly I've never
really looked at these before um but uh
I'm just curious like what we're looking
at monthly
daily the
last let's go a few months here I've
been running stuff in this account for a
while so there should be
something
apply so nothing nothing of interest but
um I mean I guess you have a lot of use
and coverage report and utilization
report could be interesting but I
imagine it's maybe you have to be using
you have to have a savings plan before
you can see this so that's probably the
reason why um but yeah hopefully that
gives you a clear idea that you know you
can just go down to those
recommendations and and see exactly what
you can save and you just add it to your
cart and then once you want to pay for
it you just choose to submit that order
and you're all good to go all right so
that's savings plans
[Music]
let's take a look here at defense in
depth to understand the layers of
security ads has to consider uh for
their data centers for their uh virtual
workloads and things that you also have
to consider when you are uh thinking
about security for your Cloud resources
so in the most interior we have data so
this is access to business and customer
data and encryption to protect your data
then we have applications so
applications are secure and free of
security vulnerabilities then you have
comput so access to Virtual machines
ports on premise and Cloud you have the
network layer so this limits
communication between resources using
segmentation and access controls you
have the perimeter itself so distributed
denial of service protection to filter
large scale attacks before they can
cause denial of service of users you
could say that's part of the network
layer and that's when I say there are
variants on this but we're just
separating it out uh explicitly there we
have identity and access so controlling
access to infrastructure and change
control and then there's the physical
physical layer so limiting access to
data centers to only authorized
Personnel you'll notice I highlighted
identity and access in yellow it's
because that is considered the new
primary um perimeter from the customer's
perspective of course adab best has
concerned about the physical perimeter
and things like that but as a as a
customer that's what you're going to be
thinking about especially with the zero
trust model and when you see these
depths the idea is that in order to get
here you have to pass through all this
stuff so if this um if this outward one
is protected pretty well then you
generally don't have to worry about the
Interiors but of course you should um
but yeah there you
[Music]
go let's take a look here at
confidentiality integrity and
availability also known as the CIA Triad
is a model describing the foundation to
security principles and their tradeoff
relationships so here is our Triad so we
have confidentiality so confidentiality
is a component of privacy that
implements to protect our data from
unauthorized viewers in practice this
can be using cryptographic keys to
encrypt our data and using keys to
encrypt our keys so envelope encryption
then we have integrity so maintaining
and ensuring the accuracy and
completeness of data over its entire
life cycle in practice utilizing asset
compliant databases for valid
transactions utilizing tamper evident or
tamper proof Hardware security modules
hsms availability so information needs
to be available when needed in practice
so high availability mitigating dos uh
decryption access so the CIA Triad was
first mentioned in N publication 1977
there have been efforts to expand and
modernize or suggest alternatives to the
CIA triab so one was in 1998 for the six
Atomic elements of information uh or in
2004 we have the N engineering
principles for uh for information
technology security so it has 33
security principles but this is still a
very popular um model for security uh
and it's just to kind of tell you like
you know you don't always get everything
you don't get all three of them
sometimes you have to trade off in your
scenario um you know and hopefully some
of the terminology here will uh resonate
as we go through more security
[Music]
content what I want to do here is just
Define the term vulnerability so
vulnerability is a whole or weakness in
an application which can be designed a
design flaw or implementation bug that
allows an attacker to cause harm to
stakeholders or applications and uh
there's a lot of great definitions of
vulnerabilities but OAS has a ton of
them and we talked about OAS when we
talk about Abus Waf uh but it's an
organization that creates security
projects that help you know what you
should protect uh or gives you a working
examples so that you can understand how
to get better at security and so they
have a lot of ones here but maybe you'll
might notice some here like using a
broken or risky cryptographic algorithm
maybe there's a memory leak least
privilege violation so that's um uh
lease privilege is something that is a
thing that you're always worried about
insecurity improper data validation
buffer overflows so you know just to
kind of set the tone of what a
vulnerability is and things you should
be thinking about
[Music]
okay let's understand what encryption is
but before we do we need to understand
what is cryptography so this is the
practice and study of techniques for
secure communication in the presence of
third parties called adversaries and
encryption is the process of encoding or
scrambling information using a key and a
cipher to store sensitive data in an
unintelligible format as a means of
protection an encryption takes in plain
text and produces produces a cipher text
so here's an example of a very old um
encryption machine this is the Enigma
machine used during World War II and it
has a different key for each day that it
was used to set the position of the
rotors and it relied on simple Cipher
substitution and so you might be asking
what is a cipher and that's what we're
going to look at next
[Music]
so what is a cipher it is an algorithm
that performs encryption or decryption
so Cipher is synomous with code uh and
the idea is that you use the code to
either unlock or or lock up the
information that you have so what is a
cipher text a cipher text is the result
of encryption performed on Plain text
via an algorithm so you lock that up you
scramble it it doesn't make sense and
you need that code to unlock ET to get
the information so a good practical
example back in the day was a code book
and this was the type of document used
for Gathering and storing cryptographic
codes or ciphers so the idea is if we
zoomed up on here notice where we have
cannot so uh and it would be0 0 and then
there would be give them Authority so
the idea is 0 0 or if you had the word
cannot it would translate to 0 and then
you use 0 to match that up to say what
does that mean and so that is kind of a
very practical example of ciphers in
[Music]
action so we just took a look at
encryption but what are cryptographic
keys so a c a cryptographic key an easy
way to think of it is a variable used in
conjunction with an encryption algorithm
in order to encrypt or decrypt data and
there are different kinds of um ones we
have so we have symmetric encryption so
this is where we have the same key that
is used for encoding and decoding uh and
a very popular one and the one you'll
see on AWS is called Advanced encryption
standard AES so just take a look at that
graphic very closely so we have one key
and it is used to encrypt so it produces
the cipher and then or Cipher text we
should say and then it will uh decrypt
and we will get our plain text so one
single key then we have asymmetric
encryption so two keys are used one to
encode and one to decode and a very
popular one here is RSA if you're
wondering what those uh those words are
it's three people's names put together
who helped uh invent this type of
algorithm and so here we have uh one key
for ecrypt and one key for decrypt and
they're two different Keys all
[Music]
right all right let's look at the
concept of hashing and salting so for
hashing we have a hashing function and
this accepts arbitrary size values and
Maps it to a fixed size data structure
hashing can reduce the size of a store
value and hashing is a one-way process
and is deterministic so a deterministic
function always returns the same output
output for the same input so if we have
something like John Smith and we pass it
to the hash function it's going to
create something that is not human
readable but it'll say something like 02
Fae X XY whatever um and it will always
produce the same thing if the same key
or you know value is being input there
so the reason we use hashing functions
or hashing General is to Hash passwords
so hash functions are used to store
passwords in a database so that the
password does not reside in a plain text
format so you've heard about all these
data reaches where they've stored the
password in plain text this is the thing
that helps us avoid that issue um and
the thing again is it because it's one
way you can't take that hash and unhash
it um well there are some conditions to
it but so to authenticate a user when a
user inputs their password it is then
hashed so the one that was inputed at
the time of you know login and then that
hash is compared to the stored hash in
the database and if they match the user
is successfully logged in so in that
case we never ever had to know what the
original password looked like uh popular
hashing functions are md5 Shaw 256 or
bcrypt uh if an attacker knows the
function you are using uh and uh and
stole your database they could enumerate
a dictionary of passwords to determine
the password so they'll never see it but
they could just keep on going through
that so that's why we salt our passwords
so a salt is a random string not known
to the attacker that the hash function
accepts to mitigate the deterministic
nature of a hashing function so there
you
[Music]
go let's take a look here at digital
signatures and signing so what is a
digital signature it is a mathematical
scheme for verifying the authenticity of
digital messages or documents and a
digital signature gives us tamper
evidence so did someone mess or modify
the data is this data from uh someone we
did not expect it to be is it from the
actual sender and so we kind of have
this diagram where we have a person who
sends or is going to send a message so
they sign it and then uh Bob verifies
that it was for the person who it's from
so there are three algorithms to a
digital signature the key generation so
generates a public and private key um
then there is signing the process of
generating a digital signature with a
private key and the inputed value so
signing which is what is happening up
here signing verification verifies the
authenticity of the message with a
public key so remember the private key
is used for signing and the public key
is used for verifying SSH uses a public
and private key to authorize remote
access into a remote machine such as a
virtual machine it is common to use RSA
and we saw that RSA is a type of
algorithm earlier and so SSH hyen keyen
is a well-known command to generate a
public and private key on Linux I know
this one off the top of my head I always
know to do this um and so what is code
signing so when you use a digital
signature to ensure computer code has
not been tampered and so that's just a
like subset of digital signatur so you
can use this as a means to get into a
virtual machine or you can use signing
as a means to make sure that the code
being committed to your repository is
who you expect it to be from so there
you
[Music]
go let's talk about in transit versus at
rest encryption so encryption and
Transit this is data that is secure when
moving between locations and the
algorithms here are TLS and SSL then you
have encryption at rest so this is data
that is secure when residing on storage
or within a database so we're looking at
AES or RSA which we both covered
previously uh these algorithms so ones
that we did not cover was TLS and SSL so
we'll cover them now so TLS transport
layer security is an encryption protocol
for data Integrity between two or more
commun communicating computer
application so 1.0 and 1.1 are no longer
used but TLS 1.2 and 1.3 is the current
best practice then we have SSL secure
socket layers so an encrypted protocol
for data Integrity between two or more
communicating uh computer application so
1.0 2.0 and 3.0 are deprecated um and
honestly I always get these two mixed up
and I always fig fig uh uh get confused
which is being used but um you know
they're always changing on us but just
understand generally what these concepts
are and that you're familiar with the
terms
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at common
compliance programs so these are a set
of internal policies and procedures for
a company to comply with laws rules and
regulations or to uphold business
reputation so here we have a bunch of
different compliance programs and so
some popular ones are like Hippa or um
PCI CSS the question is should you know
these yes you should generally know the
most popular ones because you're going
to see them throughout your Cloud career
um and so just getting familiar now is a
good time uh so let's jump into it okay
so the first one I want to introduce you
to is for I ISO and they have a bunch of
different ones so ISO is the
international organization of
standardization and there other one
called IEC which is the international
electr technical commission One deals
with uh you know like uh virtual things
the other one deals with Hardware things
but they have a lot of overlapping um
compliance programs okay and so the most
popular absolutely most popular one that
I know of is the 27100 I know a lot of
organizations that are going for their
271 this is for control implementation
guidance you have the
2707 this is enhanced focus on cloud
security the 27018 this is protection of
personal data in the cloud then you have
the 2771 this is Privacy Information
Management System so framework this
outlines controls and processes to
manage data privacy and protect piis so
that's personally identifi information
then you have system and organization
control sock and this is a very popular
thing that organizations go for
especially the sock two so sock one is
18 standards and report on the
effectiveness of internal controls at
the service organization relevant to the
client's internal control over financial
reporting we have sock 2 evaluates
internal controls policies and
procedures that directly relate to the
security of the system at a organization
and sock three a report based on the
trust uh service Services criteria that
can be freely
distributed then we have PCI DSS a set
of security standards designed to ensure
that all companies that accept process
store and transmit credit card
information maintains in a secure
environment we have a federal
information procedure standards or fips
so 140 hyphen 2 This Is Us and Canadian
government standard that specifies the
security requirements for cryptographic
modules that protect sensitive
information then we have uh phipa this
is more relevant to me because I'm
actually in onario in Canada but it's
also very uh wellknown um uh one out
there outside of HIPPA so this regulates
patient protected health information
then you actually have Hippa this is the
US federal law that regulates patient
procedure health information then we
have uh Cloud security Alliance so CSA
star certification independent
third-party assessment
of a cloud provider security posture if
you've never heard of CSA they have a
very well-known fundamental uh security
certification called the cssk or ccsk I
always get that mixed up then we have uh
fed ramp which we covered earlier in
this course or in the future depending
on where we put it but um fed ramp
stands for federal risk and
authorization Management program it's a
US Government standardization approach
to security authorizations for cloud
service offerings if you want to work
with the US government or places that
sell the US government need fed ramp
that similar to criminal justice
Information Services any US state or
local agency that wants to access the
FBI's cgis database is required to
adhere to the cgis security policy then
we have gdpr uh the general data
protection regulation everyone knows
what this is in Europe maybe not so much
in North America or other places a
European Privacy Law imposes new rules
on companies governments agencies
nonprofits and other organizations that
offer goods and services to people
people in the European Union or that
collect analyze data try tied to eu's
Residents there's a lot of compliance
programs out there one that's also very
popular is fips but we'll get to that
when we talk about KMS um but yeah uh
there you
[Music]
go so I just wanted to quickly show you
here the Adis compliance programs page
where they list out all the types of
compliance programs that ad us is uh
working with and that it has different
types of certification and attestment
which we can use itus artifact or Amazon
artifact whichever prefix they decide to
use for the name there um to uh ensure
that itus has in order to meet those
Regulatory Compliance you can see them
all there and if you want to know a
little bit more about any of these you
just go ahead and click them and you can
read and they have additional
information so you have a better idea
[Music]
okay let's talk about pen testing so pen
testing is an authorized simulated Cyber
attack on a computer system performed to
evaluate the security of the system and
on AWS you are allowed to perform uh pen
testing but um there are some
restrictions so permitted services or
ec2 instances Knack gateways elbs RDS so
that's um relational database service
cloudfront Aurora API gateways Lambda
Lambda Edge functions light cell
resources elastic beanock environments
things you cannot do or you should not
be doing is DNS Zone walking via row 53
hosted zones then there's dos simulation
testing so you should not be doing do or
Doss do doses or simulated Doss or
simulated dos is okay and that doesn't
mean that you can't necessarily do them
uh again there's a lot of exceptions to
the pen testing they have a whole page
on this but generally you're not allowed
to do dsing uh Port flooding protocol
flooding request flooding can't do any
of those things for other simulated
events you need to submit a request to
ads a reply could take up to 7 days uh
you know again there's a lot of uh
little intricacies here so you'd have to
really read up on it if you're
interested in doing this
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at pen
testing on the adus platform so they
have this page here that tells you what
you're allowed to do what you're not
allowed to do um and there's some
additional things you can read into like
the stress test policy the Dos simulate
simulation testing policy which I didn't
cover in detail uh in the course content
but for whatever reason you're
interested in it I just want you to be
aware of that kind of stuff if you want
to simulate events there is a simulate
event form that you have to fill out so
you open it up and you can kind of read
about it and it gives it us a heads up
of what you're going to be doing stress
test fishing malware analysis other so
that way that if you are doing it you're
not going to get in trouble they're
aware of what you are doing okay so
that's pretty much
[Music]
it hey this is Brown from exam Pro and
we are taking a look at itus artifact
which is a selfs serve portal for on
demand access to ibus compliance reports
so here's an example of a a bunch of
different compliance reports that adus
could be meeting and the idea is that
when you go to this portal within the
adus management conso you'll have a huge
list of reports that you can go and
access so here I'm searching for Canada
to get the government of Canada partner
package and then I go ahead and I
download that report as a PDF and then
within the PDF we can click a link to
get the downloadable Excel and that's
pretty much what it is it's just if you
want to see that adus is being compliant
for different
[Music]
programs hey this is Andrew Brown from
exam Pro and we're going to take a look
at adus artifact so in the top here
we're going to type in
artifact and not be confused with code
artifact which I guess is a new service
there's just always releasing new
Services a and so here we have a video
and some things but uh it's not too hard
all we got to do is go to view reports
and from here we have all the types of
compliance programs or Regulatory
Compliance programs that ad is uh
meeting and we can do is search for
something so we type in Canada and
that's the government of Canada partner
package and I can go ahead and download
that report so when you download it you
really want to open this up
in um you're going to really want to
open this up in um Adobe Acrobat because
if you don't open it up in Adobe acrobat
you're not going to be able to access
the
downloadblack reader and once you have
it open and I'm just moving it over here
this is what you're going to see and um
it's going to say like hey um oops no I
don't want to do that so please scroll
to the next page to view the artifact
download and so I think that if we go
here you know they say scroll to the
next page page but I'm pretty sure we
can just go here on the left hand side
and this is what we're looking for that
Excel spreadsheet so we're going to save
that
attachment or actually we just going to
open it
up open this
file okay and we'll give it a moment I
have Excel
installed and there we
go there it is okay so I know it's a
little bit odd way to get to those um uh
certificates or reports but that's just
how it works um but yeah I mean that's
the idea is like if you need to prove
that ads is meeting whatever those
standards are you can just type them in
whatever it is I like maybe there like
fed ramp right whatever it is and
download those certificate attestment
whatever um and just double check that
ads is Meeting those standards
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at AIS
inspector but before we can answer what
it does let's talk about hardening so
hardening is the act of eliminating as
many security risk risks as possible
hardening is common for virtual machines
where you run a collection of C Security
checks known as a security Benchmark so
adus inspector runs a security Benchmark
against specific ec2 instances and you
can run a variety of security benchmarks
and you can perform Network and host
assessments and so here's an example of
those two check boxes there which you'd
say which assessments you want to do so
the idea is you have to install the edus
agent on your ec2 instance you run an
assessment for your assessment Target
you review your findings and remediate
secur issues and one very popular
Benchmark you can run is the CIS which
has 699 checks so if you don't know what
CIS it stands for the center of Internet
Security uh and so they are this
organization that has a bunch of um uh
security controls or check marks uh that
are published that they suggest that you
should check on your
[Music]
machine hey this is Andrew Brown from
exam Pro and we're looking at dos so
dods is a type of ious attack to disrupt
normal traffic by flooding a website
with a large amount of fake traffic so
the idea is we have an attacker and the
victim the victim is us and it could be
our virtual machines our cloud services
the idea is that it's some kind of uh
resource which um can take in uh
incoming requests over the Internet so
the idea is the attacker is utilizing
the internet and so they may control a
bunch of uh virtual machines or servers
that're loaded up with malicious
software and the idea is that the
attacker is going to tell them all to
send a flood of traffic over the
Internet uh at your uh Computing
resource and uh this is where your
website is going to either start to
stall or it's going to become
unavailable for your users and so the
idea here is that you know if you want
to protect against dos you need some
kind of Dos protection traditionally
those used to be like third party
services that you uh would have to pay
for and and it would sit in front of uh
your load balcer or your uh n server but
now the great thing with cloud service
providers is that generally their
networks have built in DOS protection so
the idea is just by having your compute
or your resources on AWS you're going to
get uh built-in protection for free via
aw shield and we'll talk about that
[Music]
next hey this is Andrew Brown from exam
Pro and we are taking a look at it
Shield which is a managed dos Protection
Service that safeguards applications
running on ad
so when you route your traffic through
R3 or cloudfront you are using a shield
standard so here's a diagram to kind of
show you that it's not just those
services but these are the most common
ones where you'll have a point of entry
into AWS so here we could also be
including elastic IP Aus Global
accelerator but the idea is that when
you uh go through these Services into
the Aus Network it has Shield built in
and so you're going to get that
protection before those uh before that
traffic reaches your uh cloud service
and in this case we're showing ec2
instances so Shield protects against
layers three four and seven attacks uh
layer 3 four and 7even is based off the
OSI model which is a um a fundamental
networking concept so seven is for the
application layer four is the transport
Layer Three is the network layer um
there are two different types of plans
for a shield we have Shield standard
which is free and then Shield Advance
which starts at 3,000 USD per year plus
some additional uh costs based on usage
of the size of the tack or what services
you're using how much traffic is moving
in and out so protection against the
most common dos attacks is what Shield
standard does uh you have access to
tools and best practices to build dos
Brazilian architecture it's
automatically available on all aable
services for additional protection
against larger and more sophisticated
attacks that's where Shield Advance
comes into play it's available for
specific a
services so R 53 cloudfront elb Aus
Global accelerator elastic IP uh and
some notable features here is visibility
reporting on layer 3 4 and 7even you're
only going to get seven if you are using
awaf with it uh you have access to team
and support so these are DOs experts but
you're only going to get it if you're
paying for business or Enterprise
support as you're paying for this as
well uh you also get dos cost protection
just ensure that you know your bills
don't go crazy
uh and it comes with an SLA so you have
a guarantee that it's going to work both
plants integrate with itless web
application uh firewall so Waf to give
you that layer 7even application
protection so understand that if you're
not using Waf you're not going to be
having that layer 7even production
[Music]
okay hey this is Andre Brown from exam
Pro and we are looking at Amazon guard
Duty so before we look at that we need
to understand what is an IDs IPS so an
intrusion detection system and intrusion
protection system is used as a device or
software application that monitors and
network or systems for malicious
activity or policy violations so guard
duty is a threat detection service which
is IDs IPS that continuously monitors
for malicious and suspicious activity
and unauthorized Behavior it uses
machine learning to analyze the
following itus logs your cloud trail
logs your VPC flow logs your DN logs and
what it will do is report back to you
and say hey um there's this issue here
and this is actually one that's very
easy to replicate it's just saying
somebody is using the root credentials
and it's suggesting that you should not
be doing that right because you're never
supposed to be uh invoking API calls
with the root credentials or you should
be limiting that you'll might also
notice that if you want to investigate
you can kind of follow up that with uh
Amazon detective or adus detective
whichever uh prefix they decided to put
on that service it will alert you of
findings which you can automate an
incident uh response via cloudwatch
events which this uh it's been renamed
to event Bridge so you know or third
party services so you can follow up a
remediation action um and here is a
graphic of Amazon guard Duty just a bit
up closer so you can see all the
findings and you can just see you have a
lot of detailed information there
[Music]
okay hey this is Brown from exam Pro and
we're going to take a look at guard Duty
so guard duty is um an intrusion
protection and detection uh service and
so what I've done is I've um I've done
some bad practices purposely so that I
can show you um some information in
there so I'm going to go over to guard
Duty okay and you do have to turn guard
Duty on and so once guard duty is on
you're going to start getting reports
coming in so notice here that we have
some anomalous Behavior 8 days ago and
so uh that's B he's uh my co-founder
he's also named as well and so we can
kind of see some details here about
who's accessing what and what they were
doing he's not doing anything malicious
but we can have an idea where they're
from even shows generally where he is
which he is near Thunder Bay and his his
provider would be
TB um and you can see that he is making
uh API calls to describe account
attributes and things like that then the
other issue is the root account so
there's MFA I turned it off so that we
can or maybe this just usage here I
actually do have it turned on I suppose
here we see root credential usage and so
it's saying hey you used it 77 times
because sometimes I go in and and use uh
the Roo account for tutorials but saying
you're using this way too much you got
to stop doing that okay so that's
something that is uh pretty interesting
with guard Duty um and it's really cost
effective and easy to turn on so you can
turn it on looks like they have a new
thing for S3 um have not looked at that
as of yet but that's kind of cool kind
of feels like that would overlap with uh
Amazon Macy but whatever and here we get
a breakdown of cost so we see cloud
trail VPC FL logs DS logs and this is
where it would be ingesting data if you
want to use that S3 protection you'd
have to probably be turning or creating
a custom Cloud watch trail that has data
events to consume that information um
you know so you know hopefully that
gives you kind of an idea of things you
can do and you can also centralize guard
Duty uh into one account so you can have
one thing that takes care of everything
and and move all the data across all
your accounts into a single place so
that's kind of interesting and you can
set up follow followups um and it's
possible that uh I not seeing this this
here but generally it would show
you uh it would show you a way of like
triggering into Cloud watch probably you
could do it pragmatically this is
something interesting like the list
management you can add trusted IPS or
threat list so if there's people that
you know are fine you can just Whit list
them or if there's people that you know
that are bad make sure that they are
never allowed to get through so that's
pretty much it with guard Duty okay
let's take a look here at Amazon Macy so
Macy is a fully managed service that
continuously monitors S3 data access
activity for anomalies and generates
detailed alerts when it detects risks of
unauthorized access or inav virgent data
leaks so Macy works by using machine
learning to analyze your cloud trail
logs and Macy has a variety of alerts so
we have anomaly access config compliance
credential loss data compliance file
hosting identity numeration information
loss um location anomaly open
permissions privilege escalation
ransomware service disruption suspicious
access and mayy will identify your most
at risk users which could lead to
compromise so here's just one little
kind of uh tidbit from the um app itself
where you have the total users and they
categorize them into different uh risks
I can't remember which flag means what
in here uh Amazon Macy is an okay
Service uh it's it's very important if
you're storing things in
S3 but uh I don't I don't use it very
often to be
[Music]
honest hey this is Andie Brown from exam
Pro and we are taking a look at adus
virtual private Network also known as
VPN so itus VPN lets you establish a
secure and private tunnel from your
network or device to the idus global
Network it's very important to emphasize
the word secure here uh because when
you're using Direct Connect that will
will establish a private connection but
it's not using any kind of protocol to
secure that data in transit whereas a
VPN will be using a secure protocol
there are two options here we have adus
site tosite VPN so securely connect on
premise Network or branch office site to
VPC and adabs client VPN that securely
connect users to adabs or on premises
networks one thing that we need to
understand alongside VPN is IPC this
stands for Internet Protocol security
and is a secure network protocol Suite
that authenticates and encrypts the
packets of data to provide secure
encrypted communication between two
computers over an Internet Protocol
Network and it is used in vpns and Abus
definitely uses it
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at Abus web
application firewall also known as WF
which protects you uh protects your web
application from common web exploits so
the idea here is you write your own
rules to allow or deny traffic based on
the contents of an HTP requests you use
a rule set from a trusted adus security
partner in the adus Waf rule Marketplace
Waf can be attached to either cloudfront
or an application load balancer so here
is that diagram the idea is you see
cloudfront with the WAFF or ALB with the
w and what it does is it can protect uh
web applications from attacks covered
and the OAS 10 uh top 10 most dangerous
attacks if you don't know OAS they're
the open web application security
project and they basically have all
these uh security projects which are
things to say hey these are things that
you should commonly protect against or
they might have like example
applications that uh serve as a means to
learn security so when we look at the
top 10 it's injection broken
authentication sensitive data exposure
XML external entities so xxe broken
Access Control security
misconfigurations cross-site scripting
so xss uh insecure deserialization using
components with known vulnerabilities
and insufficient logging and monitoring
so there you
[Music]
go hey this is Andrew Brown from exam
Pro and we are going to take a quick
look at adus web application firewall
also known as Waf and so um in this
account I have to have a Waf running uh
so we don't have to create one uh we
already have something we can take a
look here so I'm going to go to Waf and
shield and then on the left hand side
you'll Noti this is a global Service but
on the leand side we're going to be
looking for our web acl's and so the
idea is that when you want a w you
create a web ACL and then within within
that web ACL you have uh the overview
and then you have you can kind of show
you kind of the traffic that's going on
here we can have our rules and so um
there's a lot of different kind of
manage rule groups that you can use so
these are ones that are provided by AWS
so and a lot of these some of these can
be paid some of these are free so you
see there's these free rule groups where
you're like hey I don't want any
anomymous IPS you checkbox that on you
know or I want to protect against SQL
injection now the interesting thing is
that abis has this capacity in it so um
you can't add all of these you can add a
certain amount of capacity before you
have to um um uh pay for more or
something like that it's just kind of a
way to um uh kind of cap the amount of
stuff that you can put in in terms of
rules um but there's a lot of other um
rule groups from third party services
like security companies that know what
they're doing so if you like Fort Net's
OS top 10 you can uh subscribe to that
in the marketplace and be able to use it
but uh yeah so that's how you apply
rules there's something called bot
control I've never used this before get
real-time visibility into bot AC on your
resource and controllers what Bots allow
and block from your resources that
sounds really cool I cannot stand bots
so I might turn that on myself or take a
look at the cost there and see what we
can find out but that's pretty much it
with Waf um one thing I would say is
that you can block out specific IP
addresses or whitel list specific IP
addresses and you might do that through
rules I'm just going to see yeah like
maybe the bypass here and so these IP
addresses are some of our um uh Cloud
support Engineers where they're using
our admid panel and um uh WF is being
too aggressive in terms of protection
and so sometimes you have to uh say hey
allow this IP address and let my um you
know let my cloud support engineer be
able to use the mid panel because
they're not malicious okay so that's one
little exception there but that's pretty
much it okay
[Music]
hey this is Andrew Brown from exam Pro
and we are taking a look at Hardware
security modules also known as HSM and
it's a piece of Hardware designed to
store encryption keys and it holds keys
in memory and never writes on the disk
so the idea is that if the HSM was shut
down uh that key would be gone and that
would be a guarantee of protection
because nobody could you know take the
drive and steal it so here is an example
of an HSM uh these are extremely
expensive so you definitely don't want
to have to buy them yourselves uh they
generally follow fips so fips is the
federal information processing standard
so it's a us and Canadian government
standard that specifies the security
requirements for cryptographic modules
that protect sensitive information fips
is something you want to definitely
remember um and there are two different
um uh protocols here there's actually a
bunch of different uh fips versions but
we have fips 142 level two and then fips
143 level 3 so let's talk about the
difference here so hsms that are
multi-tenant are going to be using fips
142 hyphen 2 level two compliant where
you have multiple customers virtually
isolated on the
HSM and then there are hsms that are
single tenant and so they're going to be
utilizing fips 140 hyphen 2 level three
compliant so a single customer on a
dedicated
HSM and so the reason why we have these
two levels is that when you have
multiple tenants you can say oh right
this thing is uh has temper evidence so
we can see that somebody was trying to
break into it but there's no guarantee
of uh T it being tamper proof where
level three is tamper proof there's also
uh fips 140 hyphen 3 which is the new uh
the newer um standard but not all uh
Cloud resources uh can meet that
standard just because of how they offer
the service uh so again fips 142 is
really good but just understand that
there are other ones out there and it's
very easy to get fips 1402 level three
mixed up with fips 140 hyphen 3
something that I always had um a hard
time uh remembering the distinguishing
between those two so for multi-tenant
this is where we're using ads Key
Management Service and for single tenant
we're using adus Cloud HSM and the only
time you're really using Cloud HSM is if
you're a large Enterprise and you need
that Regulatory Compliance of getting
fips 140 hi 2 level three
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at Key
Management Service also known as KMS and
it is a manage service that makes it
easy for you to create and control the
encryption Keys you use to encrypt your
data so KMS is a multi-tenant HSM so
it's a Hardware security module and many
adaa services are integrated to use KMS
Toc your data with a simple checkbox and
K KMS uses envelope encryption so here's
that example of a simple checkbox in
this case it's for RDS and what you'll
do is choose a master key A lot of times
ads will have a default for uh key for
you that's managed by them that is free
to use which is really great uh so for
KMS it's using envelope encryption so
when you encrypt your data your data is
protected but you have to protect your
encryption key when you encrypt your
data key with a master key as an
additional layer of security so that's
it works so just to make this really
clear I have my data I use this key to
encrypt this data and I need to protect
this key so I use another key to encrypt
uh this key which forms an envelope and
then I store this uh master key in KMS
and this one's considered the data key
all
[Music]
right hey this is Andrew Brown from exam
Pro and we're going to take a look at
Key Management service also known as KMS
so type in KMS on the top here and we'll
pop over here and KMS is a way for you
to create your own keys or you can use
adus manage keys so up here and not all
these appear right away but as you use
Services um you will adus will generate
out manage keys for you and these are
free you can uh create your own Keys um
and these cost a dollar each so if I go
ahead here and create a key I can choose
whether it's symmetric or asymmetric
which we definitely learned in the
course which is nice for asymmetric you
can make it encrypt and decrypt
sign and verify and they're just kind of
narrowing down the type of key you would
use um for this you know if I went to
symmetric I go here I'm just kind of
seeing if I can enter the uh actual
material into the key here um so I'm
just going to keep clicking through here
U my custom key generally you don't
really need to do this but um you know
if it's interesting you can set up
administrators to say who's allowed to
administer the key and then you have
someone that um is allowed to use the
key you usually want to keep those two
accounts separate you don't want to have
the same person administrating and using
the key okay keep those two separate and
so we would have a key policy so you can
change this to say the rules that is
allowed to use um and then we can go
here and hit finish and so there we now
have our own custom key and one thing we
can
do is it's possible to rotate out these
Keys when you need to be um but anyway
when we want to use canas it's built to
basically everything and we've seen it
multiple times throughout this course
when we've gone over to ec2 we'll just
go take a peek at a few different places
here so when we've gone to go launch an
ec2 instance and we go over to uh
storage we say
select and review or next and we go over
to storage notice that here this is
using encryption right so I can choose
that or even my custom key if you're in
Dynamo DB or anywhere else it's always
something like a checkbox and you choose
your key so that's pretty much all there
really is to KMS it's very easy to use
and there you
[Music]
go hey this is Andrew Brown from exam
Pro and we are going to take a look here
at Cloud HSM it is a single tenant uh
HSM as a service that automates Hardware
provisioning software patching High
availability and backups so here's the
idea is that you have your adus Cloud
HSM you have your developers interacting
with it your application interacting
with it you have HSM client installed in
your uh ec2 instance so that it can
access uh the cloud HSM keys so adus
Cloud HSM enables you to generate and
use your encryption keys on fips 140
hyphen 2 level 3 validated Hardware it's
built on open HSM industry standards to
integrate with things like PK uh
cs1 Java cryptography uh extension so
jce Microsoft crypto and G libraries you
can transfer your keys to other
commercial commercial HSM Solutions to
make it easy for you to migrate keys on
or off ads configure ads KMS to use adus
cloud HSM uh cluster as a custom uh key
store rather than the default KMS key
store uh so Cloud HSM is way more
expensive than KMS KMS is like free or a
dollar per key where Cloud HSM is a
fixed cost per month because you are
getting a dedicated piece of Hardware um
and there's not a lot of stuff around it
so other than the ad KMS integration a
lot of times it can be really hard to
use this as well so the only time you're
really going to be using Cloud HSM is if
you're an Enterprise and you need to
meet fips 140 hyphen 2 level three
compliancy
[Music]
okay hey this is Andrew Brown from exam
Pro and we are taking a look at know
your initialism so a lot of adus
services and Concepts and Cloud
Technologies use initial isms to just
kind of shorten uh common things that we
need to use on a frequent basis and it's
going to really help if you learn these
because then what you can do is
substitute them when you are uh seeing a
service name or something particular and
that's going to get you through content
a lot faster um and in the wild you're
going to see these all over the place
because people aren't going to say the
full name they're going to say the
initialism so let's go through them so
for IM it's identity and access
management for S3 that's simp Le storage
for S SWS it's uh swf that's simple
workflow service SNS is simple
notification service sqs is simple Q
service SCS is simple email service SSM
is simple systems manager but uh you
know when we see the name it's usually
just systems manager but we still use
the uh initialism SSM then there's RDS
relational database service VPC virtual
private Cloud VPN virtual private
Network CFN cloud formation
WF web application firewall and that is
a very common initialism not just adus
but outside of it as well mq for Amazon
active mq ASG for auto scaling groups
Tam for technical account manager elb
for elastic load bouncer ALB for the
application load bouncer NLB for the
network load bouncer G wlb for the
Gateway load balancer clb for the
classic load balancer ec2 for elastic
cloud or Cloud compute e CS for elastic
container service ECR for elastic
container repository EBS for elastic
block storage EMR for elastic map
produce EFS for elastic fall store EB or
EB for elastic beant stock es for
elastic search eeks for elastic kuber
netti service msk for managed kofka
service and if you think I got the S and
K backwards I did not for whatever
reason it's msk uh then uh there's AIS
resource manager which is known as Ram
ACM for Amazon certificate manager Pol
for principal of lease privilege which
is a concept not a service iot internet
things this is not a service but is a
tech concept or Cloud concept RI for
reserved instances and I'm sure there
are more but these are the ones that I
know off the top of my head uh and
they're in my uh usual use case uh for
what I'm doing day to-day but a lot of
times you'll probably just end up need
to remember ASG
elb um ec2 S3 things like that
[Music]
okay all right let's compare adus config
and app config which both have config in
the name but there are two completely
different services so adus config and
app config so adus config is a
governance tool for compliance as code
you can create rules that will check to
see if resources are configured the way
you expect them to be if a resource
drifts from the expected configuration
you are notified or adus config can auto
remediate correct the configuration back
to the expected state for app config it
is used to automate the process of
deploying application configuration
variable changes to your web application
you can write a validator to ensure uh
the changed variable will not break your
web app uh you can monitor deployments
and automate Integrations to catch
errors or roll backs so config is for
compliance governance app config is for
conf application configur configuration
varibles so there you
[Music]
go well let us take a look at SNS versus
sqs and uh these things have something
in common and it's they both connect
apps via messages uh so they're for
application integration so let's take a
look at SNS so simple notification
service and then simple Q service okay
so SNS is intended to pass along
messages via a pub sub model whereas sqs
cues up messages and has a guaranteed
delivery so the idea with SNS you send
notifications to subscribers of topics
via multiple protocols so it can be H
HTTP email sqs SMS and SNS is generally
used for sending plane text emails which
is triggered via other aab services the
best example here is billing alarms I
know we mentioned this but I like to
repeat it so that you absolutely know uh
it can retry sending in the case of
failures of https so it does have a
retry attempt but that doesn't mean
there's a guarantee of delivery it's
really good for web hooks simple
internal emails triggering Lambda
functions if you had to compare the to
third party Services it's similar to
Pusher or uh pubnub so sqs is uh the
idea here is that messages are placed
into a queue applications pull the queue
using the itus SDK you can uh uh retain
a message for up to 14 days you can send
them in sequential order a sequential
order or in parallel you can ensure only
one message is sent you can ensure
messages are delivered at least once
it's really good for delayed tasks
queuing up emails um comparable uh stuff
would be something like rabbit mq or uh
Ruby on Rails sidekick
[Music]
okay hey this is Andy Brown from exam
Pro and we're doing variation study with
SNS versus SCS versus pinpoint versus
workmail and so SNS and SCS get confused
uh quite often but all of these Services
uh have something in common they all
send emails but uh the utility of email
is completely different for each one so
the first one is simple notification
service is for practical and internal
emails so you send notifications to
subscribers of topics via multiple
protocols so it's not just for email it
can handle HTTP it can send sqs it can
send SNS me or SMS messages so um
messages to your phone um but uh it does
send emails and so SNS is generally used
for sending plain text emails which is
triggered via other IT services the best
example of this is a building alarm so
most exam questions are going to be
talking about SNS because lots of
services can trigger um SNS for
notifications and so that's the idea
it's like oh um you know did somebody
spend up a server send off an email
through via SNS uh did we spend too much
money here you know all sorts of things
can go through SNS to send out emails
and you need to know what are topics and
subscriptions regarding SNS then you
have sces so simple email service and
this is for transactional emails and
when I say transaction emails I'm
talking about emails that should be
triggered based on inapp action so sign
up reset password invoices um so a
cloud-based email service that is
similar to this would be like send grid
SCS sends HTML emails uh SNS cannot so
that is the distinction is that SCS can
do HTML and plain text but SNS just do
does plain text and you would not use
SNS for transactional emails SCS can
receive inbound emails SCS can create
email templates custom domain name
emails so when you use SNS it's whatever
Amazon gives you it's going to be some
weird address but SCS is whatever custom
domain you want you can also monitor
email reputation for SCS then you have
Amazon pinpoint and so this is for
promotional emails so these when we say
promotional we're talking about emails
for marketing so you can create email
campaigns you can segment your contacts
you can create customer Journeys via
emails um it can do a be email testing
and so sces and pinpoint get mixed up
because a lot of people think well can I
just use my transaction emails for
promotion emails absolutely you can it's
not recommended because um you know
pinpoint has a lot more functionality
around promotional emails they're built
differently uh and so you know just
understand that those two have
overlapping responsibilities but
generally should use them for what
they're for then you have Amazon
workmail and this is just an email web
client so it's similar to Gmail or
Outlook you can create company emails
read write and send emails from a web
client within the adus Management
console so there you
[Music]
go let us compare Amazon inspector
versus adus trusted advisor so both of
these are security tools and they both
perform audits but what they do is
slightly different so Amazon inspector
audits a single ec2 instance that you've
selected or I suppose you could select
multiple e2s it generates a report from
a long list of Security checks um and so
trusted advisor has checks but uh the
the key difference here is that it
doesn't generate out a PDF report though
I'm sure you could export CSV data if
you wanted to and then turn that into a
report uh it it gives you a holistic
view of recommendations across multiple
services and best practices so for
example if you have an open port on
these security groups that can tell you
about about that you should enable MFA
on your root account when using trusted
advisor things like that um one thing
though is that trust advisor isn't just
for security it does checks across um uh
five different things um but they both
do security and they both technically do
checks
[Music]
okay so there are a few services that
have connected the name you'd think
they' be related in some way but they
absolutely are not and they don't even
have similar functionality but let's
take a look here so we know the
difference the first is direct connect
it is a dedicated fiber optics
connection from your data center tows
it's intended for large Enterprises with
their own Data Center and they need an
insanely fast and private connection
directly uh to AWS and you'll notice
they give private empasis because if you
need a secure connection you need to
apply uh an adus virtual private network
connection on top of direct connect then
you have Amazon connect this is a call
center as a service get a toll-free
number accept inbound and outbound calls
set up automated phone systems uh so if
you ever heard of an interactive voice
system and IVs this is basically what
Amazon connect is you have media connect
this is the new version of elastic
transcoder it it converts videos to
different video types so if you have
let's say a th videos and you need to
transcode them into different video
formats maybe you need to apply
watermarks insert introduction videos in
in front of each one uh this is what you
use media connect for
[Music]
okay just in case you see elastic
transcoder as an option I just want you
to know what it is compared to Media
connect so both these services are used
for transcoding and technically elastic
transcoder is old way and iTab this
Elemental media convert or just media
convert is the new way so elastic
transcoder was the original transcoding
service it may still have promatic apis
or workflows not available in media
convert so this could be reasons why we
see Legacy customers still using it or
you know it's just too much effort for
them to upgrade to the new one it
transcodes videos to streaming formats
uh media convert is more robust
transcoding service that can perform
various operations during transcoding so
it also transcodes videos to streaming
different streaming formats but it
overlays images it inserts uh video
clips extracts captions data it has a
robust UI so generally it's recommended
to use the uh media convert in terms of
costs are basically the same so there's
no reason not to use media convert
[Music]
okay so it artifact versus Amazon
inspector get commonly mixed up all the
time but both artifact and inspector
compal out PDF reports so that's where
the confusion comes from but let's talk
about what is different about the
reports so Abus artifact and Abus
inspector so for artifact you're
answering why should an Enterprise trust
AWS it generates a security report
that's based on global compliance
framework such as sock or PCI or a
variety of others where Amazon inspector
is all about how do we know this ec2
instance is secure can you prove it so
it runs a script that analyzes your ec2
instance then generates a PDF report
telling you which Security checks had
passed um so the idea here is it's an
audit tool for security of P2 instances
so there you
[Music]
go so let us compare elb versus ALB
versus NLB versus J wlb versus clb uh
because you know when I was first
learning AWS I was getting confused
because there was elastic load balancer
but there was these other ones so what
gives right so what's happening here is
that there is a main service called
elastic load balancer elb and it has
four different types of possible load
Bal bouncers so we'll go through all the
types so the first is application load
bouncer commonly uh initialized as ALB
and so this operates on layer 7 for
https this makes sense because that is
the application layer and it has some
special powers in terms of routing rules
so the idea here is you can create rules
to change routing based on information
found within the htps request so let's
say you wanted some uh routes to go that
have a particular subdomain to this
server and a different sub domain to
another one you could do that and
because it is an application load
balancer uh you can attach a web
application firewall for protection you
can't attach this on the NLB or other
ones because they're not application
based so that is just a little caveat
there then you have Network load bouncer
uh commonly abbreviated to NLB this
operates on layer three and four so
we're talking TCP UDP this is great for
when you have Extreme Performance that
that requires T TCP and TLS traffic it's
capable of handling millions of requests
per seconds uh while maintaining ultra
low latency it's optimized for sudden
and volatile traffic patterns while
using a single static IP address per
availability Zone uh if you're making
video games this is what they like to
use is the network load balcer but it
has other utilities outside of that then
you have Gateway load bouncer G wlb this
is when you need to deploy a fleet of
third-party virtual appliances that
support uh I don't know how to say that
in abbreviation but I'll just uh say
it's G NE v um and there's not much we
need to know outside of that okay then
there is the classic load balancer uh
commonly initializes clb this operates
on layer 3 four and 7 it's intended for
applications that were built within the
ec2 classic Network it doesn't support
Target groups so albs nlbs uh use Target
groups which is just an easier way of
grouping together um a bunch of uh
Target resources like compute uh that
we're going to load balance to and with
classic load balance you just directly
assign ec2 instances uh and it's going
to be retired on August 15th of 2022 so
yeah it looks like it can do a lot of
stuff but um it also doesn't have any of
the superpowers of these specialized
ones and so uh there's no reason to keep
it around and generally you should not
be using it um and so yeah that's about
it


----AWS Training----

Week 1 - 9-13 June 2025			
Training Name 	Description	Hours	
Cloud Practitioner Essentials	This course is for individuals who seek an overall understanding of the Amazon Web Services (AWS) Cloud, independent of specific technical roles. You will learn about AWS Cloud concepts, AWS services, security, architecture, pricing, and support to build your AWS Cloud knowledge. This course also helps you prepare for the AWS Certified Cloud Practitioner exam.	6	
AWS SimuLearn: Cloud Practitioner Learning Plan	A comprehensive introduction to AWS cloud concepts and services through immersive simulations and 12 hands-on lab exercises.	12	
Cloud Quest: Cloud Practitioner	AWS Cloud Quest: Cloud Practitioner is a role-playing learning game that helps you develop practical cloud skills through interactive learning and hands-on activities using AWS services. Complete all your assignments to earn your digital badge.	12	
AWS Card Clash	AWS Card Clash is a 3D virtual card game to help you, regardless of your experience, hone your knowledge of AWS Cloud architecture, and explore AWS services and solution design. It uses strategic turn-based gameplay that keeps learners engaged as they gain understanding of AWS services, and how they interface within a solution.	5	
	Total	35	
			
			
Week 2 - 16-20 June 2025			
Training Name 	Description	Hours	
AWS Technical Essentials	AWS Technical Essentials introduces you to essential AWS services and common solutions. The course covers the fundamental AWS concepts related to compute, database, storage, networking, monitoring, and security. You will start working in AWS through hands-on course experiences. The course covers the concepts necessary to increase your understanding of AWS services, so that you can make informed decisions about solutions that meet business requirements. Throughout the course, you will gain information on how to build, compare, and apply highly available, fault tolerant, scalable, and cost-effective cloud solutions.	4	
AWS Well Architected Foundations	The AWS Well-Architected Framework helps you make informed decisions about your customers' architectures and understand the impacts of design decisions. By using this framework, you will become aware of the risks in your architecture and ways to mitigate them. This course is designed to provide a deep dive into the Well-Architected Framework and its six pillars. You will learn about the AWS Well-Architected Review process and how to use the AWS Well-Architected Tool to complete reviews. After the course is complete, and you have passed the assessment with a 80 percent or higher score, you will be granted a digital Well-Architected Proficient badge. This badge verifies the achievement of completing this course and passing the assessment.	3	
Cloud Essentials Learning Plan (includes labs)	This Learning Plan is designed to help individuals who are new to AWS Cloud technology and seek an overall understanding of the AWS Cloud, independent of specific technical roles. The digital training included in this Learning Plan will expose you to foundational AWS Cloud concepts, AWS services, security, architecture, pricing, and support. This Learning Plan can also help prepare you for the AWS Certified Cloud Practitioner certification exam.	13	
AWS Card Clash - an Architecture Design Game	A 3D virtual card game that helps all skill levels gain a working knowledge of AWS services, and how they interface within a solution through turn-based gameplay. Augment certification prep while learning the basics of solution design.		
AWS Cloud Quest: Cloud Practitioner	Receive the guidance you need to start your cloud career. Cloud Quest will assist you along a foundational learning path at your own pace to learn how to build real-life solutions and gain hands-on experience with AWS services. Earn an official AWS credential by completing all your assignments.	13	
Getting Started with Cloud Acquisition	In this course, you will learn how to buy AWS Cloud effectively, from the very start of the procurement effort. The course includes an in-depth review of cloud acquisition best practices, the benefits of working with the AWS Partner Network (APN), and how to overcome common challenges. The course aims to upskill you with the key acquisition knowledge you need to speak to all stakeholders in your organization and help maximize the benefits of cloud adoption.	1.3	
AWS Billing and Cost Management	In this course, you will learn about the AWS Billing and Cost Management service, and the functionality it offers. You will learn about some of the features available to help you analyze your cloud spending, as well as some of the features available to help you manage cloud spending. You will also have access to additional resources to dive deeper into the features covered in the course.	0.2	
AWS Well-Architected Foundations	The AWS Well-Architected Framework helps you make informed decisions about your customers' architectures and understand the impacts of design decisions. By using this framework, you will become aware of the risks in your architecture and ways to mitigate them. This course is designed to provide a deep dive into the Well-Architected Framework and its six pillars. You will learn about the AWS Well-Architected Review process and how to use the AWS Well-Architected Tool to complete reviews.	3	
	Total	37.5	
			
			
Week 3 - 23-27 June 2025			
Training Name 	Description	Hours	
AWS Cloud Quest: Recertify Cloud Practitioner	A new recertification option for AWS Certified Cloud Practitioners who are due to recertify within six months. With AWS Cloud Quest, no exam is required and you will deepen your AWS knowledge with hands-on experience. This is a free beta available until the end of January, 2025 .	13	
AWS Foundations: Getting Started with the AWS Cloud Essentials	Learn about the foundations of getting started in the AWS Cloud. In this course, you learn about the AWS Cloud architecture and the services in the Compute, Storage, Database, Networking, and Security categories. This course can help you can build and validate an overall understanding of the AWS Cloud, key terminology, and help advance your AWS Cloud skills.	1	
Introduction to the AWS Cloud Adoption Framework (CAF)	This is a digital, introductory level course that will provide an overview of the AWS Cloud Adoption Framework (CAF) and how it is used to accelerate cloud transformation journeys. You will learn about the AWS CAF 3.0 structure and how it can guide successful cloud transformation. Learners can expect to leave this course with a mental model for how customers experience their journey to the cloud and next steps to participating in hand-on workshops to enable transformation.	1	
Getting Started with DevOps	This beginner-level course is for technical learners in the development and operations domains who are interested in learning the basic concepts of DevOps on AWS.Using discussions, interactive content, and demonstrations, you will learn about culture, practices, and tools used in a DevOps environment. You will also explore concepts for developing and delivering secure applications at high velocity on AWS.	1	
Amazon Elastic Block Store Primer	This course explains Amazon EBS features and benefits and calls out common and practical use cases. Further, it describes technical options and specifications and explains how to monitor Amazon EBS volumes.	1.5	
Introduction to Containers	This is an introductory course designed for participants with little-to-no previous knowledge of containers. It will teach you the history and concepts behind containerization, provide an introduction to specific technologies used within the container ecosystem and discuss the importance of containers in microservice architectures.	15	
Advanced Testing Practices Using AWS DevOps Tools	This advanced course demonstrates how to integrate testing and security into continuous integration (CI), continuous delivery (CD), and continuous deployment (CD) pipelines. You will learn how to implement advanced testing practices in various phases of the application lifecycle to deliver application updates in a safer and more reliable manner. You will learn how to use AWS DevOps tools, such as AWS CodeCommit, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline, to implement the advanced testing practices.	2.5	
	Total	35	
			
			
Weeks 4-7 - 30 June - 25 July 2025			
Training Name 	Description	Hours	
Developer Learning Plan	This Learning Plan is designed to help Developers who want to learn how to develop modern applications on AWS. The digital training included in this Learning Plan will expose you to developing with serverless and container technologies, as well as the foundation of DevOps on AWS, while the self-paced lab activities provide a mechanism to build your skills. This Learning Plan can also help prepare you for the AWS Certified Developer - Associate certification exam.	17	
Solution Architect Knowledge Badge Readiness Path	This Learning Path helps you build knowledge on how to design applications and large distributed systems on AWS. This Learning Path presents domain-specific content and includes courses, knowledge checks, and a knowledge badge assessment. This path is a guide and presents learning in a structured order, it can be used as presented or you can select the content that is most beneficial.	50.5	
Development Work for Cyber Fortress		UTC	

